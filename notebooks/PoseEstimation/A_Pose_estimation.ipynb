{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "import imgaug.augmenters as iaa\n",
    "#import numpy as np\n",
    "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "\n",
    "# get the FLOPs\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table, parameter_count, flop_count_str\n",
    "#import torchprofile\n",
    "# # decrease Cuda memory usage\n",
    "# from torch.cuda.amp import GradScaler, autocast # use gradscaler amd mixed precision training\n",
    "\n",
    "# Pretrained encoders\n",
    "import torchvision.models as models\n",
    "\n",
    "# plot tensor to check loss function\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# saving results\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking torch and tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow.python.platform import build_info as tf_build_info\n",
    "#print(tf.__version__)\n",
    "#print(\"CUDA Version:\", tf_build_info.cuda_version)\n",
    "#print(\"cuDNN Version:\", tf_build_info.cudnn_version)\n",
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TensorFlow version:',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA version: \", tf.sysconfig.get_build_info()[\"cuda_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cuDNN version: \", tf.sysconfig.get_build_info()[\"cudnn_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print('CUDA version:',torch.version.cuda)\n",
    "print('cuDNN version:',torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "print(\"CUDA device name: \", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLC imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "print(deeplabcut.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deeplabcut.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLC imports (used with DEEPLABCUT env)\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2           \n",
    "import os\n",
    "import math      \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLEAP-io imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sleap-io_v2 env\n",
    "import sleap_io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLEAP imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sleap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we are using the cloned repository\n",
    "# output should read something like: /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/sleap/sleap/__init__.py\n",
    "# if it is a path to the conda env lib, then it is incorrect\n",
    "print(sleap.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleap.versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleap.system_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for model evaluation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "mpl.style.use(\"seaborn-deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "import os\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.profiler.model_analyzer import profile\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis imports\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas as pd\n",
    "import h5py\n",
    "# import os\n",
    "# import pickle\n",
    "# import json\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and run predictions on the aquarium dataset\n",
    "from PIL import Image\n",
    "import re\n",
    "import sleap.io\n",
    "import sleap.io.video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Important Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ids into list \n",
    "def load_file_to_list(file_path):\n",
    "    \"\"\"\n",
    "    loads a text file to a list with each entry on a new line becoming a new entry in the list.\n",
    "\n",
    "    :param file_path: Path to the file where the list should be saved.\n",
    "    :return list of data from file\n",
    "    \"\"\"\n",
    "    # Open the file for writing\n",
    "    lst = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Write each item on a new line\n",
    "        for line in file:\n",
    "            lst.append(line.strip())\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the image data into an arr\n",
    "# in the same order as the annotations and ids are stored (use id list for this)\n",
    "\n",
    "# The load image data function may take a while to run\n",
    "\n",
    "def load_image_data(ids_to_load, image_folder, crop_ext):\n",
    "\n",
    "  # list for loading image data\n",
    "  selected_imgs = []\n",
    "\n",
    "  # for loop for loading image data that is present in the list of ids\n",
    "  for i, img_id in enumerate(ids_to_load):\n",
    "\n",
    "    # load the image\n",
    "    img_path = os.path.join(image_folder, img_id+crop_ext)\n",
    "    #print(img_path)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    #print(img)\n",
    "\n",
    "    # change the img to RGB from BGR as plt uses RGB colour scale\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # scaling the pixel values to [0, 1] (you don't need to scal them back)\n",
    "    img = img/255\n",
    "\n",
    "    selected_imgs.append(img)\n",
    "\n",
    "  # Convert the list of images to a NumPy array\n",
    "  selected_imgs_array = np.array(selected_imgs)\n",
    "  \n",
    "  return selected_imgs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(df, path):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame to a .json file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be converted to JSON.\n",
    "    path (str): The path (including file name) where the .json file will be saved.\n",
    "    \"\"\"\n",
    "    df.to_json(path, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_to_df(path):\n",
    "    \"\"\"\n",
    "    Converts a .json file to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The path to the .json file that will be read.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame created from the JSON file.\n",
    "    \"\"\"\n",
    "    #print(path)\n",
    "    df = pd.read_json(path, orient='records')\n",
    "    #print(f\"JSON file has been successfully converted to DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lists(df_to_list, list_of_cols):\n",
    "\n",
    "  # create temp lists\n",
    "  keypoints_temp = []\n",
    "\n",
    "  # step through the rows and\n",
    "  for _, row in df_to_list.iterrows():\n",
    "\n",
    "    # extract the data arrays\n",
    "    keypoints_data = row[list_of_cols].values\n",
    "\n",
    "    # adding data to the list\n",
    "    keypoints_temp.append(keypoints_data)\n",
    "\n",
    "  # Convert the list to a NumPy array and make sure that they are float32\n",
    "  keypoints_array = np.array(keypoints_temp, dtype=np.float32)\n",
    "  \n",
    "  return keypoints_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dtypes_df_full_annotation_abs(df):\n",
    "    df['vid_id'] = df['vid_id'].astype(str)\n",
    "    df['img_id'] = df['img_id'].astype(str)\n",
    "    df['bbox_id'] = df['bbox_id'].astype(str)\n",
    "    df['bbox_c_x'] = df['bbox_c_x'].astype('float32')\n",
    "    df['bbox_c_y'] = df['bbox_c_y'].astype('float32')\n",
    "    df['bbox_w'] = df['bbox_w'].astype('float32')\n",
    "    df['bbox_h'] = df['bbox_h'].astype('float32')\n",
    "    df['Head_x'] = df['Head_x'].astype('float32')\n",
    "    df['Head_y'] = df['Head_y'].astype('float32')\n",
    "    df['Beak_x'] = df['Beak_x'].astype('float32')\n",
    "    df['Beak_y'] = df['Beak_y'].astype('float32')\n",
    "    df['Body_top_x'] = df['Body_top_x'].astype('float32')\n",
    "    df['Body_top_y'] = df['Body_top_y'].astype('float32')\n",
    "    df['RFlipper_mid_x'] = df['RFlipper_mid_x'].astype('float32')\n",
    "    df['RFlipper_mid_y'] = df['RFlipper_mid_y'].astype('float32')\n",
    "    df['LFlipper_mid_x'] = df['LFlipper_mid_x'].astype('float32')\n",
    "    df['LFlipper_mid_y'] = df['LFlipper_mid_y'].astype('float32')\n",
    "    df['Body_bottom_x'] = df['Body_bottom_x'].astype('float32')\n",
    "    df['Body_bottom_y'] = df['Body_bottom_y'].astype('float32')\n",
    "    df['RFoot_x'] = df['RFoot_x'].astype('float32')\n",
    "    df['RFoot_y'] = df['RFoot_y'].astype('float32')\n",
    "    df['LFoot_x'] = df['LFoot_x'].astype('float32')\n",
    "    df['LFoot_y'] = df['LFoot_y'].astype('float32')\n",
    "    df['kp_outside_best_bbox'] = df['kp_outside_best_bbox'].astype('float32')\n",
    "    df['kp_missing'] = df['kp_missing'].astype('float32')\n",
    "    df['kp_primary_missing'] = df['kp_primary_missing'].astype(bool)\n",
    "    df['img_width'] = df['img_width'].astype('float32')\n",
    "    df['img_height'] = df['img_height'].astype('float32')\n",
    "    df['bbox_max_h_w'] = df['bbox_max_h_w'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnorm_keypoints(img_size, keypoints, kp_to_null=None):\n",
    "    \"\"\"\n",
    "    De-normalizes keypoints based on image size and returns the de-normalized keypoints along with \n",
    "    the positions of any missing or nullified keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    - img_size: Tuple of the image dimensions (height, width).\n",
    "    - keypoints: List of normalized keypoints (with values between -1 and 1).\n",
    "    - kp_to_null: Optional. List of indices where the keypoints should be nulled (set to NaN).\n",
    "\n",
    "    Returns:\n",
    "    - new_keypoints: List of de-normalized keypoints where each coordinate is scaled back to the \n",
    "                     image's pixel dimensions.\n",
    "    - missing_kp: List of indices where the keypoints were either originally set to -10 (indicating \n",
    "                  missing keypoints) or explicitly nullified by the kp_to_null list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract image width and height\n",
    "    readjust_x = img_size[0]  # width of the image\n",
    "    readjust_y = img_size[1]  # height of the image\n",
    "\n",
    "    new_keypoints = []  # List to store the de-normalized keypoints\n",
    "    missing_kp = []     # List to store the indices of missing or nullified keypoints\n",
    "\n",
    "    # Iterate through each keypoint\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        # Null keypoints if they are -10 or if they are specified in kp_to_null\n",
    "        if keypoint == -10 or (kp_to_null and i in kp_to_null):\n",
    "            keypoint = np.nan  # Set keypoint to NaN\n",
    "            missing_kp.append(i)  # Record the index of the missing or nullified keypoint\n",
    "\n",
    "        # De-normalize the x-coordinates\n",
    "        if i % 2 == 0:  # Even indices are x-coordinates\n",
    "            keypoint = keypoint * readjust_x + readjust_x / 2\n",
    "        # De-normalize the y-coordinates\n",
    "        else:  # Odd indices are y-coordinates\n",
    "            keypoint = keypoint * readjust_y + readjust_y / 2\n",
    "\n",
    "        new_keypoints.append(keypoint)  # Append the de-normalized keypoint to the list\n",
    "\n",
    "    return new_keypoints, missing_kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_keypoints(img_size, keypoints):\n",
    "    \"\"\"\n",
    "    Normalizes keypoints based on image size and replaces any NaN values with -10.\n",
    "\n",
    "    Parameters:\n",
    "    - img_size: Tuple of the image dimensions (width, height).\n",
    "    - keypoints: List of de-normalized keypoints where each coordinate is in pixel dimensions.\n",
    "\n",
    "    Returns:\n",
    "    - norm_keypoints: List of normalized keypoints where each coordinate is scaled to the range \n",
    "                      [-1, 1] relative to the image size, with NaNs replaced by -10.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract image width and height\n",
    "    readjust_x = img_size[0]  # width of the image\n",
    "    readjust_y = img_size[1]  # height of the image\n",
    "\n",
    "    norm_keypoints = []  # List to store the normalized keypoints\n",
    "\n",
    "    # Iterate through each keypoint\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        # Replace NaN values with -10\n",
    "        if np.isnan(keypoint):\n",
    "            keypoint = -10.0\n",
    "        else:\n",
    "            # Normalize the x-coordinates\n",
    "            if i % 2 == 0:  # Even indices are x-coordinates\n",
    "                keypoint = (keypoint - readjust_x / 2) / readjust_x\n",
    "            # Normalize the y-coordinates\n",
    "            else:  # Odd indices are y-coordinates\n",
    "                keypoint = (keypoint - readjust_y / 2) / readjust_y\n",
    "\n",
    "        norm_keypoints.append(keypoint)  # Append the normalized keypoint to the list\n",
    "\n",
    "    return norm_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize keypoints for an array of images\n",
    "def unnorm_keypoints_arr(kp_arr, img_arr):\n",
    "    \"\"\"\n",
    "    Denormalizes keypoints for each image in the array based on the corresponding image size.\n",
    "    It converts normalized keypoints (range [-1, 1]) back to pixel coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - kp_arr: Array of normalized keypoints, where each entry is a list of keypoints for an image.\n",
    "              The keypoints are expected to be in the format [x1, y1, x2, y2, ...].\n",
    "    - img_arr: Array of images. The size of each image is used to scale the keypoints back \n",
    "               to their pixel coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - kp_abs_arr: Array of denormalized keypoints where each entry corresponds to the denormalized \n",
    "                  keypoints for the corresponding image in `img_arr`.\n",
    "    \"\"\"\n",
    "\n",
    "    kp_abs_list = []  # List to store the denormalized keypoints for each image\n",
    "\n",
    "    # Iterate through each set of keypoints and corresponding image\n",
    "    for i, kp in enumerate(kp_arr):\n",
    "        img_size = img_arr[i].shape  # Get the size of the current image (height, width, channels)\n",
    "\n",
    "        # Denormalize the keypoints based on the image size\n",
    "        kp_abs, missing_kp = unnorm_keypoints(img_size, kp_arr[i])\n",
    "\n",
    "        # Save the denormalized keypoints to the list\n",
    "        kp_abs_list.append(kp_abs)\n",
    "    \n",
    "    # Convert the list of denormalized keypoints to a NumPy array\n",
    "    kp_abs_arr = np.array(kp_abs_list)\n",
    "\n",
    "    return kp_abs_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize keypoints for an array of images\n",
    "def norm_keypoints_arr(kp_arr, img_arr):\n",
    "    \"\"\"\n",
    "    Normalizes keypoints for each image in the array based on the corresponding image size.\n",
    "    It converts keypoints from pixel coordinates back to normalized coordinates (range [-1, 1]).\n",
    "\n",
    "    Parameters:\n",
    "    - kp_arr: Array of keypoints, where each entry is a list of keypoints for an image.\n",
    "              The keypoints are expected to be in the format [x1, y1, x2, y2, ...] \n",
    "              with pixel coordinates.\n",
    "    - img_arr: Array of images. The size of each image is used to scale the keypoints \n",
    "               to normalized coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - kp_norm_arr: Array of normalized keypoints where each entry corresponds to the normalized \n",
    "                   keypoints for the corresponding image in `img_arr`.\n",
    "    \"\"\"\n",
    "        \n",
    "    kp_norm_list = []  # List to store the normalized keypoints for each image\n",
    "\n",
    "    # Iterate through each set of keypoints and corresponding image\n",
    "    for i, kp in enumerate(kp_arr):\n",
    "        img_size = img_arr[i].shape  # Get the size of the current image (height, width, channels)\n",
    "\n",
    "        # Normalize the keypoints based on the image size\n",
    "        kp_norm = norm_keypoints(img_size, kp_arr[i])\n",
    "\n",
    "        # Save the normalized keypoints to the list\n",
    "        kp_norm_list.append(kp_norm)\n",
    "    \n",
    "    # Convert the list of normalized keypoints to a NumPy array\n",
    "    kp_norm_arr = np.array(kp_norm_list)  \n",
    "\n",
    "    return kp_norm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation to images and keypoints\n",
    "def apply_aug(img_arr_orig, kp_arr_orig, aug, num_of_kp=8):\n",
    "    \"\"\"\n",
    "    Applies augmentation to a batch of images and their corresponding keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    - img_arr_orig: Original array of images. Shape should be (num_imgs, height, width, channels).\n",
    "    - kp_arr_orig: Original array of keypoints. Shape should be (num_imgs, num_of_kp*2), where each \n",
    "                   keypoint is represented by its x and y coordinates in pixel values.\n",
    "    - aug: An imgaug augmentation sequence or augmenter to apply to the images and keypoints.\n",
    "    - num_of_kp: Optional. Number of keypoints per image (default is 8).\n",
    "\n",
    "    Returns:\n",
    "    - img_arr_aug: Augmented array of images. Same shape as `img_arr_orig`.\n",
    "    - kp_arr_aug: Augmented array of keypoints. Same shape as `kp_arr_orig`.\n",
    "    \"\"\"\n",
    "    # print(img_arr_orig.shape)\n",
    "    #print(kp_arr_orig.shape)\n",
    "    \n",
    "    # Initialize lists to store augmented images and keypoints\n",
    "    aug_img = []  # List for augmented images\n",
    "    aug_kp = []   # List for augmented keypoints\n",
    "\n",
    "    # Get the number of images in the batch\n",
    "    num_imgs = img_arr_orig.shape[0]\n",
    "    #print(num_imgs)\n",
    "\n",
    "    # Loop over each image and its corresponding keypoints\n",
    "    for i in range(num_imgs):\n",
    "        image = img_arr_orig[i]  # Extract the i-th image\n",
    "        #print(image.shape)\n",
    "        \n",
    "        # Convert keypoints to KeypointsOnImage format for imgaug\n",
    "        keypoints = kp_arr_orig[i]\n",
    "        #print(keypoints)\n",
    "        kps = [Keypoint(x=keypoints[j*2], y=keypoints[j*2+1]) for j in range(num_of_kp)]\n",
    "        kps_on_image = KeypointsOnImage(kps, shape=image.shape)\n",
    "        \n",
    "        # Apply the augmentation to the image and keypoints\n",
    "        image_aug, kps_aug = aug(image=image, keypoints=kps_on_image)\n",
    "        \n",
    "        # Convert augmented keypoints back to the original flattened format [x1, y1, x2, y2, ...]\n",
    "        keypoints_aug = []\n",
    "        for kp in kps_aug.keypoints:\n",
    "            keypoints_aug.extend([kp.x, kp.y])\n",
    "        \n",
    "        # Append the augmented image and keypoints to their respective lists\n",
    "        aug_img.append(image_aug)\n",
    "        aug_kp.append(keypoints_aug)\n",
    "\n",
    "    # Convert the lists of augmented images and keypoints back to NumPy arrays\n",
    "    img_arr_aug = np.array(aug_img)\n",
    "    kp_arr_aug = np.array(aug_kp)\n",
    "\n",
    "    return img_arr_aug, kp_arr_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_padding(image):\n",
    "    \"\"\"\n",
    "    Detects if padding is on the x-axis (left and right) or y-axis (top and bottom)\n",
    "    of the image and calculates the padding size on one side.\n",
    "\n",
    "    Parameters:\n",
    "    - image: A NumPy array representing the image. The shape should be (width, height, channels).\n",
    "\n",
    "    Returns:\n",
    "    - is_padding_x: True if padding is on the x-axis, False if padding is on the y-axis.\n",
    "    - padding_size: The size of the padding on one side in pixels.\n",
    "    \"\"\"\n",
    "\n",
    "    width, height, _ = image.shape\n",
    "    \n",
    "    # Check for padding along the x-axis (left and right)\n",
    "    left_column = image[:, 0, :]#image[0, :, :]  # The first column (left side)\n",
    "    right_column = image[:, -1, :] #image[-1, :, :]  # The last column (right side)\n",
    "\n",
    "    # Check for padding along the y-axis (top and bottom)\n",
    "    top_row = image[:, 0, :]  # The first row (top side)\n",
    "    bottom_row = image[:, -1, :]  # The last row (bottom side)\n",
    "    #print(image[:, 5, :] *255)\n",
    "    #print(left_column*255)\n",
    "    \n",
    "    # Check if the columns are fully black (indicating padding)\n",
    "    if np.all(left_column*255 < 30) and np.all(right_column*255 < 30):\n",
    "        # Padding is along the x-axis\n",
    "        is_padding_x = True\n",
    "        #plot_img(image)\n",
    "        # Calculate padding size\n",
    "        #padding_size = np.sum(image[0, :, 0]*255 < 30) // 2  # Count black pixels on one side\n",
    "        # if padding_size > 60:\n",
    "        sum1 = np.sum(image[5, :, 0]*255 < 20) // 2\n",
    "        sum2 = np.sum(image[10, :, 0]*255 < 20) // 2\n",
    "        sum3 = np.sum(image[60, :, 0]*255 < 20) // 2\n",
    "        sum4 = np.sum(image[110, :, 0]*255 < 20) // 2\n",
    "        sum5 = np.sum(image[-60, :, 0]*255 < 20) // 2\n",
    "        sum6 = np.sum(image[-10, :, 0]*255 < 20) // 2\n",
    "        sum7 = np.sum(image[-5, :, 0]*255 < 20) // 2\n",
    "        padding_size = min(sum1, sum2, sum3, sum4, sum5, sum6, sum7)\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        # if padding_size > 60:\n",
    "        #     sum1 = np.sum(image[5, :, 0]*255 < 10) // 2\n",
    "        #     sum2 = np.sum(image[10, :, 0]*255 < 10) // 2\n",
    "        #     sum3 = np.sum(image[60, :, 0]*255 < 10) // 2\n",
    "        #     sum4 = np.sum(image[110, :, 0]*255 < 10) // 2\n",
    "        #     sum5 = np.sum(image[-60, :, 0]*255 < 10) // 2\n",
    "        #     sum6 = np.sum(image[-10, :, 0]*255 < 10) // 2\n",
    "        #     sum7 = np.sum(image[-5, :, 0]*255 < 10) // 2\n",
    "        #     average = (sum1 + sum2 + sum3 + sum4 + sum5 + sum6 + sum7) // 7  # Floor division for rounding down\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        if padding_size > 20: \n",
    "            padding_size = 20\n",
    "\n",
    "    else:\n",
    "        # Padding is along the y-axis (top and bottom)\n",
    "        is_padding_x = False\n",
    "        # Calculate padding size\n",
    "        padding_size = np.sum(image[:, 0, 0]*255 < 30) // 2  # Count black pixels on one side\n",
    "        # if padding_size > 60:\n",
    "        sum1 = np.sum(image[:, 5, 0]*255 < 20) // 2\n",
    "        sum2 = np.sum(image[:, 10, 0]*255 < 20) // 2\n",
    "        sum3 = np.sum(image[:, 60, 0]*255 < 20) // 2\n",
    "        sum4 = np.sum(image[:, 110, 0]*255 < 20) // 2\n",
    "        sum5 = np.sum(image[:, -60, 0]*255 < 20) // 2\n",
    "        sum6 = np.sum(image[:, -10, 0]*255 < 20) // 2\n",
    "        sum7 = np.sum(image[:, -5, 0]*255 < 20) // 2\n",
    "        padding_size = min(sum1, sum2, sum3, sum4, sum5, sum6, sum7)\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        # if padding_size > 60:\n",
    "        #     sum1 = np.sum(image[:, 5, 0]*255 < 10) // 2\n",
    "        #     sum2 = np.sum(image[:, 10, 0]*255 < 10) // 2\n",
    "        #     sum3 = np.sum(image[:, 60, 0]*255 < 10) // 2\n",
    "        #     sum4 = np.sum(image[:, 110, 0]*255 < 10) // 2\n",
    "        #     sum5 = np.sum(image[:, -60, 0]*255 < 10) // 2\n",
    "        #     sum6 = np.sum(image[:, -10, 0]*255 < 10) // 2\n",
    "        #     sum7 = np.sum(image[:, -5, 0]*255 < 10) // 2\n",
    "        #     average = (sum1 + sum2 + sum3 + sum4 + sum5 + sum6 + sum7) // 7  # Floor division for rounding down\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        if padding_size > 20: \n",
    "            padding_size = 20\n",
    "\n",
    "    return is_padding_x, padding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aug_translate(train_imgs_array, train_kp_array_abs):\n",
    "\n",
    "    \n",
    "    # Get the number of images in the batch\n",
    "    num_imgs = train_imgs_array.shape[0]\n",
    "    # print(num_imgs)\n",
    "\n",
    "    # creat empty arrays\n",
    "    train_imgs_array_aug_trans = np.empty((0, train_imgs_array.shape[1], train_imgs_array.shape[2], train_imgs_array.shape[3]), dtype=train_imgs_array.dtype)\n",
    "    train_kp_array_aug_trans = np.empty((0, train_kp_array_abs.shape[1]), dtype=train_kp_array_abs.dtype)\n",
    "\n",
    "    # print(train_imgs_array_aug_trans.shape)\n",
    "    # print(train_kp_array_aug_trans.shape)\n",
    "\n",
    "    # Loop over each image and its corresponding keypoints\n",
    "    for i in range(num_imgs):\n",
    "        image = train_imgs_array[i]  # Extract the i-th image\n",
    "        kp = train_kp_array_abs[i]\n",
    "        # print(i)\n",
    "        # print(image.shape)\n",
    "        # print(kp.shape)\n",
    "\n",
    "        is_padding_x, padding_size = detect_padding(image)\n",
    "        # print(f'this: {i}')\n",
    "        # print(is_padding_x)\n",
    "        # print(padding_size)\n",
    "\n",
    "        if is_padding_x:\n",
    "            seq_trans_x_left = iaa.Sequential([\n",
    "                iaa.TranslateX(px=(-padding_size, -padding_size)),\n",
    "            ])\n",
    "            seq_trans_x_right = iaa.Sequential([\n",
    "                iaa.TranslateX(px=(padding_size, padding_size)),\n",
    "            ])\n",
    "\n",
    "            # Convert to shape (1, 220, 220, 3) and (1, 16)\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            #print(is_padding_x)\n",
    "            #print(image.shape)\n",
    "            #print(i)\n",
    "            kp = np.expand_dims(kp, axis=0)\n",
    "\n",
    "            # apply augmentations\n",
    "            single_trans_x_left_img_arr, single_trans_x_left_kp_arr = apply_aug(image, kp, seq_trans_x_left)\n",
    "            single_trans_x_right_img_arr, single_trans_x_right_kp_arr = apply_aug(image, kp, seq_trans_x_right)\n",
    "\n",
    "            #save to image array\n",
    "            train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_x_left_img_arr), axis=0)\n",
    "            train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_x_right_img_arr), axis=0)\n",
    "            #save to kp array\n",
    "            train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_x_left_kp_arr), axis=0)\n",
    "            train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_x_right_kp_arr), axis=0)\n",
    "\n",
    "        else :\n",
    "            seq_trans_y_up = iaa.Sequential([\n",
    "                iaa.TranslateY(px=(-padding_size, -padding_size)),\n",
    "            ])\n",
    "            seq_trans_y_down = iaa.Sequential([\n",
    "                iaa.TranslateY(px=(padding_size, padding_size)),\n",
    "            ])\n",
    "\n",
    "            # Convert to shape (1, 220, 220, 3) and (1, 16)\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            #print(is_padding_x)\n",
    "            #print(image.shape)\n",
    "            #print(i)\n",
    "            kp = np.expand_dims(kp, axis=0)\n",
    "\n",
    "            # apply augmentations\n",
    "            single_trans_y_up_img_arr, single_trans_y_up_kp_arr = apply_aug(image, kp, seq_trans_y_up)\n",
    "            single_trans_y_down_img_arr, single_trans_y_down_kp_arr = apply_aug(image, kp, seq_trans_y_down)\n",
    "\n",
    "            #save to image array\n",
    "            train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_y_up_img_arr), axis=0)\n",
    "            train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_y_down_img_arr), axis=0)\n",
    "            #save to kp array\n",
    "            train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_y_up_kp_arr), axis=0)\n",
    "            train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_y_down_kp_arr), axis=0)\n",
    "\n",
    "\n",
    "    return train_imgs_array_aug_trans, train_kp_array_aug_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_out_of_img_kp(arr):\n",
    "    \"\"\"\n",
    "    Finds and replaces the elements in the array that are outside the frame but not the missing ones.\n",
    "    Specifically, elements greater than 0.5 are replaced with 0.5, and elements less than -0.5 but \n",
    "    greater than -9.0 are replaced with -0.5.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: A NumPy array of shape (n, 16).\n",
    "\n",
    "    Returns:\n",
    "    - modified_arr: The modified NumPy array with replaced values.\n",
    "    - count_replacements: The number of elements that were replaced.\n",
    "    \"\"\"\n",
    "    # Make a copy of the array to avoid modifying the original array\n",
    "    modified_arr = arr.copy()\n",
    "\n",
    "    # Replace elements greater than 0.5 with 0.5\n",
    "    count_pos_replacements = np.sum(modified_arr > 0.5)\n",
    "    modified_arr[modified_arr > 0.5] = 0.49\n",
    "\n",
    "    # Replace elements less than -0.5 but greater than -9.0 with -0.5\n",
    "    count_neg_replacements = np.sum((modified_arr < -0.5) & (modified_arr > -9.0))\n",
    "    modified_arr[(modified_arr < -0.5) & (modified_arr > -9.0)] = -0.49\n",
    "\n",
    "    # Total count of replacements\n",
    "    count_replacements = count_pos_replacements + count_neg_replacements\n",
    "\n",
    "    return modified_arr, count_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_out_of_img_kp(arr):\n",
    "    \"\"\"\n",
    "    Finds and replaces the elements in the array that are outside the frame but not the missing ones.\n",
    "    Specifically, elements greater than 0.5 are replaced with 0.5, and elements less than -0.5 but \n",
    "    greater than -9.0 are replaced with -0.5.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: A NumPy array of shape (n, 16).\n",
    "\n",
    "    Returns:\n",
    "    - modified_arr: The modified NumPy array with replaced values.\n",
    "    - count_replacements: The number of elements that were replaced.\n",
    "    \"\"\"\n",
    "    # Make a copy of the array to avoid modifying the original array\n",
    "    modified_arr = arr.copy()\n",
    "\n",
    "    # Replace elements greater than 0.5 with 0.5\n",
    "    count_pos_replacements = np.sum(modified_arr > 0.5)\n",
    "    modified_arr[modified_arr > 0.5] = 0.49\n",
    "\n",
    "    # Replace elements less than -0.5 but greater than -9.0 with -0.5\n",
    "    count_neg_replacements = np.sum((modified_arr < -0.5) & (modified_arr > -9.0))\n",
    "    modified_arr[(modified_arr < -0.5) & (modified_arr > -9.0)] = -0.49\n",
    "\n",
    "    # Total count of replacements\n",
    "    count_replacements = count_pos_replacements + count_neg_replacements\n",
    "\n",
    "    return modified_arr, count_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error, ignoring the invisible keypoints.\n",
    "    Assuming that -10.0 indicates an invisible keypoint.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints from both\n",
    "    # the predictions and the true values\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # Compute the Mean Squared Error only on the visible keypoints\n",
    "    mse = F.mse_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mae(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error, ignoring the invisible keypoints.\n",
    "    Assuming that -10.0 indicates an invisible keypoint.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints from both\n",
    "    # the predictions and the true values\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    \n",
    "    # Compute the mean absolute Error only on the visible keypoints\n",
    "    mae = F.l1_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD ANGLE LOSS FUNCTION\n",
    "\n",
    "# def find_tensor_angle(input_tensor):\n",
    "#     \"\"\"\n",
    "#     Compute angles between vectors defined in input_tensor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_tensor: Tensor with 6 columns and multiple rows.\n",
    "#     Returns:\n",
    "#     - Tensor of angles in radians with the same number of rows as input_tensor.\n",
    "#     \"\"\"\n",
    "#     # # Step 1: Create a mask for rows without -10\n",
    "#     # mask = (input_tensor != -10).all(dim=1).unsqueeze(1)  # Mask rows with -10\n",
    "\n",
    "#     # # Step 2: Adjust input_tensor using the mask\n",
    "#     # #adjusted_input_tensor = input_tensor * mask  # Set rows with -10 to 0\n",
    "\n",
    "#     # Step 3: Compute vectors BA and BC\n",
    "#     # vector_tensor = torch.zeros((adjusted_input_tensor.size(0), 4), device=input_tensor.device)\n",
    "#     # vector_tensor[:, 0] = adjusted_input_tensor[:, 0] - adjusted_input_tensor[:, 2]  # BA_x\n",
    "#     # vector_tensor[:, 1] = adjusted_input_tensor[:, 1] - adjusted_input_tensor[:, 3]  # BA_y\n",
    "#     # vector_tensor[:, 2] = adjusted_input_tensor[:, 4] - adjusted_input_tensor[:, 2]  # BC_x\n",
    "#     # vector_tensor[:, 3] = adjusted_input_tensor[:, 5] - adjusted_input_tensor[:, 3]  # BC_y\n",
    "#     vector_tensor = torch.zeros((input_tensor.size(0), 4), device=input_tensor.device)\n",
    "#     vector_tensor[:, 0] = input_tensor[:, 0] - input_tensor[:, 2]  # BA_x\n",
    "#     vector_tensor[:, 1] = input_tensor[:, 1] - input_tensor[:, 3]  # BA_y\n",
    "#     vector_tensor[:, 2] = input_tensor[:, 4] - input_tensor[:, 2]  # BC_x\n",
    "#     vector_tensor[:, 3] = input_tensor[:, 5] - input_tensor[:, 3]  # BC_y\n",
    "\n",
    "#         # DEBUG - check for nan\n",
    "#     if torch.isnan(vector_tensor).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # Step 4: Compute dot product and magnitudes\n",
    "#     dot_prod_tensor = (vector_tensor[:, 0] * vector_tensor[:, 2] + \n",
    "#                        vector_tensor[:, 1] * vector_tensor[:, 3]).unsqueeze(1)\n",
    "#     magnitude_tensor_BA = torch.linalg.norm(vector_tensor[:, :2], dim=1, keepdim=True)  # Magnitude of BA\n",
    "#     magnitude_tensor_BC = torch.linalg.norm(vector_tensor[:, 2:], dim=1, keepdim=True)  # Magnitude of BC\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(dot_prod_tensor).any():\n",
    "#         print('going to nan')\n",
    "#             # DEBUG - check for nan\n",
    "#     if torch.isnan(magnitude_tensor_BA).any():\n",
    "#         print('going to nan')\n",
    "#         # DEBUG - check for nan\n",
    "#     if torch.isnan(magnitude_tensor_BC).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "\n",
    "#     # Step 5: Compute angles in radians\n",
    "#     clamp_vals = torch.clamp((dot_prod_tensor / (magnitude_tensor_BA * magnitude_tensor_BC)), -0.99999, 0.99999) # Clamp values to the valid range for acos\n",
    "#        # DEBUG - check for nan\n",
    "#     if torch.isnan(clamp_vals).any():\n",
    "#         print('going to nan')\n",
    "    \n",
    "#     angle_tensor = torch.acos(clamp_vals)\n",
    "#        # DEBUG - check for nan\n",
    "#     if torch.isnan(clamp_vals).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "\n",
    "#     return angle_tensor\n",
    "# # # test find_tensor_angle\n",
    "# # test_tensor = torch.tensor([\n",
    "# #     [-10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # Forms a straight line, angle = 180 degrees\n",
    "# #     [1.0, 0.0, 0.0, 0.0, 1.0, 1.0],    # Forms a right-angle triangle, angle < 180 degrees\n",
    "# #     [-10.0, 0.0, -0.0, 0.0, 1.0, 0.0]    # Forms a horizontal line, angle = 180 degrees\n",
    "# # ], dtype=torch.float32)\n",
    "\n",
    "# # print(find_tensor_angle(test_tensor))\n",
    "# def tensor_abs_difference(tensor1, tensor2):\n",
    "#     \"\"\"\n",
    "#     Compute the absolute difference between two tensors, each with a single column and multiple rows.\n",
    "\n",
    "#     Parameters:\n",
    "#     - tensor1: A PyTorch tensor with shape (N, 1)\n",
    "#     - tensor2: A PyTorch tensor with shape (N, 1)\n",
    "\n",
    "#     Returns:\n",
    "#     - A PyTorch tensor with the absolute differences, shape (N, 1)\n",
    "#     \"\"\"\n",
    "#     # Ensure both tensors have the same shape\n",
    "#     if tensor1.shape != tensor2.shape:\n",
    "#         raise ValueError(\"Input tensors must have the same shape.\")\n",
    "\n",
    "#     # Compute absolute difference\n",
    "#     abs_diff = torch.abs(tensor1 - tensor2)\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(abs_diff).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     return abs_diff\n",
    "\n",
    "# # find the angle loss\n",
    "# def find_single_angle_loss(y_true_subset, y_pred_subset):\n",
    "\n",
    "#     y_true_angle = find_tensor_angle(y_true_subset)\n",
    "#     y_pred_angle = find_tensor_angle(y_pred_subset)\n",
    "\n",
    "#     # find the abs difference in angles\n",
    "#     angle_abs_differnce = tensor_abs_difference(y_true_angle, y_pred_angle)\n",
    "    \n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(angle_abs_differnce).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # normalise the abs difference (POTENTIALLY NEED TO DO THIS SO IT IS IN THE SAME RANGE AS THE MAE LOSS)\n",
    "\n",
    "#     # Step 1: Create a mask for rows without -10\n",
    "#     mask = (y_true_subset != -10).all(dim=1).unsqueeze(1)  # Mask rows with -10\n",
    "\n",
    "#     # mask angles where kp are missing\n",
    "#     angle_abs_differnce_masked = angle_abs_differnce * mask\n",
    "\n",
    "#     # normalise the abs difference (POTENTIALLY NEED TO DO THIS SO IT IS IN THE SAME RANGE AS THE MAE LOSS) - can only go up to 2*pi rad, so divide by that\n",
    "\n",
    "#     # find the mean abs difference between angles\n",
    "#     num_angles = torch.sum(mask)\n",
    "#     if num_angles == 0:\n",
    "#         num_angles = 1 \n",
    "\n",
    "#     single_angle_loss = torch.sum(angle_abs_differnce_masked)/num_angles\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(single_angle_loss).any():\n",
    "#         print('going to nan')\n",
    "    \n",
    "#     return single_angle_loss\n",
    "# def find_angle_loss(y_true, y_pred):\n",
    "\n",
    "#     beak_angle_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     beak_angle_true[:,0] = y_true[:,2]\n",
    "#     beak_angle_true[:,1] = y_true[:,3]\n",
    "#     beak_angle_true[:,2] = y_true[:,0]\n",
    "#     beak_angle_true[:,3] = y_true[:,1]\n",
    "#     beak_angle_true[:,4] = y_true[:,4]\n",
    "#     beak_angle_true[:,5] = y_true[:,5]\n",
    "#     beak_angle_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     beak_angle_pred[:,0] = y_pred[:,2]\n",
    "#     beak_angle_pred[:,1] = y_pred[:,3]\n",
    "#     beak_angle_pred[:,2] = y_pred[:,0]\n",
    "#     beak_angle_pred[:,3] = y_pred[:,1]\n",
    "#     beak_angle_pred[:,4] = y_pred[:,4]\n",
    "#     beak_angle_pred[:,5] = y_pred[:,5]\n",
    "\n",
    "#     beak_angle_loss = find_single_angle_loss(beak_angle_true, beak_angle_pred)\n",
    "\n",
    "#     head_angle_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     head_angle_true[:,0] = y_true[:,0]\n",
    "#     head_angle_true[:,1] = y_true[:,1]\n",
    "#     head_angle_true[:,2] = y_true[:,4]\n",
    "#     head_angle_true[:,3] = y_true[:,5]\n",
    "#     head_angle_true[:,4] = y_true[:,10]\n",
    "#     head_angle_true[:,5] = y_true[:,11]\n",
    "#     head_angle_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     head_angle_pred[:,0] = y_pred[:,0]\n",
    "#     head_angle_pred[:,1] = y_pred[:,1]\n",
    "#     head_angle_pred[:,2] = y_pred[:,4]\n",
    "#     head_angle_pred[:,3] = y_pred[:,5]\n",
    "#     head_angle_pred[:,4] = y_pred[:,10]\n",
    "#     head_angle_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     head_angle_loss = find_single_angle_loss(head_angle_true, head_angle_pred)\n",
    "\n",
    "#     right_wing_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     right_wing_true[:,0] = y_true[:,6]\n",
    "#     right_wing_true[:,1] = y_true[:,7]\n",
    "#     right_wing_true[:,2] = y_true[:,4]\n",
    "#     right_wing_true[:,3] = y_true[:,5]\n",
    "#     right_wing_true[:,4] = y_true[:,10]\n",
    "#     right_wing_true[:,5] = y_true[:,11]\n",
    "#     right_wing_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     right_wing_pred[:,0] = y_pred[:,6]\n",
    "#     right_wing_pred[:,1] = y_pred[:,7]\n",
    "#     right_wing_pred[:,2] = y_pred[:,4]\n",
    "#     right_wing_pred[:,3] = y_pred[:,5]\n",
    "#     right_wing_pred[:,4] = y_pred[:,10]\n",
    "#     right_wing_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     right_wing_loss = find_single_angle_loss(right_wing_true, right_wing_pred)\n",
    "\n",
    "#     left_wing_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     left_wing_true[:,0] = y_true[:,8]\n",
    "#     left_wing_true[:,1] = y_true[:,9]\n",
    "#     left_wing_true[:,2] = y_true[:,4]\n",
    "#     left_wing_true[:,3] = y_true[:,5]\n",
    "#     left_wing_true[:,4] = y_true[:,10]\n",
    "#     left_wing_true[:,5] = y_true[:,11]\n",
    "#     left_wing_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     left_wing_pred[:,0] = y_pred[:,8]\n",
    "#     left_wing_pred[:,1] = y_pred[:,9]\n",
    "#     left_wing_pred[:,2] = y_pred[:,4]\n",
    "#     left_wing_pred[:,3] = y_pred[:,5]\n",
    "#     left_wing_pred[:,4] = y_pred[:,10]\n",
    "#     left_wing_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     left_wing_loss = find_single_angle_loss(left_wing_true, left_wing_pred)\n",
    "\n",
    "#     right_foot_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     right_foot_true[:,0] = y_true[:,12]\n",
    "#     right_foot_true[:,1] = y_true[:,13]\n",
    "#     right_foot_true[:,2] = y_true[:,10]\n",
    "#     right_foot_true[:,3] = y_true[:,11]\n",
    "#     right_foot_true[:,4] = y_true[:,4]\n",
    "#     right_foot_true[:,5] = y_true[:,5]\n",
    "#     right_foot_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     right_foot_pred[:,0] = y_pred[:,12]\n",
    "#     right_foot_pred[:,1] = y_pred[:,13]\n",
    "#     right_foot_pred[:,2] = y_pred[:,10]\n",
    "#     right_foot_pred[:,3] = y_pred[:,11]\n",
    "#     right_foot_pred[:,4] = y_pred[:,4]\n",
    "#     right_foot_pred[:,5] = y_pred[:,5]\n",
    "\n",
    "#     right_foot_loss = find_single_angle_loss(right_foot_true, right_foot_pred)\n",
    "\n",
    "#     left_foot_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     left_foot_true[:,0] = y_true[:,14]\n",
    "#     left_foot_true[:,1] = y_true[:,15]\n",
    "#     left_foot_true[:,2] = y_true[:,10]\n",
    "#     left_foot_true[:,3] = y_true[:,11]\n",
    "#     left_foot_true[:,4] = y_true[:,4]\n",
    "#     left_foot_true[:,5] = y_true[:,5]\n",
    "#     left_foot_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     left_foot_pred[:,0] = y_pred[:,14]\n",
    "#     left_foot_pred[:,1] = y_pred[:,15]\n",
    "#     left_foot_pred[:,2] = y_pred[:,10]\n",
    "#     left_foot_pred[:,3] = y_pred[:,11]\n",
    "#     left_foot_pred[:,4] = y_pred[:,4]\n",
    "#     left_foot_pred[:,5] = y_pred[:,5]\n",
    "    \n",
    "#     left_foot_loss = find_single_angle_loss(left_foot_true, left_foot_pred)\n",
    "\n",
    "\n",
    "#     total_angle_loss = beak_angle_loss + head_angle_loss + right_wing_loss + left_wing_loss + right_foot_loss + left_foot_loss\n",
    "\n",
    "#     angle_loss = total_angle_loss/6\n",
    "\n",
    "#     return angle_loss\n",
    "# def masked_simpleAngles(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Computes the mean squared error, ignoring the invisible keypoints.\n",
    "#     Assuming that -10.0 indicates an invisible keypoint.\n",
    "#     \"\"\"\n",
    "#     # DEBUG \n",
    "#     if torch.isnan(y_true).any():\n",
    "#         print('going to nan')\n",
    "#     if torch.isnan(y_pred).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # find the angle loss \n",
    "#     angle_loss = find_angle_loss(y_true, y_pred)\n",
    "\n",
    "#     # normalise the loss\n",
    "#     angle_loss = angle_loss/torch.pi\n",
    "\n",
    "#     # Create a mask where keypoints are visible\n",
    "#     mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "#     # Apply the mask to filter out invisible keypoints from both\n",
    "#     # the predictions and the true values\n",
    "#     y_true_masked = y_true * mask\n",
    "#     y_pred_masked = y_pred * mask\n",
    "\n",
    "#     # Compute the Mean Squared Error only on the visible keypoints\n",
    "#     mae =  F.l1_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    \n",
    "\n",
    "#     loss = (mae * 0.7) + (angle_loss * 0.3) \n",
    "#     # DEBUG\n",
    "#     if torch.isnan(angle_loss).any():\n",
    "#         print('going to nan')\n",
    "        \n",
    "#     print('mae loss:',mae.item(), '    angle_loss:', angle_loss.item(), '     total_loss:', loss.item())\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_tensor_angle(input_tensor):\n",
    "#     \"\"\"\n",
    "#     Compute angles between vectors defined in input_tensor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_tensor: Tensor with 6 columns and multiple rows.\n",
    "#     Returns:\n",
    "#     - Tensor of angles in radians with the same number of rows as input_tensor.\n",
    "#     \"\"\"\n",
    "#     # # Step 1: Create a mask for rows without -10\n",
    "#     # mask = (input_tensor != -10).all(dim=1).unsqueeze(1)  # Mask rows with -10\n",
    "\n",
    "#     # # Step 2: Adjust input_tensor using the mask\n",
    "#     # #adjusted_input_tensor = input_tensor * mask  # Set rows with -10 to 0\n",
    "\n",
    "#     # Step 3: Compute vectors BA and BC\n",
    "#     # vector_tensor = torch.zeros((adjusted_input_tensor.size(0), 4), device=input_tensor.device)\n",
    "#     # vector_tensor[:, 0] = adjusted_input_tensor[:, 0] - adjusted_input_tensor[:, 2]  # BA_x\n",
    "#     # vector_tensor[:, 1] = adjusted_input_tensor[:, 1] - adjusted_input_tensor[:, 3]  # BA_y\n",
    "#     # vector_tensor[:, 2] = adjusted_input_tensor[:, 4] - adjusted_input_tensor[:, 2]  # BC_x\n",
    "#     # vector_tensor[:, 3] = adjusted_input_tensor[:, 5] - adjusted_input_tensor[:, 3]  # BC_y\n",
    "#     vector_tensor = torch.zeros((input_tensor.size(0), 4), device=input_tensor.device)\n",
    "#     vector_tensor[:, 0] = input_tensor[:, 0] - input_tensor[:, 2]  # BA_x\n",
    "#     vector_tensor[:, 1] = input_tensor[:, 1] - input_tensor[:, 3]  # BA_y\n",
    "#     vector_tensor[:, 2] = input_tensor[:, 4] - input_tensor[:, 2]  # BC_x\n",
    "#     vector_tensor[:, 3] = input_tensor[:, 5] - input_tensor[:, 3]  # BC_y\n",
    "\n",
    "#         # DEBUG - check for nan\n",
    "#     if torch.isnan(vector_tensor).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # Step 4: Compute dot product and magnitudes\n",
    "#     dot_prod_tensor = (vector_tensor[:, 0] * vector_tensor[:, 2] + \n",
    "#                        vector_tensor[:, 1] * vector_tensor[:, 3]).unsqueeze(1)\n",
    "#     magnitude_tensor_BA = torch.linalg.norm(vector_tensor[:, :2], dim=1, keepdim=True)  # Magnitude of BA\n",
    "#     magnitude_tensor_BC = torch.linalg.norm(vector_tensor[:, 2:], dim=1, keepdim=True)  # Magnitude of BC\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(dot_prod_tensor).any():\n",
    "#         print('going to nan')\n",
    "#             # DEBUG - check for nan\n",
    "#     if torch.isnan(magnitude_tensor_BA).any():\n",
    "#         print('going to nan')\n",
    "#         # DEBUG - check for nan\n",
    "#     if torch.isnan(magnitude_tensor_BC).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "\n",
    "#     # Step 5: Compute angles in radians\n",
    "#     clamp_vals = torch.clamp((dot_prod_tensor / (magnitude_tensor_BA * magnitude_tensor_BC)), -0.99999, 0.99999) # Clamp values to the valid range for acos\n",
    "#        # DEBUG - check for nan\n",
    "#     if torch.isnan(clamp_vals).any():\n",
    "#         print('going to nan')\n",
    "    \n",
    "#     angle_tensor = torch.acos(clamp_vals)\n",
    "#        # DEBUG - check for nan\n",
    "#     if torch.isnan(clamp_vals).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "\n",
    "#     return angle_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test find_tensor_angle\n",
    "# test_tensor = torch.tensor([\n",
    "#     [-10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # Forms a straight line, angle = 180 degrees\n",
    "#     [1.0, 0.0, 0.0, 0.0, 1.0, 1.0],    # Forms a right-angle triangle, angle < 180 degrees\n",
    "#     [-10.0, 0.0, -0.0, 0.0, 1.0, 0.0]    # Forms a horizontal line, angle = 180 degrees\n",
    "# ], dtype=torch.float32)\n",
    "\n",
    "# print(find_tensor_angle(test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tensor_abs_difference(tensor1, tensor2):\n",
    "#     \"\"\"\n",
    "#     Compute the absolute difference between two tensors, each with a single column and multiple rows.\n",
    "\n",
    "#     Parameters:\n",
    "#     - tensor1: A PyTorch tensor with shape (N, 1)\n",
    "#     - tensor2: A PyTorch tensor with shape (N, 1)\n",
    "\n",
    "#     Returns:\n",
    "#     - A PyTorch tensor with the absolute differences, shape (N, 1)\n",
    "#     \"\"\"\n",
    "#     # Ensure both tensors have the same shape\n",
    "#     if tensor1.shape != tensor2.shape:\n",
    "#         raise ValueError(\"Input tensors must have the same shape.\")\n",
    "\n",
    "#     # Compute absolute difference\n",
    "#     abs_diff = torch.abs(tensor1 - tensor2)\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(abs_diff).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     return abs_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the angle loss\n",
    "# def find_single_angle_loss(y_true_subset, y_pred_subset):\n",
    "\n",
    "#     y_true_angle = find_tensor_angle(y_true_subset)\n",
    "#     y_pred_angle = find_tensor_angle(y_pred_subset)\n",
    "\n",
    "#     # find the abs difference in angles\n",
    "#     angle_abs_differnce = tensor_abs_difference(y_true_angle, y_pred_angle)\n",
    "    \n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(angle_abs_differnce).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # normalise the abs difference (POTENTIALLY NEED TO DO THIS SO IT IS IN THE SAME RANGE AS THE MAE LOSS)\n",
    "\n",
    "#     # Step 1: Create a mask for rows without -10\n",
    "#     mask = (y_true_subset != -10).all(dim=1).unsqueeze(1)  # Mask rows with -10\n",
    "\n",
    "#     # mask angles where kp are missing\n",
    "#     angle_abs_differnce_masked = angle_abs_differnce * mask\n",
    "\n",
    "#     # normalise the abs difference (POTENTIALLY NEED TO DO THIS SO IT IS IN THE SAME RANGE AS THE MAE LOSS) - can only go up to 2*pi rad, so divide by that\n",
    "\n",
    "#     # find the mean abs difference between angles\n",
    "#     num_angles = torch.sum(mask)\n",
    "#     if num_angles == 0:\n",
    "#         num_angles = 1 \n",
    "\n",
    "#     single_angle_loss = torch.sum(angle_abs_differnce_masked)/num_angles\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(single_angle_loss).any():\n",
    "#         print('going to nan')\n",
    "    \n",
    "#     return single_angle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_angle_loss(y_true, y_pred):\n",
    "\n",
    "#     beak_angle_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     beak_angle_true[:,0] = y_true[:,2]\n",
    "#     beak_angle_true[:,1] = y_true[:,3]\n",
    "#     beak_angle_true[:,2] = y_true[:,0]\n",
    "#     beak_angle_true[:,3] = y_true[:,1]\n",
    "#     beak_angle_true[:,4] = y_true[:,4]\n",
    "#     beak_angle_true[:,5] = y_true[:,5]\n",
    "#     beak_angle_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     beak_angle_pred[:,0] = y_pred[:,2]\n",
    "#     beak_angle_pred[:,1] = y_pred[:,3]\n",
    "#     beak_angle_pred[:,2] = y_pred[:,0]\n",
    "#     beak_angle_pred[:,3] = y_pred[:,1]\n",
    "#     beak_angle_pred[:,4] = y_pred[:,4]\n",
    "#     beak_angle_pred[:,5] = y_pred[:,5]\n",
    "\n",
    "#     beak_angle_loss = find_single_angle_loss(beak_angle_true, beak_angle_pred)\n",
    "\n",
    "#     head_angle_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     head_angle_true[:,0] = y_true[:,0]\n",
    "#     head_angle_true[:,1] = y_true[:,1]\n",
    "#     head_angle_true[:,2] = y_true[:,4]\n",
    "#     head_angle_true[:,3] = y_true[:,5]\n",
    "#     head_angle_true[:,4] = y_true[:,10]\n",
    "#     head_angle_true[:,5] = y_true[:,11]\n",
    "#     head_angle_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     head_angle_pred[:,0] = y_pred[:,0]\n",
    "#     head_angle_pred[:,1] = y_pred[:,1]\n",
    "#     head_angle_pred[:,2] = y_pred[:,4]\n",
    "#     head_angle_pred[:,3] = y_pred[:,5]\n",
    "#     head_angle_pred[:,4] = y_pred[:,10]\n",
    "#     head_angle_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     head_angle_loss = find_single_angle_loss(head_angle_true, head_angle_pred)\n",
    "\n",
    "#     right_wing_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     right_wing_true[:,0] = y_true[:,6]\n",
    "#     right_wing_true[:,1] = y_true[:,7]\n",
    "#     right_wing_true[:,2] = y_true[:,4]\n",
    "#     right_wing_true[:,3] = y_true[:,5]\n",
    "#     right_wing_true[:,4] = y_true[:,10]\n",
    "#     right_wing_true[:,5] = y_true[:,11]\n",
    "#     right_wing_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     right_wing_pred[:,0] = y_pred[:,6]\n",
    "#     right_wing_pred[:,1] = y_pred[:,7]\n",
    "#     right_wing_pred[:,2] = y_pred[:,4]\n",
    "#     right_wing_pred[:,3] = y_pred[:,5]\n",
    "#     right_wing_pred[:,4] = y_pred[:,10]\n",
    "#     right_wing_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     right_wing_loss = find_single_angle_loss(right_wing_true, right_wing_pred)\n",
    "\n",
    "#     left_wing_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     left_wing_true[:,0] = y_true[:,8]\n",
    "#     left_wing_true[:,1] = y_true[:,9]\n",
    "#     left_wing_true[:,2] = y_true[:,4]\n",
    "#     left_wing_true[:,3] = y_true[:,5]\n",
    "#     left_wing_true[:,4] = y_true[:,10]\n",
    "#     left_wing_true[:,5] = y_true[:,11]\n",
    "#     left_wing_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     left_wing_pred[:,0] = y_pred[:,8]\n",
    "#     left_wing_pred[:,1] = y_pred[:,9]\n",
    "#     left_wing_pred[:,2] = y_pred[:,4]\n",
    "#     left_wing_pred[:,3] = y_pred[:,5]\n",
    "#     left_wing_pred[:,4] = y_pred[:,10]\n",
    "#     left_wing_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     left_wing_loss = find_single_angle_loss(left_wing_true, left_wing_pred)\n",
    "\n",
    "#     right_foot_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     right_foot_true[:,0] = y_true[:,12]\n",
    "#     right_foot_true[:,1] = y_true[:,13]\n",
    "#     right_foot_true[:,2] = y_true[:,10]\n",
    "#     right_foot_true[:,3] = y_true[:,11]\n",
    "#     right_foot_true[:,4] = y_true[:,4]\n",
    "#     right_foot_true[:,5] = y_true[:,5]\n",
    "#     right_foot_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     right_foot_pred[:,0] = y_pred[:,12]\n",
    "#     right_foot_pred[:,1] = y_pred[:,13]\n",
    "#     right_foot_pred[:,2] = y_pred[:,10]\n",
    "#     right_foot_pred[:,3] = y_pred[:,11]\n",
    "#     right_foot_pred[:,4] = y_pred[:,4]\n",
    "#     right_foot_pred[:,5] = y_pred[:,5]\n",
    "\n",
    "#     right_foot_loss = find_single_angle_loss(right_foot_true, right_foot_pred)\n",
    "\n",
    "#     left_foot_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     left_foot_true[:,0] = y_true[:,14]\n",
    "#     left_foot_true[:,1] = y_true[:,15]\n",
    "#     left_foot_true[:,2] = y_true[:,10]\n",
    "#     left_foot_true[:,3] = y_true[:,11]\n",
    "#     left_foot_true[:,4] = y_true[:,4]\n",
    "#     left_foot_true[:,5] = y_true[:,5]\n",
    "#     left_foot_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     left_foot_pred[:,0] = y_pred[:,14]\n",
    "#     left_foot_pred[:,1] = y_pred[:,15]\n",
    "#     left_foot_pred[:,2] = y_pred[:,10]\n",
    "#     left_foot_pred[:,3] = y_pred[:,11]\n",
    "#     left_foot_pred[:,4] = y_pred[:,4]\n",
    "#     left_foot_pred[:,5] = y_pred[:,5]\n",
    "    \n",
    "#     left_foot_loss = find_single_angle_loss(left_foot_true, left_foot_pred)\n",
    "\n",
    "\n",
    "#     total_angle_loss = beak_angle_loss + head_angle_loss + right_wing_loss + left_wing_loss + right_foot_loss + left_foot_loss\n",
    "\n",
    "#     angle_loss = total_angle_loss/6\n",
    "\n",
    "#     return angle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NEW ANGLE LOSS FUNCTION\n",
    "\n",
    "# def find_tensor_angle(input_tensor):\n",
    "#     \"\"\"\n",
    "#     Compute angles between vectors defined in input_tensor, returning angles in [0, 2pi).\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_tensor: Tensor with 6 columns and multiple rows.\n",
    "\n",
    "#     Returns:\n",
    "#     - Tensor of angles in radians with the same number of rows as input_tensor.\n",
    "#     \"\"\"\n",
    "#     # Step 3: Compute vectors BA and BC\n",
    "#     vector_tensor = torch.zeros((input_tensor.size(0), 4), device=input_tensor.device)\n",
    "#     vector_tensor[:, 0] = input_tensor[:, 0] - input_tensor[:, 2]  # BA_x\n",
    "#     vector_tensor[:, 1] = input_tensor[:, 1] - input_tensor[:, 3]  # BA_y\n",
    "#     vector_tensor[:, 2] = input_tensor[:, 4] - input_tensor[:, 2]  # BC_x\n",
    "#     vector_tensor[:, 3] = input_tensor[:, 5] - input_tensor[:, 3]  # BC_y\n",
    "\n",
    "#     # Step 4: Compute dot product, magnitudes, and cross product\n",
    "#     dot_prod_tensor = (vector_tensor[:, 0] * vector_tensor[:, 2] + \n",
    "#                        vector_tensor[:, 1] * vector_tensor[:, 3]).unsqueeze(1)\n",
    "#     # magnitude_tensor_BA = torch.linalg.norm(vector_tensor[:, :2], dim=1, keepdim=True)\n",
    "#     # magnitude_tensor_BC = torch.linalg.norm(vector_tensor[:, 2:], dim=1, keepdim=True)\n",
    "#     cross_prod_tensor = (vector_tensor[:, 0] * vector_tensor[:, 3] - \n",
    "#                          vector_tensor[:, 1] * vector_tensor[:, 2]).unsqueeze(1)\n",
    "\n",
    "#     # Compute angle in radians\n",
    "#     angle_tensor = torch.atan2(cross_prod_tensor, dot_prod_tensor)\n",
    "\n",
    "#     # Convert angle to [0, 2pi)\n",
    "#     angle_tensor = torch.where(angle_tensor < 0, angle_tensor + 2 * torch.pi, angle_tensor)\n",
    "\n",
    "#     return angle_tensor\n",
    "\n",
    "# def tensor_angle_difference(tensor1, tensor2):\n",
    "#     \"\"\"\n",
    "#     Compute the angular difference between two tensors (in [0, 2pi))\n",
    "\n",
    "#     Parameters:\n",
    "#     - tensor1: A PyTorch tensor with angles in radians.\n",
    "#     - tensor2: A PyTorch tensor with angles in radians.\n",
    "\n",
    "#     Returns:\n",
    "#     - A PyTorch tensor with angular differences in [0, pi].\n",
    "#     \"\"\"\n",
    "#     angle_diff = torch.abs(tensor1 - tensor2)\n",
    "#     return torch.where(angle_diff > torch.pi, 2 * torch.pi - angle_diff, angle_diff)\n",
    "\n",
    "# def find_single_angle_loss(y_true_subset, y_pred_subset):\n",
    "#     \"\"\"\n",
    "#     Compute the mean angular loss between y_true_subset and y_pred_subset.\n",
    "\n",
    "#     Parameters:\n",
    "#     - y_true_subset: Tensor of true coordinates.\n",
    "#     - y_pred_subset: Tensor of predicted coordinates.\n",
    "\n",
    "#     Returns:\n",
    "#     - The mean angular loss.\n",
    "#     \"\"\"\n",
    "#     y_true_angle = find_tensor_angle(y_true_subset)\n",
    "#     y_pred_angle = find_tensor_angle(y_pred_subset)\n",
    "\n",
    "#     # Compute angular difference\n",
    "#     angle_abs_difference = tensor_angle_difference(y_true_angle, y_pred_angle)\n",
    "\n",
    "#     # Create a mask for valid rows (without -10)\n",
    "#     mask = (y_true_subset != -10).all(dim=1).unsqueeze(1) # Mask rows with -10\n",
    "#     angle_abs_difference_masked = angle_abs_difference * mask # mask angles where kp are missing\n",
    "\n",
    "#     # Calculate mean angular loss\n",
    "#     num_angles = torch.sum(mask)\n",
    "#     if num_angles == 0:\n",
    "#         num_angles = 1\n",
    "\n",
    "#     single_angle_loss = torch.sum(angle_abs_difference_masked) / num_angles\n",
    "#     return single_angle_loss\n",
    "\n",
    "# def create_angle_tensor(subset, indices):\n",
    "#     \"\"\"\n",
    "#     Helper function to generate a tensor containing the coordinates of keypoints needed to compute angles.\n",
    "#     \"\"\"\n",
    "#     tensor = torch.zeros((subset.size(0), 6), device=subset.device)\n",
    "    \n",
    "#     for i, index in enumerate(indices):\n",
    "#         tensor[:, i] = subset[:, index] \n",
    "\n",
    "#     return tensor\n",
    "\n",
    "\n",
    "# def find_angle_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Compute the average angular loss across all sets of angles.\n",
    "\n",
    "#     Parameters:\n",
    "#     - y_true: Tensor of true keypoint coordinates.\n",
    "#     - y_pred: Tensor of predicted keypoint coordinates.\n",
    "\n",
    "#     Returns:\n",
    "#     - The average angular loss.\n",
    "#     \"\"\"\n",
    "\n",
    "#     indices_list = [\n",
    "#         (2, 3, 0, 1, 4, 5),  # Beak\n",
    "#         (0, 1, 4, 5, 10, 11),  # Head\n",
    "#         (6, 7, 4, 5, 10, 11),  # Right wing\n",
    "#         (8, 9, 4, 5, 10, 11),  # Left wing\n",
    "#         (12, 13, 10, 11, 4, 5),  # Right foot\n",
    "#         (14, 15, 10, 11, 4, 5)  # Left foot\n",
    "#     ]\n",
    "\n",
    "#     total_loss = 0\n",
    "#     # DEBUG\n",
    "#     # i = 1\n",
    "#     # total_loss_old = 0\n",
    "    \n",
    "#     for indices in indices_list:\n",
    "#         true_tensor = create_angle_tensor(y_true, indices)\n",
    "#         pred_tensor = create_angle_tensor(y_pred, indices)\n",
    "#         total_loss += find_single_angle_loss(true_tensor, pred_tensor)\n",
    "\n",
    "#         #DEBUG\n",
    "#         # print(\"this is loss for\", i, \":\", (total_loss-total_loss_old)/i)\n",
    "#         # i+=1\n",
    "#         # total_loss_old = total_loss.clone()\n",
    "#         #print(id(total_loss), id(total_loss_old))\n",
    "\n",
    "\n",
    "#     return total_loss / len(indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final angle loss function\n",
    "def find_tensor_angle(input_tensor):\n",
    "    \"\"\"\n",
    "    Compute angles between vectors defined in input_tensor, returning angles in [0, 2pi).\n",
    "\n",
    "    Parameters:\n",
    "    - input_tensor: Tensor with 6 columns and multiple rows.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of angles in radians with the same number of rows as input_tensor.\n",
    "    \"\"\"\n",
    "    # Step 3: Compute vectors BA and BC\n",
    "    vector_tensor = torch.zeros((input_tensor.size(0), 4), device=input_tensor.device)\n",
    "    vector_tensor[:, 0] = input_tensor[:, 0] - input_tensor[:, 2]  # BA_x\n",
    "    vector_tensor[:, 1] = input_tensor[:, 1] - input_tensor[:, 3]  # BA_y\n",
    "    vector_tensor[:, 2] = input_tensor[:, 4] - input_tensor[:, 2]  # BC_x\n",
    "    vector_tensor[:, 3] = input_tensor[:, 5] - input_tensor[:, 3]  # BC_y\n",
    "\n",
    "    # Step 4: Compute dot product, magnitudes, and cross product\n",
    "    dot_prod_tensor = (vector_tensor[:, 0] * vector_tensor[:, 2] + \n",
    "                       vector_tensor[:, 1] * vector_tensor[:, 3]).unsqueeze(1)\n",
    "    # magnitude_tensor_BA = torch.linalg.norm(vector_tensor[:, :2], dim=1, keepdim=True)\n",
    "    # magnitude_tensor_BC = torch.linalg.norm(vector_tensor[:, 2:], dim=1, keepdim=True)\n",
    "    cross_prod_tensor = (vector_tensor[:, 0] * vector_tensor[:, 3] - \n",
    "                         vector_tensor[:, 1] * vector_tensor[:, 2]).unsqueeze(1)\n",
    "\n",
    "    # Compute angle in radians\n",
    "    angle_tensor = torch.atan2(cross_prod_tensor, dot_prod_tensor)\n",
    "\n",
    "    # Convert angle to [0, 2pi)\n",
    "    angle_tensor = torch.where(angle_tensor < 0, angle_tensor + 2 * torch.pi, angle_tensor)\n",
    "\n",
    "    return angle_tensor\n",
    "\n",
    "def tensor_angle_difference(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Compute the angular difference between two tensors (in [0, 2pi))\n",
    "\n",
    "    Parameters:\n",
    "    - tensor1: A PyTorch tensor with angles in radians.\n",
    "    - tensor2: A PyTorch tensor with angles in radians.\n",
    "\n",
    "    Returns:\n",
    "    - A PyTorch tensor with angular differences in [0, pi].\n",
    "    \"\"\"\n",
    "    angle_diff = torch.abs(tensor1 - tensor2)\n",
    "    return torch.where(angle_diff > torch.pi, 2 * torch.pi - angle_diff, angle_diff)\n",
    "\n",
    "def find_single_angle_loss(y_true_subset, y_pred_subset):\n",
    "    \"\"\"\n",
    "    Compute the mean angular loss between y_true_subset and y_pred_subset.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true_subset: Tensor of true coordinates.\n",
    "    - y_pred_subset: Tensor of predicted coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - The mean angular loss.\n",
    "    \"\"\"\n",
    "    y_true_angle = find_tensor_angle(y_true_subset)\n",
    "    y_pred_angle = find_tensor_angle(y_pred_subset)\n",
    "\n",
    "    # Compute angular difference\n",
    "    angle_abs_difference = tensor_angle_difference(y_true_angle, y_pred_angle)\n",
    "\n",
    "    # Create a mask for valid rows (without -10)\n",
    "    mask = (y_true_subset != -10).all(dim=1).unsqueeze(1) # Mask rows with -10\n",
    "    angle_abs_difference_masked = angle_abs_difference * mask # mask angles where kp are missing\n",
    "\n",
    "    # Calculate mean angular loss\n",
    "    num_angles = torch.sum(mask)\n",
    "    if num_angles == 0:\n",
    "        num_angles = 1\n",
    "\n",
    "    single_angle_loss = torch.sum(angle_abs_difference_masked) / num_angles\n",
    "    return single_angle_loss\n",
    "\n",
    "def create_angle_tensor(subset, indices):\n",
    "    \"\"\"\n",
    "    Helper function to generate a tensor containing the coordinates of keypoints needed to compute angles.\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros((subset.size(0), 6), device=subset.device)\n",
    "    \n",
    "    for i, index in enumerate(indices):\n",
    "        tensor[:, i] = subset[:, index] \n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def find_angle_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the average angular loss across all sets of angles.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Tensor of true keypoint coordinates.\n",
    "    - y_pred: Tensor of predicted keypoint coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - The average angular loss.\n",
    "    \"\"\"\n",
    "\n",
    "    indices_list = [\n",
    "        (2, 3, 0, 1, 4, 5),  # Beak\n",
    "        (0, 1, 4, 5, 10, 11),  # Head\n",
    "        (6, 7, 4, 5, 10, 11),  # Right wing\n",
    "        (8, 9, 4, 5, 10, 11),  # Left wing\n",
    "        (12, 13, 10, 11, 4, 5),  # Right foot\n",
    "        (14, 15, 10, 11, 4, 5)  # Left foot\n",
    "    ]\n",
    "\n",
    "    total_loss = 0\n",
    "    # DEBUG\n",
    "    # i = 1\n",
    "    # total_loss_old = 0\n",
    "    \n",
    "    for indices in indices_list:\n",
    "        true_tensor = create_angle_tensor(y_true, indices)\n",
    "        pred_tensor = create_angle_tensor(y_pred, indices)\n",
    "        total_loss += find_single_angle_loss(true_tensor, pred_tensor)\n",
    "\n",
    "        #DEBUG\n",
    "        # print(\"this is loss for\", i, \":\", (total_loss-total_loss_old)/i)\n",
    "        # i+=1\n",
    "        # total_loss_old = total_loss.clone()\n",
    "        #print(id(total_loss), id(total_loss_old))\n",
    "    # DEBUG \n",
    "    #print(total_loss)\n",
    "\n",
    "    return total_loss / len(indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def masked_simpleAngles(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Computes the mean squared error, ignoring the invisible keypoints.\n",
    "#     Assuming that -10.0 indicates an invisible keypoint.\n",
    "#     \"\"\"\n",
    "#     # DEBUG \n",
    "#     # if torch.isnan(y_true).any():\n",
    "#     #     print('going to nan')\n",
    "#     # if torch.isnan(y_pred).any():\n",
    "#     #     print('going to nan')\n",
    "\n",
    "#     # find the angle loss \n",
    "#     angle_loss = find_angle_loss(y_true, y_pred)\n",
    "\n",
    "#     # normalise the loss\n",
    "#     angle_loss = angle_loss/torch.pi\n",
    "\n",
    "#     # Create a mask where keypoints are visible\n",
    "#     mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "#     # Apply the mask to filter out invisible keypoints from both\n",
    "#     # the predictions and the true values\n",
    "#     y_true_masked = y_true * mask\n",
    "#     y_pred_masked = y_pred * mask\n",
    "\n",
    "#     # Compute the Mean Squared Error only on the visible keypoints\n",
    "#     mae =  F.l1_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    \n",
    "\n",
    "#     loss = (mae * 0.5) + (angle_loss * 0.5) \n",
    "#     # DEBUG\n",
    "#     # if torch.isnan(angle_loss).any():\n",
    "#     #     print('going to nan')\n",
    "        \n",
    "#     #print('mae loss:',mae.item(), '    angle_loss:', angle_loss.item(), '     total_loss:', loss.item())\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_simpleAngles(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error, ignoring the invisible keypoints.\n",
    "    Assuming that -10.0 indicates an invisible keypoint.\n",
    "    \"\"\"\n",
    "    # DEBUG \n",
    "    if torch.isnan(y_true).any():\n",
    "        print('going to nan')\n",
    "    if torch.isnan(y_pred).any():\n",
    "        print('going to nan')\n",
    "\n",
    "    # find the angle loss \n",
    "    angle_loss = find_angle_loss(y_true, y_pred)\n",
    "\n",
    "    # normalise the loss\n",
    "    angle_loss = angle_loss/torch.pi\n",
    "    #print(angle_loss)\n",
    "\n",
    "    # adjust angle to be same order of magnitude as mae\n",
    "    angle_loss = angle_loss/10\n",
    "\n",
    "    # Create a mask where keypoints are visible\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints from both\n",
    "    # the predictions and the true values\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # Compute the Mean Squared Error only on the visible keypoints\n",
    "    mae =  F.l1_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    \n",
    "\n",
    "    loss = (mae * 0.7) + (angle_loss * 0.3) \n",
    "    # DEBUG\n",
    "    if torch.isnan(angle_loss).any():\n",
    "        print('going to nan')\n",
    "        \n",
    "    # print('mae loss:',mae.item(), '    angle_loss:', angle_loss.item(), '     total_loss:', loss.item())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_polarCoords(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error, ignoring the invisible keypoints.\n",
    "    Assuming that -10.0 indicates an invisible keypoint.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints from both\n",
    "    # the predictions and the true values\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # Compute the Mean Squared Error only on the visible keypoints\n",
    "    mse = F.mse_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCK\n",
    "# put in a function that will use the max bbox if primary kp is missing\n",
    "def pck_metric(y_true, y_pred, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Computes the Percentage of Correct Keypoints (PCK) metric.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (torch.Tensor): The ground truth keypoints (batch_size, num_keypoints*2).\n",
    "    y_pred (torch.Tensor): The predicted keypoints (batch_size, num_keypoints*2).\n",
    "    threshold (float): The distance threshold for a keypoint to be considered correct.\n",
    "                       Typically set relative to the size of the bounding box (e.g., 0.2).\n",
    "    \n",
    "    Returns:\n",
    "    float: The percentage of correct keypoints.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible (not equal to -10)\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "    #print(mask)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # print(y_true_masked)\n",
    "    # print(y_pred_masked)\n",
    "\n",
    "    # Compute the Euclidean distance between the predicted and true keypoints\n",
    "    distances = torch.sqrt((y_pred_masked[:, ::2] - y_true_masked[:, ::2]) ** 2 +\n",
    "                           (y_pred_masked[:, 1::2] - y_true_masked[:, 1::2]) ** 2)\n",
    "    \n",
    "    #print(distances)\n",
    "    \n",
    "    # Normalize the distances (relative to the max and min y coord)\n",
    "    Norm_max_min_kp = torch.max(y_true_masked[:, 1::2], dim=1)[0] - torch.min(y_true_masked[:, 1::2], dim=1)[0]\n",
    "    # Normalise based on the distance between the head and the bottom of the body (position 0, 1 and )\n",
    "    #print(y_true[:, 0],y_true[:,10],y_true[:, 1],y_true[:, 11])\n",
    "    #print((y_true[:, 0] - y_true[:,10]) ** 2)\n",
    "    #print((y_true[:, 1] - y_true[:, 11]) ** 2)\n",
    "    Norm_head_lowerbody = torch.sqrt((y_true[:, 0] - y_true[:,10]) ** 2 +\n",
    "                        (y_true[:, 1] - y_true[:, 11]) ** 2)\n",
    "    #print(Norm_head_lowerbody)\n",
    "    normalized_distances = distances / Norm_head_lowerbody[:, None]\n",
    "    #print(distances)\n",
    "    #print(normalized_distances)\n",
    "\n",
    "    # Count the correct keypoints (distance <= threshold)\n",
    "    correct_keypoints = (normalized_distances <= threshold).float() * mask[:, ::2]\n",
    "    #print(correct_keypoints)\n",
    "\n",
    "    # Calculate the PCK as the percentage of correct keypoints\n",
    "    pck = correct_keypoints.sum() / mask[:, ::2].sum()\n",
    "    return pck#.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DeepPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "#         super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "#         # The feature extractor part of the model, composed of several convolutional layers.\n",
    "#         self.features = nn.Sequential(\n",
    "#             # Conv2d: Input channels = 3 (RGB image), Output channels = 96, kernel size = 11x11,\n",
    "#             # stride = 4, padding = 4. \n",
    "#             # Input: (batch_size, 3, 220, 220)\n",
    "#             # Output: (batch_size, 48, 55, 55)\n",
    "#             nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 48, 55, 55)\n",
    "#             # Output: (batch_size, 48, 27, 27)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 48, Output channels = 128, kernel size = 5x5,\n",
    "#             # stride = 1, padding = 2.\n",
    "#             # Input: (batch_size, 48, 27, 27)\n",
    "#             # Output: (batch_size, 128, 27, 27)\n",
    "#             nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 128, 27, 27)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 128, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 192, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 192, Output channels = 128, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 192, 13, 13)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         # The classifier part of the model, composed of fully connected layers.\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten the input tensor\n",
    "#             # Input: (batch_size, 128, 6, 6)\n",
    "#             # Output: (batch_size, 128 * 6 * 6) = (batch_size, 4608)\n",
    "#             nn.Flatten(),\n",
    "            \n",
    "#             # Linear layer with input size 4608 and output size 4096\n",
    "#             # Input: (batch_size, 4608)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             # nn.Linear(128 * 6 * 6, 4096),\n",
    "#             nn.Linear(128 * 6 * 6, 64),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             #nn.ReLU(inplace=True),\n",
    "#             nn.LeakyReLU(inplace=True), # leakyReLu is used in the RepNet\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             # nn.Dropout(0.6),\n",
    "#             #nn.Dropout(0.3),\n",
    "            \n",
    "#             # Linear layer with input size 4096 and output size 4096\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             #nn.Linear(4096, 4096),\n",
    "#             nn.Linear(64, 64),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             #nn.ReLU(inplace=True),\n",
    "#             nn.LeakyReLU(inplace=True), # leakyReLu is used in the RepNet\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             # nn.Dropout(0.6),\n",
    "#             #nn.Dropout(0.3),\n",
    "            \n",
    "#             # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "#             # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, nkeypoints * 2)\n",
    "#             # nn.Linear(4096, nkeypoints * 2)\n",
    "#             nn.Linear(64, nkeypoints * 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Define the forward pass through the network.\n",
    "#         # Pass input `x` through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEEPPOSE (Encoder test 5 - GAP, flatten, FC)\n",
    "# class DeepPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "#         super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "#         # The feature extractor part of the model, composed of several convolutional layers.\n",
    "#         self.features = nn.Sequential(\n",
    "#             # Conv2d: Input channels = 3 (RGB image), Output channels = 96, kernel size = 11x11,\n",
    "#             # stride = 4, padding = 4. \n",
    "#             # Input: (batch_size, 3, 220, 220)\n",
    "#             # Output: (batch_size, 48, 55, 55)\n",
    "#             nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 48, 55, 55)\n",
    "#             # Output: (batch_size, 48, 27, 27)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 48, Output channels = 128, kernel size = 5x5,\n",
    "#             # stride = 1, padding = 2.\n",
    "#             # Input: (batch_size, 48, 27, 27)\n",
    "#             # Output: (batch_size, 128, 27, 27)\n",
    "#             nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 128, 27, 27)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 128, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 192, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 192, Output channels = 128, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 192, 13, 13)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             #nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "#             # GAP Layer rather (for consistancy)\n",
    "#             nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "#         )\n",
    "        \n",
    "#         # The classifier part of the model, composed of fully connected layers.\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten\n",
    "#             nn.Flatten(),\n",
    "\n",
    "#             #linear layer with output the same as the num of kp\n",
    "#             nn.Linear(128,nkeypoints*2)\n",
    "\n",
    "#             # # Flatten the input tensor\n",
    "#             # # Input: (batch_size, 128, 6, 6)\n",
    "#             # # Output: (batch_size, 128 * 6 * 6) = (batch_size, 4608)\n",
    "#             # nn.Flatten(),\n",
    "            \n",
    "#             # # Linear layer with input size 4608 and output size 4096\n",
    "#             # # Input: (batch_size, 4608)\n",
    "#             # # Output: (batch_size, 4096)\n",
    "#             # # nn.Linear(128 * 6 * 6, 4096),\n",
    "#             # nn.Linear(128 * 6 * 6, 64),\n",
    "            \n",
    "#             # # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             # #nn.ReLU(inplace=True),\n",
    "#             # nn.LeakyReLU(inplace=True), # leakyReLu is used in the RepNet\n",
    "            \n",
    "#             # # Dropout layer with 60% dropout rate\n",
    "#             # # nn.Dropout(0.6),\n",
    "#             # #nn.Dropout(0.3),\n",
    "            \n",
    "#             # # Linear layer with input size 4096 and output size 4096\n",
    "#             # # Input: (batch_size, 4096)\n",
    "#             # # Output: (batch_size, 4096)\n",
    "#             # #nn.Linear(4096, 4096),\n",
    "#             # nn.Linear(64, 64),\n",
    "            \n",
    "#             # # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             # #nn.ReLU(inplace=True),\n",
    "#             # nn.LeakyReLU(inplace=True), # leakyReLu is used in the RepNet\n",
    "            \n",
    "#             # # Dropout layer with 60% dropout rate\n",
    "#             # # nn.Dropout(0.6),\n",
    "#             # #nn.Dropout(0.3),\n",
    "            \n",
    "#             # # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "#             # # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "#             # # Input: (batch_size, 4096)\n",
    "#             # # Output: (batch_size, nkeypoints * 2)\n",
    "#             # # nn.Linear(4096, nkeypoints * 2)\n",
    "#             # nn.Linear(64, nkeypoints * 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Define the forward pass through the network.\n",
    "#         # Pass input `x` through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEEPPOSE (Encoder test 6 version - with GAP)\n",
    "# class DeepPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "#         super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "#         # The feature extractor part of the model, composed of several convolutional layers.\n",
    "#         self.features = nn.Sequential(\n",
    "#             # Conv2d: Input channels = 3 (RGB image), Output channels = 48, kernel size = 11x11,\n",
    "#             # stride = 4, padding = 4. \n",
    "#             # Input: (batch_size, 3, 220, 220)\n",
    "#             # Output: (batch_size, 48, 55, 55)\n",
    "#             nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 48, 55, 55)\n",
    "#             # Output: (batch_size, 48, 27, 27)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 48, Output channels = 128, kernel size = 5x5,\n",
    "#             # stride = 1, padding = 2.\n",
    "#             # Input: (batch_size, 48, 27, 27)\n",
    "#             # Output: (batch_size, 128, 27, 27)\n",
    "#             nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 128, 27, 27)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 128, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 192, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             #------------CHANGE----------------\n",
    "#             # Here I have also changed to 192 output units instead of the 128 \n",
    "#             # Conv2d: Input channels = 192, Output channels = 128, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 192, 13, 13)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             #nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#             #-----------CHANGE------------------\n",
    "#             # Replace maxpool2d with a global pooling to reduce size\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             nn.AdaptiveAvgPool2d(1)\n",
    "#         )\n",
    "        \n",
    "#         # The classifier part of the model, composed of fully connected layers.\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten the input tensor\n",
    "#             # Input: (batch_size, 128, 6, 6)\n",
    "#             # Output: (batch_size, 128 * 6 * 6) = (batch_size, 4608)\n",
    "#             nn.Flatten(),\n",
    "            \n",
    "#             #------------CHANGE----------------\n",
    "#             # Here I have also changed to 192 units \n",
    "#             # Linear layer with input size 4608 and output size 4096\n",
    "#             # Input: (batch_size, 4608)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             nn.Linear(192, 4096),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             nn.Dropout(0.6),\n",
    "            \n",
    "#             # --------CHANGE---------------\n",
    "#             # Removed this FC layer and subsequent activation and dropout as only 7 layers are supposed to be present\n",
    "#             # # Linear layer with input size 4096 and output size 4096\n",
    "#             # # Input: (batch_size, 4096)\n",
    "#             # # Output: (batch_size, 4096)\n",
    "#             # nn.Linear(4096, 4096),\n",
    "            \n",
    "#             # # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             # nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # # Dropout layer with 60% dropout rate\n",
    "#             # nn.Dropout(0.6),\n",
    "            \n",
    "#             # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "#             # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, nkeypoints * 2)\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Define the forward pass through the network.\n",
    "#         # Pass input `x` through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEEPPOSE (Encoder test 12 version - with GAP - remove some layers to make smaller encoder) - removed the 2 x 192 layers\n",
    "# class DeepPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "#         super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "#         # The feature extractor part of the model, composed of several convolutional layers.\n",
    "#         self.features = nn.Sequential(\n",
    "#             # Conv2d: Input channels = 3 (RGB image), Output channels = 48, kernel size = 11x11,\n",
    "#             # stride = 4, padding = 4. \n",
    "#             # Input: (batch_size, 3, 220, 220)\n",
    "#             # Output: (batch_size, 48, 55, 55)\n",
    "#             nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 48, 55, 55)\n",
    "#             # Output: (batch_size, 48, 27, 27)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 48, Output channels = 128, kernel size = 5x5,\n",
    "#             # stride = 1, padding = 2.\n",
    "#             # Input: (batch_size, 48, 27, 27)\n",
    "#             # Output: (batch_size, 128, 27, 27)\n",
    "#             nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 128, 27, 27)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 128, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # -------- REMOVE THE NEXT TWO CONV LAYERS ------\n",
    "#             # # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "#             # # stride = 1, padding = 1.\n",
    "#             # # Input: (batch_size, 192, 13, 13)\n",
    "#             # # Output: (batch_size, 192, 13, 13)\n",
    "#             # nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # # # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             # nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # #------------CHANGE----------------\n",
    "#             # # Here I have also changed to 192 output units instead of the 128 \n",
    "#             # # Conv2d: Input channels = 192, Output channels = 128, kernel size = 3x3,\n",
    "#             # # stride = 1, padding = 1.\n",
    "#             # # Input: (batch_size, 192, 13, 13)\n",
    "#             # # Output: (batch_size, 128, 13, 13)\n",
    "#             # nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # -------- END REMOVE THE NEXT TWO CONV LAYERS ------\n",
    "\n",
    "#             # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             #nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#             #-----------CHANGE------------------\n",
    "#             # Replace maxpool2d with a global pooling to reduce size\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             nn.AdaptiveAvgPool2d(1)\n",
    "#         )\n",
    "        \n",
    "#         # The classifier part of the model, composed of fully connected layers.\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten the input tensor\n",
    "#             # Input: (batch_size, 128, 6, 6)\n",
    "#             # Output: (batch_size, 128 * 6 * 6) = (batch_size, 4608)\n",
    "#             nn.Flatten(),\n",
    "            \n",
    "#             #------------CHANGE----------------\n",
    "#             # Here I have also changed to 192 units \n",
    "#             # Linear layer with input size 4608 and output size 4096\n",
    "#             # Input: (batch_size, 4608)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             nn.Linear(192, 4096),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             nn.Dropout(0.6),\n",
    "            \n",
    "#             # --------CHANGE---------------\n",
    "#             # Removed this FC layer and subsequent activation and dropout as only 7 layers are supposed to be present\n",
    "#             # # Linear layer with input size 4096 and output size 4096\n",
    "#             # # Input: (batch_size, 4096)\n",
    "#             # # Output: (batch_size, 4096)\n",
    "#             # nn.Linear(4096, 4096),\n",
    "            \n",
    "#             # # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             # nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # # Dropout layer with 60% dropout rate\n",
    "#             # nn.Dropout(0.6),\n",
    "            \n",
    "#             # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "#             # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, nkeypoints * 2)\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Define the forward pass through the network.\n",
    "#         # Pass input `x` through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate test - DEEPPOSE (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class DeepPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "        super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "        # The feature extractor part of the model, composed of several convolutional layers.\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv2d: Input channels = 3 (RGB image), Output channels = 48, kernel size = 11x11,\n",
    "            # stride = 4, padding = 4. \n",
    "            # Input: (batch_size, 3, 220, 220)\n",
    "            # Output: (batch_size, 48, 55, 55)\n",
    "            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "            # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "            nn.LocalResponseNorm(5),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            # Input: (batch_size, 48, 55, 55)\n",
    "            # Output: (batch_size, 48, 27, 27)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2d: Input channels = 48, Output channels = 128, kernel size = 5x5,\n",
    "            # stride = 1, padding = 2.\n",
    "            # Input: (batch_size, 48, 27, 27)\n",
    "            # Output: (batch_size, 128, 27, 27)\n",
    "            nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "            # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "            nn.LocalResponseNorm(5),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            # Input: (batch_size, 128, 27, 27)\n",
    "            # Output: (batch_size, 128, 13, 13)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2d: Input channels = 128, Output channels = 192, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 128, 13, 13)\n",
    "            # Output: (batch_size, 192, 13, 13)\n",
    "            nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 192, 13, 13)\n",
    "            # Output: (batch_size, 192, 13, 13)\n",
    "            nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            #------------CHANGE----------------\n",
    "            # Here I have also changed to 192 output units instead of the 128 \n",
    "            # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 192, 13, 13)\n",
    "            # Output: (batch_size, 128, 13, 13)\n",
    "            nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Replace maxpool2d with a global pooling to reduce size\n",
    "            # Input: (batch_size, 128, 13, 13)\n",
    "            # Output: (batch_size, 128, 6, 6)\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # The classifier part of the model, composed of fully connected layers.\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Flatten the input tensor\n",
    "            # Input: (batch_size, 128, 6, 6)\n",
    "            # Output: (batch_size, 128 * 6 * 6) = (batch_size, 4608)\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            #------------CHANGE----------------\n",
    "            # Here I have also changed to 192 units \n",
    "            # Linear layer with input size 4608 and output size 4096\n",
    "            # Input: (batch_size, 4608)\n",
    "            # Output: (batch_size, 4096)\n",
    "            nn.Linear(192, 4096),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Dropout layer with 60% dropout rate\n",
    "            nn.Dropout(0.6),\n",
    "                       \n",
    "            # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "            # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "            # Input: (batch_size, 4096)\n",
    "            # Output: (batch_size, nkeypoints * 2)\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass through the network.\n",
    "        # Pass input `x` through the feature extractor\n",
    "        x = self.features(x)\n",
    "        # Pass the result through the classifier to get the final output\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - AlexNet pretrained (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class AlexNetEncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(AlexNetEncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained ResNet-50 model\n",
    "        #resnet50 = models.resnet50(pretrained=True)\n",
    "        #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "        # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "        # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "        # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "        self.encoder = nn.Sequential(*list(alexnet.children())[:-2])\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        \n",
    "        # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the decoder part of the model (fully connected layers)\n",
    "        # The input size depends on the output size of the ResNet encoder\n",
    "        # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6), # tried 0.3 - didnt work\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        # Pass the input through the encoder (ResNet-50)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Global average pooling to decrease output size\n",
    "        x = self.GAP(x)\n",
    "\n",
    "        # Flatten the output from the encoder\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass the flattened tensor through the decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DeepPoseModel()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data_PyTorch(img_arr, kp_arr, batch_size, train_flag=True):\n",
    "#     '''\n",
    "#     conLoad data into PT dataLoader in specified batch size\n",
    "    \n",
    "#     Params\n",
    "#     img_arr: images loaded into an array (i,255,255,3) and are converted to (i,3,255,255)\n",
    "#     kp_arr: array of keypoints (i, num_kp*2)\n",
    "#     batch_size: batch size \n",
    "\n",
    "#     Return:\n",
    "#     PT_Dataset: containing input (x) and groundtruth (y)\n",
    "#     PT_DataLoader: Dataloader containing dataset and batch size\n",
    "\n",
    "#     '''\n",
    "\n",
    "#     # create tensors from arrays and load them to the GPU\n",
    "#     img_tensor = torch.tensor(img_arr, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "#     kp_tensor = torch.tensor(kp_arr, dtype=torch.float32)#.to('cuda')\n",
    "\n",
    "#     # Create a TensorDataset and DataLoader for training data\n",
    "#     dataset = TensorDataset(img_tensor, kp_tensor)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train_flag)\n",
    "\n",
    "#     return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet-50 original\n",
    "# # ResNet-50\n",
    "# class ResNet50EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # Fully connected layer 1\n",
    "#             nn.Linear(2048 * 7 * 7, 64),  # Input size: 2048*7*7, Output size: 4096\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # Fully connected layer 2\n",
    "#             nn.Linear(64, 64),  # Input size: 4096, Output size: 4096\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # Fully connected layer 3 (final output layer)\n",
    "#             nn.Linear(64, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-50 (Encoder test 5 - pool, flattern, FC)\n",
    "# class ResNet50EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the ONLY the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet50.children())[:-1])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Linear(2048, nkeypoints * 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-50 (Encoder test 6 - original decoder FC-4096 with GAP) - dropout changed to 0.3 - dropout changed to 0.85\n",
    "# class ResNet50EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "#         # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "#         self.encoder = nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(2048, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.85),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "\n",
    "#             # # Fully connected layer 1\n",
    "#             # nn.Linear(2048 * 7 * 7, 64),  # Input size: 2048*7*7, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 2\n",
    "#             # nn.Linear(64, 64),  # Input size: 4096, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 3 (final output layer)\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-50 (Encoder test 7 - original decoder FC-4096 with GAP and frozen layers - first 3 blocks frozen, last 2 are not=[-3] ) - prevent overfitting?\n",
    "# # !!!!! I have also adjusted the adam optimiser so check there in the train loop !!!!\n",
    "# class ResNet50EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "#         # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "#         self.encoder = nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "#         # Freeze shallower layers (everything before layer3) - see GPT for layers of ResNet\n",
    "#         # for param in list(self.encoder.children())[:-3]:  # Freeze everything before `layer3` (Conv2d-79) - the final two blocks are unfrozen\n",
    "#         for param in list(self.encoder.children())[:-2]:  # Freeze everything before `layer4` (Conv2d-141) - the final block only unfrozen\n",
    "#             for sub_param in param.parameters():\n",
    "#                 sub_param.requires_grad = False\n",
    "\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(2048, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-50 (Encoder test 9 - original decoder FC-4096 with GAP) - tried with 0.85 dropout\n",
    "# class ResNet50EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "#         # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "#         self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(2048, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.85), # tried 0.3 - didnt work\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "\n",
    "#             # # Fully connected layer 1\n",
    "#             # nn.Linear(2048 * 7 * 7, 64),  # Input size: 2048*7*7, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 2\n",
    "#             # nn.Linear(64, 64),  # Input size: 4096, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 3 (final output layer)\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Global average pooling to decrease output size\n",
    "#         x = self.GAP(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Learning rate test - ResNet50 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 16)\n",
    "# class ResNet50EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "#         # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "#         self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(2048, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6), # tried 0.3 - didnt work\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Global average pooling to decrease output size\n",
    "#         x = self.GAP(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - ResNet50 Encoder test Final 2 - Using best loss and trained from scratch - GAP, flat, 4096, DropOut 0.6, batch 16\n",
    "class ResNet50EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained ResNet-50 model\n",
    "        #resnet50 = models.resnet50(pretrained=True)\n",
    "        #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        #resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        resnet50 = models.resnet50()#(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "        # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "        # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "        # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "        self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        \n",
    "        # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the decoder part of the model (fully connected layers)\n",
    "        # The input size depends on the output size of the ResNet encoder\n",
    "        # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6), # tried 0.3 - didnt work\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        # Pass the input through the encoder (ResNet-50)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Global average pooling to decrease output size\n",
    "        x = self.GAP(x)\n",
    "\n",
    "        # Flatten the output from the encoder\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass the flattened tensor through the decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-34\n",
    "# class ResNet34EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet34EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet34 = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "#         # trying to use the same structure but \n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # Fully connected layer 1\n",
    "#             #nn.Linear(512 * 7 * 7, 4096),  # Input size: 512*7*7, Output size: 4096\n",
    "#             # DEBUG try changing fully connected to use 64 units like RepNet\n",
    "#             nn.Linear(512 * 7 * 7, 64),  # Input size: 512*7*7, Output size: 64\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "#             #DEBUG try changing dropout\n",
    "#             #nn.Dropout(0.3),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # Fully connected layer 2\n",
    "#             #nn.Linear(4096, 4096),  # Input size: 4096, Output size: 4096\n",
    "#             # DEBUG try changing fully connected to use 64 units like RepNet\n",
    "#             nn.Linear(64, 64),  # Input size: 64, Output size: 64\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "#             #DEBUG try changing dropout\n",
    "#             #nn.Dropout(0.3),  # Dropout to reduce overfitting - 0.3 works better with resnet\n",
    "\n",
    "#             # Fully connected layer 3 (final output layer)\n",
    "#             #nn.Linear(4096, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#             # DEBUG try changing fully connected to use 64 units like RepNet\n",
    "#             nn.Linear(64, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-34 (encoder test 5)\n",
    "# class ResNet34EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet34EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet34 = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "#         # adjusting to only use the current output\n",
    "#         self.encoder = nn.Sequential(*list(resnet34.children())[:-1])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-34 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 512, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(512, nkeypoints*2),\n",
    "#             # # Fully connected layer 1\n",
    "#             # nn.Linear(512 * 7 * 7, 64),  # Input size: 512*7*7, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 2\n",
    "#             # nn.Linear(64, 64),  # Input size: 4096, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 3 (final output layer)\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-34 (Encoder test 9 - original decoder FC-4096 with own GAP, 0.3 dropout)\n",
    "# class ResNet34EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet34EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet34 = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "#         # adjusting to only use the current output\n",
    "#         self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-34 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 512, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(512, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "    \n",
    "#         # Global average pooling to decrease output size\n",
    "#         x = self.GAP(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - ResNet34 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class ResNet34EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(ResNet34EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained ResNet-50 model\n",
    "        #resnet50 = models.resnet50(pretrained=True)\n",
    "        #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        resnet34 = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "        # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "        # self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "        # adjusting to only use the current output\n",
    "        self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "        \n",
    "        # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Define the decoder part of the model (fully connected layers)\n",
    "        # The input size depends on the output size of the ResNet encoder\n",
    "        # ResNet-34 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 512, 7, 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        # Pass the input through the encoder (ResNet-50)\n",
    "        x = self.encoder(x)\n",
    "    \n",
    "        # Global average pooling to decrease output size\n",
    "        x = self.GAP(x)\n",
    "\n",
    "        # Flatten the output from the encoder\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass the flattened tensor through the decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-18 (Encoder test 8 - original 4096 decoder with pooling layer, smaller encoder - tried with and without dropout)\n",
    "# class ResNet18EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet18EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(512, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.6),\n",
    "#             # nn.Dropout(0.25),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-18 (Encoder test 2, 3, 4)\n",
    "# class ResNet18EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet18EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # Fully connected layer 1\n",
    "#             nn.Linear(512 * 7 * 7, 64),  # Input size: 512*7*7, Output size: 4096\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # Fully connected layer 2\n",
    "#             nn.Linear(64, 64),  # Input size: 4096, Output size: 4096\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # Fully connected layer 3 (final output layer)\n",
    "#             nn.Linear(64, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-18 (Encoder test 9 - original 4096 decoder with OWN GAP layer, smaller encoder)\n",
    "# class ResNet18EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet18EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(512, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Global average pooling to decrease output size\n",
    "#         x = self.GAP(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-18 (Encoder test 11 - original 4096 decoder with OWN GAP layer, smaller encoder, no dropout, train from scratch)\n",
    "# class ResNet18EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet18EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet18 = models.resnet18() # default is none weights\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(512, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Global average pooling to decrease output size\n",
    "#         x = self.GAP(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - ResNet18 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class ResNet18EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(ResNet18EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained ResNet-50 model\n",
    "        #resnet50 = models.resnet50(pretrained=True)\n",
    "        #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT) # default is none weights\n",
    "        \n",
    "\n",
    "        # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "        self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the decoder part of the model (fully connected layers)\n",
    "        # The input size depends on the output size of the ResNet encoder\n",
    "        # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        # Pass the input through the encoder (ResNet-50)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Global average pooling to decrease output size\n",
    "        x = self.GAP(x)\n",
    "\n",
    "        # Flatten the output from the encoder\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass the flattened tensor through the decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientNetB0EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB0EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B0 model\n",
    "#         efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(1280 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientNetB0EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB0EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B0 model\n",
    "#         efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # nn.Linear(1280 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, 64),\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "#             nn.Linear(1280, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - EfficientNetB0 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class EfficientNetB0EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(EfficientNetB0EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained EfficientNet-B0 model\n",
    "        efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "\n",
    "        # Remove the classification head (i.e., avgpool and fc layers)\n",
    "        # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "        self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            nn.Linear(1280, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Learning rate test - EfficientNetB2 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "# class EfficientNetB2EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB2EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B0 model\n",
    "#         efficientnet = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT) #default is imgnet1k v1\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "\n",
    "#             nn.Linear(1408, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 \n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - EfficientNetB2 Encoder test Final 2 - Using best loss and trained from scratch - GAP, flat, 4096, DropOut 0.6, batch 16\n",
    "class EfficientNetB2EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained EfficientNet-B2 encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(EfficientNetB2EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained EfficientNet-B0 model\n",
    "        #efficientnet = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT) #default is imgnet1k v1\n",
    "        efficientnet = models.efficientnet_b2()#(weights=models.EfficientNet_B2_Weights.DEFAULT) #default is imgnet1k v1\n",
    "\n",
    "        # Remove the classification head (i.e., avgpool and fc layers)\n",
    "        # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "        self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            nn.Linear(1408, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 \n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientNetB4EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB4EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B0 model\n",
    "#         efficientnet = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT) #default is imgnet1k v1\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # nn.Linear(1792 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, 64),\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "#             nn.Linear(1792, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientNetB4EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB4EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B0 model\n",
    "#         efficientnet = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT) #default is imgnet1k v1\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # nn.Linear(1792 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, 64),\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "#             nn.Linear(1792, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - EfficientNetB4 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class EfficientNetB4EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(EfficientNetB4EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained EfficientNet-B0 model\n",
    "        efficientnet = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n",
    "\n",
    "        # Remove the classification head (i.e., avgpool and fc layers)\n",
    "        # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "        self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            nn.Linear(1792, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientNetB7EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB7EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B0 model\n",
    "#         efficientnet = models.efficientnet_b7(weights=models.EfficientNet_B7_Weights.DEFAULT) #default is imgnet1k v1\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(2560 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientNetB7EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB7EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B0 model\n",
    "#         efficientnet = models.efficientnet_b7(weights=models.EfficientNet_B7_Weights.DEFAULT) #default is imgnet1k v1\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # nn.Linear(2560 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, 64),\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "#             nn.Linear(2560, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MobileNetV2EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV2EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#         #     nn.Linear(1280 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(0.3),\n",
    "\n",
    "#         #     nn.Linear(64, 64),\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(0.3),\n",
    "\n",
    "#         #     nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "\n",
    "#             nn.Linear(1280, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MobileNetV2EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV2EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#         #     nn.Linear(1280 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(0.3),\n",
    "\n",
    "#         #     nn.Linear(64, 64),\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(0.3),\n",
    "\n",
    "#         #     nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "\n",
    "#             nn.Linear(1280, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.3),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MobileNetV2EncoderPoseModel (encoder test 11) - train from scratch \n",
    "# class MobileNetV2EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV2EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v2() # removed weights=models.MobileNet_V2_Weights.IMAGENET1K_V1 as default is from scratch\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#         #     nn.Linear(1280 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(0.3),\n",
    "\n",
    "#         #     nn.Linear(64, 64),\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(0.3),\n",
    "\n",
    "#         #     nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "\n",
    "#             nn.Linear(1280, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.3),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Learning rate test - MobileNetV2 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "# class MobileNetV2EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV2EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT) # removed weights=models.MobileNet_V2_Weights.IMAGENET1K_V2 as default is from scratch\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#         #     nn.Linear(1280 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(0.3),\n",
    "\n",
    "#         #     nn.Linear(64, 64),\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(0.3),\n",
    "\n",
    "#         #     nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "\n",
    "#             nn.Linear(1280, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - MobileNetV2 (Encoder test Final 2 - Using best loss and trained from scratch - GAP, flat, 4096, DropOut 0.6, batch 16)\n",
    "class MobileNetV2EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(MobileNetV2EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained MobileNetV2 model\n",
    "        #mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT) # removed weights=models.MobileNet_V2_Weights.IMAGENET1K_V2 as default is from scratch\n",
    "        mobilenet = models.mobilenet_v2()\n",
    "\n",
    "        # Remove the classification head (classifier block)\n",
    "        # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "        self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "        #     nn.Linear(1280 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(0.3),\n",
    "\n",
    "        #     nn.Linear(64, 64),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(0.3),\n",
    "\n",
    "        #     nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "\n",
    "            nn.Linear(1280, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2EncoderPoseModel()\n",
    "dummy_input = torch.randn(1, 3, 220, 220)  # Simulating input\n",
    "output = model(dummy_input)  # Forward pass through EfficientNet-B0\n",
    "print(output.shape)  # Check actual encoder output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MobileNetV3LargeEncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV3LargeEncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)  #IMAGENET1K_V2\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "        \n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(960 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MobileNetV3LargeEncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV3LargeEncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)  #IMAGENET1K_V2\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "    \n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # nn.Linear(960 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, 64),\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "#             nn.Linear(960, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.3),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - MobileNetV3Large (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class MobileNetV3LargeEncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained MobileNetV3-Large encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(MobileNetV3LargeEncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained MobileNetV3-Large model\n",
    "        mobilenet = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
    "\n",
    "        # Remove the classification head (classifier block)\n",
    "        # MobileNetV3-Large's feature extractor ends with Conv2d outputting 1280 channels\n",
    "        self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(960, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV3LargeEncoderPoseModel()\n",
    "dummy_input = torch.randn(1, 3, 220, 220)  # Simulating input\n",
    "output = model(dummy_input)  # Forward pass through EfficientNet-B0\n",
    "print(output.shape)  # Check actual encoder output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MobileNetV3SmallEncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV3SmallEncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)  #IMAGENET1K_V1\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(576 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(64, 64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MobileNetV3SmallEncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV3SmallEncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)  #IMAGENET1K_V1\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # nn.Linear(576 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, 64),\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#             nn.Linear(576, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.3),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - MobileNetV3Large (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class MobileNetV3SmallEncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained MobileNetV3-Large encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(MobileNetV3SmallEncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained MobileNetV3-Large model\n",
    "        mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "        # Remove the classification head (classifier block)\n",
    "        # MobileNetV3-Large's feature extractor ends with Conv2d outputting 1280 channels\n",
    "        self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(576, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV3SmallEncoderPoseModel()\n",
    "dummy_input = torch.randn(1, 3, 220, 220)  # Simulating input\n",
    "output = model(dummy_input)  # Forward pass through EfficientNet-B0\n",
    "print(output.shape)  # Check actual encoder output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timestamped_dir(descriptor, base_dir='/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/'):\n",
    "    \"\"\"\n",
    "    Creates a directory with a timestamp appended to the base directory name.\n",
    "    Returns the path to the created directory.\n",
    "    \n",
    "    Parameters:\n",
    "    descriptor: string describing the run generally model_dataDescriptor\n",
    "    base_dir (str): The base directory name. Default is './training_results'.\n",
    "    \n",
    "    Returns:\n",
    "    str: The path to the created directory.\n",
    "    \"\"\"\n",
    "    # Get the current datetime and format it as YYYY-MM-DD_HH-MM-SS\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    base_dir_descriptor = f\"{base_dir}{descriptor}\"\n",
    "    \n",
    "    # Create the final directory name with the timestamp\n",
    "    final_dir = f\"{base_dir_descriptor}_{timestamp}\"\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    \n",
    "    return final_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_data, val_data, save_dir, data_descriptor='Loss', show_plot=False):\n",
    "    # Plot the loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_data, label=f'Training {data_descriptor}')\n",
    "    plt.plot(val_data, label=f'Validation {data_descriptor}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(f'{data_descriptor}')\n",
    "    plt.title(f'Training and Validation {data_descriptor} Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, f'{data_descriptor}_plot.png')\n",
    "    plt.savefig(plot_path)\n",
    "    #print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "    # Optionally, display the plot\n",
    "    if show_plot == True:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_stats_and_models(model, epoch, val_loss, val_pck, save_dir, \n",
    "                     best_val_loss=None, best_val_pck=None, \n",
    "                     final_model=False, train_loss_list=None, val_loss_list=None, train_pck_list=None, val_pck_list=None):\n",
    "    \"\"\"\n",
    "    Saves the best models based on validation loss, PCK value, and final model.\n",
    "    Saves the train and val curves and results for training\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to be saved.\n",
    "    - epoch (int): The current epoch number.\n",
    "    - val_loss (float): The current validation loss.\n",
    "    - val_pck (float): The current validation PCK value.\n",
    "    - save_dir (str): The directory where the models will be saved.\n",
    "    - best_val_loss (float): The best validation loss seen so far.\n",
    "    - best_val_pck (float): The best validation PCK value seen so far.\n",
    "    - final_model (bool): If True, saves the final model after all epochs.\n",
    "    - train_loss_list (list): List of all the loss values from each epoch\n",
    "    \n",
    "    Returns:\n",
    "    - best_val_loss (float): Updated best validation loss.\n",
    "    - best_val_pck (float): Updated best validation PCK value.\n",
    "    - model_save_path_best_val_loss\n",
    "    - model_save_path_best_val_pck\n",
    "    - final_model_path\n",
    "    \"\"\"\n",
    "    model_save_path_best_val_loss = None\n",
    "    model_save_path_best_val_pck = None\n",
    "    \n",
    "    # Check if the current model has the lowest validation loss\n",
    "    if best_val_loss is None or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_name = f'best_val_loss_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth'\n",
    "        model_save_path_best_val_loss = os.path.join(save_dir, model_name)\n",
    "        torch.save(model.state_dict(), model_save_path_best_val_loss)\n",
    "        print(f'New best model saved with lowest validation loss to {model_save_path_best_val_loss}')\n",
    "    \n",
    "    # Check if the current model has the highest validation PCK\n",
    "    if best_val_pck is None or val_pck > best_val_pck:\n",
    "        best_val_pck = val_pck\n",
    "        model_save_path_best_val_pck = os.path.join(save_dir, f'best_val_pck_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth')\n",
    "        torch.save(model.state_dict(), model_save_path_best_val_pck)\n",
    "        print(f'New best model saved with highest validation PCK to {model_save_path_best_val_pck}')\n",
    "    \n",
    "    # Save the final model and perform final stats evaluation and save\n",
    "    if final_model:\n",
    "        final_model_path = os.path.join(save_dir, f'final_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth')\n",
    "        torch.save(model.state_dict(), final_model_path)\n",
    "        print(f'Final model saved to {final_model_path}')\n",
    "        plot_training_curves(train_loss_list, val_loss_list, save_dir, 'Loss', show_plot=True)\n",
    "        plot_training_curves(train_pck_list, val_pck_list, save_dir, data_descriptor='PCK@0.1', show_plot=True)\n",
    "        return best_val_loss, best_val_pck, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path\n",
    "    \n",
    "    return best_val_loss, best_val_pck, model_save_path_best_val_loss, model_save_path_best_val_pck, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(aug, train_imgs_array, train_kp_array): \n",
    "    #\n",
    "    print('augmenting data...')\n",
    "    \n",
    "    # unnorm kp\n",
    "    train_kp_array_abs = unnorm_keypoints_arr(train_kp_array, train_imgs_array)\n",
    "\n",
    "    # specify augmentations\n",
    "    # lrflip\n",
    "    seq_lrflip = iaa.Sequential([\n",
    "        iaa.Fliplr(1.0)\n",
    "    ])\n",
    "    # rotate clock\n",
    "    seq_rotate_clock = iaa.Sequential([\n",
    "        iaa.Affine(rotate=(5, 20)),\n",
    "    ])\n",
    "    #rotate anticlock\n",
    "    seq_rotate_anticlock = iaa.Sequential([\n",
    "        iaa.Affine(rotate=(-20, -5)),\n",
    "    ])\n",
    "\n",
    "    # apply augmentation\n",
    "    #lrflip\n",
    "    train_imgs_array_aug_lrflip, train_kp_array_aug_lrflip_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_lrflip)\n",
    "    # rotate clock\n",
    "    train_imgs_array_aug_rclock, train_kp_array_aug_rclock_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_rotate_clock)\n",
    "    #rotate anticlock\n",
    "    train_imgs_array_aug_ranticlock, train_kp_array_aug_ranticlock_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_rotate_anticlock)\n",
    "    #translat\n",
    "    train_imgs_array_aug_trans, train_kp_array_aug_trans = apply_aug_translate(train_imgs_array, train_kp_array_abs)\n",
    "\n",
    "    # norm the aug kp\n",
    "    #lrflip\n",
    "    train_kp_array_aug_lrflip_norm = norm_keypoints_arr(train_kp_array_aug_lrflip_abs, train_imgs_array_aug_lrflip)  \n",
    "    # rotate clock\n",
    "    train_kp_array_aug_rclock_norm = norm_keypoints_arr(train_kp_array_aug_rclock_abs, train_imgs_array_aug_rclock)\n",
    "    #rotate anticlock\n",
    "    train_kp_array_aug_ranticlock_norm = norm_keypoints_arr(train_kp_array_aug_ranticlock_abs, train_imgs_array_aug_ranticlock)\n",
    "    #translat\n",
    "    train_kp_array_aug_trans_norm = norm_keypoints_arr(train_kp_array_aug_trans, train_imgs_array_aug_trans)\n",
    "\n",
    "    # combine augmented arrays to original array\n",
    "    #save to image array\n",
    "    train_imgs_array_aug = np.concatenate((train_imgs_array, train_imgs_array_aug_lrflip), axis=0)\n",
    "    train_imgs_array_aug = np.concatenate((train_imgs_array_aug, train_imgs_array_aug_rclock), axis=0)\n",
    "    train_imgs_array_aug = np.concatenate((train_imgs_array_aug, train_imgs_array_aug_ranticlock), axis=0)\n",
    "    train_imgs_array_aug = np.concatenate((train_imgs_array_aug, train_imgs_array_aug_trans), axis=0)\n",
    "    #save to kp array\n",
    "    train_kp_array_aug = np.concatenate((train_kp_array, train_kp_array_aug_lrflip_norm), axis=0)\n",
    "    train_kp_array_aug = np.concatenate((train_kp_array_aug, train_kp_array_aug_rclock_norm), axis=0)\n",
    "    train_kp_array_aug = np.concatenate((train_kp_array_aug, train_kp_array_aug_ranticlock_norm), axis=0)\n",
    "    train_kp_array_aug = np.concatenate((train_kp_array_aug, train_kp_array_aug_trans_norm), axis=0)\n",
    "\n",
    "    if aug == 2:\n",
    "        #put additional augmentations here and then concat the arrays\n",
    "        pass\n",
    "\n",
    "    return train_imgs_array_aug, train_kp_array_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset, augmentation, crop_extension):\n",
    "\n",
    "    print('laoding data ...')\n",
    "\n",
    "    DATA_PARENT_PATH = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/'\n",
    "\n",
    "    if dataset == 1: # Simple dataset \n",
    "\n",
    "        # variables\n",
    "        dataset_name = 'PE_Simple'\n",
    "        crop_extension = '_crop_220x220.jpg'# cropsize extension\n",
    "\n",
    "        # loading ids to a list\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_test_bbox.txt'\n",
    "        ids_test_bbox = load_file_to_list(path)\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_val_bbox.txt'\n",
    "        ids_val_bbox=load_file_to_list(path)\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_train_bbox.txt'\n",
    "        ids_train_bbox=load_file_to_list(path)\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_test.txt'\n",
    "        ids_test=load_file_to_list(path)\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_val.txt'\n",
    "        ids_val=load_file_to_list(path)\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_train.txt'\n",
    "        ids_train=load_file_to_list(path)\n",
    "\n",
    "        # load image data to array \n",
    "        img_dir = '/images'\n",
    "        path = DATA_PARENT_PATH + dataset_name + img_dir + '/test'\n",
    "        test_imgs_array = load_image_data(ids_test_bbox, path, crop_extension)\n",
    "        path = DATA_PARENT_PATH + dataset_name + img_dir + '/val'\n",
    "        val_imgs_array = load_image_data(ids_val_bbox, path, crop_extension)\n",
    "        path = DATA_PARENT_PATH + dataset_name + img_dir + '/train'\n",
    "        train_imgs_array = load_image_data(ids_train_bbox, path, crop_extension)\n",
    "\n",
    "        # load annoation to df and set datatyoes\n",
    "        anno_dir = '/annotation'\n",
    "        path = DATA_PARENT_PATH + dataset_name + anno_dir + '/test_annotation_simple.json'\n",
    "        df_full_annotation_norm_test = json_to_df(path)\n",
    "        df_full_annotation_norm_test = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_test)\n",
    "        path = DATA_PARENT_PATH + dataset_name + anno_dir + '/val_annotation_simple.json'\n",
    "        df_full_annotation_norm_val = json_to_df(path)\n",
    "        df_full_annotation_norm_val = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_val)\n",
    "        path = DATA_PARENT_PATH + dataset_name + anno_dir + '/train_annotation_simple.json'\n",
    "        df_full_annotation_norm_train = json_to_df(path)\n",
    "        df_full_annotation_norm_train = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_train)\n",
    "\n",
    "        # create lists with col names (potentially remove)\n",
    "        id_cols = df_full_annotation_norm_test.iloc[:, :3].columns.to_list()\n",
    "        bbox_cols = df_full_annotation_norm_test.iloc[:, 3:7].columns.to_list()\n",
    "        kp_cols = df_full_annotation_norm_test.iloc[:, 7:23].columns.to_list()\n",
    "\n",
    "        # load the kps to arrays\n",
    "        test_kp_array = create_data_lists(df_full_annotation_norm_test, kp_cols)\n",
    "        val_kp_array = create_data_lists(df_full_annotation_norm_val, kp_cols)\n",
    "        train_kp_array = create_data_lists(df_full_annotation_norm_train, kp_cols)\n",
    "\n",
    "        if augmentation > 1: \n",
    "            train_imgs_array, train_kp_array = augment_data(augmentation, train_imgs_array, train_kp_array)\n",
    "        \n",
    "        train_kp_array, _ = replace_out_of_img_kp(train_kp_array)\n",
    "\n",
    "        return train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model, dataset, aug, optimizer, loss, lr, batch_size, num_epochs, crop_extension):\n",
    "\n",
    "    print('training ...')\n",
    "\n",
    "    # load data\n",
    "    train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array = load_data(dataset, aug, crop_extension)\n",
    "\n",
    "    # define model\n",
    "    if model == 1:\n",
    "        model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "        descriptor = 'DeepPose'\n",
    "    if model == 2:\n",
    "        model = ResNet50EncoderPoseModel(nkeypoints=8).to('cuda')\n",
    "        descriptor = 'ResNet50EncoderPoseModel'\n",
    "        # DEBUG\n",
    "        # for param in model.encoder.parameters():\n",
    "        #     print(param.requires_grad)  # Should be True\n",
    "        # END DEBUG\n",
    "    if model == 3:\n",
    "        model = ResNet34EncoderPoseModel(nkeypoints=8).to('cuda')\n",
    "        descriptor = 'ResNet34EncoderPoseModel'\n",
    "        # DEBUG\n",
    "        # for param in model.encoder.parameters():\n",
    "        #     print(param.requires_grad)  # Should be True\n",
    "        # END DEBUG\n",
    "    if model == 4:\n",
    "        model = ResNet18EncoderPoseModel(nkeypoints=8).to('cuda')\n",
    "        descriptor = 'ResNet18EncoderPoseModel'\n",
    "    if model == 5:\n",
    "        model = EfficientNetB0EncoderPoseModel(nkeypoints=8).to('cuda')\n",
    "        descriptor = 'EfficientNetB0EncoderPoseModel'\n",
    "    if model == 6:\n",
    "        model = EfficientNetB4EncoderPoseModel(nkeypoints=8).to('cuda')\n",
    "        descriptor = 'EfficientNetB4EncoderPoseModel'\n",
    "    if model == 7:\n",
    "        model = EfficientNetB2EncoderPoseModel(nkeypoints=8).to('cuda')\n",
    "        descriptor = 'EfficientNetB2EncoderPoseModel'\n",
    "    if model == 8:\n",
    "        model = MobileNetV2EncoderPoseModel(nkeypoints=8).to('cuda')\n",
    "        descriptor = 'MobileNetV2EncoderPoseModel'\n",
    "    if model == 9:\n",
    "        model = MobileNetV3LargeEncoderPoseModel(nkeypoints=8).to('cuda')\n",
    "        descriptor = 'MobileNetV3LargeEncoderPoseModel'\n",
    "    if model == 10:\n",
    "        model = MobileNetV3SmallEncoderPoseModel(nkeypoints=8).to('cuda')\n",
    "        descriptor = 'MobileNetV3SmallEncoderPoseModel'\n",
    "    if model == 11:\n",
    "        model = AlexNetEncoderPoseModel(nkeypoints=8).to('cuda')\n",
    "        descriptor = 'AlexNetEncoderPoseModel'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if dataset == 1:\n",
    "        descriptor = descriptor + '_Simple'\n",
    "    \n",
    "    if aug == 1:\n",
    "        descriptor = descriptor + '_noAug'\n",
    "    if aug == 2:\n",
    "        descriptor = descriptor + '_simpleAug'\n",
    "    if aug == 3:\n",
    "        descriptor = descriptor + '_largeAug'\n",
    "\n",
    "    if optimizer == 1:\n",
    "        # Define your optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # Adding in a scheduler\n",
    "        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        descriptor = descriptor + '_Adam'\n",
    "    if optimizer == 2:\n",
    "        # this is adam for if fine tuning the encoder (ie there are some frozen layers)\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "        descriptor = descriptor + '_AdamFineTune'\n",
    "\n",
    "    # Initialise the learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                 milestones=[90, 120],  # Epochs where LR drops\n",
    "                                                 gamma=0.1)  # Factor by which LR is reduced\n",
    "\n",
    "\n",
    "    if loss == 1:\n",
    "        descriptor = descriptor + '_MSE'\n",
    "    if loss == 2:\n",
    "        descriptor = descriptor + '_MAE'\n",
    "    if loss == 3:\n",
    "        descriptor = descriptor + '_simpleAngles'\n",
    "    if loss == 4:\n",
    "        descriptor = descriptor + '_polarCoords'\n",
    "\n",
    "    \n",
    "    # get naming convention\n",
    "    descriptor = descriptor + '_' + str(lr) + '_' + str(batch_size) + '_' + str(num_epochs)\n",
    "    # create save dir\n",
    "    save_dir = create_timestamped_dir(descriptor)\n",
    "    \n",
    "    # create tensors from arrays and load to a PT dataloader\n",
    "    #train\n",
    "    train_imgs_tensor = torch.tensor(train_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "    train_kp_tensor = torch.tensor(train_kp_array, dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(train_imgs_tensor, train_kp_tensor)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # DEBUG\n",
    "    # print(train_kp_array[0:10])\n",
    "    # print(train_kp_tensor[0:10])\n",
    "    #val\n",
    "    val_imgs_tensor = torch.tensor(val_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "    val_kp_tensor = torch.tensor(val_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "    val_dataset = TensorDataset(val_imgs_tensor, val_kp_tensor)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "    #test\n",
    "    test_imgs_tensor = torch.tensor(test_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "    test_kp_tensor = torch.tensor(test_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "    test_dataset = TensorDataset(test_imgs_tensor, test_kp_tensor)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "        \n",
    "    # Lists to store the training and validation loss for each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_pck_list = []\n",
    "    val_pck_list = []\n",
    "    best_val_loss = None\n",
    "    best_val_pck = None\n",
    "    model_save_path_best_val_loss = None\n",
    "    model_save_path_best_val_pck = None\n",
    "\n",
    "    print('start training loop ...')\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        running_pck_01 = 0.0\n",
    "        running_pck_val_01 = 0.0\n",
    "\n",
    "        # loop for a single batch\n",
    "        for batch_images, batch_keypoints in train_dataloader:\n",
    "            \n",
    "            # DEBUG \n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "            #             print(f\"Gradient for {name} has NaN or Inf!\")\n",
    "\n",
    "            # if torch.isnan(batch_images).any() or torch.isnan(batch_keypoints).any():\n",
    "            #         print(\"NaN found in input data!\")\n",
    "            # if torch.isinf(batch_images).any() or torch.isinf(batch_keypoints).any():\n",
    "            #         print(\"Inf found in input data!\")\n",
    "\n",
    "            # Move the data to the GPU\n",
    "            batch_images = batch_images.to('cuda')\n",
    "            batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_images)\n",
    "            # DEBUG \n",
    "            # print(outputs)\n",
    "            # # DEBUG - check for nan\n",
    "            # if torch.isnan(outputs).any():\n",
    "            #     print('going to nan')\n",
    "\n",
    "            # Compute the loss\n",
    "            if loss == 1:\n",
    "                batch_loss = masked_mse(batch_keypoints, outputs)\n",
    "            if loss == 2:\n",
    "                batch_loss = masked_mae(batch_keypoints, outputs)\n",
    "            if loss == 3:\n",
    "                batch_loss = masked_simpleAngles(batch_keypoints, outputs)\n",
    "                # DEBUG - check for nan\n",
    "                # if torch.isnan(batch_loss).any():\n",
    "                #     print('going to nan')\n",
    "            if loss == 4:\n",
    "                batch_loss = masked_polarCoords(batch_keypoints, outputs)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            batch_loss.backward()\n",
    "            # DEBUG\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(f\"Gradient for {name}: {param.grad.norm().item():.6f}\")\n",
    "            #     else:\n",
    "            #         print(f\"Gradient for {name}: None\")\n",
    "\n",
    "            # Check if gradients are being updated\n",
    "            # for name, param in model.encoder.named_parameters():\n",
    "            #     if param.requires_grad:  # Ensure the layer is trainable\n",
    "            #         if param.grad is None:\n",
    "            #             print(f\"⚠️ WARNING: {name} has no gradients!\")\n",
    "            #         elif torch.isnan(param.grad).any():\n",
    "            #             print(f\"⚠️ WARNING: {name} has NaN gradients!\")\n",
    "            #         else:\n",
    "            #             print(f\"✅ {name} gradient mean: {param.grad.abs().mean():.6f}\")\n",
    "\n",
    "            # performing gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # DEBUG\n",
    "            # print('clipped grads:')\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(f\"Gradient for {name}: {param.grad.norm().item():.6f}\")\n",
    "            #     else:\n",
    "            #         print(f\"Gradient for {name}: None\")\n",
    "\n",
    "            # DEBUG\n",
    "            # for param_group in optimizer.param_groups:\n",
    "            #     print(\"Current learning rate:\", param_group['lr'])\n",
    "\n",
    "            # Check if gradients are being updated\n",
    "            # if epoch % 5 == 0:\n",
    "            #     for name, param in model.encoder.named_parameters():\n",
    "            #         if param.requires_grad:  # Ensure the layer is trainable\n",
    "            #             if param.grad is None:\n",
    "            #                 print(f\"⚠️ WARNING: {name} has no gradients!\")\n",
    "            #             elif torch.isnan(param.grad).any():\n",
    "            #                 print(f\"⚠️ WARNING: {name} has NaN gradients!\")\n",
    "            #             else:\n",
    "            #                 print(f\"✅ {name} gradient mean: {param.grad.abs().mean():.6f}\")\n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss\n",
    "            running_train_loss += batch_loss.item()\n",
    "\n",
    "            # compute metrics\n",
    "            pck_01 = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "\n",
    "            # accumulate metrics\n",
    "            running_pck_01 += pck_01.item()\n",
    "\n",
    "        # calculate average loss for epoch\n",
    "        avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "        # calculate average pck for epoch\n",
    "        avg_pck_01 = running_pck_01 / len(train_dataloader)\n",
    "    \n",
    "        # populate train losses list for evaluation\n",
    "        train_losses.append(avg_train_loss)\n",
    "        # populate train pck list for evaluation\n",
    "        train_pck_list.append(avg_pck_01)\n",
    "\n",
    "        # evalution for training phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad(): # dont update weights\n",
    "\n",
    "            # evaluation loop for a single batch\n",
    "            for batch_images, batch_keypoints in val_dataloader:\n",
    "\n",
    "                # Move the data to the GPU\n",
    "                batch_images = batch_images.to('cuda')\n",
    "                batch_keypoints = batch_keypoints.to('cuda')\n",
    "                \n",
    "                # forward pass\n",
    "                outputs = model(batch_images)\n",
    "                # Compute the loss\n",
    "                if loss == 1:\n",
    "                    batch_loss = masked_mse(batch_keypoints, outputs)\n",
    "                if loss == 2:\n",
    "                    batch_loss = masked_mae(batch_keypoints, outputs)\n",
    "                if loss == 3:\n",
    "                    batch_loss = masked_simpleAngles(batch_keypoints, outputs)\n",
    "                if loss == 4:\n",
    "                    batch_loss = masked_polarCoords(batch_keypoints, outputs)\n",
    "\n",
    "                # Accumulate the loss\n",
    "                running_val_loss += batch_loss.item()\n",
    "\n",
    "                # compute metrics\n",
    "                pck_01_val = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "\n",
    "                # accumulate metrics\n",
    "                running_pck_val_01 += pck_01_val.item()\n",
    "\n",
    "        # calculate average loss for epoch\n",
    "        avg_val_loss = running_val_loss / len(val_dataloader)\n",
    "        # calculate average pck for epoch\n",
    "        avg_val_pck_01 = running_pck_val_01 / len(val_dataloader)\n",
    "\n",
    "        # populate train losses list for evaluation\n",
    "        val_losses.append(avg_val_loss)\n",
    "        # populate train pck list for evaluation\n",
    "        val_pck_list.append(avg_val_pck_01)\n",
    "\n",
    "        # Update the learning rate using the scheduler\n",
    "        scheduler.step()  #  Calls MultiStepLR to adjust the learning rate\n",
    "\n",
    "\n",
    "        # save best performing models based on the PCK and loss as well as the stats\n",
    "        best_val_loss, best_val_pck, model_save_path_best_val_loss_temp, model_save_path_best_val_pck_temp, _ = save_stats_and_models(\n",
    "        model, epoch + 1, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "        best_val_loss, best_val_pck)\n",
    "\n",
    "        if model_save_path_best_val_loss_temp:\n",
    "            model_save_path_best_val_loss = model_save_path_best_val_loss_temp\n",
    "        \n",
    "        if model_save_path_best_val_pck_temp:\n",
    "            model_save_path_best_val_pck = model_save_path_best_val_pck_temp\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train PCK0.1: {avg_pck_01:.4f}, Val PCK0.1: {avg_val_pck_01:.4f}')\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            plot_training_curves(train_losses, val_losses, save_dir, 'Loss', show_plot=True)\n",
    "            plot_training_curves(train_pck_list, val_pck_list, save_dir, data_descriptor='PCK@0.1', show_plot=True)\n",
    "        \n",
    "    best_val_loss, best_val_pck, _, _, final_model_path = save_stats_and_models(model, num_epochs, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "                    best_val_loss, best_val_pck, final_model=True, train_loss_list=train_losses, \n",
    "                    val_loss_list=val_losses, train_pck_list=train_pck_list, val_pck_list=val_pck_list)\n",
    "    \n",
    "    return save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader,\\\n",
    "        train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_eval(model_path, model_class, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a PyTorch model from a .pth file.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path (str): The path to the .pth model file.\n",
    "    - model_class (torch.nn.Module): The class of the model to instantiate.\n",
    "    - device (str): The device to load the model onto ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module): The loaded PyTorch model.\n",
    "    \"\"\"\n",
    "    # Instantiate the model class\n",
    "    model = model_class().to(device)\n",
    "    \n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pck(model, dataloader, threshold=0.2, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluates the average PCK over an entire dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model to evaluate.\n",
    "    - dataloader: A DataLoader providing the data to evaluate on.\n",
    "    - threshold: The PCK threshold distance (default is 0.2).\n",
    "    - device: The device to perform computations on (default is 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - average_pck: The average PCK over the entire dataset.\n",
    "    \"\"\"\n",
    "    total_pck = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_images, batch_keypoints in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_keypoints = batch_keypoints.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(batch_images)\n",
    "\n",
    "            # Compute PCK for the current batch\n",
    "            pck = pck_metric(batch_keypoints, outputs, threshold)\n",
    "            total_pck += pck.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    average_pck = total_pck / num_batches\n",
    "    \n",
    "    return average_pck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pck_per_keypoint(model, dataloader, num_keypoints=8, threshold=0.2, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluates the average PCK for each keypoint individually.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model to evaluate.\n",
    "    - dataloader: A DataLoader providing the data to evaluate on.\n",
    "    - num_keypoints: The number of keypoints in the dataset.\n",
    "    - threshold: The PCK threshold distance (default is 0.2).\n",
    "    - device: The device to perform computations on (default is 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - keypoint_pcks: A list of average PCK values for each keypoint.\n",
    "    \"\"\"\n",
    "    #model.eval()  # Set the model to evaluation mode\n",
    "    total_pck_per_keypoint = torch.zeros(num_keypoints, device=device)\n",
    "    total_visable_kp = torch.zeros(num_keypoints, device=device)\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_images, batch_keypoints in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_keypoints = batch_keypoints.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(batch_images)\n",
    "\n",
    "            # Create a mask for visible keypoints\n",
    "            mask = (batch_keypoints != -10.0).float().to(device)\n",
    "\n",
    "            # Compute the Euclidean distances for each keypoint\n",
    "            distances = torch.sqrt((outputs[:, ::2] - batch_keypoints[:, ::2]) ** 2 +\n",
    "                                   (outputs[:, 1::2] - batch_keypoints[:, 1::2]) ** 2)\n",
    "\n",
    "            # Normalize the distances\n",
    "            Norm_head_lowerbody = torch.sqrt((batch_keypoints[:, 0] - batch_keypoints[:,10]) ** 2 +\n",
    "                                             (batch_keypoints[:, 1] - batch_keypoints[:, 11]) ** 2)\n",
    "            normalized_distances = distances / Norm_head_lowerbody[:, None]\n",
    "\n",
    "            # Compute correct keypoints (distance <= threshold) for each keypoint\n",
    "            correct_keypoints_per_keypoint = (normalized_distances <= threshold).float() * mask[:, ::2]\n",
    "\n",
    "            # Accumulate PCK per keypoint\n",
    "            total_pck_per_keypoint += correct_keypoints_per_keypoint.sum(dim=0)\n",
    "            total_visable_kp += mask[:, ::2].sum(dim=0)\n",
    "            num_batches += 1\n",
    "\n",
    "    # Average PCK per keypoint\n",
    "    #keypoint_pcks = (total_pck_per_keypoint / mask[:, ::2].sum(dim=0)).cpu().numpy()\n",
    "    keypoint_pcks = (total_pck_per_keypoint / total_visable_kp).cpu().numpy()\n",
    "    return keypoint_pcks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pck_evaluation(model, val_dataloader, test_dataloader):\n",
    "\n",
    "    print('calculating PCK ...')\n",
    "\n",
    "    # create lists for pck at different thresholds\n",
    "    avg_pck_test_list = []\n",
    "    avg_pck_val_list = []\n",
    "    avg_pck_per_kp_val_list = []\n",
    "    avg_pck_per_kp_test_list = []\n",
    "    \n",
    "    # create a for loop to get PCK at 0.01 to 0.2\n",
    "    for i in range (1, 21):\n",
    "\n",
    "        # get pck threshold\n",
    "        pck_threshold = (i/100)\n",
    "\n",
    "        # calculate average pck\n",
    "        avg_pck_val = evaluate_pck(model, val_dataloader, threshold=pck_threshold)\n",
    "        avg_pck_test = evaluate_pck(model, test_dataloader, threshold=pck_threshold)\n",
    "\n",
    "        # calculate average pck per kp\n",
    "        avg_pck_per_kp_val = evaluate_pck_per_keypoint(model, val_dataloader, threshold=pck_threshold)\n",
    "        avg_pck_per_kp_test = evaluate_pck_per_keypoint(model, test_dataloader, threshold=pck_threshold)\n",
    "\n",
    "        if i == 5:\n",
    "            # capture pck@0.05\n",
    "            avg_pck_val_005 = avg_pck_val\n",
    "            avg_pck_test_005 = avg_pck_test\n",
    "\n",
    "        if i == 10:\n",
    "            # capture pck@0.1\n",
    "            avg_pck_val_01 = avg_pck_val\n",
    "            avg_pck_test_01 = avg_pck_test\n",
    "\n",
    "        if i == 20:\n",
    "            # capture pck@0.2\n",
    "            avg_pck_val_02 = avg_pck_val\n",
    "            avg_pck_test_02 = avg_pck_test\n",
    "\n",
    "        # save to lists\n",
    "        avg_pck_test_list.append(avg_pck_test)\n",
    "        avg_pck_val_list.append(avg_pck_val)\n",
    "        avg_pck_per_kp_val_list.append(avg_pck_per_kp_val)\n",
    "        avg_pck_per_kp_test_list.append(avg_pck_per_kp_test)\n",
    "\n",
    "    return avg_pck_val_list, avg_pck_test_list, avg_pck_per_kp_val_list, avg_pck_per_kp_test_list, \\\n",
    "        avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_inference_time(model, dummy_input):\n",
    "        \n",
    "    # Warm up GPU to avoid initial overheads\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "\n",
    "    # Synchronize GPU and measure the time\n",
    "    torch.cuda.synchronize()  # Ensure all previous CUDA operations are complete\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(dummy_input)\n",
    "    torch.cuda.synchronize()  # Wait for all CUDA operations to finish\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    return (end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_inference_time(model, dummy_input):\n",
    "\n",
    "    # move model and dummy data to cpu\n",
    "    model.to('cpu')\n",
    "    dummy_input = dummy_input.to('cpu')\n",
    "\n",
    "    # Warm up to avoid initial overheads affecting the time\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "\n",
    "    # Time the forward pass\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(dummy_input)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # move model and dummy iput back to gpu\n",
    "    model.to('cuda')\n",
    "    dummy_input.to('cuda')\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    return (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pck_to_dict(arr_list):\n",
    "        \n",
    "    # Define the keys for the dictionary\n",
    "    keys = ['head', 'beak', 'body_top', 'rflipper', 'lflipper', 'body_bottom', 'rfoot', 'lfoot']\n",
    "\n",
    "    # Initialize the dictionary with empty lists for each key\n",
    "    results_dict = {key: [] for key in keys}\n",
    "\n",
    "    # Populate the dictionary with values from the arrays\n",
    "    for array in arr_list:\n",
    "        for i, key in enumerate(keys):\n",
    "            results_dict[key].append(array[i])\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_to_dict(save_dir, avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02,\n",
    "                         total_params, total_flops, gpu_inf_time, cpu_inf_time, param_dict, flops_extend, avg_pck_test_list, \n",
    "                         avg_pck_per_kp_test_list, avg_pck_val_list, avg_pck_per_kp_val_list, num_train_imgs, num_val_imgs, num_test_imgs):\n",
    "\n",
    "    description = save_dir.split('/')[-1]\n",
    "    #print(description)\n",
    "\n",
    "    #avg_pck_test_dict = load_pck_to_dict(avg_pck_test_list)\n",
    "    avg_pck_per_kp_test_dict = load_pck_to_dict(avg_pck_per_kp_test_list)\n",
    "    #avg_pck_val_dict = load_pck_to_dict(avg_pck_val_list)\n",
    "    avg_pck_per_kp_val_dict = load_pck_to_dict(avg_pck_per_kp_val_list)\n",
    "\n",
    "    results_dict = {\n",
    "    'description': '',  # Placeholder for a string description\n",
    "    'pck005': None,  # Placeholder for PCK@0.05 variable\n",
    "    'pck01': None,  # Placeholder for PCK@0.1 variable\n",
    "    'pck02': None,  # Placeholder for PCK@0.2 variable\n",
    "    'total_params': None,  # Placeholder for total parameters variable\n",
    "    'GFLOPs': None,  # Placeholder for GFLOPs variable\n",
    "    'GPU_inf(ms)': None,  # Placeholder for GPU inference time variable\n",
    "    'CPU_inf(ms)': None,  # Placeholder for CPU inference time variable\n",
    "    'param_dict': {},  # Placeholder for parameter dictionary\n",
    "    'flops_dict': {},  # Placeholder for FLOPs dictionary\n",
    "    'PCK001-02': [],  # Placeholder for PCK@0.01-0.2 list\n",
    "    'PCK001-02_per_kp': {},  # Placeholder for PCK per joint dictionary\n",
    "    'val_PCK001-02': [],  # Placeholder for PCK@0.01-0.2 list\n",
    "    'val_PCK001-02_per_kp': {},  # Placeholder for PCK per joint dictionary\n",
    "    'val_pck005': None,  # Placeholder for PCK@0.05 variable\n",
    "    'val_pck01': None,  # Placeholder for PCK@0.1 variable\n",
    "    'val_pck02': None,  # Placeholder for PCK@0.2 variable\n",
    "    'num_train_imgs': None, # number of train imgs\n",
    "    'num_val_imgs': None, # number of train imgs\n",
    "    'num_test_imgs': None, # number of train imgs\n",
    "}\n",
    "    \n",
    "    results_dict['description'] = description\n",
    "    results_dict['pck005'] = avg_pck_test_005 \n",
    "    results_dict['pck01'] = avg_pck_test_01  \n",
    "    results_dict['pck02'] = avg_pck_test_02 \n",
    "    results_dict['total_params'] = total_params  \n",
    "    results_dict['GFLOPs'] = (total_flops/1e9)\n",
    "    results_dict['GPU_inf(ms)'] = gpu_inf_time*1000  \n",
    "    results_dict['CPU_inf(ms)'] = cpu_inf_time*1000  \n",
    "    results_dict['param_dict'] = param_dict  \n",
    "    results_dict['flops_dict'] = flops_extend \n",
    "    results_dict['PCK001-02'] = avg_pck_test_list\n",
    "    results_dict['PCK001-02_per_kp'] = avg_pck_per_kp_test_dict\n",
    "    results_dict['val_PCK001-02'] = avg_pck_val_list\n",
    "    results_dict['val_PCK001-02_per_kp'] = avg_pck_per_kp_val_dict\n",
    "    results_dict['val_pck005'] = avg_pck_val_005 \n",
    "    results_dict['val_pck01'] = avg_pck_val_01  \n",
    "    results_dict['val_pck02'] = avg_pck_val_02\n",
    "    results_dict['num_train_imgs'] = num_train_imgs\n",
    "    results_dict['num_val_imgs'] = num_val_imgs\n",
    "    results_dict['num_test_imgs'] = num_test_imgs\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numpy_types(obj):\n",
    "    \"\"\"\n",
    "    Convert numpy types in an object to their native Python equivalents.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()  # Convert numpy arrays to lists\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()  # Convert numpy scalars to native Python types\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(element) for element in obj]\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_json(data_dict, save_dir):\n",
    "    \"\"\"\n",
    "    Saves a dictionary to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict: The dictionary to save.\n",
    "    - save_dir: The directory path where the JSON file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Convert any numpy types in the dictionary to native Python types\n",
    "    data_dict = convert_numpy_types(data_dict)\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "    # json name\n",
    "    results_json = save_dir+'/results.json'\n",
    "    \n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(results_json, 'w') as json_file:\n",
    "        json.dump(data_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_comparison(img, pred_keypoints, true_keypoints, save_dir, img_num, nkeypoints=8, keypoint_labels=None, connections = [(0, 1), (0, 2), (2, 3), (2, 4), (2, 5), (5, 6), (5, 7)]):\n",
    "#     \"\"\"\n",
    "#     Plots predicted keypoints vs. ground truth keypoints on the same image.\n",
    "\n",
    "#     Parameters:\n",
    "#     - img: The image on which to plot the keypoints.\n",
    "#     - pred_keypoints: The predicted keypoints (flattened x, y coordinates).\n",
    "#     - true_keypoints: The ground truth keypoints (flattened x, y coordinates).\n",
    "#     - save_dir: Directory to save the result to\n",
    "#     - img_num: image number that is getting compared\n",
    "#     - nkeypoints:  Optional The number of keypoints (default=8).\n",
    "#     - keypoint_labels: Optional list of keypoint labels to display next to the keypoints.\n",
    "#     - connections: OPtional list of tupels defining the connections between kps\n",
    "#     \"\"\"\n",
    "\n",
    "#     fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "#     plt.imshow(img)\n",
    "    \n",
    "#     # Extract x and y coordinates for predicted keypoints\n",
    "#     pred_x_keypoints = pred_keypoints[::2]\n",
    "#     pred_y_keypoints = pred_keypoints[1::2]\n",
    "    \n",
    "#     # Extract x and y coordinates for ground truth keypoints\n",
    "#     true_x_keypoints = true_keypoints[::2]\n",
    "#     true_y_keypoints = true_keypoints[1::2]\n",
    "\n",
    "#     # Plot skeleton for true keypoints\n",
    "#     for (i, j) in connections:\n",
    "#         plt.plot([true_x_keypoints[i], true_x_keypoints[j]], \n",
    "#                  [true_y_keypoints[i], true_y_keypoints[j]], \n",
    "#                  'r-', linewidth=1)\n",
    "\n",
    "#     # Plot skeleton for predicted keypoints\n",
    "#     for (i, j) in connections:\n",
    "#         plt.plot([pred_x_keypoints[i], pred_x_keypoints[j]], \n",
    "#                  [pred_y_keypoints[i], pred_y_keypoints[j]], \n",
    "#                  'g-', linewidth=1)\n",
    "    \n",
    "#     # Plot predicted keypoints\n",
    "#     plt.scatter(pred_x_keypoints, pred_y_keypoints, marker='o', c='g', s=100, label='Predicted', edgecolor='black')\n",
    "    \n",
    "#     # Plot ground truth keypoints\n",
    "#     plt.scatter(true_x_keypoints, true_y_keypoints, marker='x', c='r', s=100, label='Ground Truth')\n",
    "    \n",
    "#     # If labels are provided, add them to the plot\n",
    "#     if keypoint_labels is not None:\n",
    "#         for i, (x, y) in enumerate(zip(true_x_keypoints, true_y_keypoints)):\n",
    "#             plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "#                      bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "#     # If labels are provided, add them to the plot\n",
    "#     if keypoint_labels is not None:\n",
    "#         for i, (x, y) in enumerate(zip(pred_x_keypoints, pred_y_keypoints)):\n",
    "#             plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "#                      bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "#     # Add a legend to differentiate between predicted and ground truth keypoints\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Save the plot\n",
    "#     plot_path = os.path.join(save_dir, f'Comparison of predicted and ground truth for img {img_num}.png')\n",
    "#     plt.savefig(plot_path)\n",
    "#     #print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_and_plot(model_path, start_img, end_img, model_class=DeepPoseModel, device='cuda'):\n",
    "#     \"\"\"\n",
    "#     Loads a model, predicts keypoints for a range of images, and plots the predicted keypoints \n",
    "#     versus ground truth keypoints on the same image. The images with plotted keypoints are then \n",
    "#     saved to a specified directory.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model_path: The file path to the saved model's .pth file.\n",
    "#     - start_img: The starting index of the images in the validation set to process.\n",
    "#     - end_img: The ending index of the images in the validation set to process (exclusive).\n",
    "#     - model_class: Optional. The class of the model architecture to instantiate and load \n",
    "#                    with the saved weights (default=DeepPoseModel).\n",
    "#     - device: Optional. The device to run the model on ('cuda' for GPU, 'cpu' for CPU; default='cuda').\n",
    "    \n",
    "#     Returns:\n",
    "#     - None. The function saves the images with plotted keypoints to the directory derived from the \n",
    "#             model path.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # get img lists\n",
    "#     img_arr = val_imgs_array[start_img:end_img,:,:,:]\n",
    "#     true_kp_arr = val_kp_array[start_img:end_img,:]\n",
    "\n",
    "#     # Load the model\n",
    "#     model = load_model(model_path, model_class, device=device)\n",
    "\n",
    "#     # Get predictions\n",
    "#     predictions = predict(model, img_arr, device=device)\n",
    "#     #print(predictions)\n",
    "\n",
    "#     # DeNorm predictions \n",
    "#     predictions_abs = []\n",
    "#     true_kp_arr_abs = []\n",
    "#     for i, kp in enumerate(predictions):\n",
    "\n",
    "#         img_size = img_arr[i].shape\n",
    "#         #print(img_size)\n",
    "\n",
    "#         #unNorm each prediction\n",
    "#         true_kp_abs, missing_kp = unnorm_keypoints(img_size, true_kp_arr[i])\n",
    "#         #print(missing_kp)\n",
    "#         kp_abs, missing_kp = unnorm_keypoints(img_size, kp, kp_to_null=missing_kp)\n",
    "#         #print(missing_kp)\n",
    "        \n",
    "\n",
    "#         # save result to new list\n",
    "#         predictions_abs.append(kp_abs)\n",
    "#         true_kp_arr_abs.append(true_kp_abs)\n",
    "\n",
    "#     #print(predictions_abs)\n",
    "\n",
    "#     # get the save directory parent (where the images will be saved)\n",
    "#     save_dir = model_path.rsplit('/',1)[0]\n",
    "\n",
    "#     # labels\n",
    "#     labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "\n",
    "#     for i, kp in enumerate(predictions_abs):\n",
    "\n",
    "#         plot_comparison(img_arr[i], predictions_abs[i], true_kp_arr_abs[i], save_dir, img_num=i+start_img)#, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(model, images, img_is_tensor=False, device='cuda'):\n",
    "#     \"\"\"\n",
    "#     Generates predictions from a PyTorch model given an array of images.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model (torch.nn.Module): The PyTorch model to use for predictions.\n",
    "#     - images (np.array): Array of images (e.g., shape: (num_images, 220, 220, 3)).\n",
    "#     - device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "#     Returns:\n",
    "#     - predictions (np.array): Array of predictions (e.g., keypoints for each image).\n",
    "#     \"\"\"\n",
    "#     # Convert images to PyTorch tensor and move to the specified device\n",
    "#     if not img_is_tensor:\n",
    "#         images_tensor = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "#     # Forward pass through the model to get predictions\n",
    "#     with torch.no_grad():\n",
    "#         predictions = model(images_tensor)\n",
    "    \n",
    "#     # Convert predictions back to a NumPy array and move to CPU if necessary\n",
    "#     predictions = predictions.cpu().numpy() if device == 'cuda' else predictions.numpy()\n",
    "    \n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, images, img_is_tensor=False, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generates predictions from a PyTorch model given an array of images.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to use for predictions.\n",
    "    - images (np.array): Array of images (e.g., shape: (num_images, 220, 220, 3)).\n",
    "    - device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - predictions (np.array): Array of predictions (e.g., keypoints for each image).\n",
    "    \"\"\"\n",
    "    # Convert images to PyTorch tensor and move to the specified device\n",
    "    if not img_is_tensor:\n",
    "        images_tensor = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "    # Forward pass through the model to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images_tensor)\n",
    "    \n",
    "    # Convert predictions back to a NumPy array and move to CPU if necessary\n",
    "    predictions = predictions.cpu().numpy() if device == 'cuda' else predictions.numpy()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(img, pred_keypoints, true_keypoints, save_dir, img_num, nkeypoints=8, keypoint_labels=None, connections = [(0, 1), (0, 2), (2, 3), (2, 4), (2, 5), (5, 6), (5, 7)]):\n",
    "    \"\"\"\n",
    "    Plots predicted keypoints vs. ground truth keypoints on the same image.\n",
    "\n",
    "    Parameters:\n",
    "    - img: The image on which to plot the keypoints.\n",
    "    - pred_keypoints: The predicted keypoints (flattened x, y coordinates).\n",
    "    - true_keypoints: The ground truth keypoints (flattened x, y coordinates).\n",
    "    - save_dir: Directory to save the result to\n",
    "    - img_num: image number that is getting compared\n",
    "    - nkeypoints:  Optional The number of keypoints (default=8).\n",
    "    - keypoint_labels: Optional list of keypoint labels to display next to the keypoints.\n",
    "    - connections: OPtional list of tupels defining the connections between kps\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Extract x and y coordinates for predicted keypoints\n",
    "    pred_x_keypoints = pred_keypoints[::2]\n",
    "    pred_y_keypoints = pred_keypoints[1::2]\n",
    "    \n",
    "    # Extract x and y coordinates for ground truth keypoints\n",
    "    true_x_keypoints = true_keypoints[::2]\n",
    "    true_y_keypoints = true_keypoints[1::2]\n",
    "\n",
    "    # Plot skeleton for true keypoints\n",
    "    for (i, j) in connections:\n",
    "        plt.plot([true_x_keypoints[i], true_x_keypoints[j]], \n",
    "                 [true_y_keypoints[i], true_y_keypoints[j]], \n",
    "                 'r-', linewidth=1)\n",
    "\n",
    "    # Plot skeleton for predicted keypoints\n",
    "    for (i, j) in connections:\n",
    "        plt.plot([pred_x_keypoints[i], pred_x_keypoints[j]], \n",
    "                 [pred_y_keypoints[i], pred_y_keypoints[j]], \n",
    "                 'g-', linewidth=1)\n",
    "    \n",
    "    # Plot predicted keypoints\n",
    "    plt.scatter(pred_x_keypoints, pred_y_keypoints, marker='o', c='g', s=100, label='Predicted', edgecolor='black')\n",
    "    \n",
    "    # Plot ground truth keypoints\n",
    "    plt.scatter(true_x_keypoints, true_y_keypoints, marker='x', c='r', s=100, label='Ground Truth')\n",
    "    \n",
    "    # If labels are provided, add them to the plot\n",
    "    if keypoint_labels is not None:\n",
    "        for i, (x, y) in enumerate(zip(true_x_keypoints, true_y_keypoints)):\n",
    "            plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "                     bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "    if keypoint_labels is not None:\n",
    "        for i, (x, y) in enumerate(zip(pred_x_keypoints, pred_y_keypoints)):\n",
    "            plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "                     bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # Add a legend to differentiate between predicted and ground truth keypoints\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the plot\n",
    "    part = save_dir.split('/')[-1]\n",
    "    plot_path = os.path.join(save_dir, f'{part}_vs_GT_img_{img_num}.png')\n",
    "    plt.savefig(plot_path)\n",
    "    #print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    # Close the figure to prevent it from displaying\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(model, img_arr, kp_arr, start_img, end_img, save_dir, keypoint_display=False, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a model, predicts keypoints for a range of images, and plots the predicted keypoints \n",
    "    versus ground truth keypoints on the same image. The images with plotted keypoints are then \n",
    "    saved to a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path: The file path to the saved model's .pth file.\n",
    "    - start_img: The starting index of the images in the validation set to process.\n",
    "    - end_img: The ending index of the images in the validation set to process (exclusive).\n",
    "    - model_class: Optional. The class of the model architecture to instantiate and load \n",
    "                   with the saved weights (default=DeepPoseModel).\n",
    "    - device: Optional. The device to run the model on ('cuda' for GPU, 'cpu' for CPU; default='cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - None. The function saves the images with plotted keypoints to the directory derived from the \n",
    "            model path.\n",
    "    \"\"\"\n",
    "\n",
    "    # get img lists\n",
    "    img_arr = img_arr[start_img:end_img,:,:,:]\n",
    "    true_kp_arr = kp_arr[start_img:end_img,:]\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = predict(model, img_arr, device=device)\n",
    "    #print(predictions)\n",
    "\n",
    "    # DeNorm predictions \n",
    "    predictions_abs = []\n",
    "    true_kp_arr_abs = []\n",
    "    for i, kp in enumerate(predictions):\n",
    "\n",
    "        img_size = img_arr[i].shape\n",
    "        #print(img_size)\n",
    "\n",
    "        #unNorm each prediction\n",
    "        true_kp_abs, missing_kp = unnorm_keypoints(img_size, true_kp_arr[i])\n",
    "        #print(missing_kp)\n",
    "        kp_abs, missing_kp = unnorm_keypoints(img_size, kp, kp_to_null=missing_kp)\n",
    "        #print(missing_kp)\n",
    "        \n",
    "\n",
    "        # save result to new list\n",
    "        predictions_abs.append(kp_abs)\n",
    "        true_kp_arr_abs.append(true_kp_abs)\n",
    "\n",
    "    # labels\n",
    "    labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'RFoot', 'LFoot']\n",
    "\n",
    "    for i, kp in enumerate(predictions_abs):\n",
    "\n",
    "        if keypoint_display:\n",
    "            plot_comparison(img_arr[i], predictions_abs[i], true_kp_arr_abs[i], save_dir, img_num=i+start_img, keypoint_labels=labels)\n",
    "        else:\n",
    "            plot_comparison(img_arr[i], predictions_abs[i], true_kp_arr_abs[i], save_dir, img_num=i+start_img)#, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not running directly off the back of training then set this to true\n",
    "# straight_eval = True\n",
    "\n",
    "def run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=None, batch_size=1, val_dataloader=None, \\\n",
    "             test_dataloader=None, train_imgs_array=None, val_imgs_array=None, test_imgs_array=None, val_kp_array=None, \\\n",
    "                test_kp_array=None, straight_eval=True):\n",
    "    if straight_eval:\n",
    "        # run just evalutation\n",
    "\n",
    "        # set crop extension and batch_size\n",
    "        crop_extension = '_crop_220x220.jpg'# cropsize extension\n",
    "        #batch_size = 1\n",
    "\n",
    "        # set model paths\n",
    "        #save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02'\n",
    "        #model_save_path_best_val_loss = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02/best_val_loss_model_epoch_22_PCK_0.6433_loss_0.0053.pth'\n",
    "        #model_save_path_best_val_pck = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02/best_val_loss_model_epoch_22_PCK_0.6433_loss_0.0053.pth'\n",
    "        #final_model_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02/final_model_epoch_30_PCK_0.5835_loss_0.0076.pth'\n",
    "\n",
    "        # load the data\n",
    "        train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array = load_data(1, 1, crop_extension)\n",
    "        #val\n",
    "        val_imgs_tensor = torch.tensor(val_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "        val_kp_tensor = torch.tensor(val_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "        val_dataset = TensorDataset(val_imgs_tensor, val_kp_tensor)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "        #test\n",
    "        test_imgs_tensor = torch.tensor(test_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "        test_kp_tensor = torch.tensor(test_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "        test_dataset = TensorDataset(test_imgs_tensor, test_kp_tensor)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "\n",
    "        #train.. DEBUG\n",
    "        train_imgs_tensor = torch.tensor(train_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "        train_kp_tensor = torch.tensor(train_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "        train_dataset = TensorDataset(train_imgs_tensor, train_kp_tensor)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "\n",
    "    # create evaluation dir\n",
    "    save_dir_eval = create_timestamped_dir('/evaluation', save_dir)\n",
    "\n",
    "    if model == 1:\n",
    "        model_class = DeepPoseModel\n",
    "        input_size = (1, 3, 220, 220)#(torch.randn((1,3,220,220)),)\n",
    "    if model == 2:\n",
    "        model_class = ResNet50EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "        input_size = (1, 3, 220, 220)\n",
    "        descriptor = 'ResNet50EncoderPose'\n",
    "    if model == 3:\n",
    "        model_class = ResNet34EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "        input_size = (1, 3, 220, 220)\n",
    "        descriptor = 'ResNet34EncoderPose'\n",
    "    if model == 4:\n",
    "        model_class = ResNet18EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "        input_size = (1, 3, 220, 220)\n",
    "        descriptor = 'ResNet18EncoderPose'\n",
    "    if model == 5:\n",
    "        model_class = EfficientNetB0EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "        input_size = (1, 3, 220, 220)\n",
    "        descriptor = 'EfficientNetB0EncoderPoseModel'\n",
    "    if model == 6:\n",
    "        model_class = EfficientNetB4EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "        input_size = (1, 3, 220, 220)\n",
    "        descriptor = 'EfficientNetB0EncoderPoseModel_HalfFC'\n",
    "    if model == 7:\n",
    "        model_class = EfficientNetB2EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "        input_size = (1, 3, 220, 220)\n",
    "        descriptor = 'EfficientNetB1EncoderPoseModel_HalfFC'\n",
    "    if model == 8:\n",
    "        model_class = MobileNetV2EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "        input_size = (1, 3, 220, 220)\n",
    "        descriptor = 'MobileNetV2EncoderPoseModel'\n",
    "    if model == 9:\n",
    "        model_class = MobileNetV3LargeEncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "        input_size = (1, 3, 220, 220)\n",
    "        descriptor = 'MobileNetV2EncoderPoseModel_HalfFC'\n",
    "    if model == 10:\n",
    "        model_class = MobileNetV3SmallEncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "        input_size = (1, 3, 220, 220)\n",
    "        descriptor = 'MobileNetV2EncoderPoseModel_HalfFC'\n",
    "    if model == 11:\n",
    "        model_class = AlexNetEncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "        input_size = (1, 3, 220, 220)\n",
    "\n",
    "    # load model for evaluation\n",
    "    model = load_model_eval(model_save_path_best_val_pck, model_class)\n",
    "\n",
    "    avg_pck_val_list, avg_pck_test_list, avg_pck_per_kp_val_list, avg_pck_per_kp_test_list,\\\n",
    "        avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02 \\\n",
    "        = full_pck_evaluation(model, val_dataloader, test_dataloader)\n",
    "\n",
    "    print('Finding other metrics ...')\n",
    "\n",
    "    # Calculate number of FLOPs\n",
    "    dummy_input = torch.randn(input_size).to('cuda') # move the dummy input to GPU\n",
    "    flops = FlopCountAnalysis(model, dummy_input)\n",
    "    total_flops = flops.total()\n",
    "    flops_extend = flops.by_module_and_operator() #* 2\n",
    "    # flops_2 = torchprofile.profile_macs(model, dummy_input) * 2\n",
    "    # print(f'Total FLOPs: {flops_2}')\n",
    "    # print(flop_count_table(flops))\n",
    "\n",
    "    # Calculate the number of params\n",
    "    param_dict = parameter_count(model)\n",
    "    total_params = param_dict['']\n",
    "    # print(total_params)\n",
    "\n",
    "    # Calculate inference time GPU\n",
    "    gpu_inf_time = gpu_inference_time(model, dummy_input)\n",
    "    # print(gpu_inf_time)\n",
    "    # print(gpu_inf_time*1e3)\n",
    "\n",
    "    # Calculate inference time CPU\n",
    "    cpu_inf_time = cpu_inference_time(model, dummy_input)\n",
    "    # print(cpu_inf_time)\n",
    "    # print(cpu_inf_time*1e3)\n",
    "\n",
    "    # load results to a dict\n",
    "    results = load_results_to_dict(save_dir, avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02,\\\n",
    "                        total_params, total_flops, gpu_inf_time, cpu_inf_time, param_dict, flops_extend, avg_pck_test_list, \\\n",
    "                        avg_pck_per_kp_test_list, avg_pck_val_list, avg_pck_per_kp_val_list, train_imgs_array.shape[0], \\\n",
    "                        val_imgs_array.shape[0], test_imgs_array.shape[0])\n",
    "\n",
    "    # save \n",
    "    print('saving metrics ...')\n",
    "    save_dict_to_json(results, save_dir_eval)\n",
    "\n",
    "    # plot and save images\n",
    "    print('plotting and saving some result images ...')\n",
    "    # get random images with seed so that it is consistant\n",
    "    # Set a fixed seed for reproducibility\n",
    "    fixed_seed = 42\n",
    "    random.seed(fixed_seed)\n",
    "\n",
    "    # val - create loop to produce and save 5 random images to the save dir\n",
    "    # Generate unique random numbers for validation\n",
    "    val_random_nums = random.sample(range(val_imgs_array.shape[0]), 5)\n",
    "    # I am replacing the above with all the val images\n",
    "    val_random_nums = val_imgs_array.shape[0]\n",
    "\n",
    "    #for i, random_num in enumerate(val_random_nums):\n",
    "    for i in range(val_random_nums):\n",
    "\n",
    "        print('VALIDATION', i)\n",
    "        #predict_and_plot(model, val_imgs_array, val_kp_array, random_num, random_num+1, save_dir_eval+'/val_predictions')\n",
    "        predict_and_plot(model, val_imgs_array, val_kp_array, i, i+1, save_dir_eval+'/val_predictions')\n",
    "\n",
    "\n",
    "    # test - create loop to produce and save 5 random images to the save dir\n",
    "    # Generate unique random numbers for testing\n",
    "    test_random_nums = random.sample(range(test_imgs_array.shape[0]), 15)\n",
    "    # I am replacing the above with all the val images\n",
    "    test_random_nums = test_imgs_array.shape[0]\n",
    "\n",
    "    #for i, random_num in enumerate(test_random_nums):\n",
    "    for i in range(test_random_nums):\n",
    "\n",
    "        print(f'TEST {i}')\n",
    "        # get a random image in the list\n",
    "        #predict_and_plot(model, test_imgs_array, test_kp_array, random_num, random_num+1, save_dir_eval+'/test_predictions')\n",
    "        predict_and_plot(model, test_imgs_array, test_kp_array, i, i+1, save_dir_eval+'/test_predictions')\n",
    "\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is for running an evalutation on the train dataset\n",
    "# # if not running directly off the back of training then set this to true\n",
    "# # straight_eval = True\n",
    "# def run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=None, batch_size=1, val_dataloader=None, \\\n",
    "#              test_dataloader=None, train_imgs_array=None, val_imgs_array=None, test_imgs_array=None, val_kp_array=None, \\\n",
    "#                 test_kp_array=None, straight_eval=True):\n",
    "#     if straight_eval:\n",
    "#         # run just evalutation\n",
    "\n",
    "#         # set crop extension and batch_size\n",
    "#         crop_extension = '_crop_220x220.jpg'# cropsize extension\n",
    "#         #batch_size = 1\n",
    "\n",
    "#         # set model paths\n",
    "#         #save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02'\n",
    "#         #model_save_path_best_val_loss = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02/best_val_loss_model_epoch_22_PCK_0.6433_loss_0.0053.pth'\n",
    "#         #model_save_path_best_val_pck = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02/best_val_loss_model_epoch_22_PCK_0.6433_loss_0.0053.pth'\n",
    "#         #final_model_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02/final_model_epoch_30_PCK_0.5835_loss_0.0076.pth'\n",
    "\n",
    "#         # load the data\n",
    "#         train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array = load_data(1, 1, crop_extension)\n",
    "#         #val\n",
    "#         val_imgs_tensor = torch.tensor(val_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "#         val_kp_tensor = torch.tensor(val_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "#         val_dataset = TensorDataset(val_imgs_tensor, val_kp_tensor)\n",
    "#         val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "#         #test\n",
    "#         test_imgs_tensor = torch.tensor(test_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "#         test_kp_tensor = torch.tensor(test_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "#         test_dataset = TensorDataset(test_imgs_tensor, test_kp_tensor)\n",
    "#         test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "\n",
    "#         #train.. DEBUG\n",
    "#         train_imgs_tensor = torch.tensor(train_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "#         train_kp_tensor = torch.tensor(train_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "#         train_dataset = TensorDataset(train_imgs_tensor, train_kp_tensor)\n",
    "#         train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "\n",
    "#     # create evaluation dir\n",
    "#     save_dir_eval = create_timestamped_dir('/evaluation', save_dir)\n",
    "\n",
    "#     if model == 1:\n",
    "#         model_class = DeepPoseModel\n",
    "#         input_size = (1, 3, 220, 220)#(torch.randn((1,3,220,220)),)\n",
    "#     if model == 2:\n",
    "#         model_class = ResNet50EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "#         input_size = (1, 3, 220, 220)\n",
    "#         descriptor = 'ResNet50EncoderPose'\n",
    "#     if model == 3:\n",
    "#         model_class = ResNet34EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "#         input_size = (1, 3, 220, 220)\n",
    "#         descriptor = 'ResNet34EncoderPose'\n",
    "#     if model == 4:\n",
    "#         model_class = ResNet18EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "#         input_size = (1, 3, 220, 220)\n",
    "#         descriptor = 'ResNet18EncoderPose'\n",
    "#     if model == 5:\n",
    "#         model_class = EfficientNetB0EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "#         input_size = (1, 3, 220, 220)\n",
    "#         descriptor = 'EfficientNetB0EncoderPoseModel'\n",
    "#     if model == 6:\n",
    "#         model_class = EfficientNetB4EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "#         input_size = (1, 3, 220, 220)\n",
    "#         descriptor = 'EfficientNetB0EncoderPoseModel_HalfFC'\n",
    "#     if model == 7:\n",
    "#         model_class = EfficientNetB7EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "#         input_size = (1, 3, 220, 220)\n",
    "#         descriptor = 'EfficientNetB1EncoderPoseModel_HalfFC'\n",
    "#     if model == 8:\n",
    "#         model_class = MobileNetV2EncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "#         input_size = (1, 3, 220, 220)\n",
    "#         descriptor = 'MobileNetV2EncoderPoseModel'\n",
    "#     if model == 9:\n",
    "#         model_class = MobileNetV3LargeEncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "#         input_size = (1, 3, 220, 220)\n",
    "#         descriptor = 'MobileNetV2EncoderPoseModel_HalfFC'\n",
    "#     if model == 10:\n",
    "#         model_class = MobileNetV3SmallEncoderPoseModel#(nkeypoints=8).to('cuda')\n",
    "#         input_size = (1, 3, 220, 220)\n",
    "#         descriptor = 'MobileNetV2EncoderPoseModel_HalfFC'\n",
    "\n",
    "#     # load model for evaluation\n",
    "#     model = load_model_eval(model_save_path_best_val_pck, model_class)\n",
    "\n",
    "#     avg_pck_val_list, avg_pck_test_list, avg_pck_per_kp_val_list, avg_pck_per_kp_test_list,\\\n",
    "#         avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02 \\\n",
    "#         = full_pck_evaluation(model, val_dataloader, train_dataloader)#test_dataloader)\n",
    "\n",
    "#     print('Finding other metrics ...')\n",
    "\n",
    "#     # Calculate number of FLOPs\n",
    "#     dummy_input = torch.randn(input_size).to('cuda') # move the dummy input to GPU\n",
    "#     flops = FlopCountAnalysis(model, dummy_input)\n",
    "#     total_flops = flops.total()\n",
    "#     flops_extend = flops.by_module_and_operator() #* 2\n",
    "#     # flops_2 = torchprofile.profile_macs(model, dummy_input) * 2\n",
    "#     # print(f'Total FLOPs: {flops_2}')\n",
    "#     # print(flop_count_table(flops))\n",
    "\n",
    "#     # Calculate the number of params\n",
    "#     param_dict = parameter_count(model)\n",
    "#     total_params = param_dict['']\n",
    "#     # print(total_params)\n",
    "\n",
    "#     # Calculate inference time GPU\n",
    "#     gpu_inf_time = gpu_inference_time(model, dummy_input)\n",
    "#     # print(gpu_inf_time)\n",
    "#     # print(gpu_inf_time*1e3)\n",
    "\n",
    "#     # Calculate inference time CPU\n",
    "#     cpu_inf_time = cpu_inference_time(model, dummy_input)\n",
    "#     # print(cpu_inf_time)\n",
    "#     # print(cpu_inf_time*1e3)\n",
    "\n",
    "#     # load results to a dict\n",
    "#     results = load_results_to_dict(save_dir, avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02,\\\n",
    "#                         total_params, total_flops, gpu_inf_time, cpu_inf_time, param_dict, flops_extend, avg_pck_test_list, \\\n",
    "#                         avg_pck_per_kp_test_list, avg_pck_val_list, avg_pck_per_kp_val_list, train_imgs_array.shape[0], \\\n",
    "#                         val_imgs_array.shape[0], train_imgs_array.shape[0])\n",
    "\n",
    "#     # save \n",
    "#     print('saving metrics ...')\n",
    "#     save_dict_to_json(results, save_dir_eval)\n",
    "\n",
    "#     # plot and save images\n",
    "#     print('plotting and saving some result images ...')\n",
    "#     # get random images with seed so that it is consistant\n",
    "#     # Set a fixed seed for reproducibility\n",
    "#     fixed_seed = 42\n",
    "#     random.seed(fixed_seed)\n",
    "\n",
    "#     # val - create loop to produce and save 5 random images to the save dir\n",
    "#     # Generate unique random numbers for validation\n",
    "#     val_random_nums = random.sample(range(val_imgs_array.shape[0]), 5)\n",
    "#     # I am replacing the above with all the val images\n",
    "#     val_random_nums = val_imgs_array.shape[0]\n",
    "\n",
    "#     #for i, random_num in enumerate(val_random_nums):\n",
    "#     for i in range(val_random_nums):\n",
    "\n",
    "#         print('VALIDATION', i)\n",
    "#         #predict_and_plot(model, val_imgs_array, val_kp_array, random_num, random_num+1, save_dir_eval+'/val_predictions')\n",
    "#         predict_and_plot(model, val_imgs_array, val_kp_array, i, i+1, save_dir_eval+'/val_predictions')\n",
    "\n",
    "\n",
    "#     # test - create loop to produce and save 5 random images to the save dir\n",
    "#     # Generate unique random numbers for testing\n",
    "#     test_random_nums = random.sample(range(train_imgs_array.shape[0]), 15)\n",
    "#     # I am replacing the above with all the val images\n",
    "#     test_random_nums = train_imgs_array.shape[0]\n",
    "\n",
    "#     #for i, random_num in enumerate(test_random_nums):\n",
    "#     for i in range(test_random_nums):\n",
    "\n",
    "#         print(f'TRAIN {i}')\n",
    "#         # get a random image in the list\n",
    "#         #predict_and_plot(model, test_imgs_array, test_kp_array, random_num, random_num+1, save_dir_eval+'/test_predictions')\n",
    "#         predict_and_plot(model, train_imgs_array, train_kp_array, i, i+1, save_dir_eval+'/train_predictions')\n",
    "\n",
    "#     print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_and_keypoint(img, keypoints, nkeypoints=8, keypoint_labels=None):\n",
    "  fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "  plt.imshow(img)\n",
    "  #print(keypoints)\n",
    "  x_keypoints = keypoints[::2]\n",
    "  y_keypoints = keypoints[1::2]\n",
    "  #print(x_keypoints)\n",
    "  #print(y_keypoints)\n",
    "  plt.scatter(x_keypoints, y_keypoints, marker='.', c=np.arange(nkeypoints), cmap='jet')\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "  if keypoint_labels is not None:\n",
    "      for i, (x, y) in enumerate(zip(x_keypoints, y_keypoints)):\n",
    "          plt.text(x, y, keypoint_labels[i], fontsize=12, color='white', \n",
    "                    bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_cols(df):\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1. Model input and run train and evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a model\n",
    "model = 8 # 1. DeepPose, 2. ResNet50EncoderPose, 3. ResNet34EncoderPose, 4. ResNet18EncoderPose\n",
    "# 5. EfficientNetB0EncoderPoseModel, 6. EfficientNetB4EncoderPoseModel # 7.EfficientNetB2EncoderPoseModel\n",
    "# 8. MobileNetV2EncoderPoseModel, 9. MobileNetV3LargeEncoderPoseModel, 10. MobileNetV3SmallEncoderPoseModel 11. AlexNetEncoderPoseModel\n",
    "dataset = 1 # 1. Simple - need to think how to handle crop image size - this should be moved to the model rather maybe\n",
    "augmentation = 2 # 1. no aug, 2. simple aug, 3. large aug*\n",
    "batch_size = 16\n",
    "num_epochs = 250 #250 #300\n",
    "learning_rate = 0.001#0.000001\n",
    "optimizer = 1 # 1. Adam, 2. AdamFineTune\n",
    "loss_function = 2 # 1. MSE (Gaussian/L2), 2. MAE (LaPlace/L1), 3. angles, 4. polar\n",
    "crop_extension = '_crop_220x220.jpg'# cropsize extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = [3, 2, 1]\n",
    "# augmentation = [1,2]\n",
    "\n",
    "# for i in loss_function:\n",
    "#     for j in augmentation:\n",
    "#         # run just train\n",
    "#         save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader, \\\n",
    "#             train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\\\n",
    "#                 = run_train(model, dataset, j, optimizer, i, learning_rate, batch_size, num_epochs, crop_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = #11#[1,4,5,8,9]#[1,2,3,4,8,9,10,5,6,7]#[1,9,10]#[2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# for j in model:\n",
    "#     # run both train and evaluation\n",
    "\n",
    "#     # train\n",
    "#     save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader, \\\n",
    "#         train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\\\n",
    "#             = run_train(j, dataset, augmentation, optimizer, loss_function, learning_rate, batch_size, num_epochs, crop_extension)\n",
    "\n",
    "#     # evaluate\n",
    "#     run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=j,batch_size=batch_size, val_dataloader=val_dataloader, \\\n",
    "#                 test_dataloader=test_dataloader, train_imgs_array=train_imgs_array, val_imgs_array=val_imgs_array, test_imgs_array=test_imgs_array,\\\n",
    "#                     val_kp_array=val_kp_array, test_kp_array=test_kp_array, straight_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing learning rate\n",
    "# model = [11] #[6]#[7,8,9,10] # run 6 at less batch size #[1,2,3,4,5,6,\n",
    "# learning_rate = [0.001, 0.0001, 0.00001]\n",
    "\n",
    "# for i in model:\n",
    "#     for j in learning_rate:\n",
    "#         # run both train and evaluation\n",
    "\n",
    "#         # train\n",
    "#         save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader, \\\n",
    "#             train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\\\n",
    "#                 = run_train(i, dataset, augmentation, optimizer, loss_function, j, batch_size, num_epochs, crop_extension)\n",
    "\n",
    "#         # evaluate\n",
    "#         run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=i,batch_size=batch_size, val_dataloader=val_dataloader, \\\n",
    "#                     test_dataloader=test_dataloader, train_imgs_array=train_imgs_array, val_imgs_array=val_imgs_array, test_imgs_array=test_imgs_array,\\\n",
    "#                         val_kp_array=val_kp_array, test_kp_array=test_kp_array, straight_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run both train and evaluation\n",
    "\n",
    "# train\n",
    "save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader, \\\n",
    "    train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\\\n",
    "        = run_train(model, dataset, augmentation, optimizer, loss_function, learning_rate, batch_size, num_epochs, crop_extension)\n",
    "\n",
    "# evaluate\n",
    "run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=model, batch_size=batch_size, val_dataloader=val_dataloader, \\\n",
    "             test_dataloader=test_dataloader, train_imgs_array=train_imgs_array, val_imgs_array=val_imgs_array, test_imgs_array=test_imgs_array,\\\n",
    "                 val_kp_array=val_kp_array, test_kp_array=test_kp_array, straight_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run just train\n",
    "save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader, \\\n",
    "    train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\\\n",
    "        = run_train(model, dataset, augmentation, optimizer, loss_function, learning_rate, batch_size, num_epochs, crop_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/Deep_pose_results_double_flop_value/best/DeepPose_Simple_simpleAug_Adam_5e-05_16_100_2024-08-31_10-49-38/evaluation_2024-08-31_10-59-05'\n",
    "model_save_path_best_val_loss = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/Deep_pose_results_double_flop_value/best/DeepPose_Simple_simpleAug_Adam_5e-05_16_100_2024-08-31_10-49-38/best_val_loss_model_epoch_96_PCK_0.5897_loss_0.0100.pth'\n",
    "model_save_path_best_val_pck = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/Deep_pose_results_double_flop_value/best/DeepPose_Simple_simpleAug_Adam_5e-05_16_100_2024-08-31_10-49-38/best_val_pck_model_epoch_95_PCK_0.5913_loss_0.0109.pth'\n",
    "final_model_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/Deep_pose_results_double_flop_value/best/DeepPose_Simple_simpleAug_Adam_5e-05_16_100_2024-08-31_10-49-38/final_model_epoch_100_PCK_0.5881_loss_0.0119.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleAngle\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_simpleAug_Adam_simpleAnglesNew_5e-05_16_250_2024-12-17_12-54-02'\n",
    "model_save_path_best_val_loss = ' '\n",
    "model_save_path_best_val_pck = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_simpleAug_Adam_simpleAngles_5e-05_16_250_2024-12-17_12-54-02/best_val_pck_model_epoch_231_PCK_0.6187_loss_0.0601.pth'\n",
    "final_model_path = ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_simpleAug_Adam_MSE_5e-05_16_250_2024-12-17_14-24-03'\n",
    "model_save_path_best_val_loss = ' '\n",
    "model_save_path_best_val_pck = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_simpleAug_Adam_MSE_5e-05_16_250_2024-12-17_14-24-03/best_val_pck_model_epoch_189_PCK_0.5992_loss_0.0097.pth'\n",
    "final_model_path = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_simpleAug_Adam_MAE_5e-05_16_250_2024-12-17_13-45-42'\n",
    "model_save_path_best_val_loss = ' '\n",
    "model_save_path_best_val_pck = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_simpleAug_Adam_MAE_5e-05_16_250_2024-12-17_13-45-42/best_val_pck_model_epoch_216_PCK_0.6535_loss_0.0554.pth'\n",
    "final_model_path = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/ResNet34EncoderPose_Simple_simpleAug_Adam_MAE_5e-05_1_250_2025-01-27_19-53-22'\n",
    "model_save_path_best_val_loss = ' '\n",
    "model_save_path_best_val_pck = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/AlexNetEncoderPoseModel_Simple_simpleAug_Adam_MAE_0.001_12_250_2025-02-19_15-01-02/best_val_pck_model_epoch_70_PCK_0.5495_loss_0.0558.pth'\n",
    "final_model_path = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/AlexNetEncoderPoseModel_Simple_simpleAug_Adam_MAE_0.001_12_250_2025-02-19_15-01-02'\n",
    "model_save_path_best_val_loss = ' '\n",
    "model_save_path_best_val_pck = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/AlexNetEncoderPoseModel_Simple_simpleAug_Adam_MAE_0.001_12_250_2025-02-19_15-01-02/best_val_pck_model_epoch_70_PCK_0.5495_loss_0.0558.pth'\n",
    "final_model_path = ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laoding data ...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'model_class' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run just evaluation (ensure you have the correct model number set)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path_best_val_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path_best_val_pck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[108], line 86\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model, batch_size, val_dataloader, test_dataloader, train_imgs_array, val_imgs_array, test_imgs_array, val_kp_array, test_kp_array, straight_eval)\u001b[0m\n\u001b[1;32m     83\u001b[0m     input_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m220\u001b[39m, \u001b[38;5;241m220\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# load model for evaluation\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model_eval(model_save_path_best_val_pck, \u001b[43mmodel_class\u001b[49m)\n\u001b[1;32m     88\u001b[0m avg_pck_val_list, avg_pck_test_list, avg_pck_per_kp_val_list, avg_pck_per_kp_test_list,\\\n\u001b[1;32m     89\u001b[0m     avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02 \\\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;241m=\u001b[39m full_pck_evaluation(model, val_dataloader, test_dataloader)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinding other metrics ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'model_class' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# run just evaluation (ensure you have the correct model number set)\n",
    "run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=model, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run both train and evaluation\n",
    "#THIS IS FOR THE DROP OUT\n",
    "# train\n",
    "save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader, \\\n",
    "    train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\\\n",
    "        = run_train(model, dataset, augmentation, optimizer, loss_function, learning_rate, batch_size, num_epochs, crop_extension)\n",
    "\n",
    "# evaluate\n",
    "run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=model, batch_size=batch_size, val_dataloader=val_dataloader, \\\n",
    "             test_dataloader=test_dataloader, train_imgs_array=train_imgs_array, val_imgs_array=val_imgs_array, test_imgs_array=test_imgs_array,\\\n",
    "                 val_kp_array=val_kp_array, test_kp_array=test_kp_array, straight_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run both train and evaluation\n",
    "\n",
    "# train\n",
    "save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader, \\\n",
    "    train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\\\n",
    "        = run_train(model, dataset, augmentation, optimizer, loss_function, learning_rate, batch_size, num_epochs, crop_extension)\n",
    "\n",
    "# evaluate\n",
    "run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=model, batch_size=batch_size, val_dataloader=val_dataloader, \\\n",
    "             test_dataloader=test_dataloader, train_imgs_array=train_imgs_array, val_imgs_array=val_imgs_array, test_imgs_array=test_imgs_array,\\\n",
    "                 val_kp_array=val_kp_array, test_kp_array=test_kp_array, straight_eval=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create a SLEAP model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0. some info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sleap.ai/guides/gui.html\n",
    "GUI Functionality:\n",
    " - .slp files contain the information about the labels (annotations)\n",
    " - file open project to open a .slp file\n",
    " - file import to import a DLC annotation (can use the .yaml file for all the annotations from a project or a csv file for a single video)\n",
    " - file export analysis to csv/h5 exports\n",
    " - file merge data from: USE THIS WHEN OPENING UP A PREDICTIONS FILE THAT YOU WANT TO ADD TO A TRAINING DATASET\n",
    " - labels: has functionality for labeling videos - review when using for labelling\n",
    " - Predict >> Evaluation of Model >> select model >> view metrics: shows all the various metrics (how can we access these?)\n",
    " - Predict >> Export labeled frames: exports frames to a .slp file (can do just labeled or +suggested or +predicted+suggested)\n",
    " - Predict >> inference: allows you to select a model and run inference. Can export the config files and the training job package (contains a bash command for inference, a yaml file with some info and paths, the .slp file (which is over 1GB in size))?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_config.json\n",
    "- data >> labels >> training_inds/validation_inds: has the indices of the training and val images\n",
    "- data >> preprocessing: has image size \n",
    "- data >> instance cropping: has the centre point and crop size (640) and the padding (16)\n",
    "- model: has model parameters, backbone: unet, heads: centered_instance\n",
    "- optimization >> augmentation_config: all the augs (only rotation of 15 deg and random flip used)\n",
    "- optimization: has hyperparameters such as batch_size, epochs, optimizer (adam), lr, early stopping \n",
    "- outputs: saving outputs, has a tensorboard option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pkg.slp file\n",
    " - contains all the lables and frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_model.h5 \n",
    " - this is the best model parameters I assume (to check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command line instances\n",
    "https://sleap.ai/guides/cli.html \n",
    "\n",
    "I think these will be the most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0. Mock project creation (following the tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run sleap-label in the teminal to open GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.1. Create a project\n",
    "https://sleap.ai/tutorials/new-project.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.2. Define animal skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.3. Select frames for labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.4. Manual labelling of frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.5. Training model using labelled frames\n",
    "see configuring models: https://sleap.ai/guides/choosing-models.html#choosing-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.6. Inference on unlabelled frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.7. Refining predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.8. Importing additional videos from your experiment, and applying the trained model to predict animal poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tracking - we are not doing this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.9. Exporting data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.10. SLEAP data structures notebook\n",
    "https://sleap.ai/notebooks/Data_structures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.11. SLEAP customised workflows for training SLEAP notebook\n",
    "https://sleap.ai/notebooks/Interactive_and_resumable_training.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.12. SLEAP real time inference notebook\n",
    "https://sleap.ai/notebooks/Interactive_and_realtime_inference.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.13 SLEAP model evaluation notebook\n",
    "https://sleap.ai/notebooks/Model_evaluation.html\n",
    "https://sleap.ai/notebooks/Analysis_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Some exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Checking out the model.h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/models/test_run240930_124502.centered_instance.n=10/best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. viewing the .slp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sleap.load_file(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels_simpleDataset_DLC_labels.v001.slp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sleap.info\n",
    "import sleap.info.labels\n",
    "\n",
    "\n",
    "sleap.info.labels.describe_labels(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels_simpleDataset_DLC_labels.v001.slp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleap.info.labels.describe_labels(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/Test_outputs_1Oct/labels_simpleDataset_DLC_labels.v001_pr_sug.pkg.slp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. attempting to train the simple model using command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. creating the sleap datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4. Continue training from existing model using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file\n",
    "#cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_centroid_50Epoch/training_config.json\")\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_centroid_200Epoch/training_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = sleap.nn.training.Trainer.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the randomly initialized weights with the saved weights.\n",
    "#trainer.keras_model.load_weights(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_centroid_50Epoch/best_model.h5\")\n",
    "trainer.keras_model.load_weights(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_centroid_200Epoch/best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.config.optimization.epochs = 200\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch/training_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize the trainer. prt1\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize the trainer. prt2\n",
    "trainer.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the randomly initialized weights with the saved weights.\n",
    "trainer.keras_model.load_weights(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch/best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.config.optimization.epochs = 200\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Using the CLI to run the training and Inference on a folder of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an altered config file. /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/models/training_config_1Oct.json\n",
    "\n",
    "kept the same from initial training with a change in paths and nulls in the labels section (except for the skeleton). Used the created centriod_instance.json to adjust the label\n",
    "\n",
    "I will be passing the label data through the CLI command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usage: sleap-train [-h] [--video-paths VIDEO_PATHS] [--val_labels VAL_LABELS]\n",
    "                   [--test_labels TEST_LABELS] [--tensorboard] [--save_viz]\n",
    "                   [--zmq] [--run_name RUN_NAME] [--prefix PREFIX]\n",
    "                   [--suffix SUFFIX]\n",
    "                   training_job_path [labels_path]\n",
    "\n",
    "positional arguments:\n",
    "  training_job_path     Path to training job profile JSON file.\n",
    "  labels_path           Path to labels file to use for training. If specified,\n",
    "                        overrides the path specified in the training job\n",
    "                        config.\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --video-paths VIDEO_PATHS\n",
    "                        List of paths for finding videos in case paths inside\n",
    "                        labels file are not accessible.\n",
    "  --val_labels VAL_LABELS, --val VAL_LABELS\n",
    "                        Path to labels file to use for validation. If\n",
    "                        specified, overrides the path specified in the\n",
    "                        training job config.\n",
    "  --test_labels TEST_LABELS, --test TEST_LABELS\n",
    "                        Path to labels file to use for test. If specified,\n",
    "                        overrides the path specified in the training job\n",
    "                        config.\n",
    "  --base_checkpoint BASE_CHECKPOINT\n",
    "                        Path to base checkpoint (directory containing best_model.h5)\n",
    "                        to resume training from.\n",
    "  --tensorboard         Enable TensorBoard logging to the run path if not\n",
    "                        already specified in the training job config.\n",
    "  --save_viz            Enable saving of prediction visualizations to the run\n",
    "                        folder if not already specified in the training job\n",
    "                        config.\n",
    "  --zmq                 Enable ZMQ logging (for GUI) if not already specified\n",
    "                        in the training job config.\n",
    "  --run_name RUN_NAME   Run name to use when saving file, overrides other run\n",
    "                        name settings.\n",
    "  --prefix PREFIX       Prefix to prepend to run name.\n",
    "  --suffix SUFFIX       Suffix to append to run name.\n",
    "  --cpu                 Run training only on CPU. If not specified, will use\n",
    "                        available GPU.\n",
    "  --first-gpu           Run training on the first GPU, if available.\n",
    "  --last-gpu            Run training on the last GPU, if available.\n",
    "  --gpu GPU             Run training on the i-th GPU on the system. If 'auto', run on\n",
    "                        the GPU with the highest percentage of available memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attermpt 1: All Oct1 results. Did not work.\n",
    "\n",
    "sleap-train /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/models/training_config_1Oct.json /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp --val_labels /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp --test_labels /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp --run_name Oct1TestRun100Epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 2: Oct2_test1. Try using the baseline .json file\n",
    "\n",
    "centriod model\n",
    "\n",
    "sleap-train baseline.centroid.json /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp --val_labels /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp --test_labels /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp --run_name Oct2_test1_centroid_50Epoch \n",
    "\n",
    "centered-instance model\n",
    "\n",
    "sleap-train baseline_medium_rf.topdown.json /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp --val_labels /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp --test_labels /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp --run_name Oct2_test1_centeredInstance_50Epoch \n",
    "\n",
    "I created a new baseline_medium_rf.topdown.json and baseline.centroid.json with _adjusted attached\n",
    "*move them from this folder /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted to this folder /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/sleap/sleap/training_profiles\n",
    "\n",
    "and remove them after! or certain functions don't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to move the adjusted baselines into the correc\n",
    "!sleap-train baseline.centroid_adjusted.json \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\" --val_labels \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\" --test_labels \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\" --run_name \"Oct2_test1_centroid_50Epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-train baseline_medium_rf.topdown_adjusted.json \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\" --val_labels \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\" --test_labels \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\" --run_name \"Oct2_test1_medium_rf.topdown_50Epoch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I would like to run inference. I will pass a video this time\n",
    "*potentially change to a directory with images\n",
    "\n",
    "!sleap-track \"dataset/drosophila-melanogaster-courtship/20190128_113421.mp4\" --frames 0-100 -m \"models/courtship.centroid\" -m \"models/courtship.topdown_confmaps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-track \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_prediction/flap1.mp4\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_centroid_50Epoch\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-track \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_prediction/labels_gt.test.slp\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_centroid_50Epoch\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-track \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_prediction/test_inference_on_folder_images\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_centroid_50Epoch\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now I will inspect the predicted frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-inspect /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_prediction/flap1.predictions.slp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets inspect the training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-inspect /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-inspect /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch/labels_gt.train.slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-inspect /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch/labels_pr.test.slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-inspect /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_prediction/labels_gt.test.predictions.slp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Using the python script to run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to improve this the Oct2_test1 results with suggestions from here:\n",
    "https://github.com/talmolab/sleap/discussions/1595\n",
    "\n",
    "I also increased the plateau patience to 75 to avoid early stopping at roughly 20 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to improve on Oct2_test2 by increasing  the plateau patience further and selecting a centroid (the Body_top)\n",
    "\n",
    "Trying to improve on Oct3_test1 by saving the final model as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline.centroid_adjusted_python.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust config file with all model and data adjustments\n",
    "# cfg.data.preprocessing.input_scaling = 0.5\n",
    "# cfg.model.backbone.unet.max_stride = 32\n",
    "# cfg.model.backbone.unet.filters = 24\n",
    "# cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\",\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  #swap cnf with trainer.config\n",
    "cfg.model.heads.centroid.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct8_test1_centroid_baseline_batchsize3'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = sleap.nn.training.Trainer.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase number of epochs\n",
    "trainer.config.optimization.epochs = 200\n",
    "# keep the ouput images\n",
    "trainer.config.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "trainer.config.outputs.run_name = 'Oct3_test1_centroid_200Epoch'\n",
    "# changes suggested by post\n",
    "###### THERE IS AN ISSUE WITH SETTING UP THE MODEL AFTER CREATING THE TRAINER... MOVE THIS TO BEFORE #### seems it is just the model set up that is an issue\n",
    "# trainer.config.data.preprocessing.input_scaling = 0.5\n",
    "# trainer.config.model.backbone.unet.max_stride = 32\n",
    "# trainer.config.model.backbone.unet.filters = 24\n",
    "# trainer.config.model.backbone.unet.filters_rate = 1.5\n",
    "# trainer.config.model.heads.centroid.output_stride = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final model\n",
    "trainer.config.outputs.checkpointing.latest_model = True\n",
    "trainer.config.outputs.checkpointing.final_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the early stopping\n",
    "trainer.config.optimization.early_stopping.plateau_patience = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALSO MOVING THESE TO BEFORE FOR SAFETY\n",
    "# provide centroid position\n",
    "# trainer.config.data.instance_cropping.center_on_part = 'Body_top'  #swap cnf with trainer.config\n",
    "# trainer.config.model.heads.centroid.anchor_part = 'Body_top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.config.save_json('check_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the randomly initialized weights with the saved weights.\n",
    "#trainer.keras_model.load_weights(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_centroid_50Epoch/best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.topdown_adjusted_python.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Oct3_test3 removed the input scaling of 0.5 as this could not run, error:\n",
    "Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 512, 512, 3), found shape=(1, 256, 256, 3)\n",
    "\n",
    "{\n",
    "\t\"name\": \"ValueError\",\n",
    "\t\"message\": \"Exception encountered when calling layer \\\"find_instance_peaks_2\\\" (type FindInstancePeaks).\n",
    "\n",
    "Input 0 of layer \\\"model_17\\\" is incompatible with the layer: expected shape=(None, 512, 512, 3), found shape=(1, 256, 256, 3)\n",
    "\n",
    "Call arguments received:\n",
    "  • inputs=tf.Tensor(shape=(1, 512, 512, 3), dtype=float32)\",\n",
    "\t\"stack\": \"---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "/tmp/ipykernel_3052/4032920361.py in <module>\n",
    "----> 1 trainer.train()\n",
    "\n",
    "~/Desktop/Master_Dev/masters_penguin_pose_estimation/sleap/sleap/nn/training.py in train(self)\n",
    "    939             validation_steps=self.config.optimization.val_batches_per_epoch,\n",
    "    940             callbacks=self.callbacks,\n",
    "--> 941             verbose=2,\n",
    "    942         )\n",
    "    943         logger.info(f\\\"Finished training loop. [{(time() - t0) / 60:.1f} min]\\\")\n",
    "\n",
    "~/anaconda3/envs/sleap/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\n",
    "     65     except Exception as e:  # pylint: disable=broad-except\n",
    "     66       filtered_tb = _process_traceback_frames(e.__traceback__)\n",
    "---> 67       raise e.with_traceback(filtered_tb) from None\n",
    "     68     finally:\n",
    "     69       del filtered_tb\n",
    "\n",
    "~/Desktop/Master_Dev/masters_penguin_pose_estimation/sleap/sleap/nn/callbacks.py in on_epoch_end(self, epoch, logs)\n",
    "    278         \\\"\\\"\\\"Save figure at the end of each epoch.\\\"\\\"\\\"\n",
    "    279         # Call plotting function.\n",
    "--> 280         figure = self.plot_fn()\n",
    "    281 \n",
    "    282         # Check if output folder exists.\n",
    "\n",
    "~/Desktop/Master_Dev/masters_penguin_pose_estimation/sleap/sleap/nn/training.py in <lambda>()\n",
    "   1344                 self.config.outputs,\n",
    "   1345                 run_path=self.run_path,\n",
    "-> 1346                 viz_fn=lambda: visualize_example(next(training_viz_ds_iter)),\n",
    "   1347                 name=\\\"train\\\",\n",
    "   1348             )\n",
    "\n",
    "~/Desktop/Master_Dev/masters_penguin_pose_estimation/sleap/sleap/nn/training.py in visualize_example(example)\n",
    "   1324         def visualize_example(example):\n",
    "   1325             # Find peaks by evaluating model.\n",
    "-> 1326             preds = find_peaks(tf.expand_dims(example[\\\"instance_image\\\"], axis=0))\n",
    "   1327             img = example[\\\"instance_image\\\"].numpy()\n",
    "   1328             cms = preds[\\\"instance_confmaps\\\"][0][0].numpy()\n",
    "\n",
    "~/Desktop/Master_Dev/masters_penguin_pose_estimation/sleap/sleap/nn/inference.py in call(self, inputs)\n",
    "   2097 \n",
    "   2098         # Network forward pass.\n",
    "-> 2099         out = self.keras_model(crops)\n",
    "   2100 \n",
    "   2101         # Sort outputs.\n",
    "\n",
    "ValueError: Exception encountered when calling layer \\\"find_instance_peaks_2\\\" (type FindInstancePeaks).\n",
    "\n",
    "Input 0 of layer \\\"model_17\\\" is incompatible with the layer: expected shape=(None, 512, 512, 3), found shape=(1, 256, 256, 3)\n",
    "\n",
    "Call arguments received:\n",
    "  • inputs=tf.Tensor(shape=(1, 512, 512, 3), dtype=float32)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg.data.preprocessing.input_scaling = 0.5\n",
    "cfg.model.backbone.unet.max_stride = 32\n",
    "cfg.model.backbone.unet.filters = 24\n",
    "# Okay removing the input scaling. There is clearly an issue with it... \n",
    "cfg.model.backbone.unet.filters_rate = 1.5\n",
    "\n",
    "# trying centring on a body part\n",
    "#cfg.model.heads.centered_instance.anchor_part = \"Body_top\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to change crop size to because running into input issue with input_scaling = 0.5\n",
    "# cfg.data.instance_cropping.crop_size = 976\n",
    "# THE ABOVE CREATED ISSUES, TRYING TO CENTRE ON BODY PART RATHER\n",
    "#cfg.data.instance_cropping.center_on_part = \"Body_top\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize the trainer. prt1\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the randomly initialized weights with the saved weights.\n",
    "#trainer.keras_model.load_weights(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch/best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.config.optimization.epochs = 200\n",
    "trainer.config.outputs.keep_viz_images = True\n",
    "trainer.config.outputs.run_name = 'Oct3_test3_medium_rf.topdown_200Epoch'\n",
    "#trainer.config.data.preprocessing.input_scaling = 0.5\n",
    "#trainer.config.model.backbone.unet.max_stride = 32\n",
    "#trainer.config.model.backbone.unet.filters = 24\n",
    "#trainer.config.model.backbone.unet.filters_rate = 1.5\n",
    "#trainer.config.model.heads.centroid.output_stride = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final model\n",
    "trainer.config.outputs.checkpointing.latest_model = True\n",
    "trainer.config.outputs.checkpointing.final_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.config.optimization.early_stopping.plateau_patience = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize the trainer. prt2\n",
    "trainer.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oct3_test2 error\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Running train with different metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1. Oct8_test1: baseline settings\n",
    "centroid (needed a batch size of 3)  - input scale 0.5, max stride 16, filter rate 2, filters 16, batch size 3 = DONE<br>\n",
    "\n",
    "center_instance (needed a batch size of 1) - input scale 1, max stride 16, filter rate 2, filters 24, batch size 1 = DONE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CENTROID\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline.centroid.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "# cfg.data.preprocessing.input_scaling = 0.5\n",
    "# cfg.model.backbone.unet.max_stride = 32\n",
    "# cfg.model.backbone.unet.filters = 24\n",
    "# cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centroid.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 3\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct8_test1_centroid_baseline_batchsize3'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CENTER_INSTANCE\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.topdown.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "# cfg.data.preprocessing.input_scaling = 0.5\n",
    "# cfg.model.backbone.unet.max_stride = 32\n",
    "# cfg.model.backbone.unet.filters = 24\n",
    "# cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centered_instance.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 1\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct8_test1_center_instance_baseline_batchsize1'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CENTROID\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline.centroid.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 0.1\n",
    "# cfg.model.backbone.unet.max_stride = 32\n",
    "# cfg.model.backbone.unet.filters = 24\n",
    "# cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centroid.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 4\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct9_test2_centroid_receptiveField_inputScale0.1'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CENTER_INSTANCE\n",
    "\n",
    "# # load config file\n",
    "# cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.topdown.json\")\n",
    "\n",
    "# # adjust config file with all model and data adjustments\n",
    "# cfg.data.preprocessing.input_scaling = 1\n",
    "# # cfg.model.backbone.unet.max_stride = 32\n",
    "# # cfg.model.backbone.unet.filters = 24\n",
    "# # cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# # cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# # provide labeled data folder\n",
    "# cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "# cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "# cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# # provide centroid position\n",
    "# cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "# cfg.model.heads.centered_instance.anchor_part = 'Body_top'\n",
    "\n",
    "# # increase number of epochs\n",
    "# cfg.optimization.epochs = 200\n",
    "# #change batch size\n",
    "# cfg.optimization.batch_size = 1\n",
    "\n",
    "# # keep the ouput images\n",
    "# cfg.outputs.keep_viz_images = True\n",
    "# # name of run\n",
    "# cfg.outputs.run_name = 'Oct9_test2_center_instance_receptiveField_inputScale0.25'\n",
    "# cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# # save final model\n",
    "# cfg.outputs.checkpointing.latest_model = True\n",
    "# cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "# trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "# trainer.setup()\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.3 Oct9_test2: Increase receptive field of to entire animal (input scale and max stride)\n",
    "centroid:  input scale to 0.25 and max stride 32, filter rate 2, filters 16, batch size 4 = DONE <br>\n",
    "centroid:  input scale to 0.5 and max stride 64, filter rate 1.5, filters 16, batch size 4 = DONE <br>\n",
    "centroid:  input scale to 0.5 and max stride 64 <br>\n",
    "\n",
    "max stride to 32, try max batch size (decrease filters), try max filters (decrease batch size)<br>\n",
    "center instance: input scale to 1 and max stride 32  - GPU issue changing filter rate to 1.5, filters 16, batchsize 2 = DONE <br>\n",
    "center instance: input scale to 1 and max stride 32  - GPU issue changing filter rate to 1.5, filters 24, batchsize 2 = DON#<br>\n",
    "\n",
    "max stride to 64, try max batch size (decrease filters), try max filters (decrease batch size)<br>\n",
    "center instance: input scale to 1 and max stride 64  - GPU issue changing filter rate to 1.5, filters 16, batchsize 2 <br>\n",
    "center instance: input scale to 1 and max stride 64  - GPU issue changing filter rate to 1.5, filters 24, batchsize 1 <br>\n",
    "\n",
    "max stride to 16, try max batch size (decrease filters), try max filters (decrease batch size)<br>\n",
    "center instance: input scale to 1 and max stride 16  - GPU issue changing filter rate to 1.5, filters 16, batchsize 4 <br>\n",
    "center instance: input scale to 1 and max stride 16  - GPU issue changing filter rate to 1.5, filters 24, batchsize 1 <br>\n",
    "\n",
    "(may need to decrease number of filters and filter rate - try get batch size back up to 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CENTROID\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline.centroid.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 0.25\n",
    "cfg.model.backbone.unet.max_stride = 32\n",
    "# cfg.model.backbone.unet.filters = 24\n",
    "# cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centroid.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 4\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CENTROID\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline.centroid.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 0.5\n",
    "cfg.model.backbone.unet.max_stride = 64\n",
    "# cfg.model.backbone.unet.filters = 24\n",
    "cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centroid.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 4\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct9_test2_centroid_receptiveField_inputScale0.5_maxStride64_filterRate1.5_filters16'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CENTER_INSTANCE (max stride 32)\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.topdown.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 1\n",
    "cfg.model.backbone.unet.max_stride = 32\n",
    "cfg.model.backbone.unet.filters = 16\n",
    "cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centered_instance.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 3\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct9_test2_center_instance_receptiveField_inputScale1_maxStride32_filterRate1.5_filters16_batchsize3'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CENTER_INSTANCE (max stride 32)\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.topdown.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 1\n",
    "cfg.model.backbone.unet.max_stride = 32\n",
    "cfg.model.backbone.unet.filters = 24\n",
    "cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centered_instance.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 2\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct9_test2_center_instance_receptiveField_inputScale1_maxStride32_filterRate1.5_filters24_batchsize3'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CENTER_INSTANCE (max stride 64)\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.topdown.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 1\n",
    "cfg.model.backbone.unet.max_stride = 64\n",
    "cfg.model.backbone.unet.filters = 16\n",
    "cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centered_instance.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 2\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters16_batchsize2'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CENTER_INSTANCE (max stride 64)\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.topdown.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 1\n",
    "cfg.model.backbone.unet.max_stride = 64\n",
    "cfg.model.backbone.unet.filters = 24\n",
    "cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centered_instance.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 1\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CENTER_INSTANCE (max stride 16)\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.topdown.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 1\n",
    "cfg.model.backbone.unet.max_stride = 16\n",
    "cfg.model.backbone.unet.filters = 16\n",
    "cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centered_instance.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 4\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct9_test2_center_instance_receptiveField_inputScale1_maxStride16_filterRate1.5_filters16_batchsize4'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CENTER_INSTANCE (max stride 16)\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.topdown.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 1\n",
    "cfg.model.backbone.unet.max_stride = 16\n",
    "cfg.model.backbone.unet.filters = 24\n",
    "cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centered_instance.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 2\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct9_test2_center_instance_receptiveField_inputScale1_maxStride16_filterRate1.5_filters16_batchsize2'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.4. Oct9_test3: bottom up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom up\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.bottomup.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 0.75\n",
    "# cfg.model.backbone.unet.max_stride = 16\n",
    "# cfg.model.backbone.unet.filters = 16\n",
    "# cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train_BU.slp\"\n",
    "cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val_BU.slp\"\n",
    "cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test_BU.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "# cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "# cfg.model.heads.centered_instance.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 1\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Oct9_test3_bottomup_baseline'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to the DLC data\n",
    "train_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/DLC_data/train'\n",
    "val_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/DLC_data/val'\n",
    "test_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/DLC_data/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a .yaml files for each of the dataset (adjusting the config.yaml accordingly)\n",
    " - (DONT NEED THIS) make sure it only has the correct frames in the video list\n",
    " - changing the project path to the data folder and recreating the folder structure (path to the correct folder (eg train_path))\n",
    " - copy the label-data folders to the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created 3 datasets now: train, test and val\n",
    "slp_train_dataset_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_train.slp'\n",
    "slp_val_dataset_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp'\n",
    "slp_test_dataset_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7. predict and convert predictions to our own format and to dlc format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.1. select a model and load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected models. to be passed as a list of centroid and centre instance models if they are top-down (folders must include a best.h5 and a training_config.json)\n",
    "# models selected by choosing best performance (OKS mAP) on the val set\n",
    "path_to_centroid_model_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4'\n",
    "path_to_centered_instance_model_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1'\n",
    "# the above models were too big, so lets try another (second best)\n",
    "#path_to_centered_instance_model_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_center_instance_receptiveField_inputScale1_maxStride32_filterRate1.5_filters24_batchsize2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to run on the baseline models (this has worked in the past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try bottom up\n",
    "path_to_bottom_up = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test3_bottomup_baseline_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor for the top-down\n",
    "predictor = sleap.load_model([path_to_centroid_model_folder, path_to_centered_instance_model_folder], batch_size=1, peak_threshold=0.0, progress_reporting='json', max_instances=4)\n",
    "\n",
    "# NOTE the peak_threshold cannot be 0 (so I have made it 0.02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor for the bottom up\n",
    "predictor = sleap.load_model(path_to_bottom_up, batch_size=1, peak_threshold=0.1, progress_reporting='json', max_instances=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2. load the slp file containing the test images and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to slp file\n",
    "#path_test_slp_file = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.pkg.slp'\n",
    "path_test_slp_file = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sleap_gt_labels = sleap.load_file(path_test_slp_file)\n",
    "test_sleap_gt_labels = sleap.load_file(path_test_slp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_images = sio.Video.from_filename(path_img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.3 Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pedictions on images\n",
    "predictions = predictor.predict(test_sleap_gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions2 = predictor.predict(test_sleap_gt_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.3.1 Try with the CLI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run top-down with gpu on folder of images\n",
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4-Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1' --verbosity json --batch_size 1 --peak_threshold 0.0 #--cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run top-down with cpu on folder of images\n",
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4-Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1' --verbosity json --batch_size 1 --peak_threshold 0.0 --cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run top-down with cpu on slp dataset\n",
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.pkg.slp' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4-Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1' --verbosity json --batch_size 1 --peak_threshold 0.0 --cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run bottom with cpu on slp dataset\n",
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.pkg.slp' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test3_bottomup_baseline_1' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/Oct9_test3_bottomup_baseline_1' --verbosity json --batch_size 1 --peak_threshold 0.0 --cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run bottom with gpu on slp dataset\n",
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.pkg.slp' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test3_bottomup_baseline_1' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/Oct9_test3_bottomup_baseline_1' --verbosity json --batch_size 1 --peak_threshold 0.0 #--cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run bottom with gpu on image folder\n",
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test3_bottomup_baseline_1' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/Oct9_test3_bottomup_baseline_1' --verbosity json --batch_size 1 --peak_threshold 0.0 #--cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run bottom with cpu on image folder\n",
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test3_bottomup_baseline_1' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/Oct9_test3_bottomup_baseline_1' --verbosity json --batch_size 1 --peak_threshold 0.0 --cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try baseline\n",
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.pkg.slp' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct8_test1_centroid_baseline_batchsize3' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct8_test1_centered_instance_baseline_batchsize1' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/Oct8_test1_centroid_center_instance_baseline_peakThreshold0.01' --verbosity json --batch_size 1 --peak_threshold 0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.4. extract labels and save them in own format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.4.1. check the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ground truth labels\n",
    "#ground_truth = sleap.load_file('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.pkg.slp')\n",
    "# dont load from package or it wont keep the names of the files\n",
    "gt = sleap.load_file('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp')\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract first frame of predictions\n",
    "first_frame_pred = predictions[9]\n",
    "first_frame_pred.plot(scale=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame_pred.instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame_gt.instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract first frame of gt\n",
    "# first_frame_gt = ground_truth[71]\n",
    "first_frame_gt = gt[1]\n",
    "first_frame_gt.plot(scale=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.4.2. for each instance in the ground truth save the predicted instance that matches with the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise empty dfs\n",
    "def initialize_df():\n",
    "    columns = [\n",
    "        \"vid_id\", \"img_id\", \"bbox_id\", \"bbox_c_x\", \"bbox_c_y\", \"bbox_w\", \"bbox_h\",\n",
    "        \"Head_x\", \"Head_y\", \"Beak_x\", \"Beak_y\", \"Body_top_x\", \"Body_top_y\",\n",
    "        \"RFlipper_mid_x\", \"RFlipper_mid_y\", \"LFlipper_mid_x\", \"LFlipper_mid_y\",\n",
    "        \"Body_bottom_x\", \"Body_bottom_y\", \"RFoot_x\", \"RFoot_y\", \"LFoot_x\", \"LFoot_y\",\n",
    "        \"kp_outside_best_bbox\", \"kp_missing\", \"kp_primary_missing\", \"img_width\", \"img_height\", \"pred_score\"\n",
    "    ]\n",
    "    return pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the distance between two points \n",
    "def distance_between_points(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculates the Euclidean distance between two points.\n",
    "    \n",
    "    Args:\n",
    "    - point1 (tuple): A list representing the (x, y) coordinates of the first point.\n",
    "    - point2 (tuple): A list representing the (x, y) coordinates of the second point.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The distance between the two points.\n",
    "    \"\"\"\n",
    "    # Extract coordinates from the tuples\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    \n",
    "    # Calculate the distance using the Euclidean distance formula\n",
    "    distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    \n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_df_entry(vid_id, img_id, bbox_id, instance, pred = True):\n",
    "\n",
    "    # create an array of the predictions\n",
    "    kp_array = instance.points_array\n",
    "\n",
    "    # create save the prediction score if there is one otherwise make 0\n",
    "    if pred:\n",
    "        pred_score = instance.score\n",
    "        # if it is a prediction, also replace all the NaN with the corner of the bbox\n",
    "        # this so if the algorithm does not predict we can just assume a pred of far out\n",
    "        if np.isnan(kp_array):\n",
    "            print(vid_id, img_id)\n",
    "        kp_array[np.isnan(kp_array)] = instance.bounding_box[0]\n",
    "        \n",
    "    else:\n",
    "        pred_score = 0.0\n",
    "    \n",
    "    # Create a new row as a dictionary\n",
    "    new_entry = {\n",
    "        \"vid_id\": vid_id,\n",
    "        \"img_id\": img_id,\n",
    "        \"bbox_id\": bbox_id,\n",
    "        \"bbox_c_x\": instance.bounding_box[1] + instance.midpoint[0],\n",
    "        \"bbox_c_y\": instance.bounding_box[0] + instance.midpoint[1],\n",
    "        \"bbox_w\": instance.bounding_box[3] - instance.midpoint[1],\n",
    "        \"bbox_h\": instance.bounding_box[2] - instance.midpoint[0],\n",
    "        \"Head_x\": kp_array[0][0],\n",
    "        \"Head_y\": kp_array[0][1],\n",
    "        \"Beak_x\": kp_array[1][0],\n",
    "        \"Beak_y\": kp_array[1][1],\n",
    "        \"Body_top_x\": kp_array[2][0],\n",
    "        \"Body_top_y\": kp_array[2][1],\n",
    "        \"RFlipper_mid_x\": kp_array[3][0],\n",
    "        \"RFlipper_mid_y\": kp_array[3][1],\n",
    "        \"LFlipper_mid_x\": kp_array[4][0],\n",
    "        \"LFlipper_mid_y\": kp_array[4][1],\n",
    "        \"Body_bottom_x\": kp_array[5][0],\n",
    "        \"Body_bottom_y\": kp_array[5][1],\n",
    "        \"RFoot_x\": kp_array[6][0],\n",
    "        \"RFoot_y\": kp_array[6][1],\n",
    "        \"LFoot_x\": kp_array[7][0],\n",
    "        \"LFoot_y\": kp_array[7][1],\n",
    "        \"kp_outside_best_bbox\": 0.0,\n",
    "        \"kp_missing\": 0.0,\n",
    "        \"kp_primary_missing\": 0.0,\n",
    "        \"img_width\": instance.video.shape[2],\n",
    "        \"img_height\": instance.video.shape[1],\n",
    "        \"pred_score\": pred_score\n",
    "    }\n",
    "\n",
    "    #print(new_entry)\n",
    "\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ground truth data\n",
    "#gt = sleap.load_file('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp')\n",
    "\n",
    "# load the prediction\n",
    "predictions\n",
    "\n",
    "# initilise the empty df to store the gt and pred\n",
    "df_pred = initialize_df()\n",
    "df_gt = initialize_df()\n",
    "\n",
    "# count to keep track of the number of penguins\n",
    "img_count = 0\n",
    "\n",
    "# for loop to step through each of the images\n",
    "for img in gt:    \n",
    "\n",
    "    # load the prediction image as well\n",
    "    img_pred = predictions[img_count]\n",
    "\n",
    "    # get the vid_id \n",
    "    vid_id = img.video.filename.split('/')[-2]\n",
    "    #print(img.video.filename)\n",
    "\n",
    "    # get the img_id\n",
    "    img_id = str(img.frame_idx)\n",
    "\n",
    "    # bbox id is each of the instances in the image of the gt\n",
    "    bbox_id_count = 0\n",
    "    # print(img.instances)\n",
    "\n",
    "    # bug fix\n",
    "    if img_count == 19:\n",
    "        print(vid_id, img_id)\n",
    "\n",
    "    # second for loop to step through instances\n",
    "    for instance in img.instances:\n",
    "\n",
    "        # get the bbox id (this is the instance)\n",
    "        bbox_id = bbox_id_count\n",
    "        # # print(instance)\n",
    "        # # print(instance.frame)\n",
    "        # print(instance.bounding_box) # the bounding box y1, x1, y2, x2\n",
    "        # print(instance.midpoint)  # this is (x2 - x1)/2 , (y2 -y2)/2\n",
    "        # # print(instance.skeleton) # the order of the points\n",
    "        # # print(instance.points_array)\n",
    "        # # print(instance.nodes_points)\n",
    "        \n",
    "        # get the body top and body bottom points\n",
    "        body_top_coord = instance.points_array[2]\n",
    "        body_bottom_coord = instance.points_array[5]\n",
    "        #print(body_top_coord)\n",
    "        #print(body_bottom_coord)\n",
    "\n",
    "        # flag for missing gt body top\n",
    "        missing_body_top = False\n",
    "        if any(map(lambda v: v is None or np.isnan(v), body_top_coord)):\n",
    "            missing_body_top = True\n",
    "\n",
    "        # check if body top or body bottom is missing and if so use width of the bbox\n",
    "        if any(map(lambda v: v is None or np.isnan(v), body_top_coord)) or any(map(lambda v: v is None or np.isnan(v), body_bottom_coord)):\n",
    "            # this takes the x coord and one of the y coords\n",
    "            body_top_coord[0] = instance.bounding_box[1]\n",
    "            body_top_coord[1] = instance.bounding_box[0]\n",
    "            body_bottom_coord[0] = instance.bounding_box[3]\n",
    "            body_bottom_coord[1] = instance.bounding_box[0]\n",
    "\n",
    "        # find the distance between them and divide by 2\n",
    "        dist_bodytop_bodybottom = distance_between_points(body_top_coord, body_bottom_coord)\n",
    "        dist_allowed = dist_bodytop_bodybottom/2\n",
    "\n",
    "        # set best score to zero\n",
    "        best_score = 0\n",
    "        #step through all the instances predicted in the  \n",
    "        for instance_pred in img_pred.instances:\n",
    "            #print(instance.nodes_points)\n",
    "            #print(instance_pred.points_array)\n",
    "\n",
    "            # check that the centre is within the distance allowed\n",
    "            dist_gt_and_pred_bodytop = distance_between_points(instance.points_array[2], instance_pred.points_array[2])\n",
    "\n",
    "            #ensure that there is an entry even if the kp is far off\n",
    "            if best_score == 0:\n",
    "                new_df_entry = create_new_df_entry(vid_id, img_id, bbox_id, instance_pred, pred=True)\n",
    "                best_score = instance_pred.score\n",
    "            \n",
    "            if dist_gt_and_pred_bodytop < dist_allowed:\n",
    "                # check if the current score is better than the best score\n",
    "                if instance_pred.score > best_score: \n",
    "                    # best score make this the new best score \n",
    "                    best_score = instance_pred.score\n",
    "                    #update the dict entry\n",
    "                    new_df_entry = create_new_df_entry(vid_id, img_id, bbox_id, instance_pred, pred=True)\n",
    "\n",
    "        # add the best fit to the pred df\n",
    "        df_pred = df_pred.append(new_df_entry, ignore_index=True)\n",
    "\n",
    "        # now create the entry for the gt. This just ensures that they are in the correct order\n",
    "        new_df_entry_gt = create_new_df_entry(vid_id, img_id, bbox_id, instance, pred=False)\n",
    "        df_gt = df_gt.append(new_df_entry_gt, ignore_index=True)\n",
    "\n",
    "        # iterate the bbox_count\n",
    "        bbox_id_count += 1\n",
    "\n",
    "    # iterate the img count\n",
    "    img_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.4.3. save the predictions as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/a_results'\n",
    "centroid_model = path_to_centroid_model_folder.split('/')[-1]\n",
    "centered_instnace_model = path_to_centered_instance_model_folder.split('/')[-1]\n",
    "path_pred = f'{base_path}/{centroid_model}__{centered_instnace_model}_pred.json'\n",
    "path_gt = f'{base_path}/{centroid_model}__{centered_instnace_model}_gt.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/a_results'\n",
    "bottom_up_model = path_to_bottom_up.split('/')[-1]\n",
    "path_pred = f'{base_path}/{bottom_up_model}_pred.json'\n",
    "path_gt = f'{base_path}/{bottom_up_model}_gt.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_json(df_pred, path_pred)\n",
    "df_to_json(df_gt, path_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8. evaluate performance on converted sleap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.1. Extract keypoints from json and save to y_true and y_pred arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pred and gt to df\n",
    "df_pred = json_to_df('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/a_results/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4__Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1_pred.json')\n",
    "df_gt = json_to_df('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/a_results/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4__Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1_gt.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pred and gt to df\n",
    "df_pred = json_to_df('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/a_results/Oct9_test3_bottomup_baseline_1_pred.json')\n",
    "df_gt = json_to_df('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/a_results/Oct9_test3_bottomup_baseline_1_gt.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract keypoints from a df and save them to an array.\n",
    "# The resulting array should have the shape (number of entries in df, num_keypoints)\n",
    "y_pred_arr = df_pred.iloc[:,7:23].values\n",
    "y_gt_arr = df_gt.iloc[:,7:23].values\n",
    "# print(y_pred_arr.shape) \n",
    "# print(y_pred_arr[0:3])\n",
    "# df_pred.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt_arr[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.2. PCK evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pck_metric_arr(y_true, y_pred, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Computes the Percentage of Correct Keypoints (PCK) metric.\n",
    "    Mask is Nan not -10 \n",
    "    \n",
    "    Parameters:\n",
    "    y_true arr: The ground truth keypoints (batch_size, num_keypoints*2).\n",
    "    y_pred arr: The predicted keypoints (batch_size, num_keypoints*2).\n",
    "    threshold (float): The distance threshold for a keypoint to be considered correct.\n",
    "                       Typically set relative to the size of the bounding box (e.g., 0.2).\n",
    "    \n",
    "    Returns:\n",
    "    float: The percentage of correct keypoints.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible (not equal to -10) CHANGED TO NAN\n",
    "    #mask = (y_true != -10.0).float().to(y_true.device)\n",
    "    mask = ~np.isnan(y_true)#.astype(float)\n",
    "    #print(mask)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints\n",
    "    # Apply the mask to filter out NaN values (setting them to zero)\n",
    "    y_true_masked = np.where(mask, y_true, 0.0)  # Set NaNs to 0.0 in y_true\n",
    "    y_pred_masked = np.where(mask, y_pred, 0.0)  # Set corresponding values in y_pred to 0.0\n",
    "    # print(y_true_masked[0])\n",
    "    # print(y_pred_masked[0])\n",
    "\n",
    "    # Compute the Euclidean distance between the predicted and true keypoints\n",
    "    distances = np.sqrt((y_pred_masked[:, ::2] - y_true_masked[:, ::2]) ** 2 +\n",
    "                           (y_pred_masked[:, 1::2] - y_true_masked[:, 1::2]) ** 2)\n",
    "    # print(distances[0])\n",
    "    \n",
    "    # Normalize the distances (relative to the max and min y coord)\n",
    "    #Norm_max_min_kp = torch.max(y_true_masked[:, 1::2], dim=1)[0] - torch.min(y_true_masked[:, 1::2], dim=1)[0]\n",
    "    Norm_max_min_kp = np.nanmax(y_true_masked[:, 1::2], axis=1) - np.nanmin(y_true_masked[:, 1::2], axis=1)\n",
    "    # Normalise based on the distance between the head and the bottom of the body (position 0, 1 and )\n",
    "    Norm_head_lowerbody = np.sqrt((y_true[:, 0] - y_true[:,10]) ** 2 +\n",
    "                        (y_true[:, 1] - y_true[:, 11]) ** 2)\n",
    "    #print(Norm_head_lowerbody[0])\n",
    "\n",
    "    # Normalize distances by the head to lower body distance\n",
    "    normalized_distances = distances / Norm_head_lowerbody[:, None]\n",
    "    #print(normalized_distances[0])\n",
    "\n",
    "    # Count the correct keypoints (distance <= threshold)\n",
    "    correct_keypoints = (normalized_distances <= threshold) * mask[:, ::2]\n",
    "    #print(correct_keypoints[0])\n",
    "\n",
    "    # Calculate the PCK as the percentage of correct keypoints\n",
    "    pck = correct_keypoints.sum() / mask[:, ::2].sum()\n",
    "    return pck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pck_metric_arr(y_gt_arr,y_pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pck_per_kp_metric_arr(y_true, y_pred, num_keypoints=8, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Evaluates the average PCK for each keypoint individually..\n",
    "    Mask is Nan not -10 \n",
    "    \n",
    "    Parameters:\n",
    "    y_true arr: The ground truth keypoints (batch_size, num_keypoints*2).\n",
    "    y_pred arr: The predicted keypoints (batch_size, num_keypoints*2).\n",
    "    num_keypoints: The number of keypoints in the dataset.\n",
    "    threshold (float): The distance threshold for a keypoint to be considered correct.\n",
    "                       Typically set relative to the size of the bounding box (e.g., 0.2).\n",
    "    \n",
    "    Returns:\n",
    "    float: The percentage of correct keypoints.\n",
    "    \"\"\"\n",
    "    # initialise variables\n",
    "    total_pck_per_keypoint = np.zeros(num_keypoints)\n",
    "    total_visable_kp = np.zeros(num_keypoints)\n",
    "\n",
    "    # Create a mask where keypoints are visible (not equal to -10) CHANGED TO NAN\n",
    "    #mask = (y_true != -10.0).float().to(y_true.device)\n",
    "    mask = ~np.isnan(y_true)#.astype(float)\n",
    "    #print(mask)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints\n",
    "    # Apply the mask to filter out NaN values (setting them to zero)\n",
    "    y_true_masked = np.where(mask, y_true, 0.0)  # Set NaNs to 0.0 in y_true\n",
    "    y_pred_masked = np.where(mask, y_pred, 0.0)  # Set corresponding values in y_pred to 0.0\n",
    "    # print(y_true_masked[0])\n",
    "    # print(y_pred_masked[0])\n",
    "\n",
    "    # Compute the Euclidean distance between the predicted and true keypoints\n",
    "    distances = np.sqrt((y_pred_masked[:, ::2] - y_true_masked[:, ::2]) ** 2 +\n",
    "                           (y_pred_masked[:, 1::2] - y_true_masked[:, 1::2]) ** 2)\n",
    "    # print(distances[0])\n",
    "    \n",
    "    # Normalize the distances (relative to the max and min y coord)\n",
    "    #Norm_max_min_kp = torch.max(y_true_masked[:, 1::2], dim=1)[0] - torch.min(y_true_masked[:, 1::2], dim=1)[0]\n",
    "    Norm_max_min_kp = np.nanmax(y_true_masked[:, 1::2], axis=1) - np.nanmin(y_true_masked[:, 1::2], axis=1)\n",
    "    # Normalise based on the distance between the head and the bottom of the body (position 0, 1 and )\n",
    "    Norm_head_lowerbody = np.sqrt((y_true[:, 0] - y_true[:,10]) ** 2 +\n",
    "                        (y_true[:, 1] - y_true[:, 11]) ** 2)\n",
    "    # print(Norm_head_lowerbody[0])\n",
    "\n",
    "    # Normalize distances by the head to lower body distance\n",
    "    normalized_distances = distances / Norm_head_lowerbody[:, None]\n",
    "    # print(normalized_distances[0])\n",
    "\n",
    "    # Compute correct keypoints (distance <= threshold) for each keypoint\n",
    "    correct_keypoints_per_keypoint = (normalized_distances <= threshold) * mask[:, ::2]\n",
    "    # print(correct_keypoints_per_keypoint[0])\n",
    "\n",
    "    # Accumulate PCK per keypoint\n",
    "    total_pck_per_keypoint = np.sum(correct_keypoints_per_keypoint, axis=0)\n",
    "    total_visable_kp = np.sum(mask[:, ::2], axis=0)\n",
    "    # print(total_pck_per_keypoint)\n",
    "    # print(total_visable_kp)\n",
    "\n",
    "    # Calculate the PCK as the percentage of correct keypoints\n",
    "    # pck = correct_keypoints.sum() / mask[:, ::2].sum()\n",
    "\n",
    "    # Accumulate PCK per keypoint\n",
    "    keypoint_pcks = (total_pck_per_keypoint / total_visable_kp)\n",
    "    return keypoint_pcks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pck_per_kp_metric_arr(y_gt_arr, y_pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full pck evaluation\n",
    "def full_pck_evaluation_arr(y_gt_arr,y_pred_arr):\n",
    "\n",
    "    print('calculating PCK ...')\n",
    "\n",
    "    # create lists for pck at different thresholds\n",
    "    avg_pck_test_list = []\n",
    "    #avg_pck_val_list = []\n",
    "    #avg_pck_per_kp_val_list = []\n",
    "    avg_pck_per_kp_test_list = []\n",
    "    \n",
    "    # create a for loop to get PCK at 0.01 to 0.2\n",
    "    for i in range (1, 21):\n",
    "\n",
    "        # get pck threshold\n",
    "        pck_threshold = (i/100)\n",
    "\n",
    "        # calculate average pck\n",
    "        #avg_pck_val = pck_metric_arr(y_gt_arr,y_pred_arr)\n",
    "        avg_pck_test = pck_metric_arr(y_gt_arr,y_pred_arr, threshold=pck_threshold)\n",
    "\n",
    "        # calculate average pck per kp\n",
    "        #avg_pck_per_kp_val = evaluate_pck_per_keypoint(model, val_dataloader, threshold=pck_threshold)\n",
    "        avg_pck_per_kp_test = pck_per_kp_metric_arr(y_gt_arr,y_pred_arr, threshold=pck_threshold)\n",
    "\n",
    "        if i == 5:\n",
    "            # capture pck@0.05\n",
    "            #avg_pck_val_005 = avg_pck_val\n",
    "            avg_pck_test_005 = avg_pck_test\n",
    "\n",
    "        if i == 10:\n",
    "            # capture pck@0.1\n",
    "            #avg_pck_val_01 = avg_pck_val\n",
    "            avg_pck_test_01 = avg_pck_test\n",
    "\n",
    "        if i == 20:\n",
    "            # capture pck@0.2\n",
    "            #avg_pck_val_02 = avg_pck_val\n",
    "            avg_pck_test_02 = avg_pck_test\n",
    "\n",
    "        # save to lists\n",
    "        avg_pck_test_list.append(avg_pck_test)\n",
    "        #avg_pck_val_list.append(avg_pck_val)\n",
    "        #avg_pck_per_kp_val_list.append(avg_pck_per_kp_val)\n",
    "        avg_pck_per_kp_test_list.append(avg_pck_per_kp_test)\n",
    "\n",
    "    # return avg_pck_val_list, avg_pck_test_list, avg_pck_per_kp_val_list, avg_pck_per_kp_test_list, \\\n",
    "    #     avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02\n",
    "    return avg_pck_test_list, avg_pck_per_kp_test_list, avg_pck_test_005, avg_pck_test_01, avg_pck_test_02    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pck_test_list, avg_pck_per_kp_test_list, avg_pck_test_005, avg_pck_test_01, avg_pck_test_02 = full_pck_evaluation_arr(y_gt_arr, y_pred_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.3. Visual inspection and plot comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gt and pred df from the json (done previously)\n",
    "df_gt\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gt and pred kp array (done previously)\n",
    "y_gt_arr\n",
    "y_pred_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create img array and id list\n",
    "def extract_images_and_ids(df, img_dir_path):\n",
    "    \"\"\"\n",
    "    Function to step through a DataFrame, extract video ID and image ID for each entry, find the corresponding image\n",
    "    in the image directory, and return a list of images and associated video-image IDs.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing the 'vid_id' and 'img_id' fields.\n",
    "        img_dir_path (str): Path to the directory containing the images.\n",
    "    \n",
    "    Returns:\n",
    "        images_array (np.ndarray): Array of images with shape (num_img, width, height, 3).\n",
    "        id_list (list): List of 'vid_id_img_id' corresponding to each image in the array.\n",
    "    \"\"\"\n",
    "    images_list = []\n",
    "    id_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Extract 'vid_id' and 'img_id' from the current row\n",
    "        vid_id = row['vid_id']\n",
    "        img_id = row['img_id']\n",
    "        \n",
    "        # Construct the file name to search for\n",
    "        file_name = f\"frame_{vid_id}.mp4_{img_id}.jpg\"\n",
    "        file_path = os.path.join(img_dir_path, file_name)\n",
    "        \n",
    "        # Load the image using cv2\n",
    "        if os.path.exists(file_path):\n",
    "            img = cv2.imread(file_path)\n",
    "            if img is not None:\n",
    "                images_list.append(img)\n",
    "                id_list.append(f\"{vid_id}_{img_id}\")\n",
    "            else:\n",
    "                print(f\"Error loading image: {file_path}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Convert the list of images to a NumPy array with shape (num_img, width, height, 3)\n",
    "    if images_list:\n",
    "        images_array = np.array(images_list)\n",
    "    else:\n",
    "        images_array = np.empty((0, 0, 0, 3))  # Empty array if no images found\n",
    "    \n",
    "    return images_array, id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr, id_list = extract_images_and_ids(df_gt, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_sleap(img, pred_keypoints, true_keypoints, id, save_dir, nkeypoints=8, keypoint_labels=None, connections = [(0, 1), (0, 2), (2, 3), (2, 4), (2, 5), (5, 6), (5, 7)]):\n",
    "    \"\"\"\n",
    "    Plots predicted keypoints vs. ground truth keypoints on the same image.\n",
    "\n",
    "    Parameters:\n",
    "    - img: The image on which to plot the keypoints.\n",
    "    - pred_keypoints: The predicted keypoints (flattened x, y coordinates).\n",
    "    - true_keypoints: The ground truth keypoints (flattened x, y coordinates).\n",
    "    - id_: ids for naming (vid_id_img_id)\n",
    "    - save_dir: Directory to save the result to\n",
    "    - nkeypoints:  Optional The number of keypoints (default=8).\n",
    "    - keypoint_labels: Optional list of keypoint labels to display next to the keypoints.\n",
    "    - connections: OPtional list of tupels defining the connections between kps\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12), dpi=150)\n",
    "    plt.imshow(img)\n",
    "\n",
    "    # create a mask for the predicted keypoints so don't disply them\n",
    "    mask = ~np.isnan(true_keypoints)\n",
    "    pred_keypoints = np.where(mask, pred_keypoints, np.nan)\n",
    "    \n",
    "    # Extract x and y coordinates for predicted keypoints\n",
    "    pred_x_keypoints = pred_keypoints[::2]\n",
    "    pred_y_keypoints = pred_keypoints[1::2]\n",
    "    \n",
    "    # Extract x and y coordinates for ground truth keypoints\n",
    "    true_x_keypoints = true_keypoints[::2]\n",
    "    true_y_keypoints = true_keypoints[1::2]\n",
    "\n",
    "    # Plot skeleton for true keypoints\n",
    "    for (i, j) in connections:\n",
    "        plt.plot([true_x_keypoints[i], true_x_keypoints[j]], \n",
    "                 [true_y_keypoints[i], true_y_keypoints[j]], \n",
    "                 'r-', linewidth=1)\n",
    "\n",
    "    # Plot skeleton for predicted keypoints\n",
    "    for (i, j) in connections:\n",
    "        plt.plot([pred_x_keypoints[i], pred_x_keypoints[j]], \n",
    "                 [pred_y_keypoints[i], pred_y_keypoints[j]], \n",
    "                 'g-', linewidth=1)\n",
    "    \n",
    "    # Plot predicted keypoints\n",
    "    plt.scatter(pred_x_keypoints, pred_y_keypoints, marker='o', c='g', s=100, label='Predicted', edgecolor='black')\n",
    "    \n",
    "    # Plot ground truth keypoints\n",
    "    plt.scatter(true_x_keypoints, true_y_keypoints, marker='x', c='r', s=100, label='Ground Truth')\n",
    "    \n",
    "    # If labels are provided, add them to the plot\n",
    "    if keypoint_labels is not None:\n",
    "        for i, (x, y) in enumerate(zip(true_x_keypoints, true_y_keypoints)):\n",
    "            plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "                     bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "    if keypoint_labels is not None:\n",
    "        for i, (x, y) in enumerate(zip(pred_x_keypoints, pred_y_keypoints)):\n",
    "            plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "                     bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # Add a legend to differentiate between predicted and ground truth keypoints\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the plot\n",
    "  \n",
    "    plot_path = os.path.join(save_dir, f'Pred_vs_GT_img_{id}.png')\n",
    "    plt.savefig(plot_path)\n",
    "    #print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step through all the all the predictions and save them to a folder\n",
    "def plot_sleap_gt_vs_pred_dataset_save(y_gt_arr, y_pred_arr, img_arr, id_list, save_dir):\n",
    "\n",
    "    # set the labels\n",
    "    labels = labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'RFoot', 'LFoot']\n",
    "\n",
    "    #step through the gt array of kp\n",
    "    for i, gt_pred_single in enumerate(y_gt_arr):\n",
    "        plot_comparison_sleap(img_arr[i], y_pred_arr[i], y_gt_arr[i], id_list[i], save_dir, keypoint_labels=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/a_results/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4__Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/a_results/Oct9_test3_bottomup_baseline_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sleap_gt_vs_pred_dataset_save(y_gt_arr, y_pred_arr, img_arr, id_list, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.4. Find GLOPs, total params, GPU and CPU inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models \n",
    "centroid_model = tf.keras.models.load_model('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4/best_model.h5', compile=False)\n",
    "center_instance_model = tf.keras.models.load_model('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1/best_model.h5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.4.1. Find the GFlops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "total_flops = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.4.1. GFLOPs for the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.profiler.model_analyzer import profile\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "# Disable eager execution to enable graph-based profiling\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# load the models \n",
    "centroid_model = tf.keras.models.load_model('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4/best_model.h5', compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy input to pass through the model\n",
    "input_shape = (1, 288, 480, 3)  # Adjust input shape to match your model\n",
    "#dummy_input = tf.random.normal(input_shape)\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, shape=input_shape)\n",
    "# Pass the placeholder through your model\n",
    "outputs = centroid_model(inputs)\n",
    "\n",
    "# Create a session and use it to run the graph\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # Profile the model for FLOPs\n",
    "    flops = profile(\n",
    "        tf.compat.v1.get_default_graph(),\n",
    "        options=ProfileOptionBuilder.float_operation()\n",
    "    )\n",
    "\n",
    "    print('FLOPs: {}'.format(flops.total_float_ops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.8.4.1. GFLOPs for the center_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.profiler.model_analyzer import profile\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable eager execution to enable graph-based profiling\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models \n",
    "#model = sleap.load_model([path_to_centroid_model_folder, path_to_centered_instance_model_folder], batch_size=1, peak_threshold=0.0, progress_reporting='json', max_instances=4)\n",
    "#centroid_model = sleap.load_model('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4')\n",
    "#centroid_model = tf.keras.models.load_model('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4/best_model.h5')\n",
    "center_instance_model = tf.keras.models.load_model('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1/best_model.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_instance_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy input to pass through the model\n",
    "input_shape = (1, 1024, 1024, 3)  # Adjust input shape to match your model\n",
    "#dummy_input = tf.random.normal(input_shape)\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.framework import ops\n",
    "# from tensorflow.python.profiler.model_analyzer import profile\n",
    "# from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "# Disable eager execution to enable graph-based profiling\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# # Load or build your model\n",
    "# model = ...  # Replace with your model\n",
    "\n",
    "# # Create a dummy input with the same input shape as your model expects\n",
    "# input_shape = (1, 224, 224, 3)  # Replace with your actual input shape\n",
    "# dummy_input = tf.random.normal(input_shape)\n",
    "\n",
    "# Pass the placeholder through your model\n",
    "outputs = center_instance_model(inputs)\n",
    "\n",
    "# Create a session and use it to run the graph\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # Profile the model for FLOPs\n",
    "    flops = profile(\n",
    "        tf.compat.v1.get_default_graph(),\n",
    "        options=ProfileOptionBuilder.float_operation()\n",
    "    )\n",
    "\n",
    "    print('FLOPs: {}'.format(flops.total_float_ops))\n",
    "\n",
    "# # Create a session and a default graph\n",
    "# with tf.compat.v1.Session() as sess:\n",
    "#     # Create a graph for the model\n",
    "#     tf.compat.v1.keras.backend.set_session(sess)\n",
    "#     center_instance_model(dummy_input)  # Run the dummy input through the model\n",
    "\n",
    "#     # Profile the model for FLOPs\n",
    "#     flops = profile(\n",
    "#         tf.compat.v1.get_default_graph(),\n",
    "#         options=ProfileOptionBuilder.float_operation()\n",
    "#     )\n",
    "\n",
    "#     print('FLOPs: {}'.format(flops.total_float_ops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use TensorFlow Profiler to measure performance\n",
    "# log_dir = \"./logdir\"  # Directory where profiler logs will be stored\n",
    "\n",
    "# @tf.function\n",
    "# def model_fn(input_tensor):\n",
    "#     return model(input_tensor)\n",
    "\n",
    "# # Trace the model with a dummy input\n",
    "# tf.profiler.experimental.start(log_dir)\n",
    "# model_fn(dummy_input)\n",
    "# tf.profiler.experimental.stop()\n",
    "\n",
    "# print(f\"Profiling complete. View results in TensorBoard with `tensorboard --logdir={log_dir}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NONE OF THE BELOW HAS WORKED\n",
    "# model.predict(test_sleap_gt_labels)\n",
    "# path_to_centroid_model_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4'\n",
    "# path_to_centered_instance_model_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1'\n",
    "# # try the CLI\n",
    "# ! sleap-export --model /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4 --model /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1 --max_instances 1\n",
    "# predictor.export_model('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/a_results/model_export')\n",
    "# #predictions = model.predict(test_sleap_gt_labels)\n",
    "# model.export_model('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/a_results/model_export')#, max_instances=4)\n",
    "# predictor.inference_model.build\n",
    "# predictor.inference_model.input_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.python.profiler.model_analyzer import profile\n",
    "# from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "# # Build or load your TensorFlow model\n",
    "# model = ...  # Replace with your SLEAP model or another TF model\n",
    "\n",
    "# # Create a dummy input to pass through the model\n",
    "# input_shape = (1, 224, 224, 3)  # Adjust input shape to match your model\n",
    "# dummy_input = tf.random.normal(input_shape)\n",
    "\n",
    "# # Profile the model for FLOPs\n",
    "# flops = tf.profiler.profile(\n",
    "#     tf.compat.v1.get_default_graph(),\n",
    "#     options=ProfileOptionBuilder.float_operation()\n",
    "# )\n",
    "\n",
    "# print('FLOPs: {}'.format(flops.total_float_ops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.4.2 Find the total params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_params(model):\n",
    "    \"\"\"\n",
    "    Get the total number of trainable parameters in a TensorFlow model.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): The TensorFlow model.\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    for layer in model.trainable_weights:\n",
    "        total_params += tf.size(layer).numpy()\n",
    "    return total_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = get_number_of_params(centroid_model)\n",
    "print(f'Total number of trainable parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = get_number_of_params(center_instance_model)\n",
    "print(f'Total number of trainable parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.4.3. Find the inference time in seconds using a gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_inference_time(model, input_data):\n",
    "    \"\"\"\n",
    "    Get the average inference time of a TensorFlow model on GPU in milliseconds over 30 runs.\n",
    "    The average is calculated for the last 20 runs.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): The TensorFlow model.\n",
    "        input_data (tf.Tensor): The input data for inference.\n",
    "        \n",
    "    Returns:\n",
    "        float: The average inference time in milliseconds for the last 20 runs.\n",
    "    \"\"\"\n",
    "    inference_times = []\n",
    "    for _ in range(40):\n",
    "        start_time = time.time()\n",
    "        _ = model(input_data, training=False)\n",
    "        end_time = time.time()\n",
    "        inference_time_ms = (end_time - start_time) * 1000\n",
    "        inference_times.append(inference_time_ms)\n",
    "    \n",
    "    # Calculate the average inference time for the last 20 runs\n",
    "    average_inference_time = sum(inference_times[-20:]) / 20\n",
    "    return average_inference_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(centroid_model.input_shape)\n",
    "print(center_instance_model.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid\n",
    "input_data_centroid = tf.random.normal((1, 288, 480, 3))  #\n",
    "inference_time = get_inference_time(centroid_model, input_data_centroid)\n",
    "print(f'Inference time on GPU: {inference_time:.2f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#center_instance\n",
    "input_data_center_instance = tf.random.normal((1, 1024, 1024, 3))  #\n",
    "inference_time = get_inference_time(center_instance_model, input_data_center_instance)\n",
    "print(f'Inference time on GPU: {inference_time:.2f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_inf_time = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.4.3. Find the inference time in seconds using a cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_inf_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_inference_time_cpu(model, input_data):\n",
    "    \"\"\"\n",
    "    Get the average inference time of a TensorFlow model on CPU in milliseconds over 30 runs.\n",
    "    The average is calculated for the last 20 runs.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): The TensorFlow model.\n",
    "        input_data (tf.Tensor): The input data for inference.\n",
    "        \n",
    "    Returns:\n",
    "        float: The average inference time in milliseconds for the last 20 runs.\n",
    "    \"\"\"\n",
    "    with tf.device('/CPU:0'):\n",
    "        inference_times = []\n",
    "        for _ in range(40):\n",
    "            start_time = time.time()\n",
    "            _ = model(input_data, training=False)\n",
    "            end_time = time.time()\n",
    "            inference_time_ms = (end_time - start_time) * 1000\n",
    "            inference_times.append(inference_time_ms)\n",
    "        \n",
    "        # Calculate the average inference time for the last 20 runs\n",
    "        average_inference_time = sum(inference_times[-20:]) / 20\n",
    "    return average_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid\n",
    "input_data_centroid = tf.random.normal((1, 288, 480, 3))  #\n",
    "inference_time = get_inference_time_cpu(centroid_model, input_data_centroid)\n",
    "print(f'Inference time on CPU: {inference_time:.2f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#center_instance\n",
    "input_data_center_instance = tf.random.normal((1, 1024, 1024, 3))  #\n",
    "inference_time = get_inference_time_cpu(center_instance_model, input_data_center_instance)\n",
    "print(f'Inference time on CPU: {inference_time:.2f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.5 Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place holders\n",
    "avg_pck_val_005 = None\n",
    "avg_pck_val_01 = None\n",
    "avg_pck_val_02 = None\n",
    "avg_pck_per_kp_val_list =[]\n",
    "param_dict = {}\n",
    "flops_extend = {}\n",
    "avg_pck_val_list = []\n",
    "\n",
    "num_train_imgs = 360\n",
    "num_val_imgs = 60\n",
    "num_test_imgs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results to a dict\n",
    "def load_results_to_dict(save_dir, avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02,\n",
    "                         total_params, total_flops, gpu_inf_time, cpu_inf_time, param_dict, flops_extend, avg_pck_test_list, \n",
    "                         avg_pck_per_kp_test_list, avg_pck_val_list, avg_pck_per_kp_val_list, num_train_imgs, num_val_imgs, num_test_imgs):\n",
    "\n",
    "    description = save_dir.split('/')[-1]\n",
    "    #print(description)\n",
    "\n",
    "    #avg_pck_test_dict = load_pck_to_dict(avg_pck_test_list)\n",
    "    avg_pck_per_kp_test_dict = load_pck_to_dict(avg_pck_per_kp_test_list)\n",
    "    #avg_pck_val_dict = load_pck_to_dict(avg_pck_val_list)\n",
    "    avg_pck_per_kp_val_dict = load_pck_to_dict(avg_pck_per_kp_val_list)\n",
    "\n",
    "    results_dict = {\n",
    "    'description': '',  # Placeholder for a string description\n",
    "    'pck005': None,  # Placeholder for PCK@0.05 variable\n",
    "    'pck01': None,  # Placeholder for PCK@0.1 variable\n",
    "    'pck02': None,  # Placeholder for PCK@0.2 variable\n",
    "    'total_params': None,  # Placeholder for total parameters variable\n",
    "    'GFLOPs': None,  # Placeholder for GFLOPs variable\n",
    "    'GPU_inf(ms)': None,  # Placeholder for GPU inference time variable\n",
    "    'CPU_inf(ms)': None,  # Placeholder for CPU inference time variable\n",
    "    'param_dict': {},  # Placeholder for parameter dictionary\n",
    "    'flops_dict': {},  # Placeholder for FLOPs dictionary\n",
    "    'PCK001-02': [],  # Placeholder for PCK@0.01-0.2 list\n",
    "    'PCK001-02_per_kp': {},  # Placeholder for PCK per joint dictionary\n",
    "    'val_PCK001-02': [],  # Placeholder for PCK@0.01-0.2 list\n",
    "    'val_PCK001-02_per_kp': {},  # Placeholder for PCK per joint dictionary\n",
    "    'val_pck005': None,  # Placeholder for PCK@0.05 variable\n",
    "    'val_pck01': None,  # Placeholder for PCK@0.1 variable\n",
    "    'val_pck02': None,  # Placeholder for PCK@0.2 variable\n",
    "    'num_train_imgs': None, # number of train imgs\n",
    "    'num_val_imgs': None, # number of train imgs\n",
    "    'num_test_imgs': None, # number of train imgs\n",
    "}\n",
    "    \n",
    "    results_dict['description'] = description\n",
    "    results_dict['pck005'] = avg_pck_test_005 \n",
    "    results_dict['pck01'] = avg_pck_test_01  \n",
    "    results_dict['pck02'] = avg_pck_test_02 \n",
    "    results_dict['total_params'] = total_params  \n",
    "    results_dict['GFLOPs'] = (total_flops/1e9)\n",
    "    results_dict['GPU_inf(ms)'] = gpu_inf_time*1000  \n",
    "    results_dict['CPU_inf(ms)'] = cpu_inf_time*1000  \n",
    "    results_dict['param_dict'] = param_dict  \n",
    "    results_dict['flops_dict'] = flops_extend \n",
    "    results_dict['PCK001-02'] = avg_pck_test_list\n",
    "    results_dict['PCK001-02_per_kp'] = avg_pck_per_kp_test_dict\n",
    "    results_dict['val_PCK001-02'] = avg_pck_val_list\n",
    "    results_dict['val_PCK001-02_per_kp'] = avg_pck_per_kp_val_dict\n",
    "    results_dict['val_pck005'] = avg_pck_val_005 \n",
    "    results_dict['val_pck01'] = avg_pck_val_01  \n",
    "    results_dict['val_pck02'] = avg_pck_val_02\n",
    "    results_dict['num_train_imgs'] = num_train_imgs\n",
    "    results_dict['num_val_imgs'] = num_val_imgs\n",
    "    results_dict['num_test_imgs'] = num_test_imgs\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results_to_dict(save_dir, avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02,\n",
    "                         total_params, total_flops, gpu_inf_time, cpu_inf_time, param_dict, flops_extend, avg_pck_test_list, \n",
    "                         avg_pck_per_kp_test_list, avg_pck_val_list, avg_pck_per_kp_val_list, num_train_imgs, num_val_imgs, num_test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_json(results, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Model evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trained SLEAP model will be a folder containing files that specify metadata that is useful for evaluation and analysis. The exact set of files may depend on the configuration, but all models will come with:\n",
    "\n",
    "    metrics.train.npz: Metrics for the training split.\n",
    "\n",
    "    metrics.val.npz: Metrics for the validation split. This is what you’ll want to use most of the time since it wasn’t directly used for optimizing the model.\n",
    "\n",
    "    Note: A test split will also be evaluated if it was provided during training and saved to metrics.test.npz.\n",
    "\n",
    "Additionally, the following files are included and may also be useful:\n",
    "\n",
    "    best_model.h5: The actual saved model and weights. This can be loaded with tf.keras.model.load_model() but it is recommended to use sleap.load_model() instead as it takes care of adding some additional inference-only procedures.\n",
    "\n",
    "    training_config.json: The configuration for the model training job, including metadata inferred during the training procedure. It can be loaded with sleap.load_config().\n",
    "\n",
    "    labels_gt.train.slp and labels_pr.train.slp: These are SLEAP labels files containing the ground truth and predicted points for the training split. They do not contain the images, but can be used to retrieve the poses used.\n",
    "\n",
    "    labels_gt.val.slp and labels_pr.val.slp: These are SLEAP labels files containing the ground truth and predicted points for the validation split. They do not contain the images, but can be used to retrieve the poses used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLEAP metrics can be loaded using the sleap.load_metrics() API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sleap.load_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.0. Inspect the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the ground truth val dataset\n",
    "!sleap-inspect /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch/labels_gt.val.slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the predicted val dataset\n",
    "!sleap-inspect /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch/labels_pr.val.slp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Comparing the datasets, max instances varies significantly. It should be a single instance per frame. Seeing 0 and > 1 in the predictions is not good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_gt_c = sleap.load_file('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch/labels_gt.val.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_val_gt_c.export_csv('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch/labels_gt.val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_gt_c_dict = labels_val_gt_c.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a JSON file with pretty formatting\n",
    "with open(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch/labels_gt.val.json\", \"w\") as json_file:\n",
    "    json.dump(labels_val_gt_c_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_gt_c.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_pr_c = sleap.load_file('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch/labels_pr.val.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_pr_c.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_pr_c_dict = labels_val_pr_c.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a JSON file with pretty formatting\n",
    "with open(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch/labels_pr.val.json\", \"w\") as json_file:\n",
    "    json.dump(labels_val_pr_c_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: so only 35 frames were found to have instances in them. The frames without instances are primarily the penguins lying down. \n",
    "\n",
    "Are these penguins missing the body_top keypoint?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.1. Looking at the available metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the metrics for the validation split of the model we can see all of the available keys:\n",
    "print('CENTROID')\n",
    "metrics_c = sleap.load_metrics(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch\", split=\"val\")\n",
    "print(\"\\n\".join(metrics_c.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.2. Visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation (Visibility):\n",
    "\n",
    "    True Positives (TP): Cases where the model correctly predicted that a keypoint is visible.\n",
    "    False Positives (FP): Cases where the model predicted a keypoint is visible when it is not (false alarm).\n",
    "    True Negatives (TN): Cases where the model correctly predicted that a keypoint is not visible (occluded).\n",
    "    False Negatives (FN): Cases where the model predicted a keypoint is not visible when it actually is (missed detection).\n",
    "    Precision: The proportion of correctly predicted visible keypoints (TP) out of all predicted visible keypoints (TP + FP).\n",
    "    Recall: The proportion of actual visible keypoints that were correctly predicted (TP) out of all actual visible keypoints (TP + FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display visibility-related metrics\n",
    "print('\\nVisibility Metrics:')\n",
    "print(\"True Positives (TP):\", metrics_c[\"vis.tp\"])\n",
    "print(\"False Positives (FP):\", metrics_c[\"vis.fp\"])\n",
    "print(\"True Negatives (TN):\", metrics_c[\"vis.tn\"])\n",
    "print(\"False Negatives (FN):\", metrics_c[\"vis.fn\"])\n",
    "print(\"Precision:\", metrics_c[\"vis.precision\"])\n",
    "print(\"Recall:\", metrics_c[\"vis.recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS A MISLEADING METRIC FOR THIS... <br>\n",
    "<br>\n",
    "ONLY 35 OF THE FRAMES WERE FOUND TO HAVE INSTANCES OUT OF 60 (35 TP) <br>\n",
    "THERE WERE 49 INSTANCES FOUND. MEANING 14 INSTANCES WERE INCORRECTLY FOUND (14 FP)<br>\n",
    "25 INSTANCES WERE MISSED (25 FN)<br>\n",
    "AND THERE WERE NO TRUE NEGATIVES (0 TN)<br>\n",
    "<br>\n",
    "THEREFORE:<br>\n",
    "<br>\n",
    "<br>\n",
    "Visibility Metrics:<br>\n",
    "True Positives (TP): 35<br>\n",
    "False Positives (FP): 14<br>\n",
    "True Negatives (TN): 0<br>\n",
    "False Negatives (FN): 25<br>\n",
    "Precision: 0.71<br>\n",
    "Recall: 0.58<br>\n",
    "\n",
    "so we are missing over 40% of the penguins... Not good..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: this is weird. As I understand it, there should be a single centroid per instance and there should be 60 instances. And from analysis of the datasets we can see the the predictions do not match the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.3. Error distance (means very little for the centroid) - however it is interesting to see that this algorithm seems to work better than the top-down one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist.frame_idxs:\n",
    "\n",
    "    What it is: Indices of the frames in the dataset where the distances are calculated.\n",
    "    How it's calculated: The indices correspond to the frame positions in the video(s) being evaluated.\n",
    "\n",
    "dist.video_paths:\n",
    "\n",
    "    What it is: Paths to the video files associated with the frames being evaluated.\n",
    "    How it's calculated: These are the file paths where the videos used for evaluation are stored.\n",
    "\n",
    "dist.dists:\n",
    "\n",
    "    What it is: The raw distances (in pixels) between predicted and ground truth keypoints for each frame.\n",
    "    How it's calculated: Euclidean distance is computed for each keypoint across the dataset between the predicted and ground truth positions.\n",
    "\n",
    "dist.avg:\n",
    "\n",
    "    What it is: The average distance error across all frames and keypoints.\n",
    "    How it's calculated: The mean of the dist.dists values across all keypoints and frames.\n",
    "\n",
    "dist.p50:\n",
    "\n",
    "    What it is: The 50th percentile (median) of the distance errors.\n",
    "    How it's calculated: The median value of the dist.dists array, meaning 50% of the distances are below this value.\n",
    "\n",
    "dist.p75:\n",
    "\n",
    "    What it is: The 75th percentile of the distance errors.\n",
    "    How it's calculated: The value below which 75% of the dist.dists lie.\n",
    "\n",
    "dist.p90:\n",
    "\n",
    "    What it is: The 90th percentile of the distance errors.\n",
    "    How it's calculated: The value below which 90% of the dist.dists lie.\n",
    "\n",
    "dist.p95:\n",
    "\n",
    "    What it is: The 95th percentile of the distance errors.\n",
    "    How it's calculated: The value below which 95% of the dist.dists lie.\n",
    "\n",
    "dist.p99:\n",
    "\n",
    "    What it is: The 99th percentile of the distance errors.\n",
    "    How it's calculated: The value below which 99% of the dist.dists lie, representing the most extreme errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics_c[\"dist.dists\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of the localization errors:\n",
    "print('Centroid Distance Metrics:')\n",
    "#print(\"Frame indices:\", metrics_c[\"dist.frame_idxs\"])\n",
    "#print(\"Video paths:\", metrics_c[\"dist.video_paths\"])\n",
    "#print(\"Error distances (all):\", metrics_c[\"dist.dists\"])\n",
    "print(\"Error distance (avg):\", metrics_c[\"dist.avg\"])\n",
    "print(\"Error distance (50%):\", metrics_c[\"dist.p50\"])\n",
    "print(\"Error distance (75%):\", metrics_c[\"dist.p75\"])\n",
    "print(\"Error distance (90%):\", metrics_c[\"dist.p90\"])\n",
    "print(\"Error distance (95%):\", metrics_c[\"dist.p95\"])\n",
    "print(\"Error distance (99%):\", metrics_c[\"dist.p99\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_c[\"dist.dists\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_dist = metrics_c[\"dist.dists\"].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up the figure with a larger size and higher resolution\n",
    "plt.figure(figsize=(6, 3), dpi=150, facecolor=\"w\")\n",
    "\n",
    "# Create the histogram with KDE\n",
    "sns.histplot(metrics_c[\"dist.dists\"].flatten(), \n",
    "             binrange=(0, 0.0001), \n",
    "             kde=True, \n",
    "             kde_kws={\"clip\": (0, 0.0001)}, \n",
    "             stat=\"probability\")\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"Localization error (px) Centroid\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "# Display the plot inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.4. PCK (meaningless for centroid I think)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCK (Percentage of Correct Keypoints) Metrics:\n",
    "\n",
    "    pck.thresholds:\n",
    "        What it is: List of distance thresholds used for calculating PCK scores.\n",
    "        How it's calculated: These thresholds define the maximum acceptable distance (in pixels or as a fraction of object size) between predicted and ground truth keypoints for a prediction to be considered correct.\n",
    "\n",
    "    pck.pcks:\n",
    "        What it is: PCK scores for each keypoint at each threshold.\n",
    "        How it's calculated: The fraction of correctly predicted keypoints that fall within the specified thresholds, calculated per keypoint and averaged across all frames.\n",
    "\n",
    "    pck.mPCK_parts:\n",
    "        What it is: Mean PCK score per keypoint across all frames.\n",
    "        How it's calculated: The average PCK for each keypoint across all thresholds, giving a measure of how well each keypoint is predicted on average.\n",
    "\n",
    "    pck.mPCK:\n",
    "        What it is: Overall mean PCK score for all keypoints and thresholds.\n",
    "        How it's calculated: The average of pck.mPCK_parts across all keypoints, providing a summary metric of the model’s overall performance.\n",
    "\n",
    "PCK-VOC Metrics:\n",
    "\n",
    "    pck_voc.match_score_thresholds:\n",
    "        What it is: Thresholds for match scores between predicted and ground truth keypoints.\n",
    "        How it's calculated: These thresholds define the maximum acceptable distance between predicted and true keypoints for a match to be considered correct, typically based on the PCK method.\n",
    "\n",
    "    pck_voc.recall_thresholds:\n",
    "        What it is: Thresholds for recall values at different levels.\n",
    "        How it's calculated: Predefined thresholds (e.g., evenly spaced from 0 to 1) used to compute recall values at different detection levels.\n",
    "\n",
    "    pck_voc.match_scores:\n",
    "        What it is: Scores representing the match quality between predicted and ground truth keypoints.\n",
    "        How it's calculated: These scores are calculated based on how close the predicted keypoints are to the ground truth, typically using a normalized distance measure (like PCK).\n",
    "\n",
    "    pck_voc.precisions:\n",
    "        What it is: Precision values calculated at different recall thresholds.\n",
    "        How it's calculated: Precision is the fraction of correctly predicted keypoints (true positives) out of all predicted keypoints (true positives + false positives), computed at each recall level.\n",
    "\n",
    "    pck_voc.recalls:\n",
    "        What it is: Recall values calculated across different recall thresholds.\n",
    "        How it's calculated: Recall is the fraction of correctly predicted keypoints out of all ground truth keypoints (true positives + false negatives), calculated at each recall threshold.\n",
    "\n",
    "    pck_voc.AP (Average Precision):\n",
    "        What it is: The average precision score for each keypoint or class.\n",
    "        How it's calculated: The area under the Precision-Recall curve for a single keypoint or class, computed by averaging precision values across all recall thresholds.\n",
    "\n",
    "    pck_voc.AR (Average Recall):\n",
    "        What it is: The average recall score for each keypoint or class.\n",
    "        How it's calculated: The area under the Recall curve, summarizing recall performance across different thresholds for a specific keypoint or class.\n",
    "\n",
    "    pck_voc.mAP (Mean Average Precision):\n",
    "        What it is: The mean of all Average Precision (AP) values across all keypoints or classes.\n",
    "        How it's calculated: The average of AP values across all keypoints or instances, providing an overall measure of the model’s precision across different thresholds.\n",
    "\n",
    "    pck_voc.mAR (Mean Average Recall):\n",
    "        What it is: The mean of all Average Recall (AR) values across all keypoints or classes.\n",
    "        How it's calculated: The average of AR values across all keypoints or instances, giving an overall measure of recall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PCK Metrics:')\n",
    "print(\"PCK thresholds:\", metrics[\"pck.thresholds\"])\n",
    "print(\"PCK scores for each keypoint:\", metrics_c[\"pck.pcks\"])\n",
    "print(\"Mean PCK per keypoint (mPCK_parts):\", metrics_c[\"pck.mPCK_parts\"])\n",
    "print(\"Overall mean PCK (mPCK):\", metrics[\"pck.mPCK\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PCK-VOC Metrics:')\n",
    "print(\"Match score thresholds:\", metrics[\"pck_voc.match_score_thresholds\"])\n",
    "print(\"Recall thresholds:\", metrics[\"pck_voc.recall_thresholds\"])\n",
    "print(\"Match scores:\", metrics_c[\"pck_voc.match_scores\"])\n",
    "print(\"Precisions at different recall levels:\", metrics_c[\"pck_voc.precisions\"])\n",
    "print(\"Recalls at different recall levels:\", metrics_c[\"pck_voc.recalls\"])\n",
    "print(\"Average Precision (AP):\", metrics_c[\"pck_voc.AP\"])\n",
    "print(\"Average Recall (AR):\", metrics_c[\"pck_voc.AR\"])\n",
    "print(\"Mean Average Precision (mAP):\", metrics_c[\"pck_voc.mAP\"])\n",
    "print(\"Mean Average Recall (mAR):\", metrics_c[\"pck_voc.mAR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((metrics_c[\"pck_voc.match_scores\"]))\n",
    "len(metrics_c[\"pck_voc.match_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_c = {\n",
    "#     \"pck_voc.match_scores\": np.random.normal(loc=5, scale=2, size=100)  # Sample data with mean 5 and std deviation 2\n",
    "# }\n",
    "\n",
    "# Set up the figure with a larger size and higher resolution\n",
    "plt.figure(figsize=(6, 3), dpi=150, facecolor=\"w\")\n",
    "\n",
    "# Create the histogram with KDE\n",
    "sns.histplot(metrics_c[\"pck_voc.match_scores\"].flatten(), \n",
    "             binrange=(0, 1), \n",
    "             kde=True, \n",
    "             kde_kws={\"clip\": (0, 1)}, \n",
    "             stat=\"probability\")\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"PCK\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "# Display the plot inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "plt.figure(figsize=(4, 4), dpi=150, facecolor=\"w\")\n",
    "\n",
    "# Plot the precision-recall curves for every second threshold and precision\n",
    "for precision, thresh in zip(metrics_c[\"pck_voc.precisions\"], metrics_c[\"pck_voc.match_score_thresholds\"]):\n",
    "    plt.plot(metrics_c[\"pck_voc.recall_thresholds\"], precision, \"-\", label=f\"PCK @ {thresh:.2f}\")\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# Display the plot inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.5. OKS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OKS (Object Keypoint Similarity) Metrics:\n",
    "\n",
    "    oks.mOKS:\n",
    "        What it is: Mean Object Keypoint Similarity across all keypoints.\n",
    "        How it's calculated: A similarity score between predicted and ground truth keypoints, normalized by object size and considering keypoint visibility. It averages the OKS scores across all keypoints.\n",
    "\n",
    "OKS-VOC Metrics (Adapted from Object Detection Evaluation):\n",
    "\n",
    "    oks_voc.match_score_thresholds:\n",
    "        What it is: Thresholds for match scores between predicted and ground truth keypoints.\n",
    "        How it's calculated: Defined thresholds (e.g., 0.5, 0.75) used to determine whether a keypoint match is considered correct.\n",
    "\n",
    "    oks_voc.recall_thresholds:\n",
    "        What it is: Thresholds for recall values.\n",
    "        How it's calculated: Specific recall thresholds (often fixed intervals) used to evaluate recall at different levels of detection.\n",
    "\n",
    "    oks_voc.match_scores:\n",
    "        What it is: Scores representing the quality of matches between predicted and ground truth keypoints.\n",
    "        How it's calculated: Based on OKS, a score is assigned to each match, determining the closeness of the match between predicted and true keypoints.\n",
    "\n",
    "    oks_voc.precisions:\n",
    "        What it is: Precision values at various recall thresholds.\n",
    "        How it's calculated: The fraction of true positive keypoints (correct matches) out of all predicted keypoints (true positives + false positives), calculated at various recall levels.\n",
    "\n",
    "    oks_voc.recalls:\n",
    "        What it is: Recall values across different recall thresholds.\n",
    "        How it's calculated: The fraction of true positive keypoints out of all ground truth keypoints (true positives + false negatives), calculated at different recall thresholds.\n",
    "\n",
    "    oks_voc.AP (Average Precision):\n",
    "        What it is: The precision averaged across different recall levels for a specific class or keypoint.\n",
    "        How it's calculated: Area under the Precision-Recall curve, summarizing the model's precision performance at different recall levels.\n",
    "\n",
    "    oks_voc.AR (Average Recall):\n",
    "        What it is: The recall averaged across different recall thresholds for a specific class or keypoint.\n",
    "        How it's calculated: Area under the Recall curve, providing an average measure of recall over varying detection thresholds.\n",
    "\n",
    "    oks_voc.mAP (Mean Average Precision):\n",
    "        What it is: The mean of Average Precision (AP) values across all keypoints or classes.\n",
    "        How it's calculated: The mean AP score calculated by averaging the AP values for all classes/keypoints.\n",
    "\n",
    "    oks_voc.mAR (Mean Average Recall):\n",
    "        What it is: The mean of Average Recall (AR) values across all keypoints or classes.\n",
    "        How it's calculated: The mean AR score calculated by averaging the AR values across all classes/keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OKS Metrics:')\n",
    "print(\"Mean OKS (mOKS):\", metrics_c[\"oks.mOKS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OKS-VOC Metrics:')\n",
    "print(\"Match score thresholds:\", metrics_c[\"oks_voc.match_score_thresholds\"])\n",
    "print(\"Recall thresholds:\", metrics_c[\"oks_voc.recall_thresholds\"])\n",
    "print(\"Match scores:\", metrics_c[\"oks_voc.match_scores\"])\n",
    "print(\"Precisions at different recall levels:\", metrics_c[\"oks_voc.precisions\"])\n",
    "print(\"Recalls at different recall levels:\", metrics_c[\"oks_voc.recalls\"])\n",
    "print(\"Average Precision (AP):\", metrics_c[\"oks_voc.AP\"])\n",
    "print(\"Average Recall (AR):\", metrics_c[\"oks_voc.AR\"])\n",
    "print(\"Mean Average Precision (mAP):\", metrics_c[\"oks_voc.mAP\"])\n",
    "print(\"Mean Average Recall (mAR):\", metrics_c[\"oks_voc.mAR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_c[\"pck_voc.match_scores\"])\n",
    "len(metrics_c[\"pck_voc.match_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_c[\"oks_voc.match_scores\"])\n",
    "len(metrics_c[\"oks_voc.match_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "oks = metrics_c[\"oks_voc.match_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_c = {\n",
    "#     \"pck_voc.match_scores\": np.random.normal(loc=5, scale=2, size=100)  # Sample data with mean 5 and std deviation 2\n",
    "# }\n",
    "\n",
    "# Set up the figure with a larger size and higher resolution\n",
    "plt.figure(figsize=(6, 3), dpi=150, facecolor=\"w\")\n",
    "\n",
    "# Create the histogram with KDE\n",
    "sns.histplot(metrics_c[\"oks_voc.match_scores\"].flatten(), \n",
    "             binrange=(0, 1), \n",
    "             kde=True, \n",
    "             kde_kws={\"clip\": (0, 1)}, \n",
    "             stat=\"probability\")\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"OKS\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "# Display the plot inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(4, 4), dpi=150, facecolor=\"w\")\n",
    "\n",
    "# Plot the precision-recall curves for every second threshold and precision\n",
    "for precision, thresh in zip(metrics_c[\"oks_voc.precisions\"], metrics_c[\"oks_voc.match_score_thresholds\"]):\n",
    "    plt.plot(metrics_c[\"oks_voc.recall_thresholds\"], precision, \"-\", label=f\"OKS @ {thresh:.2f}\")\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# Display the plot inline\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Top-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_medium_rf.topdown_200Epoch'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.0. Inspect the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the ground truth val dataset\n",
    "!sleap-inspect {model_path}/labels_gt.val.slp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the predicted val dataset\n",
    "!sleap-inspect {model_path}/labels_pr.val.slp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Comparing the datasets, max instances is the same. This is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_gt_td = sleap.load_file(f'{model_path}/labels_gt.val.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_val_gt_c.export_csv('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch/labels_gt.val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_gt_td_dict = labels_val_gt_td.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a JSON file with pretty formatting\n",
    "with open(f\"{model_path}/labels_gt.val.json\", \"w\") as json_file:\n",
    "    json.dump(labels_val_gt_td_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_gt_td.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_pr_td = sleap.load_file(f'{model_path}/labels_pr.val.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_pr_td.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val_pr_td_dict = labels_val_pr_td.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a JSON file with pretty formatting\n",
    "with open(f\"{model_path}/labels_pr.val.json\", \"w\") as json_file:\n",
    "    json.dump(labels_val_pr_td_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: so only 55 frames were found to have instances in them. what do these frames look like?\n",
    "\n",
    "This is strange that this model picks up the instances better and the other one appears to pick up the keypoints better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.1. Looking at the available metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n TOP DOWN')\n",
    "metrics = sleap.load_metrics(model_path, split=\"val\")\n",
    "print(\"\\n\".join(metrics.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.2. Visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation (Visibility):\n",
    "\n",
    "    True Positives (TP): Cases where the model correctly predicted that a keypoint is visible.\n",
    "    False Positives (FP): Cases where the model predicted a keypoint is visible when it is not (false alarm).\n",
    "    True Negatives (TN): Cases where the model correctly predicted that a keypoint is not visible (occluded).\n",
    "    False Negatives (FN): Cases where the model predicted a keypoint is not visible when it actually is (missed detection).\n",
    "    Precision: The proportion of correctly predicted visible keypoints (TP) out of all predicted visible keypoints (TP + FP).\n",
    "    Recall: The proportion of actual visible keypoints that were correctly predicted (TP) out of all actual visible keypoints (TP + FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display visibility-related metrics\n",
    "print('\\nVisibility Metrics:')\n",
    "print(\"True Positives (TP):\", metrics[\"vis.tp\"])\n",
    "print(\"False Positives (FP):\", metrics[\"vis.fp\"])\n",
    "print(\"True Negatives (TN):\", metrics[\"vis.tn\"])\n",
    "print(\"False Negatives (FN):\", metrics[\"vis.fn\"])\n",
    "print(\"Precision:\", metrics[\"vis.precision\"])\n",
    "print(\"Recall:\", metrics[\"vis.recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: We are missing almost half the keypoints. That is not a good sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.3. Error distance (means very little for the centroid) - however it is interesting to see that this algorithm seems to work better than the top-down one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist.frame_idxs:\n",
    "\n",
    "    What it is: Indices of the frames in the dataset where the distances are calculated.\n",
    "    How it's calculated: The indices correspond to the frame positions in the video(s) being evaluated.\n",
    "\n",
    "dist.video_paths:\n",
    "\n",
    "    What it is: Paths to the video files associated with the frames being evaluated.\n",
    "    How it's calculated: These are the file paths where the videos used for evaluation are stored.\n",
    "\n",
    "dist.dists:\n",
    "\n",
    "    What it is: The raw distances (in pixels) between predicted and ground truth keypoints for each frame.\n",
    "    How it's calculated: Euclidean distance is computed for each keypoint across the dataset between the predicted and ground truth positions.\n",
    "\n",
    "dist.avg:\n",
    "\n",
    "    What it is: The average distance error across all frames and keypoints.\n",
    "    How it's calculated: The mean of the dist.dists values across all keypoints and frames.\n",
    "\n",
    "dist.p50:\n",
    "\n",
    "    What it is: The 50th percentile (median) of the distance errors.\n",
    "    How it's calculated: The median value of the dist.dists array, meaning 50% of the distances are below this value.\n",
    "\n",
    "dist.p75:\n",
    "\n",
    "    What it is: The 75th percentile of the distance errors.\n",
    "    How it's calculated: The value below which 75% of the dist.dists lie.\n",
    "\n",
    "dist.p90:\n",
    "\n",
    "    What it is: The 90th percentile of the distance errors.\n",
    "    How it's calculated: The value below which 90% of the dist.dists lie.\n",
    "\n",
    "dist.p95:\n",
    "\n",
    "    What it is: The 95th percentile of the distance errors.\n",
    "    How it's calculated: The value below which 95% of the dist.dists lie.\n",
    "\n",
    "dist.p99:\n",
    "\n",
    "    What it is: The 99th percentile of the distance errors.\n",
    "    How it's calculated: The value below which 99% of the dist.dists lie, representing the most extreme errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics[\"dist.dists\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of the localization errors:\n",
    "print('Centroid Distance Metrics:')\n",
    "#print(\"Frame indices:\", metrics[\"dist.frame_idxs\"])\n",
    "#print(\"Video paths:\", metrics[\"dist.video_paths\"])\n",
    "#print(\"Error distances (all):\", metrics[\"dist.dists\"])\n",
    "print(\"Error distance (avg):\", metrics[\"dist.avg\"])\n",
    "print(\"Error distance (50%):\", metrics[\"dist.p50\"])\n",
    "print(\"Error distance (75%):\", metrics[\"dist.p75\"])\n",
    "print(\"Error distance (90%):\", metrics[\"dist.p90\"])\n",
    "print(\"Error distance (95%):\", metrics[\"dist.p95\"])\n",
    "print(\"Error distance (99%):\", metrics[\"dist.p99\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics[\"dist.dists\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_dist = metrics[\"dist.dists\"].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up the figure with a larger size and higher resolution\n",
    "plt.figure(figsize=(6, 3), dpi=150, facecolor=\"w\")\n",
    "\n",
    "# Create the histogram with KDE\n",
    "sns.histplot(metrics[\"dist.dists\"].flatten(), \n",
    "             binrange=(0, 50), \n",
    "             kde=True, \n",
    "             kde_kws={\"clip\": (0, 50)}, \n",
    "             stat=\"probability\")\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"Localization error (px) Centroid\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "# Display the plot inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1.4. PCK (meaningless for centroid I think)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCK (Percentage of Correct Keypoints) Metrics:\n",
    "\n",
    "    pck.thresholds:\n",
    "        What it is: List of distance thresholds used for calculating PCK scores.\n",
    "        How it's calculated: These thresholds define the maximum acceptable distance (in pixels or as a fraction of object size) between predicted and ground truth keypoints for a prediction to be considered correct.\n",
    "\n",
    "    pck.pcks:\n",
    "        What it is: PCK scores for each keypoint at each threshold.\n",
    "        How it's calculated: The fraction of correctly predicted keypoints that fall within the specified thresholds, calculated per keypoint and averaged across all frames.\n",
    "\n",
    "    pck.mPCK_parts:\n",
    "        What it is: Mean PCK score per keypoint across all frames.\n",
    "        How it's calculated: The average PCK for each keypoint across all thresholds, giving a measure of how well each keypoint is predicted on average.\n",
    "\n",
    "    pck.mPCK:\n",
    "        What it is: Overall mean PCK score for all keypoints and thresholds.\n",
    "        How it's calculated: The average of pck.mPCK_parts across all keypoints, providing a summary metric of the model’s overall performance.\n",
    "\n",
    "PCK-VOC Metrics:\n",
    "\n",
    "    pck_voc.match_score_thresholds:\n",
    "        What it is: Thresholds for match scores between predicted and ground truth keypoints.\n",
    "        How it's calculated: These thresholds define the maximum acceptable distance between predicted and true keypoints for a match to be considered correct, typically based on the PCK method.\n",
    "\n",
    "    pck_voc.recall_thresholds:\n",
    "        What it is: Thresholds for recall values at different levels.\n",
    "        How it's calculated: Predefined thresholds (e.g., evenly spaced from 0 to 1) used to compute recall values at different detection levels.\n",
    "\n",
    "    pck_voc.match_scores:\n",
    "        What it is: Scores representing the match quality between predicted and ground truth keypoints.\n",
    "        How it's calculated: These scores are calculated based on how close the predicted keypoints are to the ground truth, typically using a normalized distance measure (like PCK).\n",
    "\n",
    "    pck_voc.precisions:\n",
    "        What it is: Precision values calculated at different recall thresholds.\n",
    "        How it's calculated: Precision is the fraction of correctly predicted keypoints (true positives) out of all predicted keypoints (true positives + false positives), computed at each recall level.\n",
    "\n",
    "    pck_voc.recalls:\n",
    "        What it is: Recall values calculated across different recall thresholds.\n",
    "        How it's calculated: Recall is the fraction of correctly predicted keypoints out of all ground truth keypoints (true positives + false negatives), calculated at each recall threshold.\n",
    "\n",
    "    pck_voc.AP (Average Precision):\n",
    "        What it is: The average precision score for each keypoint or class.\n",
    "        How it's calculated: The area under the Precision-Recall curve for a single keypoint or class, computed by averaging precision values across all recall thresholds.\n",
    "\n",
    "    pck_voc.AR (Average Recall):\n",
    "        What it is: The average recall score for each keypoint or class.\n",
    "        How it's calculated: The area under the Recall curve, summarizing recall performance across different thresholds for a specific keypoint or class.\n",
    "\n",
    "    pck_voc.mAP (Mean Average Precision):\n",
    "        What it is: The mean of all Average Precision (AP) values across all keypoints or classes.\n",
    "        How it's calculated: The average of AP values across all keypoints or instances, providing an overall measure of the model’s precision across different thresholds.\n",
    "\n",
    "    pck_voc.mAR (Mean Average Recall):\n",
    "        What it is: The mean of all Average Recall (AR) values across all keypoints or classes.\n",
    "        How it's calculated: The average of AR values across all keypoints or instances, giving an overall measure of recall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PCK Metrics:')\n",
    "print(\"PCK thresholds:\", metrics[\"pck.thresholds\"])\n",
    "print(\"PCK scores for each keypoint:\", metrics[\"pck.pcks\"])\n",
    "print(\"Mean PCK per keypoint (mPCK_parts):\", metrics[\"pck.mPCK_parts\"])\n",
    "print(\"Overall mean PCK (mPCK):\", metrics[\"pck.mPCK\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PCK-VOC Metrics:')\n",
    "print(\"Match score thresholds:\", metrics[\"pck_voc.match_score_thresholds\"])\n",
    "print(\"Recall thresholds:\", metrics[\"pck_voc.recall_thresholds\"])\n",
    "print(\"Match scores:\", metrics[\"pck_voc.match_scores\"])\n",
    "print(\"Precisions at different recall levels:\", metrics[\"pck_voc.precisions\"])\n",
    "print(\"Recalls at different recall levels:\", metrics[\"pck_voc.recalls\"])\n",
    "print(\"Average Precision (AP):\", metrics[\"pck_voc.AP\"])\n",
    "print(\"Average Recall (AR):\", metrics[\"pck_voc.AR\"])\n",
    "print(\"Mean Average Precision (mAP):\", metrics[\"pck_voc.mAP\"])\n",
    "print(\"Mean Average Recall (mAR):\", metrics[\"pck_voc.mAR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((metrics[\"pck_voc.match_scores\"]))\n",
    "len(metrics[\"pck_voc.match_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = {\n",
    "#     \"pck_voc.match_scores\": np.random.normal(loc=5, scale=2, size=100)  # Sample data with mean 5 and std deviation 2\n",
    "# }\n",
    "\n",
    "# Set up the figure with a larger size and higher resolution\n",
    "plt.figure(figsize=(6, 3), dpi=150, facecolor=\"w\")\n",
    "\n",
    "# Create the histogram with KDE\n",
    "sns.histplot(metrics[\"pck_voc.match_scores\"].flatten(), \n",
    "             binrange=(0, 1), \n",
    "             kde=True, \n",
    "             kde_kws={\"clip\": (0, 1)}, \n",
    "             stat=\"probability\")\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"PCK\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "# Display the plot inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "plt.figure(figsize=(4, 4), dpi=150, facecolor=\"w\")\n",
    "\n",
    "# Plot the precision-recall curves for every second threshold and precision\n",
    "for precision, thresh in zip(metrics[\"pck_voc.precisions\"], metrics[\"pck_voc.match_score_thresholds\"]):\n",
    "    plt.plot(metrics[\"pck_voc.recall_thresholds\"], precision, \"-\", label=f\"PCK @ {thresh:.2f}\")\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "# Display the plot inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.5. OKS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OKS (Object Keypoint Similarity) Metrics:\n",
    "\n",
    "    oks.mOKS:\n",
    "        What it is: Mean Object Keypoint Similarity across all keypoints.\n",
    "        How it's calculated: A similarity score between predicted and ground truth keypoints, normalized by object size and considering keypoint visibility. It averages the OKS scores across all keypoints.\n",
    "\n",
    "OKS-VOC Metrics (Adapted from Object Detection Evaluation):\n",
    "\n",
    "    oks_voc.match_score_thresholds:\n",
    "        What it is: Thresholds for match scores between predicted and ground truth keypoints.\n",
    "        How it's calculated: Defined thresholds (e.g., 0.5, 0.75) used to determine whether a keypoint match is considered correct.\n",
    "\n",
    "    oks_voc.recall_thresholds:\n",
    "        What it is: Thresholds for recall values.\n",
    "        How it's calculated: Specific recall thresholds (often fixed intervals) used to evaluate recall at different levels of detection.\n",
    "\n",
    "    oks_voc.match_scores:\n",
    "        What it is: Scores representing the quality of matches between predicted and ground truth keypoints.\n",
    "        How it's calculated: Based on OKS, a score is assigned to each match, determining the closeness of the match between predicted and true keypoints.\n",
    "\n",
    "    oks_voc.precisions:\n",
    "        What it is: Precision values at various recall thresholds.\n",
    "        How it's calculated: The fraction of true positive keypoints (correct matches) out of all predicted keypoints (true positives + false positives), calculated at various recall levels.\n",
    "\n",
    "    oks_voc.recalls:\n",
    "        What it is: Recall values across different recall thresholds.\n",
    "        How it's calculated: The fraction of true positive keypoints out of all ground truth keypoints (true positives + false negatives), calculated at different recall thresholds.\n",
    "\n",
    "    oks_voc.AP (Average Precision):\n",
    "        What it is: The precision averaged across different recall levels for a specific class or keypoint.\n",
    "        How it's calculated: Area under the Precision-Recall curve, summarizing the model's precision performance at different recall levels.\n",
    "\n",
    "    oks_voc.AR (Average Recall):\n",
    "        What it is: The recall averaged across different recall thresholds for a specific class or keypoint.\n",
    "        How it's calculated: Area under the Recall curve, providing an average measure of recall over varying detection thresholds.\n",
    "\n",
    "    oks_voc.mAP (Mean Average Precision):\n",
    "        What it is: The mean of Average Precision (AP) values across all keypoints or classes.\n",
    "        How it's calculated: The mean AP score calculated by averaging the AP values for all classes/keypoints.\n",
    "\n",
    "    oks_voc.mAR (Mean Average Recall):\n",
    "        What it is: The mean of Average Recall (AR) values across all keypoints or classes.\n",
    "        How it's calculated: The mean AR score calculated by averaging the AR values across all classes/keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OKS Metrics:')\n",
    "print(\"Mean OKS (mOKS):\", metrics[\"oks.mOKS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OKS-VOC Metrics:')\n",
    "print(\"Match score thresholds:\", metrics[\"oks_voc.match_score_thresholds\"])\n",
    "print(\"Recall thresholds:\", metrics[\"oks_voc.recall_thresholds\"])\n",
    "print(\"Match scores:\", metrics[\"oks_voc.match_scores\"])\n",
    "print(\"Precisions at different recall levels:\", metrics[\"oks_voc.precisions\"])\n",
    "print(\"Recalls at different recall levels:\", metrics[\"oks_voc.recalls\"])\n",
    "print(\"Average Precision (AP):\", metrics[\"oks_voc.AP\"])\n",
    "print(\"Average Recall (AR):\", metrics[\"oks_voc.AR\"])\n",
    "print(\"Mean Average Precision (mAP):\", metrics[\"oks_voc.mAP\"])\n",
    "print(\"Mean Average Recall (mAR):\", metrics[\"oks_voc.mAR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics[\"pck_voc.match_scores\"])\n",
    "len(metrics[\"pck_voc.match_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics[\"oks_voc.match_scores\"])\n",
    "len(metrics[\"oks_voc.match_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "oks = metrics_c[\"oks_voc.match_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_c = {\n",
    "#     \"pck_voc.match_scores\": np.random.normal(loc=5, scale=2, size=100)  # Sample data with mean 5 and std deviation 2\n",
    "# }\n",
    "\n",
    "# Set up the figure with a larger size and higher resolution\n",
    "plt.figure(figsize=(6, 3), dpi=150, facecolor=\"w\")\n",
    "\n",
    "# Create the histogram with KDE\n",
    "sns.histplot(metrics[\"oks_voc.match_scores\"].flatten(), \n",
    "             binrange=(0, 1), \n",
    "             kde=True, \n",
    "             kde_kws={\"clip\": (0, 1)}, \n",
    "             stat=\"probability\")\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"OKS\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "# Display the plot inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(4, 4), dpi=150, facecolor=\"w\")\n",
    "\n",
    "# Plot the precision-recall curves for every second threshold and precision\n",
    "for precision, thresh in zip(metrics[\"oks_voc.precisions\"], metrics[\"oks_voc.match_score_thresholds\"]):\n",
    "    plt.plot(metrics[\"oks_voc.recall_thresholds\"], precision, \"-\", label=f\"OKS @ {thresh:.2f}\")\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# Display the plot inline\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis is more like cool shit you can do with the data. Like what areas are the penguins in. What verlocity are they moving, etc.\n",
    "\n",
    "However the exporting of the file as a .h5 and the locations part will be valuable when saving predictions or annotations to a different format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the .h5 file of the .slp output from predcitions:\n",
    "- file export analysis to csv/h5 exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_medium_rf.topdown_200Epoch/labels_pr.val.005_img010.analysis.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(h5_path, \"r\") as f:\n",
    "    dset_names = list(f.keys())\n",
    "    locations = f[\"tracks\"][:].T\n",
    "    node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
    "\n",
    "print(\"===filename===\")\n",
    "print(h5_path)\n",
    "print()\n",
    "\n",
    "print(\"===HDF5 datasets===\")\n",
    "print(dset_names)\n",
    "print()\n",
    "\n",
    "print(\"===locations data shape===\")\n",
    "print(locations.shape)\n",
    "print()\n",
    "\n",
    "print(\"===nodes===\")\n",
    "for i, name in enumerate(node_names):\n",
    "    print(f\"{i}: {name}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example file, the shape of the locations matrix (the tracks dataset) is (3000, 13, 2, 2) after it is transposed (with the .T). We transpose the data when loading it in Python; no transpose is needed when using MATLAB. This is because Python and MATLAB expect matrices to be stored differently.\n",
    "\n",
    "Here’s what each dimension of the matrix means:\n",
    "\n",
    "    3000: the number of frames;\n",
    "\n",
    "    13: the number of nodes in the skeleton (we’ve also loaded and displayed the node_names dataset with the names of these 13 nodes);\n",
    "\n",
    "    2: for the x and y coordinates;\n",
    "\n",
    "    2: the number of distinct animal identities which were found (we have 2 flies in the video clip and they were tracked perfectly, so we ended up with exactly 2 track, but there may be more tracks than animals if tracking didn’t work as well). - WE ONLY HAVE A SINGLE ANIMAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(locations[0]) # print one frame predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count, node_count, _, instance_count = locations.shape\n",
    "\n",
    "print(\"frame count:\", frame_count)\n",
    "print(\"node count:\", node_count)\n",
    "print(\"instance count:\", instance_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def fill_missing(Y, kind=\"linear\"):\n",
    "    \"\"\"Fills missing values independently along each dimension after the first. So this will fill in the missing value between frames (intropelate between frames)\n",
    "    actually I am not sure now... I will have to run it to see what the output looks like\"\"\"\n",
    "\n",
    "    # Store initial shape.\n",
    "    initial_shape = Y.shape\n",
    "\n",
    "    # Flatten after first dim.\n",
    "    Y = Y.reshape((initial_shape[0], -1))\n",
    "\n",
    "    # Interpolate along each slice.\n",
    "    for i in range(Y.shape[-1]):\n",
    "        y = Y[:, i]\n",
    "\n",
    "        # Build interpolant.\n",
    "        x = np.flatnonzero(~np.isnan(y))\n",
    "        # need to add a catch in here that will catch case when kp is missing in all frames (y is all nans)\n",
    "        # PUT THE CODE ABOVE HERE (IF ALL NANS, MAKE VALUE 1 OR SOMETHING..)\n",
    "        f = interp1d(x, y[x], kind=kind, fill_value=np.nan, bounds_error=False)\n",
    "\n",
    "        # Fill missing\n",
    "        xq = np.flatnonzero(np.isnan(y))\n",
    "        y[xq] = f(xq)\n",
    "        \n",
    "        # Fill leading or trailing NaNs with the nearest non-NaN values\n",
    "        mask = np.isnan(y)\n",
    "        y[mask] = np.interp(np.flatnonzero(mask), np.flatnonzero(~mask), y[~mask])\n",
    "\n",
    "        # Save slice\n",
    "        Y[:, i] = y\n",
    "\n",
    "    # Restore to initial shape.\n",
    "    Y = Y.reshape(initial_shape)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = fill_missing(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Data Structures (looking at data structures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick overview of the data structures before we start:\n",
    "\n",
    "    Point/PredictedPoint → Contains the x and y coordinates (and score for predictions) of a landmark.\n",
    "\n",
    "    Instance/PredictedInstance → Contains a set of Point/PredictedPoints. This represent a single individual within a frame and may also contain an associated Track.\n",
    "\n",
    "    Skeleton → Defines the nodes and edges that define the set of unique landmark types that each point represents, e.g., “head”, “tail”, etc. This does not contain positions – those are stored in individual Points.\n",
    "\n",
    "    LabeledFrame → Contains a set of Instance/PredictedInstances for a single frame.\n",
    "\n",
    "    Labels → Contains a set of LabeledFrames and the associated metadata for the videos and other information related to the project or predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.0. Run predictions on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.0.1. CLI get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test images in a single folder to run inference on\n",
    "#   DONE\n",
    "# create a path to that folder\n",
    "test_image_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test'\n",
    "# create a path to the two model folders\n",
    "centroid_model_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch'\n",
    "top_down_model_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_medium_rf.topdown_200Epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference \n",
    "#   run with a batch size = 1\n",
    "#   output name date_run_modelNames_cpu/gpu_batchSize_peakThreshold\n",
    "#   verbosity is json\n",
    "#   open in gui = True - do not make this true\n",
    "#   run cpu and gpu\n",
    "#   peak_threshold = 0.2, 0.15, 0.1, 0.05, 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_medium_rf.topdown_200Epoch' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run1_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_gpu_batch_1_Threshold_0.2' --verbosity json --batch_size 1 --peak_threshold 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_medium_rf.topdown_200Epoch' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run2_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_gpu_batch_1_Threshold_0' --verbosity json --batch_size 1 --peak_threshold 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_medium_rf.topdown_200Epoch' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run3_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_gpu_batch_1_Threshold_0.1' --verbosity none --batch_size 1 --peak_threshold 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_medium_rf.topdown_200Epoch' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run4_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_gpu_batch_1_Threshold_0.05' --verbosity json --batch_size 1 --peak_threshold 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_medium_rf.topdown_200Epoch' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run5_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_gpu_batch_1_Threshold_0.02' --verbosity json --batch_size 1 --peak_threshold 0.02 --cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu run\n",
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_medium_rf.topdown_200Epoch' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run6_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.01' --verbosity json --batch_size 1 --peak_threshold 0.1 --cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sleap-track '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test1_centroid_200Epoch' -m '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct3_test3_medium_rf.topdown_200Epoch' -o '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run7_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.05' --verbosity json --batch_size 1 --peak_threshold 0.05 --cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see: \n",
    "usage: sleap-track [-h] [-m MODELS] [--frames FRAMES] [--only-labeled-frames] [--only-suggested-frames] [-o OUTPUT] [--no-empty-frames]\n",
    "                   [--verbosity {none,rich,json}] [--video.dataset VIDEO.DATASET] [--video.input_format VIDEO.INPUT_FORMAT]\n",
    "                   [--video.index VIDEO.INDEX] [--cpu | --first-gpu | --last-gpu | --gpu GPU] [--max_edge_length_ratio MAX_EDGE_LENGTH_RATIO]\n",
    "                   [--dist_penalty_weight DIST_PENALTY_WEIGHT] [--batch_size BATCH_SIZE] [--open-in-gui] [--peak_threshold PEAK_THRESHOLD]\n",
    "                   [-n MAX_INSTANCES] [--tracking.tracker TRACKING.TRACKER] [--tracking.max_tracking TRACKING.MAX_TRACKING]\n",
    "                   [--tracking.max_tracks TRACKING.MAX_TRACKS] [--tracking.target_instance_count TRACKING.TARGET_INSTANCE_COUNT]\n",
    "                   [--tracking.pre_cull_to_target TRACKING.PRE_CULL_TO_TARGET] [--tracking.pre_cull_iou_threshold TRACKING.PRE_CULL_IOU_THRESHOLD]\n",
    "                   [--tracking.post_connect_single_breaks TRACKING.POST_CONNECT_SINGLE_BREAKS]\n",
    "                   [--tracking.clean_instance_count TRACKING.CLEAN_INSTANCE_COUNT] [--tracking.clean_iou_threshold TRACKING.CLEAN_IOU_THRESHOLD]\n",
    "                   [--tracking.similarity TRACKING.SIMILARITY] [--tracking.match TRACKING.MATCH] [--tracking.robust TRACKING.ROBUST]\n",
    "                   [--tracking.track_window TRACKING.TRACK_WINDOW] [--tracking.min_new_track_points TRACKING.MIN_NEW_TRACK_POINTS]\n",
    "                   [--tracking.min_match_points TRACKING.MIN_MATCH_POINTS] [--tracking.img_scale TRACKING.IMG_SCALE]\n",
    "                   [--tracking.of_window_size TRACKING.OF_WINDOW_SIZE] [--tracking.of_max_levels TRACKING.OF_MAX_LEVELS]\n",
    "                   [--tracking.save_shifted_instances TRACKING.SAVE_SHIFTED_INSTANCES] [--tracking.kf_node_indices TRACKING.KF_NODE_INDICES]\n",
    "                   [--tracking.kf_init_frame_count TRACKING.KF_INIT_FRAME_COUNT]\n",
    "                   [data_path]\n",
    "\n",
    "positional arguments:\n",
    "  data_path             Path to data to predict on. This can be a labels (.slp) file or any supported video format.\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -m MODELS, --model MODELS\n",
    "                        Path to trained model directory (with training_config.json). Multiple models can be specified, each preceded by --model.\n",
    "  --frames FRAMES       List of frames to predict when running on a video. Can be specified as a comma separated list (e.g. 1,2,3) or a range\n",
    "                        separated by hyphen (e.g., 1-3, for 1,2,3). If not provided, defaults to predicting on the entire video.\n",
    "  --only-labeled-frames\n",
    "                        Only run inference on user labeled frames when running on labels dataset. This is useful for generating predictions to compare\n",
    "                        against ground truth.\n",
    "  --only-suggested-frames\n",
    "                        Only run inference on unlabeled suggested frames when running on labels dataset. This is useful for generating predictions for\n",
    "                        initialization during labeling.\n",
    "  -o OUTPUT, --output OUTPUT\n",
    "                        The output filename to use for the predicted data. If not provided, defaults to '[data_path].predictions.slp'.\n",
    "  --no-empty-frames     Clear any empty frames that did not have any detected instances before saving to output.\n",
    "  --verbosity {none,rich,json}\n",
    "                        Verbosity of inference progress reporting. 'none' does not output anything during inference, 'rich' displays an updating\n",
    "                        progress bar, and 'json' outputs the progress as a JSON encoded response to the console.\n",
    "  --video.dataset VIDEO.DATASET\n",
    "                        The dataset for HDF5 videos.\n",
    "  --video.input_format VIDEO.INPUT_FORMAT\n",
    "                        The input_format for HDF5 videos.\n",
    "  --video.index VIDEO.INDEX\n",
    "                        Integer index of video in .slp file to predict on. To be used with an .slp path as an alternative to specifying the video\n",
    "                        path.\n",
    "  --cpu                 Run inference only on CPU. If not specified, will use available GPU.\n",
    "  --first-gpu           Run inference on the first GPU, if available.\n",
    "  --last-gpu            Run inference on the last GPU, if available.\n",
    "  --gpu GPU             Run training on the i-th GPU on the system. If 'auto', run on the GPU with the highest percentage of available memory.\n",
    "  --max_edge_length_ratio MAX_EDGE_LENGTH_RATIO\n",
    "                        The maximum expected length of a connected pair of points as a fraction of the image size. Candidate connections longer than\n",
    "                        this length will be penalized during matching. Only applies to bottom-up (PAF) models.\n",
    "  --dist_penalty_weight DIST_PENALTY_WEIGHT\n",
    "                        A coefficient to scale weight of the distance penalty. Set to values greater than 1.0 to enforce the distance penalty more\n",
    "                        strictly. Only applies to bottom-up (PAF) models.\n",
    "  --batch_size BATCH_SIZE\n",
    "                        Number of frames to predict at a time. Larger values result in faster inference speeds, but require more memory.\n",
    "  --open-in-gui         Open the resulting predictions in the GUI when finished.\n",
    "  --peak_threshold PEAK_THRESHOLD\n",
    "                        Minimum confidence map value to consider a peak as valid.\n",
    "  -n MAX_INSTANCES, --max_instances MAX_INSTANCES\n",
    "                        Limit maximum number of instances in multi-instance models. Not available for ID models. Defaults to None.\n",
    "  --tracking.tracker TRACKING.TRACKER\n",
    "                        Options: simple, flow, simplemaxtracks, flowmaxtracks, None (default: None)\n",
    "  --tracking.max_tracking TRACKING.MAX_TRACKING\n",
    "                        If true then the tracker will cap the max number of tracks. (default: False)\n",
    "  --tracking.max_tracks TRACKING.MAX_TRACKS\n",
    "                        Maximum number of tracks to be tracked by the tracker. (default: None)\n",
    "  --tracking.target_instance_count TRACKING.TARGET_INSTANCE_COUNT\n",
    "                        Target number of instances to track per frame. (default: 0)\n",
    "  --tracking.pre_cull_to_target TRACKING.PRE_CULL_TO_TARGET\n",
    "                        If non-zero and target_instance_count is also non-zero, then cull instances over target count per frame *before* tracking.\n",
    "                        (default: 0)\n",
    "  --tracking.pre_cull_iou_threshold TRACKING.PRE_CULL_IOU_THRESHOLD\n",
    "                        If non-zero and pre_cull_to_target also set, then use IOU threshold to remove overlapping instances over count *before*\n",
    "                        tracking. (default: 0)\n",
    "  --tracking.post_connect_single_breaks TRACKING.POST_CONNECT_SINGLE_BREAKS\n",
    "                        If non-zero and target_instance_count is also non-zero, then connect track breaks when exactly one track is lost and exactly\n",
    "                        one track is spawned in frame. (default: 0)\n",
    "  --tracking.clean_instance_count TRACKING.CLEAN_INSTANCE_COUNT\n",
    "                        Target number of instances to clean *after* tracking. (default: 0)\n",
    "  --tracking.clean_iou_threshold TRACKING.CLEAN_IOU_THRESHOLD\n",
    "                        IOU to use when culling instances *after* tracking. (default: 0)\n",
    "  --tracking.similarity TRACKING.SIMILARITY\n",
    "                        Options: instance, centroid, iou (default: instance)\n",
    "  --tracking.match TRACKING.MATCH\n",
    "                        Options: hungarian, greedy (default: greedy)\n",
    "  --tracking.robust TRACKING.ROBUST\n",
    "                        Robust quantile of similarity score for instance matching. If equal to 1, keep the max similarity score (non-robust).\n",
    "                        (default: 1)\n",
    "  --tracking.track_window TRACKING.TRACK_WINDOW\n",
    "                        How many frames back to look for matches (default: 5)\n",
    "  --tracking.min_new_track_points TRACKING.MIN_NEW_TRACK_POINTS\n",
    "                        Minimum number of instance points for spawning new track (default: 0)\n",
    "  --tracking.min_match_points TRACKING.MIN_MATCH_POINTS\n",
    "                        Minimum points for match candidates (default: 0)\n",
    "  --tracking.img_scale TRACKING.IMG_SCALE\n",
    "                        For optical-flow: Image scale (default: 1.0)\n",
    "  --tracking.of_window_size TRACKING.OF_WINDOW_SIZE\n",
    "                        For optical-flow: Optical flow window size to consider at each pyramid (default: 21)\n",
    "  --tracking.of_max_levels TRACKING.OF_MAX_LEVELS\n",
    "                        For optical-flow: Number of pyramid scale levels to consider (default: 3)\n",
    "  --tracking.save_shifted_instances TRACKING.SAVE_SHIFTED_INSTANCES\n",
    "                        If non-zero and tracking.tracker is set to flow, save the shifted instances between elapsed frames (default: 0)\n",
    "  --tracking.kf_node_indices TRACKING.KF_NODE_INDICES\n",
    "                        For Kalman filter: Indices of nodes to track. (default: )\n",
    "  --tracking.kf_init_frame_count TRACKING.KF_INIT_FRAME_COUNT\n",
    "                        For Kalman filter: Number of frames to track with other tracker. 0 means no Kalman filters will be used. (default: 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  -o OUTPUT, --output OUTPUT\n",
    "                        The output filename to use for the predicted data. If not provided, defaults to '[data_path].predictions.slp'.\n",
    "\n",
    "  --verbosity {none,rich,json}\n",
    "                        Verbosity of inference progress reporting. 'none' does not output anything during inference, 'rich' displays an updating\n",
    "                        progress bar, and 'json' outputs the progress as a JSON encoded response to the console.\n",
    "\n",
    "  --cpu                 Run inference only on CPU. If not specified, will use available GPU.\n",
    "\n",
    "    --batch_size BATCH_SIZE\n",
    "                        Number of frames to predict at a time. Larger values result in faster inference speeds, but require more memory.\n",
    "\n",
    "  --open-in-gui         Open the resulting predictions in the GUI when finished.\n",
    "\n",
    "  --peak_threshold PEAK_THRESHOLD\n",
    "                        Minimum confidence map value to consider a peak as valid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without tracking on a video\n",
    "!sleap-track \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_prediction/flap1.mp4\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_centroid_50Epoch\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without tracking on a .slp dataset\n",
    "!sleap-track \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_prediction/labels_gt.test.slp\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_centroid_50Epoch\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without tracking on a folder of images\n",
    "!sleap-track \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_prediction/test_inference_on_folder_images\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_centroid_50Epoch\" -m \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct2_test1_medium_rf.topdown_50Epoch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.0.2. Inference using python\n",
    "see https://sleap.ai/notebooks/Interactive_and_realtime_inference.html\n",
    "\n",
    "I will have to come back here to look for speed metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.0.3. Combining datasets (merging .slp files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load both files\n",
    "\n",
    "# label paths\n",
    "labels_path1 = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run6_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.1/frame_flap13.mp4_0.predictions.slp'\n",
    "labels_path2 = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run6_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.1/frame_flap13.mp4_1.predictions.slp'\n",
    "labels_path3 = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run6_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.1/frame_flap13.mp4_2.predictions.slp'\n",
    "\n",
    "# load labels objects\n",
    "labels1 = sleap.load_file(labels_path1)\n",
    "labels2 = sleap.load_file(labels_path2)\n",
    "labels3 = sleap.load_file(labels_path3)\n",
    "\n",
    "test = sleap.Labels.complex_merge_between(labels1,labels3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file \n",
    "sleap.Labels.save_file(labels1, '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run6_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.1/test.slp')\n",
    "sleap.Labels.save_file(labels1,'/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run6_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.1/test.h5')\n",
    "sleap.Labels.save_file(labels1, '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run6_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.1/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run6_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.1'\n",
    "save_name = dir_path.split('/')[-1]\n",
    "save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets_in_folder(dir_path, extension='.slp'):\n",
    "    \"\"\"\n",
    "    This function processes all files with a given extension in a directory in alphabetical order.\n",
    "    \n",
    "    Parameters:\n",
    "    - dir_path (str): The path to the directory.\n",
    "    - extension (str): The file extension to filter by (e.g., '.txt').\n",
    "    \"\"\"\n",
    "    # Get all files with the specified extension, sorted alphabetically\n",
    "    files = sorted([f for f in os.listdir(dir_path) if f.endswith(extension)])\n",
    "    \n",
    "    # create counter\n",
    "    count = 0\n",
    "\n",
    "    # Process each file by calling the action function\n",
    "    for file in files:\n",
    "\n",
    "        file_path = os.path.join(dir_path, file)\n",
    "\n",
    "        if count == 0:\n",
    "            # load first label file\n",
    "            label1 = sleap.load_file(file_path)\n",
    "\n",
    "            # iterate counter\n",
    "            count += 1\n",
    "\n",
    "        else:\n",
    "            # if its not the first file then load the pred and concat to the first pred\n",
    "            next_label = sleap.load_file(file_path)\n",
    "\n",
    "            # concat\n",
    "            test = sleap.Labels.complex_merge_between(label1,next_label)\n",
    "            \n",
    "\n",
    "\n",
    "    save_name = dir_path.split('/')[-1]\n",
    "    sleap.Labels.save_file(label1, dir_path+'/'+save_name+'.slp')\n",
    "    sleap.Labels.save_file(label1, dir_path+'/'+save_name+'.json')\n",
    "    sleap.Labels.save_file(label1, dir_path+'/'+save_name+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run7_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.05'\n",
    "# dir_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/7Oct_run6_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combine_datasets_in_folder(dir_path, '.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = sleap.load_file(dir_path+'/'+'7Oct_run7_m1_Oct3_test1_centroid_200Epoc_m2_Oct3_test3_medium_rf.topdown_200Epoch_cpu_batch_1_Threshold_0.05.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1[38].plot(scale=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1[38].instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the complex_merge_between() function that will add the labels of the second dataset to the first dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. Using predictions to create a new labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can download the generated colab.predicted_suggestions.slp file and merge it into your labeling project (File -> Merge into Project… from the GUI) to get new predictions for your suggested frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9. SLEAP multi-animal (Aquarium Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sleap.ai/guides/gui.html\n",
    "GUI Functionality:\n",
    " - first run sleap-label in the cmd line to launch the GUI (in the sleap conda env)\n",
    " - .slp files contain the information about the labels (annotations)\n",
    " - file open project to open a .slp file\n",
    " - file import to import a DLC annotation (can use the .yaml file for all the annotations from a project or a csv file for a single video)\n",
    " - file export analysis to csv/h5 exports\n",
    " - file merge data from: USE THIS WHEN OPENING UP A PREDICTIONS FILE THAT YOU WANT TO ADD TO A TRAINING DATASET\n",
    " - labels: has functionality for labeling videos - review when using for labelling\n",
    " - Predict >> Evaluation of Model >> select model >> view metrics: shows all the various metrics (how can we access these?)\n",
    " - Predict >> Export labeled frames: exports frames to a .slp file (can do just labeled or +suggested or +predicted+suggested)\n",
    " - Predict >> inference: allows you to select a model and run inference. Can export the config files and the training job package (contains a bash command for inference, a yaml file with some info and paths, the .slp file (which is over 1GB in size))?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.1. Train the model using the best performing model on the simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CENTROID\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline.centroid.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 0.25\n",
    "cfg.model.backbone.unet.max_stride = 32\n",
    "cfg.model.backbone.unet.filters = 16\n",
    "cfg.model.backbone.unet.filters_rate = 2\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.AquariumDataset_DLC_pretrain1.slp\"\n",
    "#cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "#cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centroid.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 4\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Dec6_AquariumPretraining1_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/AquariumDatasetModels'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CENTER_INSTANCE (max stride 64)\n",
    "\n",
    "# load config file\n",
    "cfg = sleap.load_config(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/baseline.configs_adjusted/baseline_medium_rf.topdown.json\")\n",
    "\n",
    "# adjust config file with all model and data adjustments\n",
    "cfg.data.preprocessing.input_scaling = 1\n",
    "cfg.model.backbone.unet.max_stride = 64\n",
    "cfg.model.backbone.unet.filters = 24\n",
    "cfg.model.backbone.unet.filters_rate = 1.5\n",
    "# cfg.model.heads.centroid.output_stride = 4\n",
    "\n",
    "# provide labeled data folder\n",
    "cfg.data.labels.training_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.AquariumDataset_DLC_pretrain1.slp\"\n",
    "#cfg.data.labels.validation_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_val.slp\"\n",
    "#cfg.data.labels.test_labels = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.SimpleDataset_DLC_test.slp\"\n",
    "\n",
    "# provide centroid position\n",
    "cfg.data.instance_cropping.center_on_part = 'Body_top'  \n",
    "cfg.model.heads.centered_instance.anchor_part = 'Body_top'\n",
    "\n",
    "# increase number of epochs\n",
    "cfg.optimization.epochs = 200\n",
    "#change batch size\n",
    "cfg.optimization.batch_size = 1\n",
    "\n",
    "# keep the ouput images\n",
    "cfg.outputs.keep_viz_images = True\n",
    "# name of run\n",
    "cfg.outputs.run_name = 'Dec6_AquariumPretraining1_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1'\n",
    "cfg.outputs.runs_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/AquariumDatasetModels'\n",
    "\n",
    "# save final model\n",
    "cfg.outputs.checkpointing.latest_model = True\n",
    "cfg.outputs.checkpointing.final_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.2. Run inference on the next 100 images (Pretrain2) one video at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder path\n",
    "pretrain2_img_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/Aquarium_dataset/pretrain_aquariumDataset2_raw/images/train/pretrain2'\n",
    "# run paths\n",
    "run1 = 'penguinpi2_video_res_2656_1500_duration_60_fps_12_2024-05-29_08-09'\n",
    "run2 = 'penguinpi2_video_res_2656_1500_duration_60_fps_12_2024-05-29_10-42'\n",
    "run3 = 'penguinpi2_video_res_2656_1500_duration_60_fps_12_2024-05-30_07-15'\n",
    "run4 = 'penguinpi2_video_res_2656_1500_duration_60_fps_12_2024-06-06_09-51'\n",
    "run5 = 'penguinpi2_video_res_2656_1500_duration_60_fps_12_2024-06-13_10-16'\n",
    "run1_img_path = pretrain2_img_path + '/' + run1\n",
    "run2_img_path = pretrain2_img_path + '/' + run2\n",
    "run3_img_path = pretrain2_img_path + '/' + run3\n",
    "run4_img_path = pretrain2_img_path + '/' + run4\n",
    "run5_img_path = pretrain2_img_path + '/' + run5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9.2.1. select and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected models. to be passed as a list of centroid and centre instance models if they are top-down (folders must include a best.h5 and a training_config.json)\n",
    "# models selected by choosing best performance (OKS mAP) on the val set\n",
    "path_to_centroid_model_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/AquariumDatasetModels/Dec6_AquariumPretraining1_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4'\n",
    "path_to_centered_instance_model_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/AquariumDatasetModels/Dec6_AquariumPretraining1_center_instance_receptiveField_inputScale1_maxStride64_filterRate1.5_filters24_batchsize1'\n",
    "# the above models were too big, so lets try another (second best)\n",
    "#path_to_centered_instance_model_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_center_instance_receptiveField_inputScale1_maxStride32_filterRate1.5_filters24_batchsize2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor for the top-down\n",
    "predictor = sleap.load_model([path_to_centroid_model_folder, path_to_centered_instance_model_folder], batch_size=1, peak_threshold=0.02, progress_reporting='json', max_instances=20)\n",
    "\n",
    "# NOTE the peak_threshold cannot be 0 (so I have made it 0.02) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9.2.2. select and load images to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = run1\n",
    "# raw_image_folder = run1_img_path\n",
    "# run = run2\n",
    "# raw_image_folder = run2_img_path\n",
    "# run = run3\n",
    "# raw_image_folder = run3_img_path\n",
    "# run = run4\n",
    "# raw_image_folder = run4_img_path\n",
    "run = run5\n",
    "raw_image_folder = run5_img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_to_numpy(folder_path):\n",
    "    \"\"\"\n",
    "    Loads all images from the specified folder into a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: str, path to the folder containing images\n",
    "\n",
    "    Returns:\n",
    "    - numpy_array: A NumPy array containing all the images.\n",
    "    - image_paths: A list of file paths for all the images loaded.\n",
    "    \"\"\"\n",
    "    # List to hold each image as a NumPy array\n",
    "    images_list = []\n",
    "    # List to hold paths of the images\n",
    "    image_paths = []\n",
    "\n",
    "    # Natural sorting function\n",
    "    def natural_sort_key(filename):\n",
    "        return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', filename)]\n",
    "\n",
    "    # Iterate through all the files in the given folder with natural sorting\n",
    "    for filename in sorted(os.listdir(folder_path), key=natural_sort_key):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # Open image and convert it to RGB (or handle as required)\n",
    "            with Image.open(file_path) as img:\n",
    "                img = img.convert('RGB')  # Ensure all images are in RGB format\n",
    "                img_array = np.array(img)  # Convert image to NumPy array\n",
    "                images_list.append(img_array)  # Append to the list\n",
    "                image_paths.append(file_path)  # Append the file path\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load image {filename}: {e}\")\n",
    "\n",
    "    # Find the maximum width and height among all images\n",
    "    max_height = max([img.shape[0] for img in images_list]) if images_list else 0\n",
    "    max_width = max([img.shape[1] for img in images_list]) if images_list else 0\n",
    "\n",
    "    # Pad all images to the same size (max_height, max_width)\n",
    "    padded_images_list = []\n",
    "    for img in images_list:\n",
    "        height, width, _ = img.shape\n",
    "        padded_img = np.zeros((max_height, max_width, 3), dtype=np.uint8)\n",
    "        padded_img[:height, :width, :] = img\n",
    "        padded_images_list.append(padded_img)\n",
    "\n",
    "    # Stack all images into a single NumPy array\n",
    "    if len(padded_images_list) > 0:\n",
    "        numpy_array = np.stack(padded_images_list, axis=0)\n",
    "    else:\n",
    "        numpy_array = np.array([])  # Return an empty array if no images are loaded\n",
    "\n",
    "    return numpy_array, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr, img_path_list = load_images_to_numpy(raw_image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9.2.3. run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pedictions on images\n",
    "predictions = predictor.predict(img_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add video paths to the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check https://sleap.ai/notebooks/Data_structures.html to see the data structure of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions.videos[0].backend = sleap.Video.SingleImageVideo\n",
    "# import sleap.io\n",
    "# import sleap.io.video\n",
    "\n",
    "\n",
    "predictions.videos[0].backend = sleap.io.video.SingleImageVideo(filenames=img_path_list, height_=1500, width_=2656, channels_=3, grayscale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualise the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[1].plot(scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise\n",
    "# Visualize a frame.\n",
    "predictions[12].plot(scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(run1_img_path)\n",
    "# print(pretrain2_img_path)\n",
    "# print(pretrain2_img_path + '/slp_files' + '/' + run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_file_path = pretrain2_img_path + '/slp_files' + '/' + run #+ '_adjusted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions\n",
    "predictions.save(slp_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some additional tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloading a file\n",
    "predictions = sleap.load_file('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/Aquarium_dataset/pretrain_aquariumDataset2_raw/images/train/pretrain2/slp_files/penguinpi2_video_res_2656_1500_duration_60_fps_12_2024-05-29_08-09.pkg_adjusted.slp')\n",
    "predictions[0]\n",
    "predictions[0].video.get_frame(0)\n",
    "labeled_frame = predictions[1]  # shortcut for labels.labeled_frames[0]\n",
    "labeled_frame\n",
    "predictions.videos[0]\n",
    "labeled_frame.instances\n",
    "slp_file_path_w_img = pretrain2_img_path + '/slp_files' + '/' + run + '.pkg.slp'\n",
    "# save the predictions with images\n",
    "predictions.save(slp_file_path_w_img, with_images=True, embed_all_labeled=True)\n",
    "predictions.videos[0]\n",
    "predictions.videos[0].backend.filenames\n",
    "# labels_DLC.videos[0].make_specific_backend('SingleImageVideo', listf)\n",
    "labels_DLC = sleap.load_file('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.AquariumDataset_DLC_train.slp')\n",
    "labels_DLC.videos[3]\n",
    "labels_DLC.videos[0].backend.filenames\n",
    "# other tests\n",
    "test_labels = sleap.load_file('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/Aquarium_dataset/pretrain_aquariumDataset2_raw/images/train/pretrain2/slp_files/penguinpi2_video_res_2656_1500_duration_60_fps_12_2024-05-29_08-09_adjusted.slp')\n",
    "test_labels\n",
    "test_labels[0].video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9.2.4. Now adjust labels and add the .slp file to the pretrain file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run sleap-label\n",
    "- open the .slp file that has been saved at the slp_file_path\n",
    "- adjust the predictions (do one penguin at a time. and sometimes it is easier to copy the adjusted predictions from the previous file then to adjust the predictions)\n",
    "- save as the .slp file (add the _adjusted)\n",
    "- reopen the adjusted .slp file\n",
    "- use the “Delete All Predictions…” command in the “Predictions” menu to remove all of the predicted instances from the file. Save and you’ll be left with a file which just contains your corrections.\n",
    "\n",
    "-  Open the original project file (or whatever file you want to merge into)\n",
    "(found here: /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/labels.AquariumDataset_DLC_train.slp)\n",
    "- use the “Merge Data From…” command in the “File” menu. You’ll need to select the file from which you are getting the data to merge—this would be the file with your corrected predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create a DLC model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use deeplabcut.DownSampleVideo on data to decrease size of video (input size) <br>\n",
    "Instead of using create_training_dataset you will run create_training_model_comparison. This is to create the same test train split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. create project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new project\n",
    "config_path = deeplabcut.create_new_project('DLC_simple_dataset','model1', ['/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_videos/flap1.mp4', '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_videos/flap2.mp4'],\n",
    "              copy_videos=False, multianimal=True, working_directory = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. load the annotations previously annotated to the labelled data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.0. We have already created the labelled data so we just need to copy it into the relevant folder and adjust the config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the config file so that it looks like this one: <br>\n",
    "'/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-18/config3.yaml' <br>\n",
    "<br>\n",
    "specifically you need to edit the following:<br>\n",
    "video_sets:<br>\n",
    "individuals:<br>\n",
    "multianimalbodyparts: <br>\n",
    "skeleton: <br>\n",
    "batch_size: <br>\n",
    "<br>\n",
    "Config3.yaml has only the train data in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy the data across to the labeled_data folder. <br>\n",
    "You can copy them across from the same project as the config file is from OR (option 2 is suggested as the csv is correct in the SLEAP_model folder) copy them from: '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/DLC_data'<br>\n",
    "The video_sets in the config must match the data that is copied across. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Adjusting the labeled data so that it matched "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/config.yaml'\n",
    "#config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/CollectedData_model1.h5')\n",
    "#df2 = pd.read_hdf('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/labeled-data/flap2/CollectedData_model1.h5')\n",
    "\n",
    "#df2 = pd.read_hdf('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/training-datasets/iteration-0/UnaugmentedDataSet_DLC_simple_datasetSep15/CollectedData_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_hdf('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/Filtered_CollectedData_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all_cols(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_updated = add_missing_ids(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_all_cols(df_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the body parts to retain\n",
    "# body_parts_to_keep = ['Head', 'Beak', 'Body_top', 'RFlipper_mid', 'LFlipper_mid', 'Body_bottom', 'RFoot', 'LFoot']\n",
    "\n",
    "# # Filter the dataframe by selecting only the relevant body parts\n",
    "# filtered_df = df.loc[:, (slice(None), slice(None), body_parts_to_keep)]\n",
    "\n",
    "# # Save the filtered dataframe back to an HDF5 file\n",
    "# filtered_file_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/CollectedData_model1_edit.h5'\n",
    "# filtered_df.to_hdf(filtered_file_path, key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_hdf('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/CollectedData_model1_edit.h5',key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/CollectedData_model1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_csv_filename = f\"CollectedData_{model_name}.csv\"\n",
    "#new_csv_path = os.path.join(root, new_csv_filename)\n",
    "\n",
    "# # add ids until there are 20 ids \n",
    "# filtered_df = add_missing_ids(df)\n",
    "# new_csv_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/test.csv'\n",
    "# filtered_df.to_csv(new_csv_path, index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_ids(df, total_ids=20):\n",
    "    \"\"\"\n",
    "    Adds missing individual IDs to the DataFrame until it reaches a specified number of total IDs.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame with the labeled data and existing IDs.\n",
    "    - total_ids (int): The total number of IDs that should be in the DataFrame. Defaults to 20.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A new DataFrame with missing IDs added as NaNs.\n",
    "    \"\"\"\n",
    "    # Get the existing IDs\n",
    "    existing_ids = df.columns.get_level_values('individuals').unique()\n",
    "\n",
    "    # Check how many IDs need to be added\n",
    "    missing_ids_count = total_ids - len(existing_ids)\n",
    "    \n",
    "    if missing_ids_count <= 0:\n",
    "        # No missing IDs to add\n",
    "        return df\n",
    "    \n",
    "    # Create missing IDs\n",
    "    new_ids = [f'ID{i}' for i in range(len(existing_ids) + 1, total_ids + 1)]\n",
    "    \n",
    "    # Create empty DataFrame with NaNs for new IDs\n",
    "    new_columns = pd.MultiIndex.from_product([df.columns.get_level_values('scorer').unique(),\n",
    "                                              new_ids, \n",
    "                                              df.columns.get_level_values('bodyparts').unique(),\n",
    "                                              df.columns.get_level_values('coords').unique()],\n",
    "                                             names=df.columns.names)\n",
    "    \n",
    "    # Create DataFrame with NaNs for new IDs\n",
    "    new_df = pd.DataFrame(np.nan, index=df.index, columns=new_columns)\n",
    "    \n",
    "    # Concatenate the old DataFrame with the new one\n",
    "    result_df = pd.concat([df, new_df], axis=1)\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_scorer(df, old_scorer, new_scorer):\n",
    "    \"\"\"\n",
    "    Replaces the scorer in the MultiIndex of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame with a MultiIndex.\n",
    "    - old_scorer: The scorer value to be replaced.\n",
    "    - new_scorer: The new scorer value.\n",
    "\n",
    "    Returns:\n",
    "    - df: The DataFrame with the scorer replaced.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename the 'scorer' level of the index\n",
    "    df.columns = df.columns.set_levels(\n",
    "        [new_scorer if scorer == old_scorer else scorer for scorer in df.columns.levels[0]], level=0\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labelled_images(parent_dir, model_name, base_dir, kp_to_keep):\n",
    "    \"\"\"\n",
    "    adjust data have correct naming convention and to have only relevant keypoints, load the first .h5 file from each, and save as new .h5 and .csv.\n",
    "    Also, generate a list of video paths with crop information and save it to a .txt file in base_dir. (to be put in the config file later).\n",
    "    \n",
    "    NOTES: Adjusted csvs are not it the same format... But that's not really an issue, just something to note \n",
    "    \n",
    "    Parameters:\n",
    "    - parent_dir (str): Path to the parent directory containing nested directories.\n",
    "    - model_name (str): The model name to use for renaming the saved files.\n",
    "    - base_dir (str): Path to the base directory where the video_paths.txt will be saved.\n",
    "    - kp_to_keep (list): list of kp names that need to be kept (filter others out)\n",
    "    \"\"\"\n",
    "    # Create a list to store video paths and crop information\n",
    "    video_paths_list = []\n",
    "    \n",
    "    # Walk through the parent directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(parent_dir):\n",
    "        # If there are any .h5 files in the current directory\n",
    "        h5_files = [f for f in files if f.endswith('.h5')]\n",
    "        if h5_files:\n",
    "            # Load the first .h5 file\n",
    "            h5_file_path = os.path.join(root, h5_files[0])\n",
    "            \n",
    "            # Load .h5 into DataFrame\n",
    "            df = pd.read_hdf(h5_file_path)\n",
    "\n",
    "            # Filter the dataframe by selecting only the relevant body parts\n",
    "            #filtered_df = df.loc[:, (slice(None), slice(None), kp_to_keep)]\n",
    "\n",
    "            # replace scorer with new scorer\n",
    "            #filtered_df = replace_scorer(filtered_df, old_scorer='Ro', new_scorer='model1')\n",
    "            #filtered_df = replace_scorer(df, old_scorer='Ro', new_scorer='model1')\n",
    "\n",
    "            # add ids until there are 20 ids \n",
    "            filtered_df = add_missing_ids(df)\n",
    "            \n",
    "            # Save to new .h5 file\n",
    "            new_h5_filename = f\"CollectedData_{model_name}.h5\"\n",
    "            new_h5_path = os.path.join(root, new_h5_filename)\n",
    "            filtered_df.to_hdf(new_h5_path, key='df', mode='w')\n",
    "\n",
    "            # Save to .csv file\n",
    "            new_csv_filename = f\"CollectedData_{model_name}.csv\"\n",
    "            new_csv_path = os.path.join(root, new_csv_filename)\n",
    "            filtered_df.to_csv(new_csv_path, index=True, header=True)\n",
    "\n",
    "            # Delete the old .h5 file\n",
    "            #if os.path.exists(h5_file_path):\n",
    "                #os.remove(h5_file_path)\n",
    "                #print(f\"Deleted old file: {h5_file_path}\")\n",
    "\n",
    "            # Delete the old .csv file with the same name, if it exists\n",
    "            \n",
    "            #old_csv_file = h5_file_path.replace('.h5', '.csv')\n",
    "            #if os.path.exists(old_csv_file):\n",
    "                #os.remove(old_csv_file)\n",
    "                #print(f\"Deleted old file: {old_csv_file}\")\n",
    "            \n",
    "            # Get the directory name \n",
    "            dir_name = os.path.basename(root)\n",
    "            \n",
    "            # Create video path string\n",
    "            video_path = f\"  /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_videos/{dir_name}.mp4\"\n",
    "            crop_info = \"    crop: 0, 1920, 0, 1080\"\n",
    "            video_paths_list.append(f\"{video_path}:\\n{crop_info}\")\n",
    "\n",
    "    # Save video paths to a .txt file in the base directory\n",
    "    txt_file_path = os.path.join(base_dir, \"video_paths.txt\")\n",
    "    with open(txt_file_path, 'w') as txt_file:\n",
    "        txt_file.write(\"\\n\".join(video_paths_list))\n",
    "\n",
    "    print(\"Processing complete. Files saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/labeled-data'\n",
    "# model_name = 'model1'\n",
    "# base_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15'\n",
    "# kp_to_keep = ['Head', 'Beak', 'Body_top', 'RFlipper_mid', 'LFlipper_mid', 'Body_bottom', 'RFoot', 'LFoot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data'\n",
    "model_name = 'model1'\n",
    "base_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02'\n",
    "kp_to_keep = ['Head', 'Beak', 'Body_top', 'RFlipper_mid', 'LFlipper_mid', 'Body_bottom', 'RFoot', 'LFoot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_labelled_images(parent_dir, model_name, base_dir, kp_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. check the labels and relabel where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.check_labels(config_path, visualizeindividuals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-18/config3.yaml'\n",
    "config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paf_graph='config' use the skeleton defined\n",
    "print('hello')\n",
    "deeplabcut.create_multianimaltraining_dataset(config_path, paf_graph='config', net_type=\"dlcrnet_ms5\")#, augmenter_type='imgaug', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deeplabcut.create_multianimaltraining_dataset(config_path, paf_graph='config', engine=Engine.TENSORFLOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. Look at the breakdown of the created pickle file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pickle(file_path):\n",
    "    \"\"\"\n",
    "    Load a pickle file and return its content.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the pickle file.\n",
    "    \n",
    "    Returns:\n",
    "    - data: The data loaded from the pickle file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_pickle(data, file_path):\n",
    "    \"\"\"\n",
    "    Save data back to a pickle file.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to save.\n",
    "    - file_path (str): Path to save the pickle file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    \n",
    "    print(f\"Data saved to {file_path}\")\n",
    "\n",
    "# # Load the pickle file\n",
    "# file_path = 'path_to_your_pickle_file.pickle'\n",
    "# data = load_pickle(file_path)\n",
    "\n",
    "# # View the data\n",
    "# print(\"Data loaded from pickle file:\", data)\n",
    "\n",
    "# # Modify the data (example: if it's a dictionary, you can update a key-value pair)\n",
    "# if isinstance(data, dict):\n",
    "#     data['new_key'] = 'new_value'\n",
    "#     print(\"Updated data:\", data)\n",
    "\n",
    "# # Save the modified data back to the pickle file\n",
    "# save_pickle(data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/training-datasets/iteration-0/UnaugmentedDataSet_DLC_simple_datasetSep2/DLC_simple_dataset_model195shuffle1.pickle'\n",
    "file_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/training-datasets/iteration-0/UnaugmentedDataSet_DLC_simple_datasetSep2/Documentation_data-DLC_simple_dataset_95shuffle1.pickle'\n",
    "file_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-18/evaluation-results/iteration-2/DLC_simple_datasetSep18-trainset99shuffle1/DLC_dlcrnetms5_DLC_simple_datasetSep18shuffle1_5000-snapshot-5000_meta.pickle'\n",
    "file_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-18/evaluation-results/iteration-0/DLC_simple_datasetSep18-trainset99shuffle1/DLC_dlcrnetms5_DLC_simple_datasetSep18shuffle1_50000-snapshot-50000_meta.pickle'\n",
    "file_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-18/evaluation-results/iteration-2/DLC_simple_datasetSep18-trainset99shuffle1/DLC_dlcrnetms5_DLC_simple_datasetSep18shuffle1_5000-snapshot-5000_full.pickle'\n",
    "#file_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-18/evaluation-results/iteration-0/DLC_simple_datasetSep18-trainset99shuffle1/DLC_dlcrnetms5_DLC_simple_datasetSep18shuffle1_50000-snapshot-50000_full.pickle'\n",
    "# Load the pickle file\n",
    "\n",
    "data = load_pickle(file_path)\n",
    "\n",
    "print(data)\n",
    "\n",
    "# # Save it as a JSON file for easier editing\n",
    "# json_file_path = file_path.replace('.pickle', '.json')\n",
    "# with open(json_file_path, 'w') as json_file:\n",
    "#     json.dump(data, json_file, indent=4)\n",
    "\n",
    "# print(f\"Pickle data saved as JSON to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Train the default model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training and after the creation of the dataset we must edit the pose_config.yaml file found in the train folder. <br>\n",
    "Again you can use the pose_config.yaml from the 2024-09-18 run as a guide <br>\n",
    "adjust the following:<br>\n",
    "batch_size: 4<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-18/config3.yaml'\n",
    "#config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/config.yaml'\n",
    "config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "#deeplabcut.train_network(config_path, shuffle=1, batch_size=1)#, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth and a memory limit (in MB)\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3600)])  # Set memory limit in MB\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         # Set memory growth so TensorFlow only allocates memory as needed\n",
    "#         for gpu in gpus:\n",
    "#             print(gpu)\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "#         # Alternatively, allocate a fraction of memory, e.g., 25% of total memory\n",
    "#         tf.config.experimental.set_virtual_device_configuration(\n",
    "#             gpus[0],\n",
    "#             [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(tf.config.experimental.get_memory_info('GPU:0')['total'] * 0.25))])\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "deeplabcut.train_network(config_path, maxiters=5000, saveiters=1000)#, allow_growth=True)#, device='cpu', shuffle=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(config=config_path, plotting=True)#, rescale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.extract_save_all_maps(config_path)#, shuffle=shuffle, Indices=[0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the DeepLabCut config file\n",
    "config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/config.yaml'\n",
    "\n",
    "# Path to the folder with images for inference\n",
    "image_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/test_results'\n",
    "vid_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/videos'\n",
    "\n",
    "destfolder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/test_results'\n",
    "\n",
    "# Analyze the images in the folder using the pre-trained model\n",
    "test = deeplabcut.analyze_videos(config_path, [vid_folder], save_as_csv=True, device='cpu', shuffle=6, destfolder=destfolder, videotype = '.mp4')#  videotype='.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the DeepLabCut config file\n",
    "#config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/config.yaml'\n",
    "\n",
    "# Path to the folder with images for inference\n",
    "image_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/test_results'\n",
    "vid_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/videos'\n",
    "\n",
    "destfolder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/test_results'\n",
    "\n",
    "# Analyze the images in the folder using the pre-trained model\n",
    "#test = deeplabcut.analyze_videos(config_path, [vid_folder], save_as_csv=True, device='cpu', shuffle=4, destfolder=destfolder, videotype = '.mp4')#  videotype='.jpg')\n",
    "\n",
    "deeplabcut.analyze_time_lapse_frames(config_path, image_folder, save_as_csv=True, shuffle=1, frametype='.png',gputouse=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1. Running on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a folder with the images that you want to run a test on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder with images for inference\n",
    "image_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/test_results/test'\n",
    "#vid_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/videos'\n",
    "\n",
    "destfolder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-15/test_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = deeplabcut.analyze_time_lapse_frames(config_path, image_folder, save_as_csv=True, shuffle=1, frametype='.jpg',gputouse=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Covert from DLC to ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to directories\n",
    "base_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model'\n",
    "results_save_dir = base_dir+'/a_results'\n",
    "DLC_test_dir = base_dir+'/DLC_simple_dataset-model1-2024-10-22/test_results/test'\n",
    "run_name = 'testDLC_dlcrnetms5_DLC_simple_datasetOct22shuffle1_5000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1. Load dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pred csv to df\n",
    "path_to_csv = DLC_test_dir+'/'+ run_name+'.csv'\n",
    "df_pred_DLC = pd.read_csv(path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_DLC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json gt to df\n",
    "path_to_json = results_save_dir+'/test_gt.json'\n",
    "df_gt = json_to_df(path_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_ours = initialize_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2. Loop through ground truth and save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract the required metrics from the gt and \n",
    "# save them to our df\n",
    "# return the vid_id, img_id and bbox_id \n",
    "def create_new_df_entry_DLC(gt_row, pred_row=None, gt=True):\n",
    "\n",
    "    if gt == True:\n",
    "\n",
    "        # Create a new row as a dictionary\n",
    "        new_entry = {\n",
    "            \"vid_id\": gt_row['vid_id'],\n",
    "            \"img_id\": gt_row['img_id'],\n",
    "            \"bbox_id\": gt_row['bbox_id'],\n",
    "            \"bbox_c_x\": gt_row['bbox_c_x'],\n",
    "            \"bbox_c_y\": gt_row['bbox_c_y'],\n",
    "            \"bbox_w\": gt_row['bbox_w'],\n",
    "            \"bbox_h\": gt_row['bbox_h'],\n",
    "            \"Head_x\": 0,\n",
    "            \"Head_y\": 0,\n",
    "            \"Beak_x\": 0,\n",
    "            \"Beak_y\": 0,\n",
    "            \"Body_top_x\": 0,\n",
    "            \"Body_top_y\": 0,\n",
    "            \"RFlipper_mid_x\": 0,\n",
    "            \"RFlipper_mid_y\": 0,\n",
    "            \"LFlipper_mid_x\": 0,\n",
    "            \"LFlipper_mid_y\": 0,\n",
    "            \"Body_bottom_x\": 0,\n",
    "            \"Body_bottom_y\": 0,\n",
    "            \"RFoot_x\": 0,\n",
    "            \"RFoot_y\": 0,\n",
    "            \"LFoot_x\": 0,\n",
    "            \"LFoot_y\": 0,\n",
    "            \"kp_outside_best_bbox\": 0.0,\n",
    "            \"kp_missing\": 0.0,\n",
    "            \"kp_primary_missing\": 0.0,\n",
    "            \"img_width\": gt_row['img_width'],\n",
    "            \"img_height\": gt_row['img_height'],\n",
    "            \"pred_score\": 0.0\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # find the average pred_score\n",
    "        # select the likelihood cols\n",
    "        selected_columns = pred_row.iloc[:, 3::3]\n",
    "        # Convert the selected columns to float\n",
    "        selected_columns = selected_columns.apply(pd.to_numeric, errors='coerce')\n",
    "        # find the average\n",
    "        average_pred_score = selected_columns.mean(axis=0).iloc[0]\n",
    "\n",
    "        # Create a new row as a dictionary\n",
    "        new_entry = {\n",
    "            \"vid_id\": gt_row['vid_id'],\n",
    "            \"img_id\": gt_row['img_id'],\n",
    "            \"bbox_id\": gt_row['bbox_id'],\n",
    "            \"bbox_c_x\": gt_row['bbox_c_x'],\n",
    "            \"bbox_c_y\": gt_row['bbox_c_y'],\n",
    "            \"bbox_w\": gt_row['bbox_w'],\n",
    "            \"bbox_h\": gt_row['bbox_h'],\n",
    "            \"Head_x\": float(pred_row.iloc[0,1]),\n",
    "            \"Head_y\": float(pred_row.iloc[0,2]),\n",
    "            \"Beak_x\": float(pred_row.iloc[0,4]),\n",
    "            \"Beak_y\": float(pred_row.iloc[0,5]),\n",
    "            \"Body_top_x\": float(pred_row.iloc[0,7]),\n",
    "            \"Body_top_y\": float(pred_row.iloc[0,8]),\n",
    "            \"RFlipper_mid_x\": float(pred_row.iloc[0,10]),\n",
    "            \"RFlipper_mid_y\": float(pred_row.iloc[0,11]),\n",
    "            \"LFlipper_mid_x\": float(pred_row.iloc[0,13]),\n",
    "            \"LFlipper_mid_y\": float(pred_row.iloc[0,14]),\n",
    "            \"Body_bottom_x\": float(pred_row.iloc[0,16]),\n",
    "            \"Body_bottom_y\": float(pred_row.iloc[0,17]),\n",
    "            \"RFoot_x\": float(pred_row.iloc[0,19]),\n",
    "            \"RFoot_y\": float(pred_row.iloc[0,20]),\n",
    "            \"LFoot_x\": float(pred_row.iloc[0,22]),\n",
    "            \"LFoot_y\": float(pred_row.iloc[0,23]),\n",
    "            \"kp_outside_best_bbox\": 0.0,\n",
    "            \"kp_missing\": 0.0,\n",
    "            \"kp_primary_missing\": 0.0,\n",
    "            \"img_width\": gt_row['img_width'],\n",
    "            \"img_height\": gt_row['img_height'],\n",
    "            \"pred_score\": average_pred_score\n",
    "        }\n",
    "\n",
    "    return new_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, gt_row in df_gt.iterrows():\n",
    "    # WE WILL HAVE TO ADJUST THIS FOR THE MULTIPLE PENGUINS OPTION\n",
    "\n",
    "    # Load the gt variables that are being copied across (same as in pred - such as id) into a dict that will \n",
    "    # be put appended to the df_pred_ours at the end ( after getting the pred keypoints)\n",
    "    new_entry = create_new_df_entry_DLC(gt_row, gt=True)\n",
    "\n",
    "    # find the correct row in the df_pred_DLC\n",
    "    # first get the name looking the same \n",
    "    id_pred_DLC = 'frame_'+new_entry['vid_id']+'.mp4_'+str(new_entry['img_id'])+'.jpg'\n",
    "    # extract matching row\n",
    "    matching_row = df_pred_DLC.loc[df_pred_DLC['scorer'] == id_pred_DLC]\n",
    "\n",
    "    # now save pred to the new_entry dict\n",
    "    new_entry = create_new_df_entry_DLC(gt_row, pred_row=matching_row, gt=False)\n",
    "\n",
    "    # convert new_entry to a df\n",
    "    new_entry_df = pd.DataFrame([new_entry])\n",
    "\n",
    "    # append the new entry to the df\n",
    "    df_pred_ours = pd.concat([df_pred_ours,new_entry_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_ours.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.3. Save predictions as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pred = results_save_dir+'/'+run_name+'_pred.json'\n",
    "path_gt = results_save_dir+'/'+run_name+'_gt.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_json(df_pred_ours, path_pred)\n",
    "df_to_json(df_gt, path_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Evaluate performance on converted DLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/a_results/testDLC_dlcrnetms5_DLC_simple_datasetOct22shuffle1_5000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.1. Extract keypoints from json and save to y_true and y_pred arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pred and gt to df\n",
    "df_pred = json_to_df('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/a_results/testDLC_dlcrnetms5_DLC_simple_datasetOct22shuffle1_5000_pred.json')\n",
    "df_gt = json_to_df('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/a_results/testDLC_dlcrnetms5_DLC_simple_datasetOct22shuffle1_5000_gt.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract keypoints from a df and save them to an array.\n",
    "# The resulting array should have the shape (number of entries in df, num_keypoints)\n",
    "y_pred_arr = df_pred.iloc[:,7:23].values\n",
    "y_gt_arr = df_gt.iloc[:,7:23].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.2. PCK evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pck_metric_arr(y_gt_arr,y_pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pck_per_kp_metric_arr(y_gt_arr, y_pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pck_test_list, avg_pck_per_kp_test_list, avg_pck_test_005, avg_pck_test_01, avg_pck_test_02 = full_pck_evaluation_arr(y_gt_arr, y_pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pck_test_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pck_test_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.3. Visual inspection and plot comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gt and pred df from the json (done previously)\n",
    "df_gt\n",
    "df_pred\n",
    "# create a gt and pred kp array (done previously)\n",
    "y_gt_arr\n",
    "y_pred_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am just using the sleap one. It is the same as the DLC \n",
    "img_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/test_images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr, id_list = extract_images_and_ids(df_gt, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/a_results/testDLC_dlcrnetms5_DLC_simple_datasetOct22shuffle1_5000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_sleap_gt_vs_pred_dataset_save(y_gt_arr, y_pred_arr, img_arr, id_list, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.4. Find GLOPs, total params, GPU and CPU inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.4.1 Export the model to be able to find GFLOPs and shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_shot_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/dlc-models/iteration-0/DLC_simple_datasetOct22-trainset95shuffle1/train/snapshot-5000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_model_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1'\n",
    "exported_model_path_saved_model_name = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_graph_pb = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.pb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.4.1.0 Trying Lorenes thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut.pose_estimation_tensorflow\n",
    "\n",
    "\n",
    "dlc_config = deeplabcut.pose_estimation_tensorflow.load_config(config_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.4.1.0 Trying Es thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path, \n",
    "            verbose=1,\n",
    "            save_freq=1*batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(exported_model_path_saved_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.4.1.0 This is trying my own thing (GPT assisted) - THIS WORKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a frozen graph from a .pb file\n",
    "def load_frozen_graph(pb_file):\n",
    "    with tf.io.gfile.GFile(pb_file, \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = load_frozen_graph(frozen_graph_pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the input and output tensor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/tmp/logdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.Session(graph=graph) as sess:\n",
    "    writer = tf.compat.v1.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to do it with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gf = tf.GraphDef()   \n",
    "# m_file = open(frozen_graph_pb,'rb')\n",
    "# gf.ParseFromString(m_file.read())\n",
    "with tf.io.gfile.GFile(frozen_graph_pb, \"rb\") as f:\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "with open('somefile.txt', 'a') as the_file:\n",
    "    for n in graph_def.node:\n",
    "        the_file.write(n.name+'\\n')\n",
    "\n",
    "file = open('somefile.txt','r')\n",
    "data = file.readlines()\n",
    "print (\"\\noutput name = \")\n",
    "print (data[len(data)-1])\n",
    "\n",
    "print (\"Input name = \")\n",
    "file.seek ( 0 )\n",
    "print (file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so assuming the input and output names are above with a :0 at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/saved_model_h5_converted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.Session(graph=graph) as sess:\n",
    "    # Get the input and output tensors\n",
    "    input_tensor = graph.get_tensor_by_name('Placeholder:0')\n",
    "    output_tensor = graph.get_tensor_by_name('concat_1:0')\n",
    "    \n",
    "    # Create a SignatureDef for the model\n",
    "    signature = tf.compat.v1.saved_model.signature_def_utils.predict_signature_def(\n",
    "        inputs={'input_image': input_tensor},\n",
    "        outputs={'output_heatmaps': output_tensor}\n",
    "    )\n",
    "    \n",
    "    # Save the model\n",
    "    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(saved_model_dir)\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess,\n",
    "        [tf.saved_model.SERVING],\n",
    "        signature_def_map={'serving_default': signature}\n",
    "    )\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = tf.saved_model.load(saved_model_dir)\n",
    "\n",
    "# Get the inference function\n",
    "infer = loaded_model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the model expects images of size 256x256 with 3 channels\n",
    "# Adjust the shape according to your model's expected input\n",
    "input_image = np.zeros((1, 256, 256, 3), dtype=np.uint8)\n",
    "\n",
    "# Run inference\n",
    "output = infer(input_image=tf.constant(input_image))\n",
    "\n",
    "# Access the output\n",
    "heatmaps = output['output_heatmaps']\n",
    "print(heatmaps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing that the input and output tensors are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input Tensor:\")\n",
    "print(\"Name:\", input_tensor.name)\n",
    "print(\"Shape:\", input_tensor.shape)\n",
    "print(\"Dtype:\", input_tensor.dtype)\n",
    "\n",
    "print(\"\\nOutput Tensor:\")\n",
    "print(\"Name:\", output_tensor.name)\n",
    "print(\"Shape:\", output_tensor.shape)\n",
    "print(\"Dtype:\", output_tensor.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the model as a .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabCutModelLayer(Layer):\n",
    "    def __init__(self, saved_model_dir, **kwargs):\n",
    "        super(DeepLabCutModelLayer, self).__init__(**kwargs)\n",
    "        self.saved_model_dir = saved_model_dir\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Load the model only once during the first build\n",
    "        self.loaded_model = tf.saved_model.load(self.saved_model_dir)\n",
    "        self.infer = self.loaded_model.signatures['serving_default']\n",
    "        super(DeepLabCutModelLayer, self).build(input_shape)\n",
    "\n",
    "    @tf.function  # Add this decorator to trace the method\n",
    "    def call(self, inputs):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "        outputs = self.infer(input_image=inputs)\n",
    "        heatmaps = outputs['output_heatmaps']\n",
    "        print(\"Output heatmaps shape before reshaping:\", heatmaps.shape)\n",
    "        \n",
    "        # Reshape heatmaps to include batch dimension\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        heatmaps = tf.reshape(heatmaps, (batch_size, -1, 3))\n",
    "        \n",
    "        print(\"Output heatmaps shape after reshaping:\", heatmaps.shape)\n",
    "        return heatmaps\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(DeepLabCutModelLayer, self).get_config()\n",
    "        config.update({\n",
    "            'saved_model_dir': self.saved_model_dir,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = Input(shape=(None, None, 3), dtype=tf.float32)\n",
    "# change the input tensor shape to be fixed so makes easier to compute FLOPs\n",
    "input_tensor = Input(shape=(1920, 1080, 3), dtype=tf.float32)\n",
    "output_tensor = DeepLabCutModelLayer(saved_model_dir)(input_tensor)\n",
    "keras_model = Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy_input = np.zeros((1, 256, 256, 3), dtype=np.float32)\n",
    "dummy_input = np.zeros((1, 1920, 1080, 3), dtype=np.float32)\n",
    "output = keras_model.predict(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path_h5 = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/saved_model_h5_converted/deeplabcut_keras_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "keras_model.save(save_model_path_h5, save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.4.1.1. LOAD MODEL HERE (this is condensed below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the tensorboard profiler before the execution of the code\n",
    "tf.profiler.experimental.server.start(6009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path_h5 = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/saved_model_h5_converted/deeplabcut_keras_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabCutModelLayer(Layer):\n",
    "    def __init__(self, saved_model_dir, **kwargs):\n",
    "        super(DeepLabCutModelLayer, self).__init__(**kwargs)\n",
    "        self.saved_model_dir = saved_model_dir\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Load the model only once during the first build\n",
    "        self.loaded_model = tf.saved_model.load(self.saved_model_dir)\n",
    "        self.infer = self.loaded_model.signatures['serving_default']\n",
    "        super(DeepLabCutModelLayer, self).build(input_shape)\n",
    "\n",
    "    @tf.function  # Add this decorator to trace the method\n",
    "    def call(self, inputs):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "        outputs = self.infer(input_image=inputs)\n",
    "        heatmaps = outputs['output_heatmaps']\n",
    "        print(\"Output heatmaps shape before reshaping:\", heatmaps.shape)\n",
    "        \n",
    "        # Reshape heatmaps to include batch dimension\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        heatmaps = tf.reshape(heatmaps, (batch_size, -1, 3))\n",
    "        \n",
    "        print(\"Output heatmaps shape after reshaping:\", heatmaps.shape)\n",
    "        return heatmaps\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(DeepLabCutModelLayer, self).get_config()\n",
    "        config.update({\n",
    "            'saved_model_dir': self.saved_model_dir,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_keras_model = load_model(\n",
    "    save_model_path_h5,\n",
    "    custom_objects={'DeepLabCutModelLayer': DeepLabCutModelLayer}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the loaded model input data type\n",
    "infer = loaded_model.signatures['serving_default']\n",
    "\n",
    "print(\"Input Signature:\")\n",
    "print(infer.structured_input_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loaded model with a dummy input\n",
    "#dummy_input = np.zeros((1, 256, 256, 3), dtype=np.uint8)\n",
    "dummy_input = np.zeros((1, 1920, 1080, 3), dtype=np.float32)\n",
    "output = loaded_keras_model.predict(dummy_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below just confirms that it's working (compare to the csv in the same folder and it is very accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/Simple_testAndVal/flap1/img010.png')  # Replace with your image path\n",
    "image_resized = cv2.resize(image, (1080, 1920))\n",
    "image_float32 = image_resized.astype(np.float32)\n",
    "\n",
    "# Expand dimensions to match input shape\n",
    "input_image = np.expand_dims(image_float32, axis=0)\n",
    "\n",
    "# Run inference\n",
    "output = loaded_keras_model.predict(input_image)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output values:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.8.4.1.0 This is trying my own thing (DIDNT WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorFlow graph\n",
    "meta_path = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/dlc-models/iteration-0/DLC_simple_datasetOct22-trainset95shuffle1/train/snapshot-5000.meta\"  # Replace with the correct path to your .meta file\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Import the computational graph from the .meta file\n",
    "    saver = tf.compat.v1.train.import_meta_graph(meta_path)\n",
    "    \n",
    "    # Restore the saved weights\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/dlc-models/iteration-0/DLC_simple_datasetOct22-trainset95shuffle1/train'))  # Adjust the checkpoint directory accordingly\n",
    "    \n",
    "    # Get the default graph\n",
    "    graph1 = tf.compat.v1.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1.load_weights(snap_shot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorFlow graph\n",
    "meta_path = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.meta\" # Replace with the correct path to your .meta file\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Import the computational graph from the .meta file\n",
    "    saver = tf.compat.v1.train.import_meta_graph(meta_path)\n",
    "    \n",
    "    # Restore the saved weights\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1'))  # Adjust the checkpoint directory accordingly\n",
    "    \n",
    "    # Get the default graph\n",
    "    graph2 = tf.compat.v1.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_pretrained_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.8.4.1.0 This is trying my own thing (DIDNT WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.export_model(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.meta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.train.latest_checkpoint('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/dlc-models/iteration-0/DLC_simple_datasetOct22-trainset95shuffle1/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.train.latest_checkpoint('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1'))  # Adjust the checkpoint directory accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorFlow graph\n",
    "#meta_path = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.meta\"  # Replace with the correct path to your .meta file\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Import the computational graph from the .meta file\n",
    "    saver = tf.compat.v1.train.import_meta_graph(meta_path)\n",
    "    \n",
    "    # Restore the saved weights\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/dlc-models/iteration-0/DLC_simple_datasetOct22-trainset95shuffle1/train'))  # Adjust the checkpoint directory accordingly\n",
    "    \n",
    "    # Get the default graph\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "\n",
    "    # Iterate over all the operations in the graph and print their names\n",
    "    for op in graph.get_operations():\n",
    "        print(op.name)\n",
    "        for tensor in op.values():\n",
    "            print(f\"  - {tensor.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the graph and print out all operations.\n",
    "graph = tf.compat.v1.get_default_graph()\n",
    "for op in graph.get_operations():\n",
    "    print(op.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables and graph\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Load the meta graph\n",
    "    saver = tf.compat.v1.train.import_meta_graph(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.meta\")\n",
    "    # Restore the weights\n",
    "    saver.restore(sess, \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000\")\n",
    "\n",
    "    # Accessing input and output tensors\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "\n",
    "    # Assuming you know the input and output tensor names, retrieve them\n",
    "    input_tensor = graph.get_tensor_by_name(\"Placeholder:0\")\n",
    "    output_tensor = graph.get_tensor_by_name(\"GatherNd_1:0\")\n",
    "\n",
    "    # Create a function for inference\n",
    "    def run_inference(input_data):\n",
    "        feed_dict = {input_tensor: input_data}\n",
    "        result = sess.run(output_tensor, feed_dict=feed_dict)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables and graph\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Load the meta graph\n",
    "    saver = tf.compat.v1.train.import_meta_graph(\"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.meta\")\n",
    "    # Restore the weights\n",
    "    saver.restore(sess, \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000\")\n",
    "\n",
    "    # Accessing input and output tensors\n",
    "    graph = tf.compat.v1.get_default_graph()\n",
    "\n",
    "    # Assuming you know the input and output tensor names, retrieve them\n",
    "    input_tensor = graph.get_tensor_by_name(\"Placeholder:0\")\n",
    "    output_tensor = graph.get_tensor_by_name(\"GatherNd_1:0\")\n",
    "\n",
    "    # Create a Keras model from the loaded graph\n",
    "    input_layer = tf.keras.Input(tensor=input_tensor)\n",
    "    output_layer = tf.convert_to_tensor(output_tensor)\n",
    "\n",
    "    # Create the Keras Model\n",
    "    keras_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Save the model in HDF5 format (.h5)\n",
    "    keras_model.save('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.export_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export DeepLabCut models for the model zoo or for live inference.\n",
    "\n",
    "Saves the pose configuration, snapshot files, and frozen TF graph of the model to directory named exported-models within the project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.pose_estimation_tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/dlc-models/iteration-0/DLC_simple_datasetOct22-trainset95shuffle1/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.signatures.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deeplabcut_model(export_dir):\n",
    "    # Load the saved model from the exported directory\n",
    "    imported = tf.saved_model.load(export_dir)\n",
    "    \n",
    "    # Extract the inference function\n",
    "    inference_func = imported.signatures[\"serving_default\"]\n",
    "\n",
    "    # Create a wrapper class to convert the SavedModel function into a Keras model-like structure\n",
    "    class DeeplabCutModel(tf.keras.Model):\n",
    "        def __init__(self, inference_func):\n",
    "            super(DeeplabCutModel, self).__init__()\n",
    "            self.inference_func = inference_func\n",
    "\n",
    "        def call(self, inputs):\n",
    "            return self.inference_func(inputs)\n",
    "\n",
    "    # Return a wrapped model object\n",
    "    return DeeplabCutModel(inference_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to the exported DeepLabCut model\n",
    "exported_model_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/test'\n",
    "\n",
    "# Load the model\n",
    "model = load_deeplabcut_model(exported_model_dir)\n",
    "\n",
    "# Now you can use the functions you defined, e.g., get_number_of_params, get_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.signatures.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the default serving signature\n",
    "inference_function = model.signatures['serving_default']\n",
    "\n",
    "# Get the input details from the function's structured inputs\n",
    "for input_tensor_name, input_tensor in inference_function.structured_input_signature[1].items():\n",
    "    print(f\"Input tensor name: {input_tensor_name}\")\n",
    "    print(f\"Input shape: {input_tensor.shape}\")\n",
    "    print(f\"Input dtype: {input_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/test/saved_model.pb', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.4.1. Find the GFlops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.4.1.0 The old way of doing it (using tf 1 but we want to use tf 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.profiler.model_analyzer import profile\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "# Disable eager execution to enable graph-based profiling\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loaded_keras_model\n",
    "total_flops = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.profiler.model_analyzer import profile\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "# Disable eager execution to enable graph-based profiling\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# load the models \n",
    "loaded_keras_model = load_model(\n",
    "    save_model_path_h5,\n",
    "    custom_objects={'DeepLabCutModelLayer': DeepLabCutModelLayer},\n",
    "    compile=False\n",
    ")\n",
    "#centroid_model = tf.keras.models.load_model('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/SLEAP_model/baseline_models/Oct9_test2_centroid_receptiveField_inputScale0.25_maxStride32_filterRate2_filters16_batchsize4/best_model.h5', compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_keras_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.profiler.model_analyzer import profile\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "input_shape = (1, 1920, 1080, 3)\n",
    "input_signature = [tf.TensorSpec(shape=input_shape, dtype=tf.float32)]\n",
    "\n",
    "@tf.function(input_signature=input_signature)\n",
    "def forward_pass(inputs):\n",
    "    return loaded_keras_model(inputs)\n",
    "\n",
    "# Obtain the concrete function\n",
    "#concrete_func = forward_pass.get_concrete_function(tf.random.normal(input_shape))\n",
    "concrete_func = forward_pass.get_concrete_function(tf.TensorSpec(shape=input_shape, dtype=tf.float32))\n",
    "\n",
    "# Profile the graph to compute FLOPs\n",
    "graph_info = profile(\n",
    "    concrete_func.graph,\n",
    "    options=ProfileOptionBuilder.float_operation()\n",
    ")\n",
    "\n",
    "# Since the profiler counts multiply and accumulate as separate operations,\n",
    "# divide by 2 to get the total number of FLOPs\n",
    "flops = graph_info.total_float_ops // 2\n",
    "print('FLOPs: {:,}'.format(flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy input to pass through the model\n",
    "input_shape = (1, 1920, 1080, 3)  # Adjust input shape to match your model\n",
    "#dummy_input = tf.random.normal(input_shape)\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, shape=input_shape)\n",
    "# Pass the placeholder through your model\n",
    "outputs = loaded_keras_model(inputs)\n",
    "\n",
    "# Create a session and use it to run the graph\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # Profile the model for FLOPs\n",
    "    flops = profile(\n",
    "        tf.compat.v1.get_default_graph(),\n",
    "        options=ProfileOptionBuilder.float_operation()\n",
    "    )\n",
    "\n",
    "    print('FLOPs: {}'.format(flops.total_float_ops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "# Convert the Keras model to a ConcreteFunction\n",
    "input_shape = (1, 1920, 1080, 3)\n",
    "input_spec = tf.TensorSpec(shape=input_shape, dtype=tf.float32)\n",
    "concrete_func = tf.function(loaded_keras_model).get_concrete_function(input_spec)\n",
    "\n",
    "# Convert variables to constants\n",
    "frozen_func = convert_variables_to_constants_v2(concrete_func)\n",
    "graph_def = frozen_func.graph.as_graph_def()\n",
    "\n",
    "# Profile the graph\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.profiler import model_analyzer\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "# Build the graph\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.compat.v1.import_graph_def(graph_def, name='')\n",
    "\n",
    "    # Run the profiler\n",
    "    flops = tf.compat.v1.profiler.profile(\n",
    "        graph,\n",
    "        options=ProfileOptionBuilder.float_operation()\n",
    "    )\n",
    "\n",
    "    print('FLOPs: {:,}'.format(flops.total_float_ops // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.4.1.1 THE NEW WAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a function that runs the model\n",
    "# @tf.function\n",
    "# def model_fn(inputs):\n",
    "#     return loaded_keras_model(inputs)\n",
    "\n",
    "# # Create dummy input with fixed shape\n",
    "# input_shape = (1, 1920, 1080, 3)\n",
    "# dummy_input = tf.random.normal(input_shape)\n",
    "\n",
    "# # Warm-up run\n",
    "# model_fn(dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 1920, 1080, 3)\n",
    "dummy_input = np.random.randn(*input_shape).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that runs your model\n",
    "@tf.function\n",
    "def run_model():\n",
    "    loaded_keras_model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/tmp/logdir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try use the profiler.profile API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below is trying to use tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the log directory if it doesn't exist\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# Start the profiler\n",
    "tf.profiler.experimental.start(logdir)\n",
    "\n",
    "# Run the model function to collect profile data\n",
    "#model_fn(dummy_input)\n",
    "run_model()\n",
    "\n",
    "\n",
    "# Stop the profiler\n",
    "tf.profiler.experimental.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "# Convert the model function to a concrete function\n",
    "concrete_func = model_fn.get_concrete_function(dummy_input)\n",
    "\n",
    "# Get the frozen ConcreteFunction\n",
    "frozen_func = convert_variables_to_constants_v2(concrete_func)\n",
    "graph_def = frozen_func.graph.as_graph_def()\n",
    "\n",
    "# Create the profiler options\n",
    "opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "\n",
    "# Run the profiler\n",
    "flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph, options=opts)\n",
    "\n",
    "print('FLOPs: {}'.format(flops.total_float_ops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "total_flops = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lorenes method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #replace the input_signature with your input shape\n",
    "# #mine was 1,x,y (1 batch with width and height)\n",
    "# from tensorflow.python.profiler.model_analyzer import profile\n",
    "# from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "# #this code assumes your network is stored in a variable called \"model\"\n",
    "\n",
    "# forward_pass = tf.function(model.call, \n",
    "#                            input_signature=[tf.TensorSpec(shape=(1,) + \\\n",
    "#                                                           (hyper_parameters['input_height'],\\\n",
    "#                                                            hyper_parameters['input_width']))])\n",
    "\n",
    "# graph_info = profile(forward_pass.get_concrete_function().graph,\n",
    "#                         options=ProfileOptionBuilder.float_operation())\n",
    "\n",
    "# # The //2 is necessary since profile counts multiply and accumulate\n",
    "# # as two flops, here we report the total number of multiply accumulate ops\n",
    "# flops = graph_info.total_float_ops // 2\n",
    "# print('Flops: {:,}'.format(flops))\n",
    "\n",
    "# from tensorflow.python.profiler.model_analyzer import profile\n",
    "# from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "# # Assume your loaded Keras model is stored in 'loaded_keras_model'\n",
    "# # Define the input shape based on your model's expected input\n",
    "# input_shape = (1, 1920, 1080, 3)  # Batch size of 1, height 1920, width 1080, 3 channels\n",
    "\n",
    "# # Create a tf.function that wraps the model's call method\n",
    "# forward_pass = tf.function(\n",
    "#     loaded_keras_model.call,\n",
    "#     input_signature=[tf.TensorSpec(shape=input_shape, dtype=tf.float32)]\n",
    "# )\n",
    "\n",
    "# # Obtain the concrete function\n",
    "# concrete_func = forward_pass.get_concrete_function()\n",
    "\n",
    "# # Profile the graph to compute FLOPs\n",
    "# graph_info = profile(\n",
    "#     concrete_func.graph,\n",
    "#     options=ProfileOptionBuilder.float_operation()\n",
    "# )\n",
    "\n",
    "# # Since the profiler counts multiply and accumulate as separate operations,\n",
    "# # divide by 2 to get the total number of FLOPs\n",
    "# flops = graph_info.total_float_ops // 2\n",
    "# print('FLOPs: {:,}'.format(flops))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.4.1.1. OKAY THIS IS THE CELL TO RUN, the code is all condensed in here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Start the profiler server\n",
    "tf.profiler.experimental.server.start(6009)\n",
    "\n",
    "# Define your custom layer as before\n",
    "class DeepLabCutModelLayer(Layer):\n",
    "    def __init__(self, saved_model_dir, **kwargs):\n",
    "        super(DeepLabCutModelLayer, self).__init__(**kwargs)\n",
    "        self.saved_model_dir = saved_model_dir\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Load the model only once during the first build\n",
    "        self.loaded_model = tf.saved_model.load(self.saved_model_dir)\n",
    "        self.infer = self.loaded_model.signatures['serving_default']\n",
    "        super(DeepLabCutModelLayer, self).build(input_shape)\n",
    "\n",
    "    @tf.function  # Add this decorator to trace the method\n",
    "    def call(self, inputs):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "        outputs = self.infer(input_image=inputs)\n",
    "        heatmaps = outputs['output_heatmaps']\n",
    "        print(\"Output heatmaps shape before reshaping:\", heatmaps.shape)\n",
    "        \n",
    "        # Reshape heatmaps to include batch dimension\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        heatmaps = tf.reshape(heatmaps, (batch_size, -1, 3))\n",
    "        \n",
    "        print(\"Output heatmaps shape after reshaping:\", heatmaps.shape)\n",
    "        return heatmaps\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(DeepLabCutModelLayer, self).get_config()\n",
    "        config.update({\n",
    "            'saved_model_dir': self.saved_model_dir,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# Load your Keras model\n",
    "save_model_path_h5 = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/saved_model_h5_converted/deeplabcut_keras_model.h5'\n",
    "\n",
    "loaded_keras_model = load_model(\n",
    "    save_model_path_h5,\n",
    "    custom_objects={'DeepLabCutModelLayer': DeepLabCutModelLayer}\n",
    ")\n",
    "\n",
    "# Prepare dummy input data\n",
    "input_shape = (1, 1920, 1080, 3)\n",
    "dummy_input = np.random.randn(*input_shape).astype(np.float32)\n",
    "dummy_input = tf.convert_to_tensor(dummy_input)\n",
    "\n",
    "# Run your model to ensure it's loaded and ready\n",
    "loaded_keras_model(dummy_input)\n",
    "\n",
    "# Keep the script running so the profiler server s\n",
    "# tays alive\n",
    "print(\"Profiler server started. Keep this script running while capturing the profile.\")\n",
    "input(\"Press Enter to exit...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dummy input data\n",
    "input_shape = (1, 1920, 1080, 3)\n",
    "dummy_input = np.random.randn(*input_shape).astype(np.float32)\n",
    "dummy_input = tf.convert_to_tensor(dummy_input)\n",
    "\n",
    "# Set up the log directory\n",
    "logdir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/tmp/logdir/profile'\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# Use the Trace context manager\n",
    "with tf.profiler.experimental.Trace('my_trace', step_num=0, _r=1):\n",
    "    # Run your model\n",
    "    loaded_keras_model(dummy_input)\n",
    "\n",
    "# Optionally, wait a bit to ensure data is written\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.4.1.5 Trying this now: 8 November 2024 - GPT4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.python.profiler import model_analyzer\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "# Disable eager execution to work with static graph mode\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Define the custom DeepLabCutModelLayer\n",
    "class DeepLabCutModelLayer(Layer):\n",
    "    def __init__(self, saved_model_dir, **kwargs):\n",
    "        super(DeepLabCutModelLayer, self).__init__(**kwargs)\n",
    "        self.saved_model_dir = saved_model_dir\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.loaded_model = tf.saved_model.load(self.saved_model_dir)\n",
    "        self.infer = self.loaded_model.signatures['serving_default']\n",
    "        super(DeepLabCutModelLayer, self).build(input_shape)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        outputs = self.infer(input_image=inputs)\n",
    "        heatmaps = outputs['output_heatmaps']\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        heatmaps = tf.reshape(heatmaps, (batch_size, -1, 3))\n",
    "        return heatmaps\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DeepLabCutModelLayer, self).get_config()\n",
    "        config.update({\n",
    "            'saved_model_dir': self.saved_model_dir,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Load the pre-trained Keras model with the custom layer\n",
    "save_model_path_h5 = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/saved_model_h5_converted/deeplabcut_keras_model.h5'\n",
    "loaded_keras_model = load_model(\n",
    "    save_model_path_h5,\n",
    "    custom_objects={'DeepLabCutModelLayer': DeepLabCutModelLayer},\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "# Define input shape with flexible batch size for the profiling\n",
    "input_shape = (None, 1920, 1080, 3)\n",
    "input_signature = [tf.TensorSpec(shape=input_shape, dtype=tf.float32)]\n",
    "\n",
    "# Create a concrete function from the loaded model\n",
    "@tf.function(input_signature=input_signature)\n",
    "def forward_pass(inputs):\n",
    "    return loaded_keras_model(inputs)\n",
    "\n",
    "# Obtain the concrete function to trace the graph operations\n",
    "concrete_func = forward_pass.get_concrete_function(tf.TensorSpec(shape=input_shape, dtype=tf.float32))\n",
    "\n",
    "# Step 4: Convert to a frozen graph to make it easier to analyze\n",
    "try:\n",
    "    frozen_func = convert_variables_to_constants_v2(concrete_func)\n",
    "    frozen_graph_def = frozen_func.graph.as_graph_def()\n",
    "except ValueError as e:\n",
    "    print(f\"Error during conversion to frozen graph: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Step 5: Use TensorFlow v1 Profiler to analyze the frozen graph\n",
    "# Creating a session for running the v1 Profiler\n",
    "with tf.compat.v1.Session(graph=frozen_func.graph) as sess:\n",
    "    profiler = model_analyzer.Profiler(graph=sess.graph)\n",
    "    profile_options = ProfileOptionBuilder.float_operation()    \n",
    "    flops_info = profiler.profile_operations(options=profile_options)\n",
    "\n",
    "    # Since the profiler counts multiply and accumulate separately, divide by 2 for FLOPs\n",
    "    flops = flops_info.total_float_ops // 2\n",
    "    print('FLOPs: {:,}'.format(flops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.python.profiler import model_analyzer\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "# Disable eager execution to use graph-based profiling\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Define custom DeepLabCutModelLayer (this part stays the same)\n",
    "class DeepLabCutModelLayer(Layer):\n",
    "    def __init__(self, saved_model_dir, **kwargs):\n",
    "        super(DeepLabCutModelLayer, self).__init__(**kwargs)\n",
    "        self.saved_model_dir = saved_model_dir\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Load the model only once during the first build\n",
    "        self.loaded_model = tf.saved_model.load(self.saved_model_dir)\n",
    "        self.infer = self.loaded_model.signatures['serving_default']\n",
    "        super(DeepLabCutModelLayer, self).build(input_shape)\n",
    "\n",
    "    @tf.function  # Add this decorator to trace the method\n",
    "    def call(self, inputs):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "        outputs = self.infer(input_image=inputs)\n",
    "        heatmaps = outputs['output_heatmaps']\n",
    "        print(\"Output heatmaps shape before reshaping:\", heatmaps.shape)\n",
    "\n",
    "        # Reshape heatmaps to include batch dimension\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        heatmaps = tf.reshape(heatmaps, (batch_size, -1, 3))\n",
    "\n",
    "        print(\"Output heatmaps shape after reshaping:\", heatmaps.shape)\n",
    "        return heatmaps\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DeepLabCutModelLayer, self).get_config()\n",
    "        config.update({\n",
    "            'saved_model_dir': self.saved_model_dir,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# Load the Keras model\n",
    "save_model_path_h5 = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/saved_model_h5_converted/deeplabcut_keras_model.h5'\n",
    "loaded_keras_model = load_model(\n",
    "    save_model_path_h5,\n",
    "    custom_objects={'DeepLabCutModelLayer': DeepLabCutModelLayer},\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "# Define input signature with a flexible batch size\n",
    "input_shape = (None, 1920, 1080, 3)\n",
    "input_signature = [tf.TensorSpec(shape=input_shape, dtype=tf.float32)]\n",
    "\n",
    "# Create a concrete function for the forward pass\n",
    "@tf.function(input_signature=input_signature)\n",
    "def forward_pass(inputs):\n",
    "    return loaded_keras_model(inputs)\n",
    "\n",
    "# Obtain the concrete function\n",
    "concrete_func = forward_pass.get_concrete_function(tf.TensorSpec(shape=input_shape, dtype=tf.float32))\n",
    "\n",
    "# Step 4: Convert the model to a frozen graph\n",
    "frozen_func = convert_variables_to_constants_v2(concrete_func)\n",
    "frozen_graph = frozen_func.graph\n",
    "\n",
    "# Step 5: Use TensorFlow v1 profiler to profile the frozen graph and compute FLOPs\n",
    "with frozen_graph.as_default():\n",
    "    profiler = model_analyzer.Profiler(graph=frozen_graph)\n",
    "    profile_opts = ProfileOptionBuilder.float_operation()    \n",
    "    flops_info = profiler.profile_operations(options=profile_opts)\n",
    "\n",
    "# Since the profiler counts multiply and accumulate as separate operations, divide by 2 to get the FLOPs\n",
    "flops = flops_info.total_float_ops // 2\n",
    "print('FLOPs: {:,}'.format(flops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.profiler.model_analyzer import profile\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "save_model_path_h5 = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/saved_model_h5_converted/deeplabcut_keras_model.h5'\n",
    "\n",
    "# Disable eager execution to use graph-based profiling\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "class DeepLabCutModelLayer(Layer):\n",
    "    def __init__(self, saved_model_dir, **kwargs):\n",
    "        super(DeepLabCutModelLayer, self).__init__(**kwargs)\n",
    "        self.saved_model_dir = saved_model_dir\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Load the model only once during the first build\n",
    "        self.loaded_model = tf.saved_model.load(self.saved_model_dir)\n",
    "        self.infer = self.loaded_model.signatures['serving_default']\n",
    "        super(DeepLabCutModelLayer, self).build(input_shape)\n",
    "\n",
    "    @tf.function  # Add this decorator to trace the method\n",
    "    def call(self, inputs):\n",
    "        print(\"Input shape:\", inputs.shape)\n",
    "        outputs = self.infer(input_image=inputs)\n",
    "        heatmaps = outputs['output_heatmaps']\n",
    "        print(\"Output heatmaps shape before reshaping:\", heatmaps.shape)\n",
    "        \n",
    "        # Reshape heatmaps to include batch dimension\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        heatmaps = tf.reshape(heatmaps, (batch_size, -1, 3))\n",
    "        \n",
    "        print(\"Output heatmaps shape after reshaping:\", heatmaps.shape)\n",
    "        return heatmaps\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(DeepLabCutModelLayer, self).get_config()\n",
    "        config.update({\n",
    "            'saved_model_dir': self.saved_model_dir,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "# Disable eager execution to enable graph-based profiling\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# load the models \n",
    "loaded_keras_model = load_model(\n",
    "    save_model_path_h5,\n",
    "    custom_objects={'DeepLabCutModelLayer': DeepLabCutModelLayer},\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "input_shape = (None, 1920, 1080, 3)\n",
    "input_signature = [tf.TensorSpec(shape=input_shape, dtype=tf.float32)]\n",
    "\n",
    "@tf.function(input_signature=input_signature)\n",
    "def forward_pass(inputs):\n",
    "    return loaded_keras_model(inputs)\n",
    "\n",
    "# Obtain the concrete function\n",
    "#concrete_func = loaded_keras_model.signatures['serving_default']\n",
    "concrete_func = loaded_keras_model.__call__.get_concrete_function(tf.TensorSpec(shape=input_shape, dtype=tf.float32))\n",
    "#concrete_func = forward_pass.get_concrete_function(tf.random.normal(input_shape))\n",
    "#concrete_func = forward_pass.get_concrete_function(tf.TensorSpec(shape=input_shape, dtype=tf.float32))\n",
    "\n",
    "# Profile the graph to compute FLOPs\n",
    "graph_info = profile(\n",
    "    concrete_func.graph,\n",
    "    options=ProfileOptionBuilder.float_operation()\n",
    ")\n",
    "\n",
    "# Since the profiler counts multiply and accumulate as separate operations,\n",
    "# divide by 2 to get the total number of FLOPs\n",
    "flops = graph_info.total_float_ops // 2\n",
    "print('FLOPs: {:,}'.format(flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import GraphDef\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.python.profiler import model_analyzer\n",
    "from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n",
    "\n",
    "# Get the concrete function from the loaded keras model\n",
    "concrete_func = loaded_keras_model.__call__.get_concrete_function(tf.TensorSpec(shape=input_shape, dtype=tf.float32))\n",
    "\n",
    "# Convert the model to a graph\n",
    "frozen_func = convert_variables_to_constants_v2(concrete_func)\n",
    "frozen_func.graph.as_graph_def()\n",
    "\n",
    "# Create the graph definition\n",
    "graph_def = frozen_func.graph.as_graph_def()\n",
    "\n",
    "# Run the profiler\n",
    "profiler = model_analyzer.Profiler(graph=frozen_func.graph)\n",
    "profile_opts = ProfileOptionBuilder.float_operation()    \n",
    "flops = profiler.profile_operations(options=profile_opts)\n",
    "\n",
    "print('FLOPs: {:,}'.format(flops.total_float_ops // 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.4.1. Trying this now: 8 November 2024 - GPT o1 - GOTCHA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size needs to be 1, 1920, 1080, 3\n",
    "\n",
    "# second size 1, 256, 256, 3\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_and_fix_graph_def(pb_file, input_shapes):\n",
    "    with tf.io.gfile.GFile(pb_file, \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    for node in graph_def.node:\n",
    "        if node.name in input_shapes and node.op == 'Placeholder':\n",
    "            shape = tf.TensorShape(input_shapes[node.name]).as_proto()\n",
    "            node.attr['shape'].CopyFrom(tf.compat.v1.AttrValue(shape=shape))\n",
    "\n",
    "    return graph_def\n",
    "\n",
    "# Replace with your actual paths and tensor names\n",
    "frozen_graph_pb = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.pb'\n",
    "input_shapes = {\n",
    "    'Placeholder': [1, 256, 256, 3]  # Replace 'input_image' with your actual input node name\n",
    "}\n",
    "\n",
    "fixed_graph_def = load_and_fix_graph_def(frozen_graph_pb, input_shapes)\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.import_graph_def(fixed_graph_def, name='')\n",
    "\n",
    "    with tf.compat.v1.Session(graph=graph) as sess:\n",
    "        # Replace with your actual input and output tensor names\n",
    "        input_tensor = graph.get_tensor_by_name('Placeholder:0')\n",
    "        output_tensor = graph.get_tensor_by_name('concat_1:0')\n",
    "\n",
    "        # Create sample input data\n",
    "        sample_input = np.random.rand(1, 256, 256, 3).astype(np.float32)\n",
    "\n",
    "        # Run the graph to populate shapes\n",
    "        sess.run(output_tensor, feed_dict={input_tensor: sample_input})\n",
    "\n",
    "        # Run the profiler\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(\n",
    "            graph=sess.graph,\n",
    "            run_meta=run_meta,\n",
    "            cmd='scope',\n",
    "            options=opts\n",
    "        )\n",
    "\n",
    "        print('Total FLOPs: {}'.format(flops.total_float_ops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# def load_frozen_graph(pb_file):\n",
    "#     with tf.io.gfile.GFile(pb_file, \"rb\") as f:\n",
    "#         graph_def = tf.compat.v1.GraphDef()\n",
    "#         graph_def.ParseFromString(f.read())\n",
    "#     with tf.Graph().as_default() as graph:\n",
    "#         tf.import_graph_def(graph_def, name='')\n",
    "#     return graph\n",
    "\n",
    "# frozen_graph_pb = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.pb'\n",
    "# graph = load_frozen_graph(frozen_graph_pb)\n",
    "\n",
    "# with tf.compat.v1.Session(graph=graph) as sess:\n",
    "#     # Replace 'input:0' with your model's actual input tensor name\n",
    "#     input_tensor = graph.get_tensor_by_name('Placeholder:0')\n",
    "\n",
    "#     # Create a dummy input with appropriate shape\n",
    "#     sample_input = np.random.rand(1, 1920, 1080, 3).astype(np.float32)\n",
    "\n",
    "#     # Initialize variables if any\n",
    "#     sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "#     # Run the graph once to populate shapes\n",
    "#     sess.run(\n",
    "#         [],  # No need to fetch any outputs\n",
    "#         feed_dict={input_tensor: sample_input}\n",
    "#     )\n",
    "\n",
    "#     # Set up profiler options\n",
    "#     run_meta = tf.compat.v1.RunMetadata()\n",
    "#     opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "#     flops = tf.compat.v1.profiler.profile(\n",
    "#         graph=sess.graph,\n",
    "#         run_meta=run_meta,\n",
    "#         cmd='scope',\n",
    "#         options=opts\n",
    "#     )\n",
    "\n",
    "#     print('Total FLOPs: {}'.format(flops.total_float_ops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find from a frozen graph\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def load_frozen_graph(pb_file):\n",
    "#     with tf.io.gfile.GFile(pb_file, \"rb\") as f:\n",
    "#         graph_def = tf.compat.v1.GraphDef()\n",
    "#         graph_def.ParseFromString(f.read())\n",
    "#     with tf.Graph().as_default() as graph:\n",
    "#         tf.import_graph_def(graph_def, name='')\n",
    "#     return graph\n",
    "\n",
    "# # Replace with your actual path to the .pb file\n",
    "# frozen_graph_pb = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.pb'\n",
    "\n",
    "# graph = load_frozen_graph(frozen_graph_pb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Start a TensorFlow session with the loaded graph\n",
    "# with tf.compat.v1.Session(graph=graph) as sess:\n",
    "#     # Optionally, you can set up a RunMetadata object to store additional metadata\n",
    "#     run_meta = tf.compat.v1.RunMetadata()\n",
    "    \n",
    "#     # Set up the profiler options to compute float operations\n",
    "#     opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "    \n",
    "#     # Run the profiler on the graph\n",
    "#     flops = tf.compat.v1.profiler.profile(\n",
    "#         graph=sess.graph,\n",
    "#         run_meta=run_meta,\n",
    "#         cmd='scope',\n",
    "#         options=opts\n",
    "#     )\n",
    "\n",
    "#     # Print the total FLOPs\n",
    "#     print('Total FLOPs: {}'.format(flops.total_float_ops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "# Remove the line that disables eager execution\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# # Load your model\n",
    "# save_model_path_h5 = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/saved_model_h5_converted/deeplabcut_keras_model.h5'\n",
    "\n",
    "# loaded_keras_model = load_model(\n",
    "#     save_model_path_h5,\n",
    "#     custom_objects={'DeepLabCutModelLayer': DeepLabCutModelLayer},\n",
    "#     compile=False\n",
    "# )\n",
    "\n",
    "# # Define a function to calculate FLOPs\n",
    "# def get_flops(model, input_shape):\n",
    "#     # Create a concrete function from the Keras model\n",
    "#     @tf.function\n",
    "#     def model_fn(inputs):\n",
    "#         return model(inputs)\n",
    "\n",
    "#     # Build the concrete function with a specified input shape\n",
    "#     concrete_func = model_fn.get_concrete_function(tf.TensorSpec(input_shape, tf.float32))\n",
    "\n",
    "#     # Convert the concrete function to a frozen graph\n",
    "#     frozen_func = convert_variables_to_constants_v2(concrete_func)\n",
    "#     graph_def = frozen_func.graph.as_graph_def()\n",
    "\n",
    "#     # Use the profiler to calculate FLOPs\n",
    "#     with tf.compat.v1.Session(graph=frozen_func.graph) as sess:\n",
    "#         flops = tf.compat.v1.profiler.profile(\n",
    "#             graph=sess.graph,\n",
    "#             options=tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "#         )\n",
    "#         print(f'FLOPs: {flops.total_float_ops:,}')\n",
    "\n",
    "# # Specify your input shape\n",
    "# input_shape = (1, 1920, 1080, 3)\n",
    "\n",
    "# # Call the function to compute FLOPs\n",
    "# get_flops(loaded_keras_model, input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total flops for input side: 1, 1920, 1080, 3\n",
    "total_flops = 489530833957 # 489.5308 GFLOPs\n",
    "\n",
    "# total flops for input side: 1, 256, 256, 3\n",
    "#total_flops = 15385591845 # 15.3856 GFLOPs\n",
    "\n",
    "# total flops for input size of: 1, 400, 400, 3\n",
    "#total_flops = 37562511535 # 37.5625 GFLOPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.4.2 Find the total params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_params(model):\n",
    "    \"\"\"\n",
    "    Get the total number of trainable parameters in a TensorFlow model.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): The TensorFlow model.\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    for layer in model.trainable_weights:\n",
    "        total_params += tf.size(layer).numpy()\n",
    "    return total_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = get_number_of_params(loaded_keras_model)\n",
    "print(f'Total number of trainable parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = get_number_of_params(centroid_model)\n",
    "print(f'Total number of trainable parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # size needs to be 1, 1920, 1080, 3\n",
    "\n",
    "# # second size 1, 256, 256, 3\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# def load_and_fix_graph_def(pb_file, input_shapes):\n",
    "#     with tf.io.gfile.GFile(pb_file, \"rb\") as f:\n",
    "#         graph_def = tf.compat.v1.GraphDef()\n",
    "#         graph_def.ParseFromString(f.read())\n",
    "\n",
    "#     for node in graph_def.node:\n",
    "#         if node.name in input_shapes and node.op == 'Placeholder':\n",
    "#             shape = tf.TensorShape(input_shapes[node.name]).as_proto()\n",
    "#             node.attr['shape'].CopyFrom(tf.compat.v1.AttrValue(shape=shape))\n",
    "\n",
    "#     return graph_def\n",
    "\n",
    "# # Replace with your actual paths and tensor names\n",
    "# frozen_graph_pb = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.pb'\n",
    "# input_shapes = {\n",
    "#     'Placeholder': [1, 256, 256, 3]  # Replace 'Placeholder' with your actual input node name\n",
    "# }\n",
    "\n",
    "# fixed_graph_def = load_and_fix_graph_def(frozen_graph_pb, input_shapes)\n",
    "\n",
    "# with tf.Graph().as_default() as graph:\n",
    "#     tf.import_graph_def(fixed_graph_def, name='')\n",
    "\n",
    "#     with tf.compat.v1.Session(graph=graph) as sess:\n",
    "#         # Replace with your actual input and output tensor names\n",
    "#         input_tensor = graph.get_tensor_by_name('Placeholder:0')\n",
    "#         output_tensor = graph.get_tensor_by_name('concat_1:0')\n",
    "\n",
    "#         # Create sample input data\n",
    "#         sample_input = np.random.rand(1, 256, 256, 3).astype(np.float32)\n",
    "\n",
    "#         # Run the graph to populate shapes\n",
    "#         sess.run(output_tensor, feed_dict={input_tensor: sample_input})\n",
    "\n",
    "#         # Run the profiler to get parameter counts\n",
    "#         run_meta = tf.compat.v1.RunMetadata()\n",
    "#         opts = tf.compat.v1.profiler.ProfileOptionBuilder() \\\n",
    "#                 .with_max_depth(100) \\\n",
    "#                 .select(['params']) \\\n",
    "#                 .build()\n",
    "#         param_stats = tf.compat.v1.profiler.profile(\n",
    "#             graph=sess.graph,\n",
    "#             run_meta=run_meta,\n",
    "#             cmd='scope',\n",
    "#             options=opts\n",
    "#         )\n",
    "\n",
    "#         print('Total Parameters: {}'.format(param_stats.total_parameters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size needs to be 1, 1920, 1080, 3\n",
    "\n",
    "# second size 1, 256, 256, 3\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_and_fix_graph_def(pb_file, input_shapes):\n",
    "    with tf.io.gfile.GFile(pb_file, \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    for node in graph_def.node:\n",
    "        if node.name in input_shapes and node.op == 'Placeholder':\n",
    "            shape = tf.TensorShape(input_shapes[node.name]).as_proto()\n",
    "            node.attr['shape'].CopyFrom(tf.compat.v1.AttrValue(shape=shape))\n",
    "\n",
    "    return graph_def\n",
    "\n",
    "# Replace with your actual paths and tensor names\n",
    "frozen_graph_pb = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.pb'\n",
    "input_shapes = {\n",
    "    'Placeholder': [1, 1920, 1080, 3]  # Replace 'Placeholder' with your actual input node name\n",
    "}\n",
    "\n",
    "fixed_graph_def = load_and_fix_graph_def(frozen_graph_pb, input_shapes)\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.import_graph_def(fixed_graph_def, name='')\n",
    "\n",
    "    total_params = 0\n",
    "    param_counts = {}\n",
    "\n",
    "    for op in graph.get_operations():\n",
    "        # Look for variable or constant nodes that represent weights\n",
    "        if op.type in ['Const', 'VariableV2', 'Variable', 'VarHandleOp']:\n",
    "            # Get the tensor associated with the operation\n",
    "            try:\n",
    "                tensor = op.outputs[0]\n",
    "                shape = tensor.get_shape()\n",
    "                if shape.is_fully_defined():\n",
    "                    param_count = np.prod(shape.as_list())\n",
    "                    total_params += param_count\n",
    "                    param_counts[op.name] = param_count\n",
    "                else:\n",
    "                    print(f\"Operation {op.name} has incomplete shape {shape}\")\n",
    "            except:\n",
    "                pass  # Handle any exceptions gracefully\n",
    "\n",
    "    print('Total Parameters: {}'.format(total_params))\n",
    "\n",
    "    # Optionally, print a detailed breakdown\n",
    "    for name, count in param_counts.items():\n",
    "        print(f\"{name}: {count} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total flops for input side: 1, 1920, 1080, 3\n",
    "total_params = 24003827 # 24.0038 million params\n",
    "\n",
    "# total flops for input side: 1, 256, 256, 3\n",
    "total_params = 24003827.0 # 24.0038 million params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.4.3. Find the inference time in seconds using a gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_inf_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_inference_time(model, input_data):\n",
    "#     \"\"\"\n",
    "#     Get the average inference time of a TensorFlow model on GPU in milliseconds over 30 runs.\n",
    "#     The average is calculated for the last 20 runs.\n",
    "    \n",
    "#     Args:\n",
    "#         model (tf.keras.Model): The TensorFlow model.\n",
    "#         input_data (tf.Tensor): The input data for inference.\n",
    "        \n",
    "#     Returns:\n",
    "#         float: The average inference time in milliseconds for the last 20 runs.\n",
    "#     \"\"\"\n",
    "#     inference_times = []\n",
    "#     for _ in range(40):\n",
    "#         start_time = time.time()\n",
    "#         _ = model(input_data, training=False)\n",
    "#         end_time = time.time()\n",
    "#         inference_time_ms = (end_time - start_time) * 1000\n",
    "#         inference_times.append(inference_time_ms)\n",
    "    \n",
    "#     # Calculate the average inference time for the last 20 runs\n",
    "#     average_inference_time = sum(inference_times[-20:]) / 20\n",
    "#     return average_inference_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(centroid_model.input_shape)\n",
    "# print(center_instance_model.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # centroid\n",
    "# input_data_centroid = tf.random.normal((1, 288, 480, 3))  #\n",
    "# inference_time = get_inference_time(centroid_model, input_data_centroid)\n",
    "# print(f'Inference time on GPU: {inference_time:.2f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #center_instance\n",
    "# input_data_center_instance = tf.random.normal((1, 1024, 1024, 3))  #\n",
    "# inference_time = get_inference_time(center_instance_model, input_data_center_instance)\n",
    "# print(f'Inference time on GPU: {inference_time:.2f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size needs to be 1, 1920, 1080, 3\n",
    "\n",
    "# second size 1, 256, 256, 3\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def load_and_fix_graph_def(pb_file, input_shapes):\n",
    "    with tf.io.gfile.GFile(pb_file, \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    for node in graph_def.node:\n",
    "        if node.name in input_shapes and node.op == 'Placeholder':\n",
    "            shape = tf.TensorShape(input_shapes[node.name]).as_proto()\n",
    "            node.attr['shape'].CopyFrom(tf.compat.v1.AttrValue(shape=shape))\n",
    "\n",
    "    return graph_def\n",
    "\n",
    "# Replace with your actual paths and tensor names\n",
    "frozen_graph_pb = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.pb' # Update this path\n",
    "input_shapes = {\n",
    "    'Placeholder': [1, 1920, 1080, 3]  # Replace 'Placeholder' with your actual input node name\n",
    "}\n",
    "\n",
    "fixed_graph_def = load_and_fix_graph_def(frozen_graph_pb, input_shapes)\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.import_graph_def(fixed_graph_def, name='')\n",
    "\n",
    "    # Ensure GPU is being used\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Enable dynamic memory allocation\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"Found {len(gpus)} GPU(s)\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"No GPU found. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Start a session with GPU settings\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True  # Allow GPU memory to grow as needed\n",
    "\n",
    "    with tf.compat.v1.Session(graph=graph, config=config) as sess:\n",
    "        # Replace with your actual input and output tensor names\n",
    "        input_tensor = graph.get_tensor_by_name('Placeholder:0')\n",
    "        output_tensor = graph.get_tensor_by_name('concat_1:0')  # Update if different\n",
    "\n",
    "        # Create sample input data\n",
    "        sample_input = np.random.rand(1, 1920, 1080, 3).astype(np.float32)\n",
    "\n",
    "        # Ensure variables are initialized (if any)\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        # Create a NoOp for synchronization\n",
    "        noop = tf.no_op()\n",
    "\n",
    "        # Warm-up runs\n",
    "        print(\"Warming up...\")\n",
    "        for _ in range(20):\n",
    "            sess.run(output_tensor, feed_dict={input_tensor: sample_input})\n",
    "\n",
    "        # Measure inference time over the next 20 runs\n",
    "        print(\"Measuring inference time...\")\n",
    "        inference_times = []\n",
    "        for _ in range(500):\n",
    "            start_time = time.time()\n",
    "            sess.run([output_tensor, noop], feed_dict={input_tensor: sample_input})\n",
    "            elapsed_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "            inference_times.append(elapsed_time)\n",
    "\n",
    "        # Calculate average inference time\n",
    "        #average_time = sum(inference_times) / len(inference_times)\n",
    "        average_time = sum(inference_times[-20:]) / 20\n",
    "        print(f\"Average inference time over 20 runs: {average_time:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_inf_time = average_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.4.3. Find the inference time in seconds using a cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_inf_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_inference_time_cpu(model, input_data):\n",
    "#     \"\"\"\n",
    "#     Get the average inference time of a TensorFlow model on CPU in milliseconds over 30 runs.\n",
    "#     The average is calculated for the last 20 runs.\n",
    "    \n",
    "#     Args:\n",
    "#         model (tf.keras.Model): The TensorFlow model.\n",
    "#         input_data (tf.Tensor): The input data for inference.\n",
    "        \n",
    "#     Returns:\n",
    "#         float: The average inference time in milliseconds for the last 20 runs.\n",
    "#     \"\"\"\n",
    "#     with tf.device('/CPU:0'):\n",
    "#         inference_times = []\n",
    "#         for _ in range(40):\n",
    "#             start_time = time.time()\n",
    "#             _ = model(input_data, training=False)\n",
    "#             end_time = time.time()\n",
    "#             inference_time_ms = (end_time - start_time) * 1000\n",
    "#             inference_times.append(inference_time_ms)\n",
    "        \n",
    "#         # Calculate the average inference time for the last 20 runs\n",
    "#         average_inference_time = sum(inference_times[-20:]) / 20\n",
    "#     return average_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # centroid\n",
    "# input_data_centroid = tf.random.normal((1, 288, 480, 3))  #\n",
    "# inference_time = get_inference_time_cpu(centroid_model, input_data_centroid)\n",
    "# print(f'Inference time on CPU: {inference_time:.2f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size needs to be 1, 1920, 1080, 3\n",
    "\n",
    "# second size 1, 256, 256, 3\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def load_and_fix_graph_def(pb_file, input_shapes):\n",
    "    with tf.io.gfile.GFile(pb_file, \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    for node in graph_def.node:\n",
    "        if node.name in input_shapes and node.op == 'Placeholder':\n",
    "            shape = tf.TensorShape(input_shapes[node.name]).as_proto()\n",
    "            node.attr['shape'].CopyFrom(tf.compat.v1.AttrValue(shape=shape))\n",
    "\n",
    "    return graph_def\n",
    "\n",
    "# Replace with your actual paths and tensor names\n",
    "frozen_graph_pb = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-10-22/exported-models/DLC_DLC_simple_dataset_resnet_50_iteration-0_shuffle-1/snapshot-5000.pb' # Update this path\n",
    "input_shapes = {\n",
    "    'Placeholder': [1, 1920, 1080, 3]  # Replace 'Placeholder' with your actual input node name\n",
    "}\n",
    "\n",
    "fixed_graph_def = load_and_fix_graph_def(frozen_graph_pb, input_shapes)\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.import_graph_def(fixed_graph_def, name='')\n",
    "\n",
    "    # Force TensorFlow to use CPU\n",
    "    config = tf.compat.v1.ConfigProto(device_count={'GPU': 0})  # Disable GPU\n",
    "\n",
    "    with tf.compat.v1.Session(graph=graph, config=config) as sess:\n",
    "        # Replace with your actual input and output tensor names\n",
    "        input_tensor = graph.get_tensor_by_name('Placeholder:0')\n",
    "        output_tensor = graph.get_tensor_by_name('concat_1:0')  # Update if different\n",
    "\n",
    "        # Create sample input data\n",
    "        sample_input = np.random.rand(1, 1920, 1080, 3).astype(np.float32)\n",
    "\n",
    "        # Ensure variables are initialized (if any)\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        # Warm-up runs\n",
    "        print(\"Warming up...\")\n",
    "        for _ in range(20):\n",
    "            sess.run(output_tensor, feed_dict={input_tensor: sample_input})\n",
    "\n",
    "        # Measure inference time over the next 20 runs\n",
    "        print(\"Measuring inference time...\")\n",
    "        inference_times = []\n",
    "        for _ in range(200):\n",
    "            start_time = time.time()\n",
    "            sess.run(output_tensor, feed_dict={input_tensor: sample_input})\n",
    "            elapsed_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "            inference_times.append(elapsed_time)\n",
    "\n",
    "        # Calculate average inference time\n",
    "        average_time = sum(inference_times[-20:]) / 20\n",
    "        print(f\"Average CPU inference time over 20 runs: {average_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_inf_time = average_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.5 Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place holders\n",
    "avg_pck_val_005 = None\n",
    "avg_pck_val_01 = None\n",
    "avg_pck_val_02 = None\n",
    "avg_pck_per_kp_val_list =[]\n",
    "param_dict = {}\n",
    "flops_extend = {}\n",
    "avg_pck_val_list = []\n",
    "\n",
    "num_train_imgs = 360\n",
    "num_val_imgs = 60\n",
    "num_test_imgs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results to a dict\n",
    "def load_results_to_dict(save_dir, avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02,\n",
    "                         total_params, total_flops, gpu_inf_time, cpu_inf_time, param_dict, flops_extend, avg_pck_test_list, \n",
    "                         avg_pck_per_kp_test_list, avg_pck_val_list, avg_pck_per_kp_val_list, num_train_imgs, num_val_imgs, num_test_imgs):\n",
    "\n",
    "    description = save_dir.split('/')[-1]\n",
    "    #print(description)\n",
    "\n",
    "    #avg_pck_test_dict = load_pck_to_dict(avg_pck_test_list)\n",
    "    avg_pck_per_kp_test_dict = load_pck_to_dict(avg_pck_per_kp_test_list)\n",
    "    #avg_pck_val_dict = load_pck_to_dict(avg_pck_val_list)\n",
    "    avg_pck_per_kp_val_dict = load_pck_to_dict(avg_pck_per_kp_val_list)\n",
    "\n",
    "    results_dict = {\n",
    "    'description': '',  # Placeholder for a string description\n",
    "    'pck005': None,  # Placeholder for PCK@0.05 variable\n",
    "    'pck01': None,  # Placeholder for PCK@0.1 variable\n",
    "    'pck02': None,  # Placeholder for PCK@0.2 variable\n",
    "    'total_params': None,  # Placeholder for total parameters variable\n",
    "    'GFLOPs': None,  # Placeholder for GFLOPs variable\n",
    "    'GPU_inf(ms)': None,  # Placeholder for GPU inference time variable\n",
    "    'CPU_inf(ms)': None,  # Placeholder for CPU inference time variable\n",
    "    'param_dict': {},  # Placeholder for parameter dictionary\n",
    "    'flops_dict': {},  # Placeholder for FLOPs dictionary\n",
    "    'PCK001-02': [],  # Placeholder for PCK@0.01-0.2 list\n",
    "    'PCK001-02_per_kp': {},  # Placeholder for PCK per joint dictionary\n",
    "    'val_PCK001-02': [],  # Placeholder for PCK@0.01-0.2 list\n",
    "    'val_PCK001-02_per_kp': {},  # Placeholder for PCK per joint dictionary\n",
    "    'val_pck005': None,  # Placeholder for PCK@0.05 variable\n",
    "    'val_pck01': None,  # Placeholder for PCK@0.1 variable\n",
    "    'val_pck02': None,  # Placeholder for PCK@0.2 variable\n",
    "    'num_train_imgs': None, # number of train imgs\n",
    "    'num_val_imgs': None, # number of train imgs\n",
    "    'num_test_imgs': None, # number of train imgs\n",
    "}\n",
    "    \n",
    "    results_dict['description'] = description\n",
    "    results_dict['pck005'] = avg_pck_test_005 \n",
    "    results_dict['pck01'] = avg_pck_test_01  \n",
    "    results_dict['pck02'] = avg_pck_test_02 \n",
    "    results_dict['total_params'] = total_params  \n",
    "    results_dict['GFLOPs'] = (total_flops/1e9)\n",
    "    results_dict['GPU_inf(ms)'] = gpu_inf_time*1000  \n",
    "    results_dict['CPU_inf(ms)'] = cpu_inf_time*1000  \n",
    "    results_dict['param_dict'] = param_dict  \n",
    "    results_dict['flops_dict'] = flops_extend \n",
    "    results_dict['PCK001-02'] = avg_pck_test_list\n",
    "    results_dict['PCK001-02_per_kp'] = avg_pck_per_kp_test_dict\n",
    "    results_dict['val_PCK001-02'] = avg_pck_val_list\n",
    "    results_dict['val_PCK001-02_per_kp'] = avg_pck_per_kp_val_dict\n",
    "    results_dict['val_pck005'] = avg_pck_val_005 \n",
    "    results_dict['val_pck01'] = avg_pck_val_01  \n",
    "    results_dict['val_pck02'] = avg_pck_val_02\n",
    "    results_dict['num_train_imgs'] = num_train_imgs\n",
    "    results_dict['num_val_imgs'] = num_val_imgs\n",
    "    results_dict['num_test_imgs'] = num_test_imgs\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_results_to_dict(save_dir, avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02,\n",
    "                         total_params, total_flops, gpu_inf_time, cpu_inf_time, param_dict, flops_extend, avg_pck_test_list, \n",
    "                         avg_pck_per_kp_test_list, avg_pck_val_list, avg_pck_per_kp_val_list, num_train_imgs, num_val_imgs, num_test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_json(results, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0. Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_cols(df):\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ids into list \n",
    "def load_file_to_list(file_path):\n",
    "    \"\"\"\n",
    "    loads a text file to a list with each entry on a new line becoming a new entry in the list.\n",
    "\n",
    "    :param file_path: Path to the file where the list should be saved.\n",
    "    :return list of data from file\n",
    "    \"\"\"\n",
    "    # Open the file for writing\n",
    "    lst = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Write each item on a new line\n",
    "        for line in file:\n",
    "            lst.append(line.strip())\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(df, path):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame to a .json file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be converted to JSON.\n",
    "    path (str): The path (including file name) where the .json file will be saved.\n",
    "    \"\"\"\n",
    "    df.to_json(path, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_to_df(path):\n",
    "    \"\"\"\n",
    "    Converts a .json file to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The path to the .json file that will be read.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame created from the JSON file.\n",
    "    \"\"\"\n",
    "    print(path)\n",
    "    df = pd.read_json(path, orient='records')\n",
    "    print(f\"JSON file has been successfully converted to DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dtypes_df_full_annotation_abs(df):\n",
    "    df['vid_id'] = df['vid_id'].astype(str)\n",
    "    df['img_id'] = df['img_id'].astype(str)\n",
    "    df['bbox_id'] = df['bbox_id'].astype(str)\n",
    "    df['bbox_c_x'] = df['bbox_c_x'].astype('float32')\n",
    "    df['bbox_c_y'] = df['bbox_c_y'].astype('float32')\n",
    "    df['bbox_w'] = df['bbox_w'].astype('float32')\n",
    "    df['bbox_h'] = df['bbox_h'].astype('float32')\n",
    "    df['Head_x'] = df['Head_x'].astype('float32')\n",
    "    df['Head_y'] = df['Head_y'].astype('float32')\n",
    "    df['Beak_x'] = df['Beak_x'].astype('float32')\n",
    "    df['Beak_y'] = df['Beak_y'].astype('float32')\n",
    "    df['Body_top_x'] = df['Body_top_x'].astype('float32')\n",
    "    df['Body_top_y'] = df['Body_top_y'].astype('float32')\n",
    "    df['RFlipper_mid_x'] = df['RFlipper_mid_x'].astype('float32')\n",
    "    df['RFlipper_mid_y'] = df['RFlipper_mid_y'].astype('float32')\n",
    "    df['LFlipper_mid_x'] = df['LFlipper_mid_x'].astype('float32')\n",
    "    df['LFlipper_mid_y'] = df['LFlipper_mid_y'].astype('float32')\n",
    "    df['Body_bottom_x'] = df['Body_bottom_x'].astype('float32')\n",
    "    df['Body_bottom_y'] = df['Body_bottom_y'].astype('float32')\n",
    "    df['RFoot_x'] = df['RFoot_x'].astype('float32')\n",
    "    df['RFoot_y'] = df['RFoot_y'].astype('float32')\n",
    "    df['LFoot_x'] = df['LFoot_x'].astype('float32')\n",
    "    df['LFoot_y'] = df['LFoot_y'].astype('float32')\n",
    "    df['kp_outside_best_bbox'] = df['kp_outside_best_bbox'].astype('float32')\n",
    "    df['kp_missing'] = df['kp_missing'].astype('float32')\n",
    "    df['kp_primary_missing'] = df['kp_primary_missing'].astype(bool)\n",
    "    df['img_width'] = df['img_width'].astype('float32')\n",
    "    df['img_height'] = df['img_height'].astype('float32')\n",
    "    df['bbox_max_h_w'] = df['bbox_max_h_w'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnorm_keypoints(img_size, keypoints, kp_to_null=None):\n",
    "    \"\"\"\n",
    "    De-normalizes keypoints based on image size and returns the de-normalized keypoints along with \n",
    "    the positions of any missing or nullified keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    - img_size: Tuple of the image dimensions (height, width).\n",
    "    - keypoints: List of normalized keypoints (with values between -1 and 1).\n",
    "    - kp_to_null: Optional. List of indices where the keypoints should be nulled (set to NaN).\n",
    "\n",
    "    Returns:\n",
    "    - new_keypoints: List of de-normalized keypoints where each coordinate is scaled back to the \n",
    "                     image's pixel dimensions.\n",
    "    - missing_kp: List of indices where the keypoints were either originally set to -10 (indicating \n",
    "                  missing keypoints) or explicitly nullified by the kp_to_null list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract image width and height\n",
    "    readjust_x = img_size[0]  # width of the image\n",
    "    readjust_y = img_size[1]  # height of the image\n",
    "\n",
    "    new_keypoints = []  # List to store the de-normalized keypoints\n",
    "    missing_kp = []     # List to store the indices of missing or nullified keypoints\n",
    "\n",
    "    # Iterate through each keypoint\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        # Null keypoints if they are -10 or if they are specified in kp_to_null\n",
    "        if keypoint == -10 or (kp_to_null and i in kp_to_null):\n",
    "            keypoint = np.nan  # Set keypoint to NaN\n",
    "            missing_kp.append(i)  # Record the index of the missing or nullified keypoint\n",
    "\n",
    "        # De-normalize the x-coordinates\n",
    "        if i % 2 == 0:  # Even indices are x-coordinates\n",
    "            keypoint = keypoint * readjust_x + readjust_x / 2\n",
    "        # De-normalize the y-coordinates\n",
    "        else:  # Odd indices are y-coordinates\n",
    "            keypoint = keypoint * readjust_y + readjust_y / 2\n",
    "\n",
    "        new_keypoints.append(keypoint)  # Append the de-normalized keypoint to the list\n",
    "\n",
    "    return new_keypoints, missing_kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_keypoints(img_size, keypoints):\n",
    "    \"\"\"\n",
    "    Normalizes keypoints based on image size and replaces any NaN values with -10.\n",
    "\n",
    "    Parameters:\n",
    "    - img_size: Tuple of the image dimensions (width, height).\n",
    "    - keypoints: List of de-normalized keypoints where each coordinate is in pixel dimensions.\n",
    "\n",
    "    Returns:\n",
    "    - norm_keypoints: List of normalized keypoints where each coordinate is scaled to the range \n",
    "                      [-1, 1] relative to the image size, with NaNs replaced by -10.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract image width and height\n",
    "    readjust_x = img_size[0]  # width of the image\n",
    "    readjust_y = img_size[1]  # height of the image\n",
    "\n",
    "    norm_keypoints = []  # List to store the normalized keypoints\n",
    "\n",
    "    # Iterate through each keypoint\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        # Replace NaN values with -10\n",
    "        if np.isnan(keypoint):\n",
    "            keypoint = -10.0\n",
    "        else:\n",
    "            # Normalize the x-coordinates\n",
    "            if i % 2 == 0:  # Even indices are x-coordinates\n",
    "                keypoint = (keypoint - readjust_x / 2) / readjust_x\n",
    "            # Normalize the y-coordinates\n",
    "            else:  # Odd indices are y-coordinates\n",
    "                keypoint = (keypoint - readjust_y / 2) / readjust_y\n",
    "\n",
    "        norm_keypoints.append(keypoint)  # Append the normalized keypoint to the list\n",
    "\n",
    "    return norm_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_and_keypoint(img, keypoints, nkeypoints=8, keypoint_labels=None):\n",
    "  fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "  plt.imshow(img)\n",
    "  print(keypoints)\n",
    "  x_keypoints = keypoints[::2]\n",
    "  y_keypoints = keypoints[1::2]\n",
    "  print(x_keypoints)\n",
    "  print(y_keypoints)\n",
    "  plt.scatter(x_keypoints, y_keypoints, marker='.', c=np.arange(nkeypoints), cmap='jet')\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "  if keypoint_labels is not None:\n",
    "      for i, (x, y) in enumerate(zip(x_keypoints, y_keypoints)):\n",
    "          plt.text(x, y, keypoint_labels[i], fontsize=12, color='white', \n",
    "                    bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Extract and save only the desired keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the video names\n",
    "def get_unique_video_names(directory):\n",
    "    \"\"\"\n",
    "    Scans the given directory for video files and returns a list of unique file names without the extension.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (str): The path to the directory containing the video files.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of unique video file names without extensions.\n",
    "    \"\"\"\n",
    "    unique_names = set()\n",
    "    \n",
    "    # Supported video file extensions\n",
    "    video_extensions = {'.mp4', '.mjpeg'}\n",
    "    \n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Split the filename and extension\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        # Check if the file has a video extension\n",
    "        if ext.lower() in video_extensions:\n",
    "            unique_names.add(name)  # Add the name to the set (ensures uniqueness)\n",
    "    \n",
    "    # Convert the set to a list and return\n",
    "    return list(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple\n",
    "list_of_vids = get_unique_video_names('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_videos')\n",
    "print(len(list_of_vids))\n",
    "print(list_of_vids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and rename csv files that contain annotations\n",
    "def copy_csv_files(ids, source_dir, destination_dir):\n",
    "    \"\"\"\n",
    "    Copies CSV files from sub-directories that match the given IDs and renames them to the ID.\n",
    "    \n",
    "    Parameters:\n",
    "    ids (list): A list of IDs (sub-directory names) to search for.\n",
    "    source_dir (str): The path to the root directory containing sub-directories.\n",
    "    destination_dir (str): The path to the directory where the CSV files should be copied and renamed.\n",
    "    \"\"\"\n",
    "    # Ensure the destination directory exists\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    for id_ in ids:\n",
    "        subdir_path = os.path.join(source_dir, id_)\n",
    "        \n",
    "        if os.path.isdir(subdir_path):\n",
    "            # Find the CSV file in the sub-directory\n",
    "            for file_name in os.listdir(subdir_path):\n",
    "                if file_name.endswith('.csv'):\n",
    "                    csv_file_path = os.path.join(subdir_path, file_name)\n",
    "                    \n",
    "                    # Create the destination file path\n",
    "                    destination_file_path = os.path.join(destination_dir, f\"{id_}.csv\")\n",
    "                    \n",
    "                    # Copy the CSV file to the destination directory with the new name\n",
    "                    shutil.copy(csv_file_path, destination_file_path)\n",
    "                    print(f\"Copied {csv_file_path} to {destination_file_path}\")\n",
    "                    break  # Assuming there is only one CSV file per sub-directory\n",
    "        else:\n",
    "            print(f\"Sub-directory '{id_}' not found in '{source_dir}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = '/home/matthew/Desktop/Masters/Masters-data/Roanne Penguins 2022/Penguin Project Annotation and Videos/Penguin Annotations/P1_labeled-data'\n",
    "destination_directory = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/DLC_Annotations'\n",
    "\n",
    "copy_csv_files(list_of_vids, source_directory, destination_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load csv to a dataframe and remove the keypoints not required\n",
    "need it to look like:\n",
    "vid_id,img_id,Head,Head,Beak,Beak,Body_top,Body_top,RFlipper_mid,RFlipper_mid,LFlipper_mid,LFlipper_mid,Body_bottom,Body_bottom,RFoot,RFoot,LFoot,LFoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv into a df\n",
    "df = pd.read_csv('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/DLC_Annotations/flap1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test df function\n",
    "# print(df.head())\n",
    "# print(df.info())\n",
    "# print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step through list of videos/csv ids and create a list of dfs\n",
    "list_of_kp_df_raw = []\n",
    "\n",
    "# step through the list of ids and load each csv into a df and add to list\n",
    "for _id in list_of_vids:\n",
    "    \n",
    "    # load csv to a temp df\n",
    "    print(_id)\n",
    "    df = pd.read_csv(f'/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/DLC_Annotations/{_id}.csv')\n",
    "    list_of_kp_df_raw.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list_of_kp_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append master df from csv to df with correct kp and column names\n",
    "######  THIS WILL HAVE TO BE UPDATED WHEN HAVE LOTS OF PENGUINS. JUST ADD AN IF STATEMENT TO PUT ADDITIONAL COLUMNS\n",
    "######  AS A ADDITIONAL ENTRY AND ADD A BOUNDING BBOX NUMBER COLUMN. THIS WILL LEAD TO SOME EMPTY ROWS IF A PENGUIN\n",
    "######  ENTRERS THE FRAME. SO FINALLY REMOVE ANY EMPTY ROWS\n",
    "def consolidate_dataframes(source_dfs):\n",
    "    \"\"\"\n",
    "    Consolidates data from multiple source DataFrames into a single DataFrame with specific columns.\n",
    "    \n",
    "    Parameters:\n",
    "    source_dfs (list of pd.DataFrame): List of source DataFrames to be consolidated.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A consolidated DataFrame with the selected columns.\n",
    "    \"\"\"\n",
    "    consolidated_df = pd.DataFrame(columns=[\n",
    "        'vid_id', 'image_id', 'Head_x', 'Head_y', 'Beak_x', 'Beak_y',\n",
    "        'Body_top_x', 'Body_top_y', 'RFlipper_mid_x', 'RFlipper_mid_y',\n",
    "        'LFlipper_mid_x', 'LFlipper_mid_y', 'Body_bottom_x', 'Body_bottom_y',\n",
    "        'RFoot_x', 'RFoot_y', 'LFoot_x', 'LFoot_y'\n",
    "    ])\n",
    "\n",
    "    for df in source_dfs:\n",
    "        # Skip the first 4 rows (headers)\n",
    "        df = df.iloc[3:]\n",
    "        \n",
    "        # Create a temporary DataFrame to hold the required columns\n",
    "        temp_df = pd.DataFrame({\n",
    "            'vid_id': df.iloc[:, 1],  # Column 2 (index 1)\n",
    "            'image_id': range(len(df)),  # Sequential image_id starting from 0\n",
    "            'Head_x': df.iloc[:, 3],  # Column 4 (index 3)\n",
    "            'Head_y': df.iloc[:, 4],  # Column 5 (index 4)\n",
    "            'Beak_x': df.iloc[:, 5],  # Column 6 (index 5)\n",
    "            'Beak_y': df.iloc[:, 6],  # Column 7 (index 6)\n",
    "            'Body_top_x': df.iloc[:, 7],  # Column 8 (index 7)\n",
    "            'Body_top_y': df.iloc[:, 8],  # Column 9 (index 8)\n",
    "            'RFlipper_mid_x': df.iloc[:, 13],  # Column 14 (index 13)\n",
    "            'RFlipper_mid_y': df.iloc[:, 14],  # Column 15 (index 14)\n",
    "            'LFlipper_mid_x': df.iloc[:, 15],  # Column 16 (index 15)\n",
    "            'LFlipper_mid_y': df.iloc[:, 16],  # Column 17 (index 16)\n",
    "            'Body_bottom_x': df.iloc[:, 23],  # Column 24 (index 23)\n",
    "            'Body_bottom_y': df.iloc[:, 24],  # Column 25 (index 24)\n",
    "            'RFoot_x': df.iloc[:, 27],  # Column 28 (index 27)\n",
    "            'RFoot_y': df.iloc[:, 28],  # Column 29 (index 28)\n",
    "            'LFoot_x': df.iloc[:, 29],  # Column 30 (index 29)\n",
    "            'LFoot_y': df.iloc[:, 30]  # Column 31 (index 30)\n",
    "        })\n",
    "\n",
    "        # Append the temp_df to the consolidated DataFrame\n",
    "        consolidated_df = pd.concat([consolidated_df, temp_df], ignore_index=True)\n",
    "\n",
    "    return consolidated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_kp_df = consolidate_dataframes(list_of_kp_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_kp_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_kp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/reduced_kp_raw_Simple.json'\n",
    "df_to_json(master_kp_df, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Save df as a json (and vice versa) and set the datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(df, path):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame to a .json file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be converted to JSON.\n",
    "    path (str): The path (including file name) where the .json file will be saved.\n",
    "    \"\"\"\n",
    "    df.to_json(path, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_to_df(path):\n",
    "    \"\"\"\n",
    "    Converts a .json file to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The path to the .json file that will be read.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame created from the JSON file.\n",
    "    \"\"\"\n",
    "    print(path)\n",
    "    df = pd.read_json(path, orient='records')\n",
    "    print(f\"JSON file has been successfully converted to DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dtypes_df_full_annotation_abs(df):\n",
    "    df['vid_id'] = df['vid_id'].astype(str)\n",
    "    df['img_id'] = df['img_id'].astype(str)\n",
    "    df['bbox_id'] = df['bbox_id'].astype(str)\n",
    "    df['bbox_c_x'] = df['bbox_c_x'].astype('float32')\n",
    "    df['bbox_c_y'] = df['bbox_c_y'].astype('float32')\n",
    "    df['bbox_w'] = df['bbox_w'].astype('float32')\n",
    "    df['bbox_h'] = df['bbox_h'].astype('float32')\n",
    "    df['Head_x'] = df['Head_x'].astype('float32')\n",
    "    df['Head_y'] = df['Head_y'].astype('float32')\n",
    "    df['Beak_x'] = df['Beak_x'].astype('float32')\n",
    "    df['Beak_y'] = df['Beak_y'].astype('float32')\n",
    "    df['Body_top_x'] = df['Body_top_x'].astype('float32')\n",
    "    df['Body_top_y'] = df['Body_top_y'].astype('float32')\n",
    "    df['RFlipper_mid_x'] = df['RFlipper_mid_x'].astype('float32')\n",
    "    df['RFlipper_mid_y'] = df['RFlipper_mid_y'].astype('float32')\n",
    "    df['LFlipper_mid_x'] = df['LFlipper_mid_x'].astype('float32')\n",
    "    df['LFlipper_mid_y'] = df['LFlipper_mid_y'].astype('float32')\n",
    "    df['Body_bottom_x'] = df['Body_bottom_x'].astype('float32')\n",
    "    df['Body_bottom_y'] = df['Body_bottom_y'].astype('float32')\n",
    "    df['RFoot_x'] = df['RFoot_x'].astype('float32')\n",
    "    df['RFoot_y'] = df['RFoot_y'].astype('float32')\n",
    "    df['LFoot_x'] = df['LFoot_x'].astype('float32')\n",
    "    df['LFoot_y'] = df['LFoot_y'].astype('float32')\n",
    "    df['kp_outside_best_bbox'] = df['kp_outside_best_bbox'].astype('float32')\n",
    "    df['kp_missing'] = df['kp_missing'].astype('float32')\n",
    "    df['kp_primary_missing'] = df['kp_primary_missing'].astype(bool)\n",
    "    df['img_width'] = df['img_width'].astype('float32')\n",
    "    df['img_height'] = df['img_height'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Create a single annotation with bbox and keypoints in the correct form and linked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load keypoints df and make them the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the reduced_kp_raw_Simple_df\n",
    "df_reduced_kp_raw = json_to_df('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/reduced_kp_raw_Simple.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_kp_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the format of the keypoints\n",
    "    # 1st string\n",
    "df_reduced_kp_raw.iloc[:, 0] = df_reduced_kp_raw.iloc[:, 0].astype(str)\n",
    "\n",
    "# Ensure the second column is an integer\n",
    "df_reduced_kp_raw.iloc[:, 1] = df_reduced_kp_raw.iloc[:, 1].astype(str)\n",
    "\n",
    "# Format the remaining columns as floats with minimal decimal points\n",
    "for col in df_reduced_kp_raw.columns[2:]:\n",
    "    df_reduced_kp_raw[col] = df_reduced_kp_raw[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_kp_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bboxes into a df to be used and correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step through bounding box text files and create a df with the following output\n",
    "# vid_id,img_id,bbox_c_x, bbox_c_y, bbox_w, bbox_h\n",
    "def bbox_txt_files_to_df(directory):\n",
    "    \"\"\"\n",
    "    Processes all text files in the given directory and returns a DataFrame with the columns:\n",
    "    vid_id, img_id, bbox_c_x, bbox_c_y, bbox_w, bbox_h.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing the text files.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The processed DataFrame.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    #count = 0\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Extract vid_id and img_id from the filename\n",
    "            parts = filename.split('_')\n",
    "            vid_id = parts[1].split('.')[0]\n",
    "            img_id = parts[-1].split('.')[0]\n",
    "            #print(img_id)\n",
    "\n",
    "            # Read the text file\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                for line in file:\n",
    "                    figures = line.strip().split()\n",
    "                    if len(figures) == 5:\n",
    "                        bbox_c_x, bbox_c_y, bbox_w, bbox_h = map(float, figures[1:])\n",
    "                        data.append([vid_id, img_id, np.float32(bbox_c_x), np.float32(bbox_c_y), np.float32(bbox_w), np.float32(bbox_h)])\n",
    "                        #count += 1\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=['vid_id', 'img_id', 'bbox_c_x', 'bbox_c_y', 'bbox_w', 'bbox_h'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_annotations'\n",
    "\n",
    "bbox_df_raw = bbox_txt_files_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see where there are more than one bounding box per image\n",
    "duplicates = bbox_df_raw[bbox_df_raw.duplicated(['vid_id', 'img_id'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image size stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step through image files and create a df with the following output\n",
    "# vid_id,img_id,img_wid,img_height\n",
    "\n",
    "def image_files_to_df(directory):\n",
    "    \"\"\"\n",
    "    Processes all .jpg image files in the given directory and returns a DataFrame with the columns:\n",
    "    vid_id, img_id, img_wid, img_height.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing the image files.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The processed DataFrame.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.jpg'):\n",
    "            # Extract vid_id and img_id from the filename\n",
    "            parts = filename.split('_')\n",
    "            vid_id = parts[1].split('.')[0]\n",
    "            img_id = parts[-1].split('.')[0]\n",
    "\n",
    "            # Read the image file and get its dimensions\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with Image.open(filepath) as img:\n",
    "                img_wid, img_height = img.size\n",
    "\n",
    "            # Append the data to the list\n",
    "            data.append([vid_id, img_id, np.float32(img_wid), np.float32(img_height)])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=['vid_id', 'img_id', 'img_wid', 'img_height'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_images'\n",
    "\n",
    "imgsize_df_raw = image_files_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsize_df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsize_df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match keypoint to bbox, check for keypoints outside of bbox, match bbox ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all keypoint entries in df\n",
    "# for each keypoint filter the bbox df to have only those ids\n",
    "# do the same for the image df\n",
    "# find the bbox that will contain the most keypoints from the bbox df\n",
    "#   this will require the rescaling of the bbox\n",
    "#   find how many keypoints are outside the bbox\n",
    "#   find how many keypoints are missing\n",
    "#   find whether the primary kp are missing (True/False)\n",
    "# check if this is img_id = 0 \n",
    "#   yes: increment bbox_id starting at 0\n",
    "#   no: find bbox from previous vid_id, img_id - 1 in final df that has the best fit and make bbox id = to that bbox_id, unless distance is over 20% of img size, then make bbox_id last bbox_id + 1\n",
    "# remove the bounding box from the original bbox df - I REMOVED THIS STEP AS UNNECESSARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each keypoint filter the bbox df to have only those ids\n",
    "\n",
    "def filter_dataframe_based_on_another(vid_id, img_id, df_to_filter):\n",
    "    \"\"\"\n",
    "    filters df_to_filter to show entries with the same vid_id and img_id.\n",
    "\n",
    "    Parameters:\n",
    "    vid_id: string with vid_id\n",
    "    image_id: string with img_id\n",
    "    df_to_filter (pd.DataFrame): The DataFrame to filter based on vid_id and img_id.\n",
    "\n",
    "    Returns:\n",
    "    list of pd.DataFrame: A list of filtered DataFrames, one for each row in df_main.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Filter df_to_filter based on the current row's vid_id and img_id\n",
    "    filtered_df = df_to_filter[(df_to_filter['vid_id'] == vid_id) & (df_to_filter['img_id'] == img_id)]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the bounding box\n",
    "def denorm_bbox_df(df_bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Denormalizes bounding boxes in a DataFrame from normalized values to absolute pixel values.\n",
    "\n",
    "    Parameters:\n",
    "    df_bboxes (pd.DataFrame): The DataFrame containing bounding box coordinates.\n",
    "                              Expected columns: ['vid_id', 'image_id', 'bbox_c_x', 'bbox_c_y', 'bbox_w', 'bbox_h']\n",
    "    img_width (int): The width of the image.\n",
    "    img_height (int): The height of the image.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with denormalized bounding boxes.\n",
    "                  Columns: ['vid_id', 'image_id', 'bbox_c_x', 'bbox_c_y', 'bbox_w', 'bbox_h']\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original DataFrame\n",
    "    denorm_df = df_bbox.copy()\n",
    "    #print(denorm_df)\n",
    "    #print(img_width)\n",
    "\n",
    "    # Denormalize the bounding box coordinates\n",
    "    #print(denorm_df['bbox_c_x'])# * img_width)\n",
    "    denorm_df['bbox_c_x'] = denorm_df['bbox_c_x'] * img_width\n",
    "    denorm_df['bbox_c_y'] = denorm_df['bbox_c_y'] * img_height\n",
    "    denorm_df['bbox_w'] = denorm_df['bbox_w'] * img_width\n",
    "    denorm_df['bbox_h'] = denorm_df['bbox_h'] * img_height\n",
    "\n",
    "    return denorm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the bbox that will contain the most keypoints from the bbox df\n",
    "#   find how many keypoints are outside the bbox\n",
    "#   find how many keypoints are missing\n",
    "#   find whether the primary kp are missing (True/False)\n",
    "\n",
    "def find_best_bbox(bbox_df, keypoints_df):\n",
    "    \"\"\"\n",
    "    Finds the bounding box that contains the most keypoints and returns it along with the number\n",
    "    of keypoints that fall outside that bounding box.\n",
    "\n",
    "    Parameters:\n",
    "    bbox_df (pd.DataFrame): DataFrame with bounding boxes in absolute coordinates. \n",
    "                            \n",
    "    keypoints_df (pd.DataFrame): DataFrame with keypoints.\n",
    "                                \n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing the best bounding box and the number of keypoints outside it.\n",
    "    \"\"\"\n",
    "    best_bbox = None\n",
    "    max_keypoints_inside = -1\n",
    "    min_distance_to_origin = float('inf')\n",
    "    keypoints_outside_best_bbox = 0\n",
    "    nan_keypoint_pairs = 0\n",
    "    missing_primary_keypoint = False\n",
    "\n",
    "    # Extract keypoints and check for NaN pairs and missing primary keypoints\n",
    "    keypoints = []\n",
    "    for i in range(0, 16, 2):  # Since there are 8 keypoints (16 columns), we step by 2\n",
    "        #print(i)\n",
    "        x = keypoints_df.iloc[i+2]  \n",
    "        y = keypoints_df.iloc[i+3]\n",
    "        keypoints.append((x, y))\n",
    "\n",
    "        # Check if either x or y is NaN\n",
    "        if pd.isna(x) or pd.isna(y):\n",
    "            nan_keypoint_pairs += 1\n",
    "            if i == 4 or i == 10:\n",
    "                missing_primary_keypoint = True\n",
    "\n",
    "        #break\n",
    "    count =0\n",
    "    for _, bbox in bbox_df.iterrows():\n",
    "        #vid_id, img_id = bbox['vid_id'], bbox['img_id']\n",
    "        count=+1\n",
    "        #print(count)\n",
    "        bbox_c_x, bbox_c_y, bbox_w, bbox_h = bbox['bbox_c_x'], bbox['bbox_c_y'], bbox['bbox_w'], bbox['bbox_h']\n",
    "        \n",
    "        # Calculate the bounding box corners (x_min, y_min, x_max, y_max)\n",
    "        x_min = bbox_c_x - bbox_w / 2\n",
    "        y_min = bbox_c_y - bbox_h / 2\n",
    "        x_max = bbox_c_x + bbox_w / 2\n",
    "        y_max = bbox_c_y + bbox_h / 2\n",
    "\n",
    "        # print('xy minmax')\n",
    "        # print(x_min, y_min, x_max, y_max)\n",
    "        # print('keypoints')\n",
    "        # print(keypoints)\n",
    "        \n",
    "        # Count keypoints inside the current bbox\n",
    "        keypoints_inside = sum(x_min <= x <= x_max and y_min <= y <= y_max for x, y in keypoints)\n",
    "        #print('keypoint inside')\n",
    "        #print(keypoints_inside)\n",
    "        \n",
    "        # Calculate the distance of the bbox to the origin (0,0)\n",
    "        distance_to_origin = (x_min**2 + y_min**2)**0.5\n",
    "        \n",
    "        # Update the best bbox if this one has more keypoints inside, or the same number but is closer to the origin\n",
    "        if (keypoints_inside > max_keypoints_inside) or \\\n",
    "           (keypoints_inside == max_keypoints_inside and distance_to_origin < min_distance_to_origin):\n",
    "            best_bbox = bbox\n",
    "            max_keypoints_inside = keypoints_inside\n",
    "            min_distance_to_origin = distance_to_origin\n",
    "            \n",
    "            keypoints_outside_best_bbox = len(keypoints) - nan_keypoint_pairs - keypoints_inside\n",
    "    \n",
    "    # Convert the best bbox to a DataFrame or a list\n",
    "    if best_bbox is not None:\n",
    "        best_bbox_df = pd.DataFrame([{\n",
    "            'vid_id': best_bbox['vid_id'],\n",
    "            'img_id': best_bbox['img_id'],\n",
    "            'bbox_c_x': best_bbox['bbox_c_x'],\n",
    "            'bbox_c_y': best_bbox['bbox_c_y'],\n",
    "            'bbox_w': best_bbox['bbox_w'],\n",
    "            'bbox_h': best_bbox['bbox_h'],\n",
    "        }])\n",
    "        return best_bbox_df, keypoints_outside_best_bbox, nan_keypoint_pairs, missing_primary_keypoint\n",
    "    else:\n",
    "        return None, 8, 0, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_bbox_id(best_bbox_df, final_df_prev):\n",
    "    \"\"\"\n",
    "    Finds the bbox_id in final_df_prev that is closest in distance to the bounding box in best_bbox_df \n",
    "    UNLESS it is more that 20% of image size off, then it returns max bbox_id +1.\n",
    "\n",
    "    Parameters:\n",
    "    best_bbox_df (pd.DataFrame): DataFrame with a single entry for the best bounding box.\n",
    "    final_df_prev (pd.DataFrame): DataFrame with multiple entries, each having a bounding box.\n",
    "\n",
    "    Returns:\n",
    "    str: The bbox_id of the closest bounding box in final_df_prev UNLESS \n",
    "    \"\"\"\n",
    "    # Extract the values from the single entry in best_bbox_df\n",
    "    best_bbox_c_x = best_bbox_df['bbox_c_x'].iloc[0]\n",
    "    best_bbox_c_y = best_bbox_df['bbox_c_y'].iloc[0]\n",
    "    best_bbox_w = best_bbox_df['bbox_w'].iloc[0]\n",
    "    best_bbox_h = best_bbox_df['bbox_h'].iloc[0]\n",
    "\n",
    "    # Initialize variables to track the closest bbox\n",
    "    min_distance = float('inf')\n",
    "    closest_bbox_id = None\n",
    "\n",
    "    # Find the max distance can be (rsm of the image size x 0.25 - this is 25% of image size)\n",
    "    max_distance = np.sqrt(\n",
    "        final_df_prev['img_width'].iloc[0] ** 2 +\n",
    "        final_df_prev['img_height'].iloc[0] ** 2\n",
    "    ) * 0.25\n",
    "\n",
    "    # Iterate through each entry in final_df_prev to calculate the distance\n",
    "    for index, row in final_df_prev.iterrows():\n",
    "        # Calculate the Euclidean distance (root squared mean)\n",
    "        distance = np.sqrt(\n",
    "            (row['bbox_c_x'] - best_bbox_c_x) ** 2 +\n",
    "            (row['bbox_c_y'] - best_bbox_c_y) ** 2 +\n",
    "            (row['bbox_w'] - best_bbox_w) ** 2 +\n",
    "            (row['bbox_h'] - best_bbox_h) ** 2\n",
    "        )\n",
    "        \n",
    "        # Update the closest_bbox_id if this distance is the smallest found\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_bbox_id = row['bbox_id']\n",
    "    \n",
    "    # return max bbox_id + 1 if the distance is greater than the max distance\n",
    "    if min_distance > max_distance:\n",
    "        closest_bbox_id = str(int(final_df_prev['bbox_id'].max())+1)\n",
    "\n",
    "    return closest_bbox_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all keypoint entries in df and use the above functions to create final df\n",
    "def process_dataframe(df_kp, df_bbox, df_imgsize):\n",
    "    \"\"\"\n",
    "    Iterates over each row in the DataFrame and processes the data.\n",
    "\n",
    "    Parameters:\n",
    "    df_kp, df_bbox, df_imgsize: keypoint df, bbox df, img size df \n",
    "\n",
    "    Returns:\n",
    "    df_full_annotation: Full annotation compiled df\n",
    "    \"\"\"\n",
    "    #test\n",
    "    count = 0\n",
    "    bbox_count = 0\n",
    "    prev_img_id = -1\n",
    "    df_final = pd.DataFrame(columns=[\n",
    "        'vid_id', 'img_id', 'bbox_id', 'bbox_c_x', 'bbox_c_y', 'bbox_w', 'bbox_h',\n",
    "        'Head_x', 'Head_y', 'Beak_x', 'Beak_y','Body_top_x', 'Body_top_y','RFlipper_mid_x',\t\n",
    "        'RFlipper_mid_y', 'LFlipper_mid_x', 'LFlipper_mid_y', 'Body_bottom_x', 'Body_bottom_y', \n",
    "        'RFoot_x', 'RFoot_y', 'LFoot_x', 'LFoot_y','kp_outside_best_bbox', 'kp_missing', 'kp_primary_missing',\n",
    "        'img_width', 'img_height',\n",
    "    ])\n",
    "\n",
    "    for index, row in df_kp.iterrows():\n",
    "        #test\n",
    "        count += 1\n",
    "\n",
    "        # Access data in each row using row['column_name'] - get the vid_id and img_id\n",
    "        vid_id = row['vid_id']\n",
    "        img_id = row['image_id']\n",
    "\n",
    "        # print(row)\n",
    "\n",
    "        #filter the bbox and img_size df to only have specific img and vid id\n",
    "        df_bbox_filtered = filter_dataframe_based_on_another(vid_id, img_id, df_bbox)\n",
    "        df_imgsize_filtered = filter_dataframe_based_on_another(vid_id, img_id, df_imgsize)\n",
    "\n",
    "        # get image size \n",
    "        img_width = df_imgsize_filtered['img_wid']\n",
    "        img_height = df_imgsize_filtered['img_height']\n",
    "        # convert them to scalars that can be used in math operations\n",
    "        img_width = img_width.iloc[0]  # Convert to scalar\n",
    "        img_height = img_height.iloc[0]  # Convert to scalar\n",
    "\n",
    "        # denormalise the bbox so that the they are absolute coords\n",
    "        df_bbox_filtered_abs = denorm_bbox_df(df_bbox_filtered, img_width, img_height)\n",
    "\n",
    "        # find the bbox that will contain the most keypoints from the bbox df\n",
    "        # and find how many keypoints are outside the bbox\n",
    "        df_best_bbox, kp_outside_best_bbox, kp_missing, kp_primary_missing = find_best_bbox(df_bbox_filtered_abs, row)\n",
    "\n",
    "        # check if first image in video sequence (for matching bboxes and kp, if it is first then we don't need matching)\n",
    "        if img_id == '0':\n",
    "            #yes: just increment bbox_id starting at 0\n",
    "            if prev_img_id != img_id: # if we are on the first bbox of an img\n",
    "                bbox_count = 0\n",
    "            else: # if we not on the first one\n",
    "                bbox_count += 1\n",
    "            # set the bbox id\n",
    "            bbox_id = str(bbox_count)  \n",
    "        \n",
    "        else:\n",
    "            # no: find bbox from previous vid_id, img_id - 1 in final df that has the best fit \n",
    "            # and make bbox id = to that bbox_id, unless distance is over 20% of img size, \n",
    "            # then make bbox_id last bbox_id + 1\n",
    "\n",
    "            # 1. filter for all the entries in the df_final that are from the previous image\n",
    "            df_final_filtered_prev = filter_dataframe_based_on_another(prev_vid_id, prev_img_id, df_final)\n",
    "\n",
    "            # 2. find bbox df_final_filtered_prev that has the best fit to current best bbox \n",
    "            # and make bbox id = to that bbox_id, unless distance is over 25% of img size, \n",
    "            # then make bbox_id last bbox_id + 1\n",
    "            bbox_id = find_closest_bbox_id(df_best_bbox, df_final_filtered_prev)\n",
    "\n",
    "\n",
    "        # Store the result in a dictionary and then append to the DataFrame\n",
    "        result = {\n",
    "            'vid_id': vid_id,\n",
    "            'img_id': img_id,\n",
    "            'bbox_id': bbox_id,\n",
    "            'bbox_c_x': df_best_bbox['bbox_c_x'].iloc[0],\n",
    "            'bbox_c_y': df_best_bbox['bbox_c_y'].iloc[0],\n",
    "            'bbox_w': df_best_bbox['bbox_w'].iloc[0],\n",
    "            'bbox_h': df_best_bbox['bbox_h'].iloc[0],\n",
    "            'Head_x': row.iloc[2],\n",
    "            'Head_y': row.iloc[3],\n",
    "            'Beak_x': row.iloc[4],\n",
    "            'Beak_y': row.iloc[5],\n",
    "            'Body_top_x': row.iloc[6],\n",
    "            'Body_top_y': row.iloc[7],\n",
    "            'RFlipper_mid_x': row.iloc[8],\n",
    "            'RFlipper_mid_y': row.iloc[9],\n",
    "            'LFlipper_mid_x': row.iloc[10],\n",
    "            'LFlipper_mid_y': row.iloc[11],\n",
    "            'Body_bottom_x': row.iloc[12],\n",
    "            'Body_bottom_y': row.iloc[13],\n",
    "            'RFoot_x': row.iloc[14],\n",
    "            'RFoot_y': row.iloc[15],\n",
    "            'LFoot_x': row.iloc[16],\n",
    "            'LFoot_y': row.iloc[17],\n",
    "            'kp_outside_best_bbox': float(kp_outside_best_bbox),\n",
    "            'kp_missing': float(kp_missing),\n",
    "            'kp_primary_missing': kp_primary_missing,\n",
    "            'img_width': img_width,\n",
    "            'img_height': img_height\n",
    "        }\n",
    "        \n",
    "        df_final = df_final.append(result, ignore_index=True)\n",
    "\n",
    "        # keep track of the last img_id so if we are on the second bbox for an image we know\n",
    "        prev_img_id = img_id\n",
    "        prev_vid_id = vid_id\n",
    "        \n",
    "    return df_final\n",
    "        \n",
    "\n",
    "\n",
    "        #test\n",
    "        # if count == 50:\n",
    "        #     return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs = process_dataframe(df_reduced_kp_raw, bbox_df_raw, imgsize_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check that all kp are contained in the bbox\n",
    "A. in the simple we will adjust bbox \n",
    "B. in others we will adjust bboxs to make them slightly bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the bboxs that have kps outside of the box\n",
    "df_kp_outside_bbox = df_full_annotation_abs[(df_full_annotation_abs['kp_outside_best_bbox'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kp_outside_bbox.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple\n",
    "we will just show all the bbox that are an issue and adjust the bbox and save those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all the issue bbox rows\n",
    "df_kp_outside_bbox.head(44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPORTANT: display all cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_kp_outside_bbox.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i need to adjust the Head_y value of row 390 as this is a negative (which it cant be)\n",
    "# first lets see that we have the right row number (390) -> seen in the above output\n",
    "print(df_full_annotation_abs.loc[390, 'Head_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay now just set it to 1.5\n",
    "df_full_annotation_abs.loc[390, 'Head_y'] = float(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the kp_outside flag\n",
    "df_full_annotation_abs.loc[390, 'kp_outside_best_bbox'] = float(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_full_annotation_abs.loc[390])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets just check that it is the correct dtype\n",
    "df_full_annotation_abs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save df_final as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/df_full_annotation_abs_Simple.json'\n",
    "df_to_json(df_full_annotation_abs, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Get json annotation abs to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/df_full_annotation_abs_Simple.json'\n",
    "df_full_annotation_abs = json_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs = set_dtypes_df_full_annotation_abs(df_full_annotation_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Crop images by bbox\n",
    "save them with the name vid_id_img_id_bbox_id_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all kp entries in the df_full_annotation_abs\n",
    "# for each entry crop and save image with the naming criteria using a function that takes bbox coords as input and ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to crop image by bbox and save using id naming convention\n",
    "def crop_and_save_image(image_path, save_directory, vid_id, img_id, bbox_id, bbox_c_x, bbox_c_y, bbox_w, bbox_h):\n",
    "    \"\"\"\n",
    "    Crops an image based on the provided bounding box coordinates and saves it with a specific naming convention.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - save_directory (str): Directory where the cropped image will be saved.\n",
    "    - vid_id (str): Video ID used for naming the cropped image.\n",
    "    - img_id (str): Image ID used for naming the cropped image.\n",
    "    - bbox_id (str): Bounding box ID used for naming the cropped image.\n",
    "    - bbox_c_x (float): X-coordinate of the bounding box center.\n",
    "    - bbox_c_y (float): Y-coordinate of the bounding box center.\n",
    "    - bbox_w (float): Width of the bounding box.\n",
    "    - bbox_h (float): Height of the bounding box.\n",
    "    \n",
    "    The cropped image will be saved as `vid_id_img_id_bbox_id_crop_raw.jpg` in the save directory.\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Calculate the bounding box corners\n",
    "    left = bbox_c_x - (bbox_w / 2)\n",
    "    top = bbox_c_y - (bbox_h / 2)\n",
    "    right = bbox_c_x + (bbox_w / 2)\n",
    "    bottom = bbox_c_y + (bbox_h / 2)\n",
    "    \n",
    "    # Crop the image\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    \n",
    "    # Ensure the save directory exists\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    \n",
    "    # Create the file name\n",
    "    save_filename = f\"{vid_id}_{img_id}_{bbox_id}_crop_raw.jpg\"\n",
    "    save_path = os.path.join(save_directory, save_filename)\n",
    "    \n",
    "    # Save the cropped image\n",
    "    cropped_image.save(save_path, format='JPEG')\n",
    "    \n",
    "    print(f\"Cropped image saved as {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that gets the path to the image \n",
    "def find_image_path(image_directory, vid_id, img_id):\n",
    "    \"\"\"\n",
    "    Searches through all .jpg files in the specified directory and returns the path to the image\n",
    "    that matches the provided vid_id and img_id.\n",
    "\n",
    "    Parameters:\n",
    "    - image_directory (str): Path to the directory containing the images.\n",
    "    - vid_id (str): The video ID to match in the image file name.\n",
    "    - img_id (str): The image ID to match in the image file name.\n",
    "\n",
    "    Returns:\n",
    "    - str: The path to the matching image file, or None if no match is found.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(image_directory):\n",
    "        if filename.endswith('.jpg'):\n",
    "            # Split the filename and check if it matches the vid_id and img_id\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 3:  # Ensure there are enough parts to avoid index errors\n",
    "                file_vid_id = parts[1].split('.')[0]\n",
    "                file_img_id = parts[-1].split('.')[0]\n",
    "                if file_vid_id == vid_id and file_img_id == img_id:\n",
    "                    return os.path.join(image_directory, filename)\n",
    "    \n",
    "    # If no matching image is found, return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that steps through df and calls above function\n",
    "def crop_img_from_df(df, img_dir, save_dir):\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # Access data in each row using row['column_name'] - get the vid_id and img_id\n",
    "        vid_id = row['vid_id']\n",
    "        img_id = row['img_id']\n",
    "        bbox_id = row['bbox_id']\n",
    "        bbox_c_x = row['bbox_c_x']\n",
    "        bbox_c_y = row['bbox_c_y']\n",
    "        bbox_w = row['bbox_w']\n",
    "        bbox_h = row['bbox_h']\n",
    "\n",
    "        #print(type(vid_id))\n",
    "        #print(type(img_id))\n",
    "\n",
    "        # get the relevant image path\n",
    "        img_path = find_image_path(img_dir, vid_id, img_id)\n",
    "        #print(img_path)\n",
    "\n",
    "        #crop and save the relevant bbox in the save directory\n",
    "        crop_and_save_image(img_path, save_dir, vid_id, img_id, bbox_id, bbox_c_x, bbox_c_y, bbox_w, bbox_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_images'\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_raw'\n",
    "#df_full_annotation_abs.info()\n",
    "\n",
    "crop_img_from_df(df_full_annotation_abs, img_dir, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Resize cropped images and add padding\n",
    "resize bbox_img to fit into 220x220 but do not allow distortion of the img. Use padding rather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad_images(source_dir, save_dir):\n",
    "    \"\"\"\n",
    "    Resizes and pads images from the source directory to 220x220 pixels and saves them to the save directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_dir (str): Path to the directory containing the source images.\n",
    "    - save_dir (str): Path to the directory where the resized images will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Process each image in the source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            img_path = os.path.join(source_dir, filename)\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Resize while maintaining aspect ratio\n",
    "            img.thumbnail((220, 220), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Calculate padding to make the image 220x220\n",
    "            delta_w = 220 - img.size[0]\n",
    "            delta_h = 220 - img.size[1]\n",
    "            padding = (delta_w // 2, delta_h // 2, delta_w - (delta_w // 2), delta_h - (delta_h // 2))\n",
    "            \n",
    "            # Add padding to the image\n",
    "            padded_img = ImageOps.expand(img, padding, fill='black')\n",
    "            \n",
    "            # Rename the image\n",
    "            parts = filename.split('_')\n",
    "            base_name = '_'.join(parts[:-1])\n",
    "            extension = filename.split('.')[-1]\n",
    "            new_filename = f\"{base_name}_220x220.{extension}\"\n",
    "            \n",
    "            # Save the new image\n",
    "            save_path = os.path.join(save_dir, new_filename)\n",
    "            padded_img.save(save_path, format='JPEG')\n",
    "            print(f\"Saved resized image as {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_raw'\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_crop_220'\n",
    "resize_and_pad_images(source_dir, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Normalise the df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the keypoints (normalise by the abs value of the bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max(w/h) of the bbox\n",
    "# shift x and y coords for each annotation by the centre of the bbox\n",
    "# devide by the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_kp_by_bbox_df(df_orginal):\n",
    "\n",
    "    # copy so don't effect the original df\n",
    "    df = df_orginal.copy()\n",
    "    \n",
    "    # first create a new col that is the max value of the height and width\n",
    "    df['bbox_max_h_w'] = df[['bbox_w', 'bbox_h']].max(axis=1) \n",
    "\n",
    "    ## shift coords \n",
    "    # list of kp y cols\n",
    "    y_columns = [\n",
    "        'Head_y', 'Beak_y', 'Body_top_y', 'RFlipper_mid_y', \n",
    "        'LFlipper_mid_y', 'Body_bottom_y', 'RFoot_y', 'LFoot_y'\n",
    "    ]\n",
    "    # list of kp x cols\n",
    "    x_columns = [\n",
    "        'Head_x', 'Beak_x', 'Body_top_x', 'RFlipper_mid_x', \n",
    "        'LFlipper_mid_x', 'Body_bottom_x', 'RFoot_x', 'LFoot_x'\n",
    "    ]\n",
    "    # Subtract bbox_c_y from the selected '_y' columns\n",
    "    df[y_columns] = df[y_columns].subtract(df['bbox_c_y'], axis=0)\n",
    "    # Subtract bbox_c_x from the selected '_x' columns\n",
    "    df[x_columns] = df[x_columns].subtract(df['bbox_c_x'], axis=0)\n",
    "\n",
    "    # scale (devide) by the max of bbox width and hight (bbox_max_h_w)\n",
    "    df[y_columns] = df[y_columns].div(df['bbox_max_h_w'], axis=0)\n",
    "    df[x_columns] = df[x_columns].div(df['bbox_max_h_w'], axis=0)\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_kpnorm = norm_kp_by_bbox_df(df_full_annotation_abs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \n",
    "# with pd.option_context('display.max_columns', None):\n",
    "#     print(df_full_annotation_abs.head(2))\n",
    "\n",
    "# with pd.option_context('display.max_columns', None):\n",
    "#     print(df_test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the bbox (normalise by the size of the img)\n",
    "normalising how it is done in the obj dect (between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_bbox_by_img_df(df_orginal):\n",
    "\n",
    "    # copy so don't effect the original df\n",
    "    df = df_orginal.copy()\n",
    "\n",
    "    ## shift coords \n",
    "    # list of kp y cols\n",
    "    y_columns = [\n",
    "        'bbox_c_y', 'bbox_h'\n",
    "    ]\n",
    "    # list of kp x cols \n",
    "    # I AM NORMILISING THE bbox_max_h_w BY THE IMAGE WIDTH\n",
    "    x_columns = [\n",
    "        'bbox_c_x', 'bbox_w', 'bbox_max_h_w'\n",
    "    ]\n",
    "\n",
    "    # scale (devide) by the width and hight of the image\n",
    "    df[y_columns] = df[y_columns].div(df['img_height'], axis=0)\n",
    "    df[x_columns] = df[x_columns].div(df['img_width'], axis=0)\n",
    "\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm = norm_bbox_by_img_df(df_full_annotation_kpnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_full_annotation_abs.head(2))\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_full_annotation_norm.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the norm full annotation df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/df_full_annotation_norm_Simple.json'\n",
    "df_to_json(df_full_annotation_norm, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build Basic Regression PE Model (DeepPose based)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. load the normalised annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/df_full_annotation_norm_Simple.json'\n",
    "df_full_annotation_norm = json_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm = set_dtypes_df_full_annotation_abs(df_full_annotation_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the db easier to work with I am going to create a list with the kp col names, bbox col names, id col names\n",
    "id_cols = df_full_annotation_norm.iloc[:, :3].columns.to_list()\n",
    "bbox_cols = df_full_annotation_norm.iloc[:, 3:7].columns.to_list()\n",
    "kp_cols = df_full_annotation_norm.iloc[:, 7:23].columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_cols)\n",
    "print(bbox_cols)\n",
    "print(kp_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Remove rows where too many (or primary) keypoints are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full_annotation_norm.iloc[359])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any examples with more NaN values than the chosen threshold\n",
    "# The nan values are there when a keypoint is occluded\n",
    "\n",
    "def remove_rows_with_too_many_nans(df, columns_to_check, nan_threshold):\n",
    "    \"\"\"\n",
    "    Remove rows from the DataFrame where the number of NaN values in specified columns exceeds the threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to process.\n",
    "    - columns_to_check: A list of column names to check for NaN values.\n",
    "    - nan_threshold: The maximum allowed number of NaN values in the specified columns. Rows with more NaNs will be removed.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with rows exceeding the NaN threshold removed.\n",
    "    \"\"\"\n",
    "    # Count NaNs only in the specified columns\n",
    "    nan_counts = df[columns_to_check].isna().sum(axis=1)\n",
    "    print(type(nan_counts))\n",
    "\n",
    "    # Identify rows where NaN count is below or equal to the threshold\n",
    "    rows_to_keep = nan_counts <= nan_threshold\n",
    "    print(rows_to_keep[rows_to_keep==False].index)\n",
    "\n",
    "    # Filter the DataFrame to keep only the desired rows\n",
    "    filtered_df = df[rows_to_keep]\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function to remove the rows with more than 14 keypoint coords missing\n",
    "# the number of keypoints is 14 and each has 2 coords so there are 28 coords\n",
    "df_full_annotation_norm = remove_rows_with_too_many_nans(df_full_annotation_norm, kp_cols, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full_annotation_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any examples where the primary kp are missing. When these are missing we will not be able to use our PCK metric\n",
    "# The nan values are there when a keypoint is occluded\n",
    "\n",
    "def remove_rows_with_missing_primary_kp(df):\n",
    "    \"\"\"\n",
    "    Remove rows from the DataFrame where kp_primary_missing is set to true\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with rows not missing the primary kp.\n",
    "    \"\"\"\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        print(df[df['kp_primary_missing'] == True])\n",
    "\n",
    "    # Identify rows where NaN count is below or equal to the threshold\n",
    "    rows_to_keep = df['kp_primary_missing'] == False\n",
    "\n",
    "    # Filter the DataFrame to keep only the desired rows\n",
    "    filtered_df = df[rows_to_keep]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm = remove_rows_with_missing_primary_kp(df_full_annotation_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Replace nan with out of range (-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to train the data keypoints cannot have the value nan\n",
    "# this function removes the value nan from the keypoint df\n",
    "def convert_nans_to_neg_ten(df, columns):\n",
    "\n",
    "    df_adjusted = df.copy()\n",
    "\n",
    "    # Iterate over the specified columns\n",
    "    for col in columns:\n",
    "        # Replace NaN values with -10\n",
    "        df_adjusted[col].fillna(-10, inplace=True)\n",
    "\n",
    "    return df_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm = convert_nans_to_neg_ten(df_full_annotation_norm, kp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Split data into train, val and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. get list of ids that are in each set from obj detect folder and save to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of ids from image_obj detect\n",
    "def extract_image_ids(folder_path):\n",
    "    \"\"\"\n",
    "    Extracts image IDs from filenames in the given folder. \n",
    "    The filenames are assumed to be in the format something_vidid.something_imgid.something.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of strings in the format 'vidid_imgid'.\n",
    "    \"\"\"\n",
    "    image_ids = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if os.path.isfile(os.path.join(folder_path, filename)):  # Ensure it's a file\n",
    "            parts = filename.split('_')  # Split by the underscore\n",
    "            vidid = parts[1].split('.')[0]  # Extract vidid (part after first underscore and before first dot)\n",
    "            imgid = parts[2].split('.')[0]  # Extract imgid (part after second underscore and before second dot)\n",
    "            image_ids.append(f'{vidid}_{imgid}')\n",
    "\n",
    "    return image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unique ids list as a text file\n",
    "def save_list_to_file(list_data, file_path):\n",
    "    \"\"\"\n",
    "    Saves a list to a text file with each entry on a new line.\n",
    "\n",
    "    :param list_data: List of strings to be saved to a file.\n",
    "    :param file_path: Path to the file where the list should be saved.\n",
    "    \"\"\"\n",
    "    # Open the file for writing\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Write each item on a new line\n",
    "        for item in list_data:\n",
    "            file.write(f\"{item}\\n\")  # Add a newline after each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test imgs\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/YoloV8_dataset_Simple_parent/YoloV8_dataset_Simple/images/test'\n",
    "ids_test = extract_image_ids(path)\n",
    "print(ids_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val imgs\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/YoloV8_dataset_Simple_parent/YoloV8_dataset_Simple/images/val'\n",
    "ids_val = extract_image_ids(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train imgs\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/YoloV8_dataset_Simple_parent/YoloV8_dataset_Simple/images/train'\n",
    "ids_train = extract_image_ids(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lists to txt files\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_test.txt'\n",
    "save_list_to_file(ids_test, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_val.txt'\n",
    "\n",
    "save_list_to_file(ids_val, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_train.txt'\n",
    "\n",
    "save_list_to_file(ids_train, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. split df based on train, val, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_id_parts(df, id_list, col1, col2):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only rows where:\n",
    "    - col1 matches idpart1\n",
    "    - col2 matches idpart2\n",
    "    The ID parts are derived from the id_list, where each ID is in the format 'idpart1_idpart2'.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be filtered.\n",
    "    id_list (list): The list of IDs in the format 'idpart1_idpart2'.\n",
    "    col1 (str): The name of the first column containing idpart1.\n",
    "    col2 (str): The name of the second column containing idpart2.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A filtered DataFrame containing only the rows matching the ID parts.\n",
    "    \"\"\"\n",
    "    # Split the IDs into idpart1 and idpart2\n",
    "    id_parts = [id.split('_') for id in id_list]\n",
    "    \n",
    "    # Convert the list of tuples into a DataFrame\n",
    "    id_df = pd.DataFrame(id_parts, columns=[col1, col2])\n",
    "    \n",
    "    # Perform an inner merge to filter the DataFrame\n",
    "    filtered_df = pd.merge(df, id_df, how='inner', on=[col1, col2])\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "df_full_annotation_norm_test = filter_df_by_id_parts(df_full_annotation_norm, ids_test, 'vid_id', 'img_id')\n",
    "\n",
    "print(df_full_annotation_norm_test.info())\n",
    "\n",
    "display_all_cols(df_full_annotation_norm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val set\n",
    "df_full_annotation_norm_val = filter_df_by_id_parts(df_full_annotation_norm, ids_val, 'vid_id', 'img_id')\n",
    "\n",
    "print(df_full_annotation_norm_val.info())\n",
    "display_all_cols(df_full_annotation_norm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "df_full_annotation_norm_train = filter_df_by_id_parts(df_full_annotation_norm, ids_train, 'vid_id', 'img_id')\n",
    "\n",
    "print(df_full_annotation_norm_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df_full_annotation_norm_test.info())\n",
    "\n",
    "display_all_cols(df_full_annotation_norm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3. Save the df annotations to the processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/test_annotation_simple.json'\n",
    "df_to_json(df_full_annotation_norm_test, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save val\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/val_annotation_simple.json'\n",
    "df_to_json(df_full_annotation_norm_val, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/train_annotation_simple.json'\n",
    "df_to_json(df_full_annotation_norm_train, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4. save imgs to the processed folder based on split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images_by_ids(src_folder, dst_folder, id_list):\n",
    "    \"\"\"\n",
    "    Moves images from the source folder to the destination folder based on the specified IDs.\n",
    "    The image filenames are expected to be in the format 'something_vidid.something_imgid.something'.\n",
    "\n",
    "    Parameters:\n",
    "    src_folder (str): Path to the source folder containing the images.\n",
    "    dst_folder (str): Path to the destination folder where images will be moved.\n",
    "    id_list (list): List of IDs in the format 'idpart1_idpart2'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Ensure the destination directory exists\n",
    "    if not os.path.exists(dst_folder):\n",
    "        os.makedirs(dst_folder)\n",
    "\n",
    "    for filename in os.listdir(src_folder):\n",
    "        if os.path.isfile(os.path.join(src_folder, filename)):  # Check if it's a file\n",
    "            parts = filename.split('_')\n",
    "            vidid = parts[0]\n",
    "            imgid = parts[1]\n",
    "            \n",
    "            # Check if the extracted id combination is in the list\n",
    "            if f'{vidid}_{imgid}' in id_list:\n",
    "                src_path = os.path.join(src_folder, filename)\n",
    "                dst_path = os.path.join(dst_folder, filename)\n",
    "                shutil.move(src_path, dst_path)\n",
    "                print(f'Moved: {filename} to {dst_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "src_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_crop_220'\n",
    "dst_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/test'\n",
    "\n",
    "move_images_by_ids(src_folder, dst_folder, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "src_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_crop_220'\n",
    "dst_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/val'\n",
    "\n",
    "move_images_by_ids(src_folder, dst_folder, ids_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "src_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_crop_220'\n",
    "dst_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/train'\n",
    "\n",
    "move_images_by_ids(src_folder, dst_folder, ids_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5. save final list of all vid_id, img_id, bbox_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of full ids\n",
    "def full_ids_to_list(df, cols_to_combine):\n",
    "    \"\"\"\n",
    "    Combines the values of specified columns in each row of the DataFrame, \n",
    "    separated by an underscore, and returns a list of these combined values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    cols_to_combine (list): List of column names to combine.\n",
    "\n",
    "    Returns:\n",
    "    list: A list where each item is a combined string of the specified columns' values.\n",
    "    \"\"\"\n",
    "    # Use DataFrame's apply method to combine the columns row-wise\n",
    "    combined_list = df[cols_to_combine].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    \n",
    "    # Convert the combined Series to a list\n",
    "    return combined_list.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "ids_test_bbox = full_ids_to_list(df_full_annotation_norm_test, id_cols)\n",
    "print(ids_test_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ids_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "ids_val_bbox = full_ids_to_list(df_full_annotation_norm_val, id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "ids_train_bbox = full_ids_to_list(df_full_annotation_norm_train, id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lists to txt files\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_test_bbox.txt'\n",
    "save_list_to_file(ids_test_bbox, save_dir)\n",
    "\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_val_bbox.txt'\n",
    "\n",
    "save_list_to_file(ids_val_bbox, save_dir)\n",
    "\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_train_bbox.txt'\n",
    "\n",
    "save_list_to_file(ids_train_bbox, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Load image data into arr for train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1. load ids to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ids into list \n",
    "def load_file_to_list(file_path):\n",
    "    \"\"\"\n",
    "    loads a text file to a list with each entry on a new line becoming a new entry in the list.\n",
    "\n",
    "    :param file_path: Path to the file where the list should be saved.\n",
    "    :return list of data from file\n",
    "    \"\"\"\n",
    "    # Open the file for writing\n",
    "    lst = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Write each item on a new line\n",
    "        for line in file:\n",
    "            lst.append(line.strip())\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_test_bbox.txt'\n",
    "ids_test_bbox = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_val_bbox.txt'\n",
    "ids_val_bbox = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_train_bbox.txt'\n",
    "ids_train_bbox = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_test.txt'\n",
    "ids_test = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_val.txt'\n",
    "ids_val = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_train.txt'\n",
    "ids_train = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2. load image data to arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the image data into an arr\n",
    "# in the same order as the annotations and ids are stored (use id list for this)\n",
    "\n",
    "# The load image data function may take a while to run\n",
    "\n",
    "def load_image_data(ids_to_load, image_folder, crop_ext):\n",
    "\n",
    "  # list for loading image data\n",
    "  selected_imgs = []\n",
    "\n",
    "  # for loop for loading image data that is present in the list of ids\n",
    "  for i, img_id in enumerate(ids_to_load):\n",
    "\n",
    "    # load the image\n",
    "    img_path = os.path.join(image_folder, img_id+crop_ext)\n",
    "    print(img_path)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    #print(img)\n",
    "\n",
    "    # change the img to RGB from BGR as plt uses RGB colour scale\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # scaling the pixel values to [0, 1] (you don't need to scal them back)\n",
    "    img = img/255\n",
    "\n",
    "    selected_imgs.append(img)\n",
    "\n",
    "  # Convert the list of images to a NumPy array\n",
    "  selected_imgs_array = np.array(selected_imgs)\n",
    "  \n",
    "  return selected_imgs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "image_folder_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/test'\n",
    "crop_extension = '_crop_220x220.jpg'\n",
    "\n",
    "test_imgs_array = load_image_data(ids_test_bbox, image_folder_path, crop_extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "image_folder_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/val'\n",
    "crop_extension = '_crop_220x220.jpg'\n",
    "\n",
    "val_imgs_array = load_image_data(ids_val_bbox, image_folder_path, crop_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "image_folder_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/train'\n",
    "crop_extension = '_crop_220x220.jpg'\n",
    "\n",
    "train_imgs_array = load_image_data(ids_train_bbox, image_folder_path, crop_extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3. load annotations to df and then keypoints to arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3.1. loading dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json to a df test\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/test_annotation_simple.json'\n",
    "df_full_annotation_norm_test = json_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm_test = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json to a df val\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/val_annotation_simple.json'\n",
    "df_full_annotation_norm_val = json_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm_val = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json to a df train\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/train_annotation_simple.json'\n",
    "df_full_annotation_norm_train = json_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm_train = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the db easier to work with I am going to create a list with the kp col names, bbox col names, id col names\n",
    "id_cols = df_full_annotation_norm_test.iloc[:, :3].columns.to_list()\n",
    "bbox_cols = df_full_annotation_norm_test.iloc[:, 3:7].columns.to_list()\n",
    "kp_cols = df_full_annotation_norm_test.iloc[:, 7:23].columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kp_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3.2. loading the keypoint annotation into an arr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lists(df_to_list, list_of_cols):\n",
    "\n",
    "  # create temp lists\n",
    "  keypoints_temp = []\n",
    "\n",
    "  # step through the rows and\n",
    "  for _, row in df_to_list.iterrows():\n",
    "\n",
    "    # extract the data arrays\n",
    "    keypoints_data = row[list_of_cols].values\n",
    "\n",
    "    # adding data to the list\n",
    "    keypoints_temp.append(keypoints_data)\n",
    "\n",
    "  # Convert the list to a NumPy array and make sure that they are float32\n",
    "  keypoints_array = np.array(keypoints_temp, dtype=np.float32)\n",
    "  \n",
    "  return keypoints_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_kp_array = create_data_lists(df_full_annotation_norm_test, kp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "val_kp_array = create_data_lists(df_full_annotation_norm_val, kp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_kp_array = create_data_lists(df_full_annotation_norm_train, kp_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.51. Augment train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = train_imgs_array.shape[0]\n",
    "print(num_imgs)\n",
    "num_kp = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize keypoints for an array of images\n",
    "def unnorm_keypoints_arr(kp_arr, img_arr):\n",
    "    \"\"\"\n",
    "    Denormalizes keypoints for each image in the array based on the corresponding image size.\n",
    "    It converts normalized keypoints (range [-1, 1]) back to pixel coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - kp_arr: Array of normalized keypoints, where each entry is a list of keypoints for an image.\n",
    "              The keypoints are expected to be in the format [x1, y1, x2, y2, ...].\n",
    "    - img_arr: Array of images. The size of each image is used to scale the keypoints back \n",
    "               to their pixel coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - kp_abs_arr: Array of denormalized keypoints where each entry corresponds to the denormalized \n",
    "                  keypoints for the corresponding image in `img_arr`.\n",
    "    \"\"\"\n",
    "\n",
    "    kp_abs_list = []  # List to store the denormalized keypoints for each image\n",
    "\n",
    "    # Iterate through each set of keypoints and corresponding image\n",
    "    for i, kp in enumerate(kp_arr):\n",
    "        img_size = img_arr[i].shape  # Get the size of the current image (height, width, channels)\n",
    "\n",
    "        # Denormalize the keypoints based on the image size\n",
    "        kp_abs, missing_kp = unnorm_keypoints(img_size, kp_arr[i])\n",
    "\n",
    "        # Save the denormalized keypoints to the list\n",
    "        kp_abs_list.append(kp_abs)\n",
    "    \n",
    "    # Convert the list of denormalized keypoints to a NumPy array\n",
    "    kp_abs_arr = np.array(kp_abs_list)\n",
    "\n",
    "    return kp_abs_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize keypoints for an array of images\n",
    "def norm_keypoints_arr(kp_arr, img_arr):\n",
    "    \"\"\"\n",
    "    Normalizes keypoints for each image in the array based on the corresponding image size.\n",
    "    It converts keypoints from pixel coordinates back to normalized coordinates (range [-1, 1]).\n",
    "\n",
    "    Parameters:\n",
    "    - kp_arr: Array of keypoints, where each entry is a list of keypoints for an image.\n",
    "              The keypoints are expected to be in the format [x1, y1, x2, y2, ...] \n",
    "              with pixel coordinates.\n",
    "    - img_arr: Array of images. The size of each image is used to scale the keypoints \n",
    "               to normalized coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - kp_norm_arr: Array of normalized keypoints where each entry corresponds to the normalized \n",
    "                   keypoints for the corresponding image in `img_arr`.\n",
    "    \"\"\"\n",
    "        \n",
    "    kp_norm_list = []  # List to store the normalized keypoints for each image\n",
    "\n",
    "    # Iterate through each set of keypoints and corresponding image\n",
    "    for i, kp in enumerate(kp_arr):\n",
    "        img_size = img_arr[i].shape  # Get the size of the current image (height, width, channels)\n",
    "\n",
    "        # Normalize the keypoints based on the image size\n",
    "        kp_norm = norm_keypoints(img_size, kp_arr[i])\n",
    "\n",
    "        # Save the normalized keypoints to the list\n",
    "        kp_norm_list.append(kp_norm)\n",
    "    \n",
    "    # Convert the list of normalized keypoints to a NumPy array\n",
    "    kp_norm_arr = np.array(kp_norm_list)  \n",
    "\n",
    "    return kp_norm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation to images and keypoints\n",
    "def apply_aug(img_arr_orig, kp_arr_orig, aug, num_of_kp=8):\n",
    "    \"\"\"\n",
    "    Applies augmentation to a batch of images and their corresponding keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    - img_arr_orig: Original array of images. Shape should be (num_imgs, height, width, channels).\n",
    "    - kp_arr_orig: Original array of keypoints. Shape should be (num_imgs, num_of_kp*2), where each \n",
    "                   keypoint is represented by its x and y coordinates in pixel values.\n",
    "    - aug: An imgaug augmentation sequence or augmenter to apply to the images and keypoints.\n",
    "    - num_of_kp: Optional. Number of keypoints per image (default is 8).\n",
    "\n",
    "    Returns:\n",
    "    - img_arr_aug: Augmented array of images. Same shape as `img_arr_orig`.\n",
    "    - kp_arr_aug: Augmented array of keypoints. Same shape as `kp_arr_orig`.\n",
    "    \"\"\"\n",
    "    # print(img_arr_orig.shape)\n",
    "    #print(kp_arr_orig.shape)\n",
    "    \n",
    "    # Initialize lists to store augmented images and keypoints\n",
    "    aug_img = []  # List for augmented images\n",
    "    aug_kp = []   # List for augmented keypoints\n",
    "\n",
    "    # Get the number of images in the batch\n",
    "    num_imgs = img_arr_orig.shape[0]\n",
    "    #print(num_imgs)\n",
    "\n",
    "    # Loop over each image and its corresponding keypoints\n",
    "    for i in range(num_imgs):\n",
    "        image = img_arr_orig[i]  # Extract the i-th image\n",
    "        #print(image.shape)\n",
    "        \n",
    "        # Convert keypoints to KeypointsOnImage format for imgaug\n",
    "        keypoints = kp_arr_orig[i]\n",
    "        #print(keypoints)\n",
    "        kps = [Keypoint(x=keypoints[j*2], y=keypoints[j*2+1]) for j in range(num_of_kp)]\n",
    "        kps_on_image = KeypointsOnImage(kps, shape=image.shape)\n",
    "        \n",
    "        # Apply the augmentation to the image and keypoints\n",
    "        image_aug, kps_aug = aug(image=image, keypoints=kps_on_image)\n",
    "        \n",
    "        # Convert augmented keypoints back to the original flattened format [x1, y1, x2, y2, ...]\n",
    "        keypoints_aug = []\n",
    "        for kp in kps_aug.keypoints:\n",
    "            keypoints_aug.extend([kp.x, kp.y])\n",
    "        \n",
    "        # Append the augmented image and keypoints to their respective lists\n",
    "        aug_img.append(image_aug)\n",
    "        aug_kp.append(keypoints_aug)\n",
    "\n",
    "    # Convert the lists of augmented images and keypoints back to NumPy arrays\n",
    "    img_arr_aug = np.array(aug_img)\n",
    "    kp_arr_aug = np.array(aug_kp)\n",
    "\n",
    "    return img_arr_aug, kp_arr_aug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.51.1. Apply a lrflip to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify augmentation\n",
    "seq_lrflip = iaa.Sequential([\n",
    "    iaa.Fliplr(1.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply augmentation\n",
    "\n",
    "# unnorm kp\n",
    "train_kp_array_abs = unnorm_keypoints_arr(train_kp_array, train_imgs_array)\n",
    "\n",
    "# apply augmentation\n",
    "train_imgs_array_aug_lrflip, train_kp_array_aug_lrflip_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_lrflip)\n",
    "\n",
    "# norm the aug kp\n",
    "train_kp_array_aug_lrflip_norm = norm_keypoints_arr(train_kp_array_aug_lrflip_abs, train_imgs_array_aug_lrflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lrflip\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array_aug_lrflip[150], train_kp_array_aug_lrflip_abs[150], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check original\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array[150], train_kp_array_abs[150], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.51.2. Apply a random rotation (5:20 deg) clockwise and anticlockwise(-20:-5 deg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify augmentation\n",
    "seq_rotate_clock = iaa.Sequential([\n",
    "    iaa.Affine(rotate=(5, 20)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply augmentation\n",
    "\n",
    "# unnorm kp\n",
    "train_kp_array_abs = unnorm_keypoints_arr(train_kp_array, train_imgs_array)\n",
    "\n",
    "# apply augmentation\n",
    "train_imgs_array_aug_rclock, train_kp_array_aug_rclock_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_rotate_clock)\n",
    "\n",
    "# norm the aug kp\n",
    "train_kp_array_aug_rclock_norm = norm_keypoints_arr(train_kp_array_aug_rclock_abs, train_imgs_array_aug_rclock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rclock\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array_aug_rclock[300], train_kp_array_aug_rclock_abs[300], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check original\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array[300], train_kp_array_abs[300], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify augmentation\n",
    "seq_rotate_anticlock = iaa.Sequential([\n",
    "    iaa.Affine(rotate=(-20, -5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply augmentation\n",
    "\n",
    "# unnorm kp\n",
    "train_kp_array_abs = unnorm_keypoints_arr(train_kp_array, train_imgs_array)\n",
    "\n",
    "# apply augmentation\n",
    "train_imgs_array_aug_ranticlock, train_kp_array_aug_ranticlock_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_rotate_anticlock)\n",
    "\n",
    "# norm the aug kp\n",
    "train_kp_array_aug_ranticlock_norm = norm_keypoints_arr(train_kp_array_aug_ranticlock_abs, train_imgs_array_aug_ranticlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rclock\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array_aug_ranticlock[300], train_kp_array_aug_ranticlock_abs[300], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check original\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array[300], train_kp_array_abs[300], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.51.3. Apply a translation either up and down or left and right by the amount of padding in img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD FUNCTION\n",
    "# def detect_padding(image):\n",
    "#     \"\"\"\n",
    "#     Detects if padding is on the x-axis (left and right) or y-axis (top and bottom) \n",
    "#     of the image and calculates the padding size on one side.\n",
    "\n",
    "#     Parameters:\n",
    "#     - image: A NumPy array representing the image. The shape should be (height, width, channels).\n",
    "\n",
    "#     Returns:\n",
    "#     - is_padding_x: True if padding is on the x-axis, False if padding is on the y-axis.\n",
    "#     - padding_size: The size of the padding on one side in pixels.\n",
    "#     \"\"\"\n",
    "\n",
    "#     height, width, _ = image.shape\n",
    "    \n",
    "#     # Check for padding along the x-axis (left and right)\n",
    "#     left_column = image[:, 0, :]  # The first column (left side)\n",
    "#     right_column = image[:, -1, :]  # The last column (right side)\n",
    "#     # Check for padding along the x-axis (left and right)\n",
    "#     top_row = image[0, :, :]  # The first column (left side)\n",
    "#     bottom_row = image[-1, :, :]  # The last column (right side)\n",
    "#     print(left_column)\n",
    "#     #print(right_column)\n",
    "    \n",
    "#     # Check if the columns are fully black (indicating padding)\n",
    "#     if np.all(left_column < 1) and np.all(right_column < 1):\n",
    "#         # Padding is along the x-axis\n",
    "#         is_padding_x = True\n",
    "#         # Calculate padding size\n",
    "#         print(image[:, 30, 0]*255)\n",
    "#         plot_img(image)\n",
    "#         padding_size = np.sum(image[:, 0, 0] < 1) // 2  # Count black pixels on one side\n",
    "#     else:\n",
    "#         #plot_img(image)\n",
    "#         # Padding is along the y-axis (top and bottom)\n",
    "#         is_padding_x = False\n",
    "#         # Calculate padding size\n",
    "#         padding_size = np.sum(image[0, :, 0] < 1) // 2  # Count black pixels on one side\n",
    "#         #print(padding_size)\n",
    "\n",
    "#     return is_padding_x, padding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_padding(image):\n",
    "    \"\"\"\n",
    "    Detects if padding is on the x-axis (left and right) or y-axis (top and bottom)\n",
    "    of the image and calculates the padding size on one side.\n",
    "\n",
    "    Parameters:\n",
    "    - image: A NumPy array representing the image. The shape should be (width, height, channels).\n",
    "\n",
    "    Returns:\n",
    "    - is_padding_x: True if padding is on the x-axis, False if padding is on the y-axis.\n",
    "    - padding_size: The size of the padding on one side in pixels.\n",
    "    \"\"\"\n",
    "\n",
    "    width, height, _ = image.shape\n",
    "    \n",
    "    # Check for padding along the x-axis (left and right)\n",
    "    left_column = image[:, 0, :]#image[0, :, :]  # The first column (left side)\n",
    "    right_column = image[:, -1, :] #image[-1, :, :]  # The last column (right side)\n",
    "\n",
    "    # Check for padding along the y-axis (top and bottom)\n",
    "    top_row = image[:, 0, :]  # The first row (top side)\n",
    "    bottom_row = image[:, -1, :]  # The last row (bottom side)\n",
    "    #print(image[:, 5, :] *255)\n",
    "    #print(left_column*255)\n",
    "    \n",
    "    # Check if the columns are fully black (indicating padding)\n",
    "    if np.all(left_column*255 < 30) and np.all(right_column*255 < 30):\n",
    "        # Padding is along the x-axis\n",
    "        is_padding_x = True\n",
    "        #plot_img(image)\n",
    "        # Calculate padding size\n",
    "        #padding_size = np.sum(image[0, :, 0]*255 < 30) // 2  # Count black pixels on one side\n",
    "        # if padding_size > 60:\n",
    "        sum1 = np.sum(image[5, :, 0]*255 < 20) // 2\n",
    "        sum2 = np.sum(image[10, :, 0]*255 < 20) // 2\n",
    "        sum3 = np.sum(image[60, :, 0]*255 < 20) // 2\n",
    "        sum4 = np.sum(image[110, :, 0]*255 < 20) // 2\n",
    "        sum5 = np.sum(image[-60, :, 0]*255 < 20) // 2\n",
    "        sum6 = np.sum(image[-10, :, 0]*255 < 20) // 2\n",
    "        sum7 = np.sum(image[-5, :, 0]*255 < 20) // 2\n",
    "        padding_size = min(sum1, sum2, sum3, sum4, sum5, sum6, sum7)\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        # if padding_size > 60:\n",
    "        #     sum1 = np.sum(image[5, :, 0]*255 < 10) // 2\n",
    "        #     sum2 = np.sum(image[10, :, 0]*255 < 10) // 2\n",
    "        #     sum3 = np.sum(image[60, :, 0]*255 < 10) // 2\n",
    "        #     sum4 = np.sum(image[110, :, 0]*255 < 10) // 2\n",
    "        #     sum5 = np.sum(image[-60, :, 0]*255 < 10) // 2\n",
    "        #     sum6 = np.sum(image[-10, :, 0]*255 < 10) // 2\n",
    "        #     sum7 = np.sum(image[-5, :, 0]*255 < 10) // 2\n",
    "        #     average = (sum1 + sum2 + sum3 + sum4 + sum5 + sum6 + sum7) // 7  # Floor division for rounding down\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        if padding_size > 20: \n",
    "            padding_size = 20\n",
    "\n",
    "    else:\n",
    "        # Padding is along the y-axis (top and bottom)\n",
    "        is_padding_x = False\n",
    "        # Calculate padding size\n",
    "        padding_size = np.sum(image[:, 0, 0]*255 < 30) // 2  # Count black pixels on one side\n",
    "        # if padding_size > 60:\n",
    "        sum1 = np.sum(image[:, 5, 0]*255 < 20) // 2\n",
    "        sum2 = np.sum(image[:, 10, 0]*255 < 20) // 2\n",
    "        sum3 = np.sum(image[:, 60, 0]*255 < 20) // 2\n",
    "        sum4 = np.sum(image[:, 110, 0]*255 < 20) // 2\n",
    "        sum5 = np.sum(image[:, -60, 0]*255 < 20) // 2\n",
    "        sum6 = np.sum(image[:, -10, 0]*255 < 20) // 2\n",
    "        sum7 = np.sum(image[:, -5, 0]*255 < 20) // 2\n",
    "        padding_size = min(sum1, sum2, sum3, sum4, sum5, sum6, sum7)\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        # if padding_size > 60:\n",
    "        #     sum1 = np.sum(image[:, 5, 0]*255 < 10) // 2\n",
    "        #     sum2 = np.sum(image[:, 10, 0]*255 < 10) // 2\n",
    "        #     sum3 = np.sum(image[:, 60, 0]*255 < 10) // 2\n",
    "        #     sum4 = np.sum(image[:, 110, 0]*255 < 10) // 2\n",
    "        #     sum5 = np.sum(image[:, -60, 0]*255 < 10) // 2\n",
    "        #     sum6 = np.sum(image[:, -10, 0]*255 < 10) // 2\n",
    "        #     sum7 = np.sum(image[:, -5, 0]*255 < 10) // 2\n",
    "        #     average = (sum1 + sum2 + sum3 + sum4 + sum5 + sum6 + sum7) // 7  # Floor division for rounding down\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        if padding_size > 20: \n",
    "            padding_size = 20\n",
    "\n",
    "    return is_padding_x, padding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img_arr = train_imgs_array[0:2]\n",
    "# test_img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_kp_arr = train_kp_array[0:2]\n",
    "# test_kp_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnorm kp\n",
    "train_kp_array_abs = unnorm_keypoints_arr(train_kp_array, train_imgs_array)\n",
    "# print(train_kp_array_abs)\n",
    "# apply augmentation\n",
    "\n",
    "# Get the number of images in the batch\n",
    "num_imgs = train_imgs_array.shape[0]\n",
    "# print(num_imgs)\n",
    "\n",
    "# creat empty arrays\n",
    "train_imgs_array_aug_trans = np.empty((0, train_imgs_array.shape[1], train_imgs_array.shape[2], train_imgs_array.shape[3]), dtype=train_imgs_array.dtype)\n",
    "train_kp_array_aug_trans = np.empty((0, train_kp_array_abs.shape[1]), dtype=train_kp_array_abs.dtype)\n",
    "\n",
    "# print(train_imgs_array_aug_trans.shape)\n",
    "# print(train_kp_array_aug_trans.shape)\n",
    "\n",
    "# Loop over each image and its corresponding keypoints\n",
    "for i in range(num_imgs):\n",
    "    image = train_imgs_array[i]  # Extract the i-th image\n",
    "    kp = train_kp_array_abs[i]\n",
    "    # print(i)\n",
    "    # print(image.shape)\n",
    "    # print(kp.shape)\n",
    "\n",
    "    is_padding_x, padding_size = detect_padding(image)\n",
    "    print(f'this: {i}')\n",
    "    print(is_padding_x)\n",
    "    print(padding_size)\n",
    "\n",
    "    if is_padding_x:\n",
    "        seq_trans_x_left = iaa.Sequential([\n",
    "            iaa.TranslateX(px=(-padding_size, -padding_size)),\n",
    "        ])\n",
    "        seq_trans_x_right = iaa.Sequential([\n",
    "            iaa.TranslateX(px=(padding_size, padding_size)),\n",
    "        ])\n",
    "\n",
    "        # Convert to shape (1, 220, 220, 3) and (1, 16)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        #print(is_padding_x)\n",
    "        #print(image.shape)\n",
    "        #print(i)\n",
    "        kp = np.expand_dims(kp, axis=0)\n",
    "\n",
    "        # apply augmentations\n",
    "        single_trans_x_left_img_arr, single_trans_x_left_kp_arr = apply_aug(image, kp, seq_trans_x_left)\n",
    "        single_trans_x_right_img_arr, single_trans_x_right_kp_arr = apply_aug(image, kp, seq_trans_x_right)\n",
    "\n",
    "        #save to image array\n",
    "        train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_x_left_img_arr), axis=0)\n",
    "        train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_x_right_img_arr), axis=0)\n",
    "        #save to kp array\n",
    "        train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_x_left_kp_arr), axis=0)\n",
    "        train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_x_right_kp_arr), axis=0)\n",
    "\n",
    "    else :\n",
    "        seq_trans_y_up = iaa.Sequential([\n",
    "            iaa.TranslateY(px=(-padding_size, -padding_size)),\n",
    "        ])\n",
    "        seq_trans_y_down = iaa.Sequential([\n",
    "            iaa.TranslateY(px=(padding_size, padding_size)),\n",
    "        ])\n",
    "\n",
    "        # Convert to shape (1, 220, 220, 3) and (1, 16)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        #print(is_padding_x)\n",
    "        #print(image.shape)\n",
    "        #print(i)\n",
    "        kp = np.expand_dims(kp, axis=0)\n",
    "\n",
    "        # apply augmentations\n",
    "        single_trans_y_up_img_arr, single_trans_y_up_kp_arr = apply_aug(image, kp, seq_trans_y_up)\n",
    "        single_trans_y_down_img_arr, single_trans_y_down_kp_arr = apply_aug(image, kp, seq_trans_y_down)\n",
    "\n",
    "        #save to image array\n",
    "        train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_y_up_img_arr), axis=0)\n",
    "        train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_y_down_img_arr), axis=0)\n",
    "        #save to kp array\n",
    "        train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_y_up_kp_arr), axis=0)\n",
    "        train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_y_down_kp_arr), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_check = 231\n",
    "\n",
    "# print(train_imgs_array[image_check,:, -1, :]*255)\n",
    "# print(train_imgs_array[image_check,0, :, :]*255)\n",
    "# print(np.sum(train_imgs_array[image_check, :, 0, 0]*255 < 30))\n",
    "# print(np.sum(train_imgs_array[image_check, :, 0, 0]*255 < 30)//2)\n",
    "# plot_img(train_imgs_array[image_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_imgs_array_aug_trans.shape)\n",
    "print(train_kp_array_aug_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm the aug kp\n",
    "train_kp_array_aug_trans_norm = norm_keypoints_arr(train_kp_array_aug_trans, train_imgs_array_aug_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check trans\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array_aug_trans[561], train_kp_array_aug_trans[561], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check trans\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array_aug_trans[560], train_kp_array_aug_trans[560], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check original\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array[280], train_kp_array_abs[280], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.51.4 Combine the simple augmentation datasets with the original dataset to create simple_aug_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat empty arrays\n",
    "#train_imgs_array_aug_simple = np.empty((0, train_imgs_array.shape[1], train_imgs_array.shape[2], train_imgs_array.shape[3]), dtype=train_imgs_array.dtype)\n",
    "#train_kp_array_aug_simple = np.empty((0, train_kp_array_abs.shape[1]), dtype=train_kp_array_abs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine arrays\n",
    "#save to image array\n",
    "train_imgs_array_aug_simple = np.concatenate((train_imgs_array, train_imgs_array_aug_lrflip), axis=0)\n",
    "train_imgs_array_aug_simple = np.concatenate((train_imgs_array_aug_simple, train_imgs_array_aug_rclock), axis=0)\n",
    "train_imgs_array_aug_simple = np.concatenate((train_imgs_array_aug_simple, train_imgs_array_aug_ranticlock), axis=0)\n",
    "train_imgs_array_aug_simple = np.concatenate((train_imgs_array_aug_simple, train_imgs_array_aug_trans), axis=0)\n",
    "#save to kp array\n",
    "train_kp_array_aug_simple = np.concatenate((train_kp_array, train_kp_array_aug_lrflip_norm), axis=0)\n",
    "train_kp_array_aug_simple = np.concatenate((train_kp_array_aug_simple, train_kp_array_aug_rclock_norm), axis=0)\n",
    "train_kp_array_aug_simple = np.concatenate((train_kp_array_aug_simple, train_kp_array_aug_ranticlock_norm), axis=0)\n",
    "train_kp_array_aug_simple = np.concatenate((train_kp_array_aug_simple, train_kp_array_aug_trans_norm), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_imgs_array_aug_simple.shape)\n",
    "print(train_kp_array_aug_simple.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.51.5. Ensure that all kp are within the image frame and shift them in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_out_of_img_kp_rows(arr):\n",
    "    \"\"\"\n",
    "    Finds the number and positions of rows that contain numbers lower than -0.5 but not -10 and greater than 0.5\n",
    "    THese are keypoints that are outside the frame but not the missing ones.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: A NumPy array of shape (n, 16).\n",
    "\n",
    "    Returns:\n",
    "    - count_neg_rows: The number of rows that contain negative numbers.\n",
    "    - neg_row_indices: A list of indices of rows that contain negative numbers.\n",
    "    \"\"\"\n",
    "    # Check which rows contain negative numbers\n",
    "    neg_row_mask = np.any(((arr < -0.5) & (arr > -9.0)) | (arr > 0.5), axis=1)\n",
    "    \n",
    "    # Get the indices of rows that contain negative numbers\n",
    "    neg_row_indices = np.where(neg_row_mask)[0]\n",
    "    \n",
    "    # Count the number of rows with negative numbers\n",
    "    count_neg_rows = len(neg_row_indices)\n",
    "    \n",
    "    return count_neg_rows, neg_row_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_types = set(type(element) for element in train_kp_array_aug_simple.flatten())\n",
    "print(unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes\n",
    "train_kp_array_aug_simple.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all the kp are within the image\n",
    "print(find_out_of_img_kp_rows(train_kp_array_aug_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_out_of_img_kp(arr):\n",
    "    \"\"\"\n",
    "    Finds and replaces the elements in the array that are outside the frame but not the missing ones.\n",
    "    Specifically, elements greater than 0.5 are replaced with 0.5, and elements less than -0.5 but \n",
    "    greater than -9.0 are replaced with -0.5.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: A NumPy array of shape (n, 16).\n",
    "\n",
    "    Returns:\n",
    "    - modified_arr: The modified NumPy array with replaced values.\n",
    "    - count_replacements: The number of elements that were replaced.\n",
    "    \"\"\"\n",
    "    # Make a copy of the array to avoid modifying the original array\n",
    "    modified_arr = arr.copy()\n",
    "\n",
    "    # Replace elements greater than 0.5 with 0.5\n",
    "    count_pos_replacements = np.sum(modified_arr > 0.5)\n",
    "    modified_arr[modified_arr > 0.5] = 0.49\n",
    "\n",
    "    # Replace elements less than -0.5 but greater than -9.0 with -0.5\n",
    "    count_neg_replacements = np.sum((modified_arr < -0.5) & (modified_arr > -9.0))\n",
    "    modified_arr[(modified_arr < -0.5) & (modified_arr > -9.0)] = -0.49\n",
    "\n",
    "    # Total count of replacements\n",
    "    count_replacements = count_pos_replacements + count_neg_replacements\n",
    "\n",
    "    return modified_arr, count_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kp_array_aug_simple, num_replacements = replace_out_of_img_kp(train_kp_array_aug_simple)\n",
    "print(num_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_out_of_img_kp_rows(train_kp_array_aug_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img):\n",
    "  fig = plt.figure(figsize=(8, 25), dpi=100)\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.imshow(img)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_and_keypoint(img, keypoints, nkeypoints, keypoint_labels):\n",
    "  fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "  plt.imshow(img)\n",
    "  x_keypoints = keypoints[::2]\n",
    "  y_keypoints = keypoints[1::2]\n",
    "  plt.scatter(x_keypoints, y_keypoints, marker='.', c=np.arange(nkeypoints), cmap='jet')\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "  if keypoint_labels is not None:\n",
    "      for i, (x, y) in enumerate(zip(x_keypoints, y_keypoints)):\n",
    "          plt.text(x, y, keypoint_labels[i], fontsize=12, color='white', \n",
    "                    bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unnorm_keypoints(img_size, keypoints, kp_to_null=None):\n",
    "\n",
    "#   readjust_x = img_size[0]\n",
    "#   readjust_y = img_size[1]\n",
    "#   #print(readjust_x)\n",
    "#   #print(readjust_y)\n",
    "#   new_keypoints = []\n",
    "#   missing_kp = []\n",
    "\n",
    "#   for i, keypoint in enumerate(keypoints):\n",
    "#     # Null keypoints at specified indices\n",
    "#     #print(kp_to_null)\n",
    "#     if keypoint == -10 or (kp_to_null and i in kp_to_null):\n",
    "#       keypoint = np.nan\n",
    "#       #print(missing_kp)\n",
    "#       missing_kp.append(i)\n",
    "\n",
    "#     if i % 2 == 0:\n",
    "#       keypoint = keypoint * readjust_x + readjust_x/2\n",
    "#       #print(i, keypoint, 'x')\n",
    "#     else:\n",
    "#       keypoint = keypoint * readjust_y + readjust_y/2\n",
    "#       #print(i, keypoint, 'y')\n",
    "#     #print(keypoint)\n",
    "#     new_keypoints.append(keypoint)\n",
    "#   #print(new_keypoints)\n",
    "#   return new_keypoints, missing_kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img = 259\n",
    "chosen_img = train_imgs_array[display_img]\n",
    "chosen_img_size = chosen_img.shape\n",
    "print(chosen_img_size)\n",
    "#print(original_img_shape)\n",
    "chosen_img_keypoints = train_kp_array[display_img]\n",
    "nkeypoints = 8\n",
    "keypoint_labels = kp_cols[::2]\n",
    "\n",
    "display_keypoints, missing_kp = unnorm_keypoints(chosen_img_size, chosen_img_keypoints)\n",
    "\n",
    "plot_img_and_keypoint(chosen_img, display_keypoints, 8, keypoint_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img = 1013\n",
    "chosen_img = train_imgs_array_aug_simple[display_img]\n",
    "chosen_img_size = chosen_img.shape\n",
    "print(chosen_img_size)\n",
    "print(chosen_img_size)\n",
    "#print(original_img_shape)\n",
    "chosen_img_keypoints = train_kp_array_aug_simple_adjust[display_img]\n",
    "nkeypoints = 8\n",
    "keypoint_labels = kp_cols[::2]\n",
    "\n",
    "display_keypoints, missing_kp = unnorm_keypoints(chosen_img_size, chosen_img_keypoints)\n",
    "\n",
    "plot_img_and_keypoint(chosen_img, display_keypoints, 8, keypoint_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1. Define the Loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error, ignoring the invisible keypoints.\n",
    "    Assuming that -10.0 indicates an invisible keypoint.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints from both\n",
    "    # the predictions and the true values\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # Compute the Mean Squared Error only on the visible keypoints\n",
    "    mse = F.mse_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL IMPLEMENTATION OF THE ABOVE\n",
    "# def masked_rmse_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Computes the Root Mean Square Error (RMSE) loss, ignoring the invisible keypoints (denoted by -10).\n",
    "    \n",
    "#     Parameters:\n",
    "#     y_true (torch.Tensor): The ground truth keypoints (batch_size, num_keypoints*2).\n",
    "#     y_pred (torch.Tensor): The predicted keypoints (batch_size, num_keypoints*2).\n",
    "\n",
    "#     Returns:\n",
    "#     torch.Tensor: The computed RMSE loss.\n",
    "#     \"\"\"\n",
    "#     # Create a mask where keypoints are visible (not equal to -10)\n",
    "#     mask = (y_true != -10.0).float()\n",
    "\n",
    "#     # Apply the mask to filter out invisible keypoints\n",
    "#     y_true_masked = y_true * mask\n",
    "#     y_pred_masked = y_pred * mask\n",
    "\n",
    "#     # Compute the squared differences\n",
    "#     squared_diff = (y_pred_masked - y_true_masked) ** 2\n",
    "\n",
    "#     # Compute the mean of squared differences for visible keypoints\n",
    "#     loss = torch.sum(squared_diff) / torch.sum(mask)\n",
    "\n",
    "#     # Return the square root of the loss to get RMSE\n",
    "#     return torch.sqrt(loss)\n",
    "\n",
    "# # Example usage:\n",
    "# # Assume y_true and y_pred are your ground truth and predicted keypoints, respectively.\n",
    "# # y_true = torch.tensor([...])\n",
    "# # y_pred = torch.tensor([...])\n",
    "# # loss = masked_rmse_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2. Define the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCK\n",
    "# put in a function that will use the max bbox if primary kp is missing\n",
    "def pck_metric(y_true, y_pred, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Computes the Percentage of Correct Keypoints (PCK) metric.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (torch.Tensor): The ground truth keypoints (batch_size, num_keypoints*2).\n",
    "    y_pred (torch.Tensor): The predicted keypoints (batch_size, num_keypoints*2).\n",
    "    threshold (float): The distance threshold for a keypoint to be considered correct.\n",
    "                       Typically set relative to the size of the bounding box (e.g., 0.2).\n",
    "    \n",
    "    Returns:\n",
    "    float: The percentage of correct keypoints.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible (not equal to -10)\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "    #print(mask)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # print(y_true_masked)\n",
    "    # print(y_pred_masked)\n",
    "\n",
    "    # Compute the Euclidean distance between the predicted and true keypoints\n",
    "    distances = torch.sqrt((y_pred_masked[:, ::2] - y_true_masked[:, ::2]) ** 2 +\n",
    "                           (y_pred_masked[:, 1::2] - y_true_masked[:, 1::2]) ** 2)\n",
    "    \n",
    "    #print(distances)\n",
    "    \n",
    "    # Normalize the distances (relative to the max and min y coord)\n",
    "    # Norm_max_min_kp = torch.max(y_true_masked[:, 1::2], dim=1)[0] - torch.min(y_true_masked[:, 1::2], dim=1)[0]\n",
    "    # Normalise based on the distance between the head and the bottom of the body (position 0, 1 and )\n",
    "    #print(y_true[:, 0],y_true[:,10],y_true[:, 1],y_true[:, 11])\n",
    "    #print((y_true[:, 0] - y_true[:,10]) ** 2)\n",
    "    #print((y_true[:, 1] - y_true[:, 11]) ** 2)\n",
    "    Norm_head_lowerbody = torch.sqrt((y_true[:, 0] - y_true[:,10]) ** 2 +\n",
    "                        (y_true[:, 1] - y_true[:, 11]) ** 2)\n",
    "    #print(Norm_head_lowerbody)\n",
    "    normalized_distances = distances / Norm_head_lowerbody[:, None]\n",
    "    #print(distances)\n",
    "    #print(normalized_distances)\n",
    "\n",
    "    # Count the correct keypoints (distance <= threshold)\n",
    "    correct_keypoints = (normalized_distances <= threshold).float() * mask[:, ::2]\n",
    "    #print(correct_keypoints)\n",
    "\n",
    "    # Calculate the PCK as the percentage of correct keypoints\n",
    "    pck = correct_keypoints.sum() / mask[:, ::2].sum()\n",
    "    return pck#.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create two tensors to check pck\n",
    "\n",
    "# # Create two PyTorch tensors with the sizes (1, 16)\n",
    "# # Initialize them with random values between -1 and 1\n",
    "# tensor1_true = torch.rand(1, 16) * 2 - 1\n",
    "# tensor2_pred = tensor1_true.clone()\n",
    "\n",
    "# # creating a tensor with 2 predictions for an image (test that it will work for multiple inputs)\n",
    "# # tensor2_pred = tensor1_true.clone()\n",
    "\n",
    "# # Introduce some differences in tensor2\n",
    "# tensor2_pred[0, :8] += torch.randn(8) * 0.1  # Slightly off for the first element of the first row\n",
    "\n",
    "# tensor2_pred[0, :8] += torch.randn(8) * 0.1  # Slightly off for the first 8 elements of the first row\n",
    "# #tensor2_pred[1, 8:] += torch.randn(8) * 0.1  # Slightly off for the last 8 elements of the second row\n",
    "\n",
    "# # Ensure the values are still within the range [-1, 1]\n",
    "# tensor2_pred = torch.clamp(tensor2_pred, min=-1, max=1)\n",
    "\n",
    "# print(tensor1_true, tensor2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tensor1_true, tensor2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor1_true[0, 5] = -10\n",
    "# tensor1_true[0, 4] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pck_metric(tensor1_true, tensor2_pred, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.3. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First model (not correct sizes)\n",
    "# class DeepPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "#         super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "#         # The feature extractor part of the model, composed of several convolutional layers.\n",
    "#         self.features = nn.Sequential(\n",
    "#             # Conv2d: Input channels = 3 (RGB image), Output channels = 96, kernel size = 11x11,\n",
    "#             # stride = 4, padding = 4. \n",
    "#             # Input: (batch_size, 3, 220, 220)\n",
    "#             # Output: (batch_size, 96, 55, 55)\n",
    "#             nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # output size: (batch_size, 96, 27, 27)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 96, Output channels = 256, kernel size = 5x5,\n",
    "#             # stride = 2, padding = 2.\n",
    "#             # Input: (batch_size, 96, 27, 27)\n",
    "#             # Output: (batch_size, 256, 27, 27)\n",
    "#             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # output size: (batch_size, 96, 13, 13)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 256, Output channels = 384, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 256, 13, 13)\n",
    "#             # Output: (batch_size, 384, 13, 13)\n",
    "#             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 384, Output channels = 384, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 384, 13, 13)\n",
    "#             # Output: (batch_size, 384, 13, 13)\n",
    "#             nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 384, Output channels = 256, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 384, 13, 13)\n",
    "#             # Output: (batch_size, 256, 13, 13)\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "#             # Input: (batch_size, 256, 13, 13)\n",
    "#             # Output: (batch_size, 256, 6, 6)\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         # The classifier part of the model, composed of fully connected layers.\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten the input tensor\n",
    "#             # Input: (batch_size, 256, 6, 6)\n",
    "#             # Output: (batch_size, 256 * 6 * 6) = (batch_size, 9216)\n",
    "#             nn.Flatten(),\n",
    "            \n",
    "#             # Linear layer with input size 6400 and output size 4096\n",
    "#             # Input: (batch_size, 6400)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             nn.Linear(256 * 6 * 6, 4096),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             nn.Dropout(0.6),\n",
    "            \n",
    "#             # Linear layer with input size 4096 and output size 4096\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             nn.Linear(4096, 4096),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             nn.Dropout(0.6),\n",
    "            \n",
    "#             # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "#             # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, nkeypoints * 2)\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Define the forward pass through the network.\n",
    "#         # Pass input `x` through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "        super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "        # The feature extractor part of the model, composed of several convolutional layers.\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv2d: Input channels = 3 (RGB image), Output channels = 48, kernel size = 11x11,\n",
    "            # stride = 4, padding = 4. \n",
    "            # Input: (batch_size, 3, 220, 220)\n",
    "            # Output: (batch_size, 48, 55, 55)\n",
    "            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "            # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "            nn.LocalResponseNorm(5),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            # output size: (batch_size, 96, 27, 27)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2d: Input channels = 96, Output channels = 256, kernel size = 5x5,\n",
    "            # stride = 2, padding = 2.\n",
    "            # Input: (batch_size, 48, 27, 27)\n",
    "            # Output: (batch_size, 128, 27, 27)\n",
    "            nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "            # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "            nn.LocalResponseNorm(5),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            # output size: (batch_size, 96, 13, 13)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2d: Input channels = 256, Output channels = 384, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 256, 13, 13)\n",
    "            # Output: (batch_size, 384, 13, 13)\n",
    "            nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv2d: Input channels = 384, Output channels = 384, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 384, 13, 13)\n",
    "            # Output: (batch_size, 384, 13, 13)\n",
    "            nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv2d: Input channels = 384, Output channels = 256, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 384, 13, 13)\n",
    "            # Output: (batch_size, 256, 13, 13)\n",
    "            nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "            # Input: (batch_size, 256, 13, 13)\n",
    "            # Output: (batch_size, 256, 6, 6)\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        # The classifier part of the model, composed of fully connected layers.\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Flatten the input tensor\n",
    "            # Input: (batch_size, 256, 6, 6)\n",
    "            # Output: (batch_size, 256 * 6 * 6) = (batch_size, 9216)\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # Linear layer with input size 6400 and output size 4096\n",
    "            # Input: (batch_size, 6400)\n",
    "            # Output: (batch_size, 4096)\n",
    "            nn.Linear(128 * 6 * 6, 4096),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Dropout layer with 60% dropout rate\n",
    "            nn.Dropout(0.6),\n",
    "            \n",
    "            # Linear layer with input size 4096 and output size 4096\n",
    "            # Input: (batch_size, 4096)\n",
    "            # Output: (batch_size, 4096)\n",
    "            nn.Linear(4096, 4096),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Dropout layer with 60% dropout rate\n",
    "            nn.Dropout(0.6),\n",
    "            \n",
    "            # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "            # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "            # Input: (batch_size, 4096)\n",
    "            # Output: (batch_size, nkeypoints * 2)\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass through the network.\n",
    "        # Pass input `x` through the feature extractor\n",
    "        x = self.features(x)\n",
    "        # Pass the result through the classifier to get the final output\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepPose Model Summary\n",
    "# model = DeepPoseModel()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original ALexNet model\n",
    "# class AlexNet(nn.Module):\n",
    "#     def __init__(self, num_classes=1000):\n",
    "#         super(AlexNet, self).__init__()\n",
    "        \n",
    "#         # Define the feature extractor part of the network\n",
    "#         self.features = nn.Sequential(\n",
    "#             # 1st Convolutional Layer: 3 input channels (RGB), 64 output channels, 11x11 kernel size, stride 4, padding 2\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # 2nd Convolutional Layer: 64 input channels, 192 output channels, 5x5 kernel size, stride 1, padding 2\n",
    "#             nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # 3rd Convolutional Layer: 192 input channels, 384 output channels, 3x3 kernel size, stride 1, padding 1\n",
    "#             nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # 4th Convolutional Layer: 384 input channels, 256 output channels, 3x3 kernel size, stride 1, padding 1\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # 5th Convolutional Layer: 256 input channels, 256 output channels, 3x3 kernel size, stride 1, padding 1\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         # Define the classifier part of the network\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten the input\n",
    "#             nn.Flatten(),\n",
    "#             # 1st Fully Connected Layer: input size 256 * 6 * 6, output size 4096\n",
    "#             nn.Linear(256 * 6 * 6, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "            \n",
    "#             # 2nd Fully Connected Layer: input size 4096, output size 4096\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "            \n",
    "#             # 3rd Fully Connected Layer (output layer): input size 4096, output size num_classes\n",
    "#             nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Pass the input through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet Summary\n",
    "# model = AlexNet(num_classes=1000)  # Example model\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 224, 224), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.0 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_PyTorch(img_arr, kp_arr, batch_size, train_flag=True):\n",
    "    '''\n",
    "    Load data into PT dataset and dataLoader in specified batch size\n",
    "    \n",
    "    Params\n",
    "    img_arr: images loaded into an array (i,255,255,3) and are converted to (i,3,255,255)\n",
    "    kp_arr: array of keypoints (i, num_kp*2)\n",
    "    batch_size: batch size \n",
    "\n",
    "    Return:\n",
    "    PT_Dataset: containing input (x) and groundtruth (y)\n",
    "    PT_DataLoader: Dataloader containing dataset and batch size\n",
    "\n",
    "    '''\n",
    "\n",
    "    # create tensors from arrays and load them to the GPU\n",
    "    img_tensor = torch.tensor(img_arr, dtype=torch.float32).permute(0, 3, 1, 2).to('cuda')\n",
    "    kp_tensor = torch.tensor(kp_arr, dtype=torch.float32).to('cuda')\n",
    "\n",
    "    # Create a TensorDataset and DataLoader for training data\n",
    "    dataset = TensorDataset(img_tensor, kp_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train_flag)\n",
    "\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timestamped_dir(descriptor, base_dir='/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/'):\n",
    "    \"\"\"\n",
    "    Creates a directory with a timestamp appended to the base directory name.\n",
    "    Returns the path to the created directory.\n",
    "    \n",
    "    Parameters:\n",
    "    descriptor: string describing the run generally model_dataDescriptor\n",
    "    base_dir (str): The base directory name. Default is './training_results'.\n",
    "    \n",
    "    Returns:\n",
    "    str: The path to the created directory.\n",
    "    \"\"\"\n",
    "    # Get the current datetime and format it as YYYY-MM-DD_HH-MM-SS\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    base_dir_descriptor = f\"{base_dir}{descriptor}\"\n",
    "    \n",
    "    # Create the final directory name with the timestamp\n",
    "    final_dir = f\"{base_dir_descriptor}_{timestamp}\"\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    \n",
    "    return final_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_data, val_data, save_dir, data_descriptor='Loss', show_plot=False):\n",
    "    # Plot the loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_data, label=f'Training {data_descriptor}')\n",
    "    plt.plot(val_data, label=f'Validation {data_descriptor}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(f'{data_descriptor}')\n",
    "    plt.title(f'Training and Validation {data_descriptor} Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, f'{data_descriptor}_plot.png')\n",
    "    plt.savefig(plot_path)\n",
    "    print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "    # Optionally, display the plot\n",
    "    if show_plot == True:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_stats_and_models(model, epoch, val_loss, val_pck, save_dir, \n",
    "                     best_val_loss=None, best_val_pck=None, \n",
    "                     final_model=False, train_loss_list=None, val_loss_list=None, train_pck_list=None, val_pck_list=None):\n",
    "    \"\"\"\n",
    "    Saves the best models based on validation loss, PCK value, and final model.\n",
    "    Saves the train and val curves and results for training\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to be saved.\n",
    "    - epoch (int): The current epoch number.\n",
    "    - val_loss (float): The current validation loss.\n",
    "    - val_pck (float): The current validation PCK value.\n",
    "    - save_dir (str): The directory where the models will be saved.\n",
    "    - best_val_loss (float): The best validation loss seen so far.\n",
    "    - best_val_pck (float): The best validation PCK value seen so far.\n",
    "    - final_model (bool): If True, saves the final model after all epochs.\n",
    "    - train_loss_list (list): List of all the loss values from each epoch\n",
    "    \n",
    "    Returns:\n",
    "    - best_val_loss (float): Updated best validation loss.\n",
    "    - best_val_pck (float): Updated best validation PCK value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the current model has the lowest validation loss\n",
    "    if best_val_loss is None or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_name = f'best_val_loss_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth'\n",
    "        model_save_path_best_val_loss = os.path.join(save_dir, model_name)\n",
    "        torch.save(model.state_dict(), model_save_path_best_val_loss)\n",
    "        print(f'New best model saved with lowest validation loss to {model_save_path_best_val_loss}')\n",
    "    \n",
    "    # Check if the current model has the highest validation PCK\n",
    "    if best_val_pck is None or val_pck > best_val_pck:\n",
    "        best_val_pck = val_pck\n",
    "        model_save_path_best_val_pck = os.path.join(save_dir, f'best_val_pck_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth')\n",
    "        torch.save(model.state_dict(), model_save_path_best_val_pck)\n",
    "        print(f'New best model saved with highest validation PCK to {model_save_path_best_val_pck}')\n",
    "    \n",
    "    # Save the final model and perform final stats evaluation and save\n",
    "    if final_model:\n",
    "        final_model_path = os.path.join(save_dir, f'final_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth')\n",
    "        torch.save(model.state_dict(), final_model_path)\n",
    "        print(f'Final model saved to {final_model_path}')\n",
    "        plot_training_curves(train_loss_list, val_loss_list, save_dir, 'Loss', show_plot=True)\n",
    "        plot_training_curves(train_pck_list, val_pck_list, save_dir, data_descriptor='PCK@0.1', show_plot=True)\n",
    "    \n",
    "    return best_val_loss, best_val_pck, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where the training loop will go \n",
    "def train_loop(model, optimizer, train_dataloader, val_dataloader, num_epochs, descriptor):\n",
    "    # Create the directory to save the results to\n",
    "    save_dir = create_timestamped_dir(descriptor)\n",
    "\n",
    "    # Assuming the model, loss function, and optimizer are already defined\n",
    "    #model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "\n",
    "    # Define your optimizer\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    # Load data into PT dataset and dataloader\n",
    "    #train_dataset, train_dataloader = load_data_PyTorch(train_imgs_array, train_kp_array, 8)\n",
    "    #val_dataset, val_dataloader = load_data_PyTorch(val_imgs_array, val_kp_array, 8, train_flag=False)\n",
    "\n",
    "    # Training loop (variables)\n",
    "    #num_epochs = 30  # Adjust the number of epochs as needed\n",
    "\n",
    "    # Lists to store the training and validation loss for each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_pck_list = []\n",
    "    val_pck_list = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_pck = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        running_pck_005 = 0.0\n",
    "        running_pck_01 = 0.0\n",
    "        running_pck_02 = 0.0\n",
    "        running_pck_val_005 = 0.0\n",
    "        running_pck_val_01 = 0.0\n",
    "        running_pck_val_02 = 0.0\n",
    "        \n",
    "        for batch_images, batch_keypoints in train_dataloader:\n",
    "            # Move the data to the GPU\n",
    "            batch_images = batch_images.to('cuda')\n",
    "            batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(batch_images)\n",
    "            # Compute the loss\n",
    "            loss = masked_mse(batch_keypoints, outputs)\n",
    "            #print(loss)\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Accumulate the loss\n",
    "            running_train_loss += loss.item()\n",
    "            #print(running_train_loss)\n",
    "\n",
    "            # compute metrics\n",
    "            #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "            pck_01 = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "            #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "            #running_pck_005 += pck_005.item()\n",
    "            running_pck_01 += pck_01.item()\n",
    "            #running_pck_02 += pck_02.item()\n",
    "\n",
    "                \n",
    "        avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "        #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "        avg_pck_01 = running_pck_01 / len(train_dataloader)\n",
    "        #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "        # populate train losses list for evaluation\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        train_pck_list.append(avg_pck_01)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_keypoints in val_dataloader:\n",
    "                # Move the data to the GPU\n",
    "                batch_images = batch_images.to('cuda')\n",
    "                batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "                outputs = model(batch_images)\n",
    "                loss = masked_mse(batch_keypoints, outputs)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # compute metrics\n",
    "                #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "                pck_01_val = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "                #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "                #running_pck_val_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "                running_pck_val_01 += pck_01_val.item()\n",
    "                #running_pck_val_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "        \n",
    "        avg_val_loss = running_val_loss / len(val_dataloader)\n",
    "        #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "        avg_val_pck_01 = running_pck_val_01 / len(val_dataloader)\n",
    "        #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "        # populate val losses list for evaluation\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        val_pck_list.append(avg_val_pck_01)\n",
    "\n",
    "        # save best performing models based on the PCK and loss as well as the stats\n",
    "        best_val_loss, best_val_pck = save_stats_and_models(\n",
    "        model, epoch + 1, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "        best_val_loss, best_val_pck)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train PCK0.1: {avg_pck_01:.4f}, Val PCK0.1: {avg_val_pck_01:.4f}')\n",
    "        \n",
    "    save_stats_and_models(model, num_epochs, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "                    best_val_loss, best_val_pck, final_model=True, train_loss_list=train_losses, \n",
    "                    val_loss_list=val_losses, train_pck_list=train_pck_list, val_pck_list=val_pck_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD TRAIN LOOP\n",
    "# this is where the training loop will go \n",
    "def train_loop_mixed_precision(model, optimizer, train_dataloader, val_dataloader, num_epochs, descriptor):\n",
    "    # Create the directory to save the results to\n",
    "    save_dir = create_timestamped_dir(descriptor)\n",
    "\n",
    "    # Assuming the model, loss function, and optimizer are already defined\n",
    "    #model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "\n",
    "    # Define your optimizer\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    # Load data into PT dataset and dataloader\n",
    "    #train_dataset, train_dataloader = load_data_PyTorch(train_imgs_array, train_kp_array, 8)\n",
    "    #val_dataset, val_dataloader = load_data_PyTorch(val_imgs_array, val_kp_array, 8, train_flag=False)\n",
    "\n",
    "    # Training loop (variables)\n",
    "    #num_epochs = 30  # Adjust the number of epochs as needed\n",
    "\n",
    "    # Lists to store the training and validation loss for each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_pck_list = []\n",
    "    val_pck_list = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_pck = None\n",
    "\n",
    "    scaler = GradScaler() \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        running_pck_005 = 0.0\n",
    "        running_pck_01 = 0.0\n",
    "        running_pck_02 = 0.0\n",
    "        running_pck_val_005 = 0.0\n",
    "        running_pck_val_01 = 0.0\n",
    "        running_pck_val_02 = 0.0\n",
    "        \n",
    "        for batch_images, batch_keypoints in train_dataloader:\n",
    "            # Move the data to the GPU\n",
    "            batch_images = batch_images.to('cuda')\n",
    "            batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                outputs = model(batch_images)\n",
    "                # Compute the loss\n",
    "                loss = masked_mse(batch_keypoints, outputs)\n",
    "                #print(loss)\n",
    "\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update\n",
    "            # Accumulate the loss\n",
    "            running_train_loss += loss.item()\n",
    "            #print(running_train_loss)\n",
    "\n",
    "            # compute metrics\n",
    "            #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "            pck_01 = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "            #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "            #running_pck_005 += pck_005.item()\n",
    "            running_pck_01 += pck_01.item()\n",
    "            #running_pck_02 += pck_02.item()\n",
    "\n",
    "                \n",
    "        avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "        #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "        avg_pck_01 = running_pck_01 / len(train_dataloader)\n",
    "        #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "        # populate train losses list for evaluation\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        train_pck_list.append(avg_pck_01)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_keypoints in val_dataloader:\n",
    "                # Move the data to the GPU\n",
    "                batch_images = batch_images.to('cuda')\n",
    "                batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "                outputs = model(batch_images)\n",
    "                loss = masked_mse(batch_keypoints, outputs)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # compute metrics\n",
    "                #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "                pck_01_val = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "                #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "                #running_pck_val_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "                running_pck_val_01 += pck_01_val.item()\n",
    "                #running_pck_val_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "        \n",
    "        avg_val_loss = running_val_loss / len(val_dataloader)\n",
    "        #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "        avg_val_pck_01 = running_pck_val_01 / len(val_dataloader)\n",
    "        #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "        # populate val losses list for evaluation\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        val_pck_list.append(avg_val_pck_01)\n",
    "\n",
    "        # save best performing models based on the PCK and loss as well as the stats\n",
    "        best_val_loss, best_val_pck = save_stats_and_models(\n",
    "        model, epoch + 1, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "        best_val_loss, best_val_pck)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train PCK0.1: {avg_pck_01:.4f}, Val PCK0.1: {avg_val_pck_01:.4f}')\n",
    "        \n",
    "    save_stats_and_models(model, num_epochs, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "                    best_val_loss, best_val_pck, final_model=True, train_loss_list=train_losses, \n",
    "                    val_loss_list=val_losses, train_pck_list=train_pck_list, val_pck_list=val_pck_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.1. Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the model, loss function, and optimizer are already defined\n",
    "model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Load data into PT dataset and dataloader\n",
    "bacth_size = 2 #batch_size\n",
    "train_dataset, train_dataloader = load_data_PyTorch(train_imgs_array_aug_simple, train_kp_array_aug_simple, bacth_size)\n",
    "val_dataset, val_dataloader = load_data_PyTorch(val_imgs_array, val_kp_array, bacth_size, train_flag=False)\n",
    "\n",
    "# Training loop (variables)\n",
    "num_epochs = 30  # Adjust the number of epochs as needed\n",
    "\n",
    "descriptor = 'DeepPose_Simple_SimpleAug'\n",
    "\n",
    "#train_loop(model, optimizer, train_dataloader, val_dataloader, num_epochs, descriptor)\n",
    "\n",
    "train_loop_mixed_precision(model, optimizer, train_dataloader, val_dataloader, num_epochs, descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop test\n",
    "# Assuming the model, loss function, and optimizer are already defined\n",
    "print('start loop')\n",
    "model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Load data into PT dataset and dataloader\n",
    "batch_size = 32 #batch_size\n",
    "# create tensors from arrays \n",
    "img_tensor = torch.tensor(train_imgs_array_aug_simple, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "kp_tensor = torch.tensor(train_kp_array_aug_simple, dtype=torch.float32)#.to('cuda')\n",
    "\n",
    "# Create a TensorDataset and DataLoader for training data\n",
    "dataset = TensorDataset(img_tensor, kp_tensor)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "img_tensor = torch.tensor(val_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "kp_tensor = torch.tensor(val_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "\n",
    "# Create a TensorDataset and DataLoader for training data\n",
    "dataset = TensorDataset(img_tensor, kp_tensor)\n",
    "val_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#train_dataset, train_dataloader = load_data_PyTorch(train_imgs_array_aug_simple, train_kp_array_aug_simple, bacth_size)\n",
    "#val_dataset, val_dataloader = load_data_PyTorch(val_imgs_array, val_kp_array, bacth_size, train_flag=False)\n",
    "\n",
    "# Training loop (variables)\n",
    "num_epochs = 30  # Adjust the number of epochs as needed\n",
    "\n",
    "descriptor = 'DeepPose_Simple_SimpleAug'\n",
    "\n",
    "#train_loop(model, optimizer, train_dataloader, val_dataloader, num_epochs, descriptor)\n",
    "\n",
    "save_dir = create_timestamped_dir(descriptor)\n",
    "\n",
    "# Assuming the model, loss function, and optimizer are already defined\n",
    "#model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "\n",
    "# Define your optimizer\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Load data into PT dataset and dataloader\n",
    "#train_dataset, train_dataloader = load_data_PyTorch(train_imgs_array, train_kp_array, 8)\n",
    "#val_dataset, val_dataloader = load_data_PyTorch(val_imgs_array, val_kp_array, 8, train_flag=False)\n",
    "\n",
    "# Training loop (variables)\n",
    "#num_epochs = 30  # Adjust the number of epochs as needed\n",
    "\n",
    "# Lists to store the training and validation loss for each epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_pck_list = []\n",
    "val_pck_list = []\n",
    "\n",
    "best_val_loss = None\n",
    "best_val_pck = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_pck_005 = 0.0\n",
    "    running_pck_01 = 0.0\n",
    "    running_pck_02 = 0.0\n",
    "    running_pck_val_005 = 0.0\n",
    "    running_pck_val_01 = 0.0\n",
    "    running_pck_val_02 = 0.0\n",
    "    \n",
    "    for batch_images, batch_keypoints in train_dataloader:\n",
    "        #print(batch_images.shape)\n",
    "        # Move the data to the GPU\n",
    "        batch_images = batch_images.to('cuda')\n",
    "        batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(batch_images)\n",
    "        # Compute the loss\n",
    "        loss = masked_mse(batch_keypoints, outputs)\n",
    "        #print(loss)\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Accumulate the loss\n",
    "        running_train_loss += loss.item()\n",
    "        #print(running_train_loss)\n",
    "\n",
    "        # compute metrics\n",
    "        #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "        pck_01 = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "        #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "        #running_pck_005 += pck_005.item()\n",
    "        running_pck_01 += pck_01.item()\n",
    "        #running_pck_02 += pck_02.item()\n",
    "\n",
    "            \n",
    "    avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "    #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "    avg_pck_01 = running_pck_01 / len(train_dataloader)\n",
    "    #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "    # populate train losses list for evaluation\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_pck_list.append(avg_pck_01)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_keypoints in val_dataloader:\n",
    "            # Move the data to the GPU\n",
    "            batch_images = batch_images.to('cuda')\n",
    "            batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "            outputs = model(batch_images)\n",
    "            loss = masked_mse(batch_keypoints, outputs)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # compute metrics\n",
    "            #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "            pck_01_val = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "            #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "            #running_pck_val_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "            running_pck_val_01 += pck_01_val.item()\n",
    "            #running_pck_val_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "    \n",
    "    avg_val_loss = running_val_loss / len(val_dataloader)\n",
    "    #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "    avg_val_pck_01 = running_pck_val_01 / len(val_dataloader)\n",
    "    #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "    # populate val losses list for evaluation\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    val_pck_list.append(avg_val_pck_01)\n",
    "\n",
    "    # save best performing models based on the PCK and loss as well as the stats\n",
    "    best_val_loss, best_val_pck = save_stats_and_models(\n",
    "    model, epoch + 1, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "    best_val_loss, best_val_pck)\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train PCK0.1: {avg_pck_01:.4f}, Val PCK0.1: {avg_val_pck_01:.4f}')\n",
    "    \n",
    "save_stats_and_models(model, num_epochs, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "                best_val_loss, best_val_pck, final_model=True, train_loss_list=train_losses, \n",
    "                val_loss_list=val_losses, train_pck_list=train_pck_list, val_pck_list=val_pck_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. Evaluating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_class, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a PyTorch model from a .pth file.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path (str): The path to the .pth model file.\n",
    "    - model_class (torch.nn.Module): The class of the model to instantiate.\n",
    "    - device (str): The device to load the model onto ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module): The loaded PyTorch model.\n",
    "    \"\"\"\n",
    "    # Instantiate the model class\n",
    "    model = model_class().to(device)\n",
    "    \n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, images, img_is_tensor=False, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generates predictions from a PyTorch model given an array of images.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to use for predictions.\n",
    "    - images (np.array): Array of images (e.g., shape: (num_images, 220, 220, 3)).\n",
    "    - device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - predictions (np.array): Array of predictions (e.g., keypoints for each image).\n",
    "    \"\"\"\n",
    "    # Convert images to PyTorch tensor and move to the specified device\n",
    "    if not img_is_tensor:\n",
    "        images_tensor = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "    # Forward pass through the model to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images_tensor)\n",
    "    \n",
    "    # Convert predictions back to a NumPy array and move to CPU if necessary\n",
    "    predictions = predictions.cpu().numpy() if device == 'cuda' else predictions.numpy()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(img, pred_keypoints, true_keypoints, save_dir, img_num, nkeypoints=8, keypoint_labels=None, connections = [(0, 1), (0, 2), (2, 3), (2, 4), (2, 5), (5, 6), (5, 7)]):\n",
    "    \"\"\"\n",
    "    Plots predicted keypoints vs. ground truth keypoints on the same image.\n",
    "\n",
    "    Parameters:\n",
    "    - img: The image on which to plot the keypoints.\n",
    "    - pred_keypoints: The predicted keypoints (flattened x, y coordinates).\n",
    "    - true_keypoints: The ground truth keypoints (flattened x, y coordinates).\n",
    "    - save_dir: Directory to save the result to\n",
    "    - img_num: image number that is getting compared\n",
    "    - nkeypoints:  Optional The number of keypoints (default=8).\n",
    "    - keypoint_labels: Optional list of keypoint labels to display next to the keypoints.\n",
    "    - connections: OPtional list of tupels defining the connections between kps\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Extract x and y coordinates for predicted keypoints\n",
    "    pred_x_keypoints = pred_keypoints[::2]\n",
    "    pred_y_keypoints = pred_keypoints[1::2]\n",
    "    \n",
    "    # Extract x and y coordinates for ground truth keypoints\n",
    "    true_x_keypoints = true_keypoints[::2]\n",
    "    true_y_keypoints = true_keypoints[1::2]\n",
    "\n",
    "    # Plot skeleton for true keypoints\n",
    "    for (i, j) in connections:\n",
    "        plt.plot([true_x_keypoints[i], true_x_keypoints[j]], \n",
    "                 [true_y_keypoints[i], true_y_keypoints[j]], \n",
    "                 'r-', linewidth=1)\n",
    "\n",
    "    # Plot skeleton for predicted keypoints\n",
    "    for (i, j) in connections:\n",
    "        plt.plot([pred_x_keypoints[i], pred_x_keypoints[j]], \n",
    "                 [pred_y_keypoints[i], pred_y_keypoints[j]], \n",
    "                 'g-', linewidth=1)\n",
    "    \n",
    "    # Plot predicted keypoints\n",
    "    plt.scatter(pred_x_keypoints, pred_y_keypoints, marker='o', c='g', s=100, label='Predicted', edgecolor='black')\n",
    "    \n",
    "    # Plot ground truth keypoints\n",
    "    plt.scatter(true_x_keypoints, true_y_keypoints, marker='x', c='r', s=100, label='Ground Truth')\n",
    "    \n",
    "    # If labels are provided, add them to the plot\n",
    "    if keypoint_labels is not None:\n",
    "        for i, (x, y) in enumerate(zip(true_x_keypoints, true_y_keypoints)):\n",
    "            plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "                     bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "    if keypoint_labels is not None:\n",
    "        for i, (x, y) in enumerate(zip(pred_x_keypoints, pred_y_keypoints)):\n",
    "            plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "                     bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # Add a legend to differentiate between predicted and ground truth keypoints\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, f'Comparison of predicted and ground truth for img {img_num}.png')\n",
    "    plt.savefig(plot_path)\n",
    "    #print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(model_path, start_img, end_img, model_class=DeepPoseModel, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a model, predicts keypoints for a range of images, and plots the predicted keypoints \n",
    "    versus ground truth keypoints on the same image. The images with plotted keypoints are then \n",
    "    saved to a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path: The file path to the saved model's .pth file.\n",
    "    - start_img: The starting index of the images in the validation set to process.\n",
    "    - end_img: The ending index of the images in the validation set to process (exclusive).\n",
    "    - model_class: Optional. The class of the model architecture to instantiate and load \n",
    "                   with the saved weights (default=DeepPoseModel).\n",
    "    - device: Optional. The device to run the model on ('cuda' for GPU, 'cpu' for CPU; default='cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - None. The function saves the images with plotted keypoints to the directory derived from the \n",
    "            model path.\n",
    "    \"\"\"\n",
    "\n",
    "    # get img lists\n",
    "    img_arr = val_imgs_array[start_img:end_img,:,:,:]\n",
    "    true_kp_arr = val_kp_array[start_img:end_img,:]\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(model_path, model_class, device=device)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = predict(model, img_arr, device=device)\n",
    "    #print(predictions)\n",
    "\n",
    "    # DeNorm predictions \n",
    "    predictions_abs = []\n",
    "    true_kp_arr_abs = []\n",
    "    for i, kp in enumerate(predictions):\n",
    "\n",
    "        img_size = img_arr[i].shape\n",
    "        #print(img_size)\n",
    "\n",
    "        #unNorm each prediction\n",
    "        true_kp_abs, missing_kp = unnorm_keypoints(img_size, true_kp_arr[i])\n",
    "        #print(missing_kp)\n",
    "        kp_abs, missing_kp = unnorm_keypoints(img_size, kp, kp_to_null=missing_kp)\n",
    "        #print(missing_kp)\n",
    "        \n",
    "\n",
    "        # save result to new list\n",
    "        predictions_abs.append(kp_abs)\n",
    "        true_kp_arr_abs.append(true_kp_abs)\n",
    "\n",
    "    #print(predictions_abs)\n",
    "\n",
    "    # get the save directory parent (where the images will be saved)\n",
    "    save_dir = model_path.rsplit('/',1)[0]\n",
    "\n",
    "    # labels\n",
    "    labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "\n",
    "    for i, kp in enumerate(predictions_abs):\n",
    "\n",
    "        plot_comparison(img_arr[i], predictions_abs[i], true_kp_arr_abs[i], save_dir, img_num=i+start_img)#, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions and draw them \n",
    "model_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_2024-08-22_15-17-20/final_model_epoch_30_PCK_0.5700_loss_0.0083.pth'\n",
    "start_img = 55\n",
    "end_img = 58\n",
    "\n",
    "predict_and_plot(model_path, start_img, end_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare some poses\n",
    "val_imgs_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_kp_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Loss function evalutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0. Loss functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.1. Simple angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_tensor_angle(input_tensor):\n",
    "#     \"\"\"\n",
    "#     Compute angles between vectors defined in input_tensor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_tensor: Tensor with 6 columns and multiple rows.\n",
    "#     Returns:\n",
    "#     - Tensor of angles in radians with the same number of rows as input_tensor.\n",
    "#     \"\"\"\n",
    "#     # # Step 1: Create a mask for rows without -10\n",
    "#     # mask = (input_tensor != -10).all(dim=1).unsqueeze(1)  # Mask rows with -10\n",
    "\n",
    "#     # # Step 2: Adjust input_tensor using the mask\n",
    "#     # #adjusted_input_tensor = input_tensor * mask  # Set rows with -10 to 0\n",
    "\n",
    "#     # Step 3: Compute vectors BA and BC\n",
    "#     # vector_tensor = torch.zeros((adjusted_input_tensor.size(0), 4), device=input_tensor.device)\n",
    "#     # vector_tensor[:, 0] = adjusted_input_tensor[:, 0] - adjusted_input_tensor[:, 2]  # BA_x\n",
    "#     # vector_tensor[:, 1] = adjusted_input_tensor[:, 1] - adjusted_input_tensor[:, 3]  # BA_y\n",
    "#     # vector_tensor[:, 2] = adjusted_input_tensor[:, 4] - adjusted_input_tensor[:, 2]  # BC_x\n",
    "#     # vector_tensor[:, 3] = adjusted_input_tensor[:, 5] - adjusted_input_tensor[:, 3]  # BC_y\n",
    "#     vector_tensor = torch.zeros((input_tensor.size(0), 4), device=input_tensor.device)\n",
    "#     vector_tensor[:, 0] = input_tensor[:, 0] - input_tensor[:, 2]  # BA_x\n",
    "#     vector_tensor[:, 1] = input_tensor[:, 1] - input_tensor[:, 3]  # BA_y\n",
    "#     vector_tensor[:, 2] = input_tensor[:, 4] - input_tensor[:, 2]  # BC_x\n",
    "#     vector_tensor[:, 3] = input_tensor[:, 5] - input_tensor[:, 3]  # BC_y\n",
    "\n",
    "#         # DEBUG - check for nan\n",
    "#     if torch.isnan(vector_tensor).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # Step 4: Compute dot product and magnitudes\n",
    "#     dot_prod_tensor = (vector_tensor[:, 0] * vector_tensor[:, 2] + \n",
    "#                        vector_tensor[:, 1] * vector_tensor[:, 3]).unsqueeze(1)\n",
    "#     magnitude_tensor_BA = torch.linalg.norm(vector_tensor[:, :2], dim=1, keepdim=True)  # Magnitude of BA\n",
    "#     magnitude_tensor_BC = torch.linalg.norm(vector_tensor[:, 2:], dim=1, keepdim=True)  # Magnitude of BC\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(dot_prod_tensor).any():\n",
    "#         print('going to nan')\n",
    "#             # DEBUG - check for nan\n",
    "#     if torch.isnan(magnitude_tensor_BA).any():\n",
    "#         print('going to nan')\n",
    "#         # DEBUG - check for nan\n",
    "#     if torch.isnan(magnitude_tensor_BC).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "\n",
    "#     # Step 5: Compute angles in radians\n",
    "#     clamp_vals = torch.clamp((dot_prod_tensor / (magnitude_tensor_BA * magnitude_tensor_BC)), -0.99999, 0.99999) # Clamp values to the valid range for acos\n",
    "#        # DEBUG - check for nan\n",
    "#     if torch.isnan(clamp_vals).any():\n",
    "#         print('going to nan')\n",
    "    \n",
    "#     angle_tensor = torch.acos(clamp_vals)\n",
    "#        # DEBUG - check for nan\n",
    "#     if torch.isnan(clamp_vals).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "\n",
    "#     return angle_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tensor_abs_difference(tensor1, tensor2):\n",
    "#     \"\"\"\n",
    "#     Compute the absolute difference between two tensors, each with a single column and multiple rows.\n",
    "\n",
    "#     Parameters:\n",
    "#     - tensor1: A PyTorch tensor with shape (N, 1)\n",
    "#     - tensor2: A PyTorch tensor with shape (N, 1)\n",
    "\n",
    "#     Returns:\n",
    "#     - A PyTorch tensor with the absolute differences, shape (N, 1)\n",
    "#     \"\"\"\n",
    "#     # Ensure both tensors have the same shape\n",
    "#     if tensor1.shape != tensor2.shape:\n",
    "#         raise ValueError(\"Input tensors must have the same shape.\")\n",
    "\n",
    "#     # Compute absolute difference\n",
    "#     abs_diff = torch.abs(tensor1 - tensor2)\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(abs_diff).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     return abs_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the angle loss\n",
    "# def find_single_angle_loss(y_true_subset, y_pred_subset):\n",
    "\n",
    "#     y_true_angle = find_tensor_angle(y_true_subset)\n",
    "#     y_pred_angle = find_tensor_angle(y_pred_subset)\n",
    "\n",
    "#     # DEBUG\n",
    "#     #print('the following is the angles', y_true_angle)\n",
    "\n",
    "#     # find the abs difference in angles\n",
    "#     angle_abs_differnce = tensor_abs_difference(y_true_angle, y_pred_angle)\n",
    "    \n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(angle_abs_differnce).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # normalise the abs difference (POTENTIALLY NEED TO DO THIS SO IT IS IN THE SAME RANGE AS THE MAE LOSS)\n",
    "\n",
    "#     # Step 1: Create a mask for rows without -10\n",
    "#     mask = (y_true_subset != -10).all(dim=1).unsqueeze(1)  # Mask rows with -10\n",
    "\n",
    "#     # mask angles where kp are missing\n",
    "#     angle_abs_differnce_masked = angle_abs_differnce * mask\n",
    "\n",
    "#     # normalise the abs difference (POTENTIALLY NEED TO DO THIS SO IT IS IN THE SAME RANGE AS THE MAE LOSS) - can only go up to 2*pi rad, so divide by that\n",
    "\n",
    "#     # find the mean abs difference between angles\n",
    "#     num_angles = torch.sum(mask)\n",
    "#     if num_angles == 0:\n",
    "#         num_angles = 1 \n",
    "\n",
    "#     single_angle_loss = torch.sum(angle_abs_differnce_masked)/num_angles\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(single_angle_loss).any():\n",
    "#         print('going to nan')\n",
    "    \n",
    "#     return single_angle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_angle_loss(y_true, y_pred):\n",
    "\n",
    "#     beak_angle_true = torch.zeros((y_true.size(0), 6), device=y_true.device, dtype=torch.float32) \n",
    "#     beak_angle_true[:,0] = y_true[:,2]\n",
    "#     beak_angle_true[:,1] = y_true[:,3]\n",
    "#     beak_angle_true[:,2] = y_true[:,0]\n",
    "#     beak_angle_true[:,3] = y_true[:,1]\n",
    "#     beak_angle_true[:,4] = y_true[:,4]\n",
    "#     beak_angle_true[:,5] = y_true[:,5]\n",
    "#     beak_angle_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     beak_angle_pred[:,0] = y_pred[:,2]\n",
    "#     beak_angle_pred[:,1] = y_pred[:,3]\n",
    "#     beak_angle_pred[:,2] = y_pred[:,0]\n",
    "#     beak_angle_pred[:,3] = y_pred[:,1]\n",
    "#     beak_angle_pred[:,4] = y_pred[:,4]\n",
    "#     beak_angle_pred[:,5] = y_pred[:,5]\n",
    "\n",
    "#     #print('printing the values for beak angle true and beak angle pred',beak_angle_true, beak_angle_pred)\n",
    "\n",
    "#     beak_angle_loss = find_single_angle_loss(beak_angle_true, beak_angle_pred)\n",
    "    \n",
    "#     #DEBUG\n",
    "#     print('the following is the beak_angle_loss', beak_angle_loss)\n",
    "\n",
    "#     head_angle_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     head_angle_true[:,0] = y_true[:,0]\n",
    "#     head_angle_true[:,1] = y_true[:,1]\n",
    "#     head_angle_true[:,2] = y_true[:,4]\n",
    "#     head_angle_true[:,3] = y_true[:,5]\n",
    "#     head_angle_true[:,4] = y_true[:,10]\n",
    "#     head_angle_true[:,5] = y_true[:,11]\n",
    "#     head_angle_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     head_angle_pred[:,0] = y_pred[:,0]\n",
    "#     head_angle_pred[:,1] = y_pred[:,1]\n",
    "#     head_angle_pred[:,2] = y_pred[:,4]\n",
    "#     head_angle_pred[:,3] = y_pred[:,5]\n",
    "#     head_angle_pred[:,4] = y_pred[:,10]\n",
    "#     head_angle_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     head_angle_loss = find_single_angle_loss(head_angle_true, head_angle_pred)\n",
    "#     #DEBUG\n",
    "#     print('the following is the head_angle_loss', head_angle_loss)\n",
    "\n",
    "#     right_wing_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     right_wing_true[:,0] = y_true[:,6]\n",
    "#     right_wing_true[:,1] = y_true[:,7]\n",
    "#     right_wing_true[:,2] = y_true[:,4]\n",
    "#     right_wing_true[:,3] = y_true[:,5]\n",
    "#     right_wing_true[:,4] = y_true[:,10]\n",
    "#     right_wing_true[:,5] = y_true[:,11]\n",
    "#     right_wing_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     right_wing_pred[:,0] = y_pred[:,6]\n",
    "#     right_wing_pred[:,1] = y_pred[:,7]\n",
    "#     right_wing_pred[:,2] = y_pred[:,4]\n",
    "#     right_wing_pred[:,3] = y_pred[:,5]\n",
    "#     right_wing_pred[:,4] = y_pred[:,10]\n",
    "#     right_wing_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     right_wing_loss = find_single_angle_loss(right_wing_true, right_wing_pred)\n",
    "#     #DEBUG\n",
    "#     print('the following is the right_wing_angle_loss', right_wing_loss)\n",
    "\n",
    "#     left_wing_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     left_wing_true[:,0] = y_true[:,8]\n",
    "#     left_wing_true[:,1] = y_true[:,9]\n",
    "#     left_wing_true[:,2] = y_true[:,4]\n",
    "#     left_wing_true[:,3] = y_true[:,5]\n",
    "#     left_wing_true[:,4] = y_true[:,10]\n",
    "#     left_wing_true[:,5] = y_true[:,11]\n",
    "#     left_wing_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     left_wing_pred[:,0] = y_pred[:,8]\n",
    "#     left_wing_pred[:,1] = y_pred[:,9]\n",
    "#     left_wing_pred[:,2] = y_pred[:,4]\n",
    "#     left_wing_pred[:,3] = y_pred[:,5]\n",
    "#     left_wing_pred[:,4] = y_pred[:,10]\n",
    "#     left_wing_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     left_wing_loss = find_single_angle_loss(left_wing_true, left_wing_pred)\n",
    "#     #DEBUG\n",
    "#     print('the following is the left_wing_angle_loss', left_wing_loss)\n",
    "\n",
    "#     right_foot_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     right_foot_true[:,0] = y_true[:,12]\n",
    "#     right_foot_true[:,1] = y_true[:,13]\n",
    "#     right_foot_true[:,2] = y_true[:,10]\n",
    "#     right_foot_true[:,3] = y_true[:,11]\n",
    "#     right_foot_true[:,4] = y_true[:,4]\n",
    "#     right_foot_true[:,5] = y_true[:,5]\n",
    "#     right_foot_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     right_foot_pred[:,0] = y_pred[:,12]\n",
    "#     right_foot_pred[:,1] = y_pred[:,13]\n",
    "#     right_foot_pred[:,2] = y_pred[:,10]\n",
    "#     right_foot_pred[:,3] = y_pred[:,11]\n",
    "#     right_foot_pred[:,4] = y_pred[:,4]\n",
    "#     right_foot_pred[:,5] = y_pred[:,5]\n",
    "\n",
    "#     right_foot_loss = find_single_angle_loss(right_foot_true, right_foot_pred)\n",
    "#     #DEBUG\n",
    "#     print('the following is the right_foot_angle_loss', right_foot_loss)\n",
    "\n",
    "#     left_foot_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     left_foot_true[:,0] = y_true[:,14]\n",
    "#     left_foot_true[:,1] = y_true[:,15]\n",
    "#     left_foot_true[:,2] = y_true[:,10]\n",
    "#     left_foot_true[:,3] = y_true[:,11]\n",
    "#     left_foot_true[:,4] = y_true[:,4]\n",
    "#     left_foot_true[:,5] = y_true[:,5]\n",
    "#     left_foot_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     left_foot_pred[:,0] = y_pred[:,14]\n",
    "#     left_foot_pred[:,1] = y_pred[:,15]\n",
    "#     left_foot_pred[:,2] = y_pred[:,10]\n",
    "#     left_foot_pred[:,3] = y_pred[:,11]\n",
    "#     left_foot_pred[:,4] = y_pred[:,4]\n",
    "#     left_foot_pred[:,5] = y_pred[:,5]\n",
    "\n",
    "#     left_foot_loss = find_single_angle_loss(left_foot_true, left_foot_pred)\n",
    "#     #DEBUG\n",
    "#     print('the following is the left_foot_angle_loss', left_foot_loss)\n",
    "\n",
    "\n",
    "#     total_angle_loss = beak_angle_loss + head_angle_loss + right_wing_loss + left_wing_loss + right_foot_loss + left_foot_loss\n",
    "#     #DEBUG\n",
    "#     print('the following is the total_angle_loss', total_angle_loss)\n",
    "#     angle_loss = total_angle_loss/6\n",
    "#     #DEBUG\n",
    "#     print('the following is the avg_angle_loss', angle_loss)\n",
    "\n",
    "#     return angle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def masked_simpleAngles(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Computes the mean squared error, ignoring the invisible keypoints.\n",
    "#     Assuming that -10.0 indicates an invisible keypoint.\n",
    "#     \"\"\"\n",
    "#     # DEBUG \n",
    "#     if torch.isnan(y_true).any():\n",
    "#         print('going to nan')\n",
    "#     if torch.isnan(y_pred).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # find the angle loss \n",
    "#     angle_loss = find_angle_loss(y_true, y_pred)\n",
    "\n",
    "#     # normalise the loss\n",
    "#     angle_loss = angle_loss/torch.pi\n",
    "\n",
    "#     # adjust angle to be same order of magnitude as mae\n",
    "#     angle_loss = angle_loss/10\n",
    "\n",
    "#     # Create a mask where keypoints are visible\n",
    "#     mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "#     # Apply the mask to filter out invisible keypoints from both\n",
    "#     # the predictions and the true values\n",
    "#     y_true_masked = y_true * mask\n",
    "#     y_pred_masked = y_pred * mask\n",
    "\n",
    "#     # Compute the Mean Squared Error only on the visible keypoints\n",
    "#     mae =  F.l1_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    \n",
    "\n",
    "#     loss = (mae * 0.7) + (angle_loss * 0.3) \n",
    "#     # DEBUG\n",
    "#     if torch.isnan(angle_loss).any():\n",
    "#         print('going to nan')\n",
    "        \n",
    "#     print('mae loss:',mae.item(), '    angle_loss:', angle_loss.item(), '     total_loss:', loss.item())\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all in one\n",
    "# def find_tensor_angle(input_tensor):\n",
    "#     \"\"\"\n",
    "#     Compute angles between vectors defined in input_tensor.\n",
    "\n",
    "#     Parameters:\n",
    "#     - input_tensor: Tensor with 6 columns and multiple rows.\n",
    "#     Returns:\n",
    "#     - Tensor of angles in radians with the same number of rows as input_tensor.\n",
    "#     \"\"\"\n",
    "#     # # Step 1: Create a mask for rows without -10\n",
    "#     # mask = (input_tensor != -10).all(dim=1).unsqueeze(1)  # Mask rows with -10\n",
    "\n",
    "#     # # Step 2: Adjust input_tensor using the mask\n",
    "#     # #adjusted_input_tensor = input_tensor * mask  # Set rows with -10 to 0\n",
    "\n",
    "#     # Step 3: Compute vectors BA and BC\n",
    "#     # vector_tensor = torch.zeros((adjusted_input_tensor.size(0), 4), device=input_tensor.device)\n",
    "#     # vector_tensor[:, 0] = adjusted_input_tensor[:, 0] - adjusted_input_tensor[:, 2]  # BA_x\n",
    "#     # vector_tensor[:, 1] = adjusted_input_tensor[:, 1] - adjusted_input_tensor[:, 3]  # BA_y\n",
    "#     # vector_tensor[:, 2] = adjusted_input_tensor[:, 4] - adjusted_input_tensor[:, 2]  # BC_x\n",
    "#     # vector_tensor[:, 3] = adjusted_input_tensor[:, 5] - adjusted_input_tensor[:, 3]  # BC_y\n",
    "#     vector_tensor = torch.zeros((input_tensor.size(0), 4), device=input_tensor.device)\n",
    "#     vector_tensor[:, 0] = input_tensor[:, 0] - input_tensor[:, 2]  # BA_x\n",
    "#     vector_tensor[:, 1] = input_tensor[:, 1] - input_tensor[:, 3]  # BA_y\n",
    "#     vector_tensor[:, 2] = input_tensor[:, 4] - input_tensor[:, 2]  # BC_x\n",
    "#     vector_tensor[:, 3] = input_tensor[:, 5] - input_tensor[:, 3]  # BC_y\n",
    "\n",
    "#         # DEBUG - check for nan\n",
    "#     if torch.isnan(vector_tensor).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # Step 4: Compute dot product and magnitudes\n",
    "#     dot_prod_tensor = (vector_tensor[:, 0] * vector_tensor[:, 2] + \n",
    "#                        vector_tensor[:, 1] * vector_tensor[:, 3]).unsqueeze(1)\n",
    "#     magnitude_tensor_BA = torch.linalg.norm(vector_tensor[:, :2], dim=1, keepdim=True)  # Magnitude of BA\n",
    "#     magnitude_tensor_BC = torch.linalg.norm(vector_tensor[:, 2:], dim=1, keepdim=True)  # Magnitude of BC\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(dot_prod_tensor).any():\n",
    "#         print('going to nan')\n",
    "#             # DEBUG - check for nan\n",
    "#     if torch.isnan(magnitude_tensor_BA).any():\n",
    "#         print('going to nan')\n",
    "#         # DEBUG - check for nan\n",
    "#     if torch.isnan(magnitude_tensor_BC).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "\n",
    "#     # Step 5: Compute angles in radians\n",
    "#     clamp_vals = torch.clamp((dot_prod_tensor / (magnitude_tensor_BA * magnitude_tensor_BC)), -0.99999, 0.99999) # Clamp values to the valid range for acos\n",
    "#        # DEBUG - check for nan\n",
    "#     if torch.isnan(clamp_vals).any():\n",
    "#         print('going to nan')\n",
    "    \n",
    "#     angle_tensor = torch.acos(clamp_vals)\n",
    "#        # DEBUG - check for nan\n",
    "#     if torch.isnan(clamp_vals).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "\n",
    "#     return angle_tensor\n",
    "\n",
    "# def tensor_abs_difference(tensor1, tensor2):\n",
    "#     \"\"\"\n",
    "#     Compute the absolute difference between two tensors, each with a single column and multiple rows.\n",
    "\n",
    "#     Parameters:\n",
    "#     - tensor1: A PyTorch tensor with shape (N, 1)\n",
    "#     - tensor2: A PyTorch tensor with shape (N, 1)\n",
    "\n",
    "#     Returns:\n",
    "#     - A PyTorch tensor with the absolute differences, shape (N, 1)\n",
    "#     \"\"\"\n",
    "#     # Ensure both tensors have the same shape\n",
    "#     if tensor1.shape != tensor2.shape:\n",
    "#         raise ValueError(\"Input tensors must have the same shape.\")\n",
    "\n",
    "#     # Compute absolute difference\n",
    "#     abs_diff = torch.abs(tensor1 - tensor2)\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(abs_diff).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     return abs_diff\n",
    "\n",
    "# # find the angle loss\n",
    "# def find_single_angle_loss(y_true_subset, y_pred_subset):\n",
    "\n",
    "#     y_true_angle = find_tensor_angle(y_true_subset)\n",
    "#     y_pred_angle = find_tensor_angle(y_pred_subset)\n",
    "\n",
    "#     # DEBUG\n",
    "#     #print('the following is the angles', y_true_angle)\n",
    "\n",
    "#     # find the abs difference in angles\n",
    "#     angle_abs_differnce = tensor_abs_difference(y_true_angle, y_pred_angle)\n",
    "    \n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(angle_abs_differnce).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # normalise the abs difference (POTENTIALLY NEED TO DO THIS SO IT IS IN THE SAME RANGE AS THE MAE LOSS)\n",
    "\n",
    "#     # Step 1: Create a mask for rows without -10\n",
    "#     mask = (y_true_subset != -10).all(dim=1).unsqueeze(1)  # Mask rows with -10\n",
    "\n",
    "#     # mask angles where kp are missing\n",
    "#     angle_abs_differnce_masked = angle_abs_differnce * mask\n",
    "\n",
    "#     # normalise the abs difference (POTENTIALLY NEED TO DO THIS SO IT IS IN THE SAME RANGE AS THE MAE LOSS) - can only go up to 2*pi rad, so divide by that\n",
    "\n",
    "#     # find the mean abs difference between angles\n",
    "#     num_angles = torch.sum(mask)\n",
    "#     if num_angles == 0:\n",
    "#         num_angles = 1 \n",
    "\n",
    "#     single_angle_loss = torch.sum(angle_abs_differnce_masked)/num_angles\n",
    "\n",
    "#     # DEBUG - check for nan\n",
    "#     if torch.isnan(single_angle_loss).any():\n",
    "#         print('going to nan')\n",
    "    \n",
    "#     return single_angle_loss\n",
    "\n",
    "# def find_angle_loss(y_true, y_pred):\n",
    "\n",
    "#     beak_angle_true = torch.zeros((y_true.size(0), 6), device=y_true.device, dtype=torch.float32) \n",
    "#     beak_angle_true[:,0] = y_true[:,2]\n",
    "#     beak_angle_true[:,1] = y_true[:,3]\n",
    "#     beak_angle_true[:,2] = y_true[:,0]\n",
    "#     beak_angle_true[:,3] = y_true[:,1]\n",
    "#     beak_angle_true[:,4] = y_true[:,4]\n",
    "#     beak_angle_true[:,5] = y_true[:,5]\n",
    "#     beak_angle_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     beak_angle_pred[:,0] = y_pred[:,2]\n",
    "#     beak_angle_pred[:,1] = y_pred[:,3]\n",
    "#     beak_angle_pred[:,2] = y_pred[:,0]\n",
    "#     beak_angle_pred[:,3] = y_pred[:,1]\n",
    "#     beak_angle_pred[:,4] = y_pred[:,4]\n",
    "#     beak_angle_pred[:,5] = y_pred[:,5]\n",
    "\n",
    "#     #print('printing the values for beak angle true and beak angle pred',beak_angle_true, beak_angle_pred)\n",
    "\n",
    "#     beak_angle_loss = find_single_angle_loss(beak_angle_true, beak_angle_pred)\n",
    "    \n",
    "#     #DEBUG\n",
    "#     print('the following is the beak_angle_loss', beak_angle_loss)\n",
    "\n",
    "#     head_angle_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     head_angle_true[:,0] = y_true[:,0]\n",
    "#     head_angle_true[:,1] = y_true[:,1]\n",
    "#     head_angle_true[:,2] = y_true[:,4]\n",
    "#     head_angle_true[:,3] = y_true[:,5]\n",
    "#     head_angle_true[:,4] = y_true[:,10]\n",
    "#     head_angle_true[:,5] = y_true[:,11]\n",
    "#     head_angle_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     head_angle_pred[:,0] = y_pred[:,0]\n",
    "#     head_angle_pred[:,1] = y_pred[:,1]\n",
    "#     head_angle_pred[:,2] = y_pred[:,4]\n",
    "#     head_angle_pred[:,3] = y_pred[:,5]\n",
    "#     head_angle_pred[:,4] = y_pred[:,10]\n",
    "#     head_angle_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     head_angle_loss = find_single_angle_loss(head_angle_true, head_angle_pred)\n",
    "#     #DEBUG\n",
    "#     print('the following is the head_angle_loss', head_angle_loss)\n",
    "\n",
    "#     right_wing_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     right_wing_true[:,0] = y_true[:,6]\n",
    "#     right_wing_true[:,1] = y_true[:,7]\n",
    "#     right_wing_true[:,2] = y_true[:,4]\n",
    "#     right_wing_true[:,3] = y_true[:,5]\n",
    "#     right_wing_true[:,4] = y_true[:,10]\n",
    "#     right_wing_true[:,5] = y_true[:,11]\n",
    "#     right_wing_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     right_wing_pred[:,0] = y_pred[:,6]\n",
    "#     right_wing_pred[:,1] = y_pred[:,7]\n",
    "#     right_wing_pred[:,2] = y_pred[:,4]\n",
    "#     right_wing_pred[:,3] = y_pred[:,5]\n",
    "#     right_wing_pred[:,4] = y_pred[:,10]\n",
    "#     right_wing_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     right_wing_loss = find_single_angle_loss(right_wing_true, right_wing_pred)\n",
    "#     #DEBUG\n",
    "#     print('the following is the right_wing_angle_loss', right_wing_loss)\n",
    "\n",
    "#     left_wing_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     left_wing_true[:,0] = y_true[:,8]\n",
    "#     left_wing_true[:,1] = y_true[:,9]\n",
    "#     left_wing_true[:,2] = y_true[:,4]\n",
    "#     left_wing_true[:,3] = y_true[:,5]\n",
    "#     left_wing_true[:,4] = y_true[:,10]\n",
    "#     left_wing_true[:,5] = y_true[:,11]\n",
    "#     left_wing_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     left_wing_pred[:,0] = y_pred[:,8]\n",
    "#     left_wing_pred[:,1] = y_pred[:,9]\n",
    "#     left_wing_pred[:,2] = y_pred[:,4]\n",
    "#     left_wing_pred[:,3] = y_pred[:,5]\n",
    "#     left_wing_pred[:,4] = y_pred[:,10]\n",
    "#     left_wing_pred[:,5] = y_pred[:,11]\n",
    "\n",
    "#     left_wing_loss = find_single_angle_loss(left_wing_true, left_wing_pred)\n",
    "#     #DEBUG\n",
    "#     print('the following is the left_wing_angle_loss', left_wing_loss)\n",
    "\n",
    "#     right_foot_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     right_foot_true[:,0] = y_true[:,12]\n",
    "#     right_foot_true[:,1] = y_true[:,13]\n",
    "#     right_foot_true[:,2] = y_true[:,10]\n",
    "#     right_foot_true[:,3] = y_true[:,11]\n",
    "#     right_foot_true[:,4] = y_true[:,4]\n",
    "#     right_foot_true[:,5] = y_true[:,5]\n",
    "#     right_foot_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     right_foot_pred[:,0] = y_pred[:,12]\n",
    "#     right_foot_pred[:,1] = y_pred[:,13]\n",
    "#     right_foot_pred[:,2] = y_pred[:,10]\n",
    "#     right_foot_pred[:,3] = y_pred[:,11]\n",
    "#     right_foot_pred[:,4] = y_pred[:,4]\n",
    "#     right_foot_pred[:,5] = y_pred[:,5]\n",
    "\n",
    "#     right_foot_loss = find_single_angle_loss(right_foot_true, right_foot_pred)\n",
    "#     #DEBUG\n",
    "#     print('the following is the right_foot_angle_loss', right_foot_loss)\n",
    "\n",
    "#     left_foot_true = torch.zeros((y_true.size(0), 6), device=y_true.device) \n",
    "#     left_foot_true[:,0] = y_true[:,14]\n",
    "#     left_foot_true[:,1] = y_true[:,15]\n",
    "#     left_foot_true[:,2] = y_true[:,10]\n",
    "#     left_foot_true[:,3] = y_true[:,11]\n",
    "#     left_foot_true[:,4] = y_true[:,4]\n",
    "#     left_foot_true[:,5] = y_true[:,5]\n",
    "#     left_foot_pred = torch.zeros((y_pred.size(0), 6), device=y_pred.device) \n",
    "#     left_foot_pred[:,0] = y_pred[:,14]\n",
    "#     left_foot_pred[:,1] = y_pred[:,15]\n",
    "#     left_foot_pred[:,2] = y_pred[:,10]\n",
    "#     left_foot_pred[:,3] = y_pred[:,11]\n",
    "#     left_foot_pred[:,4] = y_pred[:,4]\n",
    "#     left_foot_pred[:,5] = y_pred[:,5]\n",
    "\n",
    "#     left_foot_loss = find_single_angle_loss(left_foot_true, left_foot_pred)\n",
    "#     #DEBUG\n",
    "#     print('the following is the left_foot_angle_loss', left_foot_loss)\n",
    "\n",
    "\n",
    "#     total_angle_loss = beak_angle_loss + head_angle_loss + right_wing_loss + left_wing_loss + right_foot_loss + left_foot_loss\n",
    "#     #DEBUG\n",
    "#     print('the following is the total_angle_loss', total_angle_loss)\n",
    "#     angle_loss = total_angle_loss/6\n",
    "#     #DEBUG\n",
    "#     print('the following is the avg_angle_loss', angle_loss)\n",
    "\n",
    "#     return angle_loss\n",
    "\n",
    "\n",
    "\n",
    "# def masked_simpleAngles(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Computes the mean squared error, ignoring the invisible keypoints.\n",
    "#     Assuming that -10.0 indicates an invisible keypoint.\n",
    "#     \"\"\"\n",
    "#     # DEBUG \n",
    "#     if torch.isnan(y_true).any():\n",
    "#         print('going to nan')\n",
    "#     if torch.isnan(y_pred).any():\n",
    "#         print('going to nan')\n",
    "\n",
    "#     # find the angle loss \n",
    "#     angle_loss = find_angle_loss(y_true, y_pred)\n",
    "\n",
    "#     # normalise the loss\n",
    "#     angle_loss = angle_loss/torch.pi\n",
    "\n",
    "#     # adjust angle to be same order of magnitude as mae\n",
    "#     angle_loss = angle_loss/10\n",
    "\n",
    "#     # Create a mask where keypoints are visible\n",
    "#     mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "#     # Apply the mask to filter out invisible keypoints from both\n",
    "#     # the predictions and the true values\n",
    "#     y_true_masked = y_true * mask\n",
    "#     y_pred_masked = y_pred * mask\n",
    "\n",
    "#     # Compute the Mean Squared Error only on the visible keypoints\n",
    "#     mae =  F.l1_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    \n",
    "\n",
    "#     loss = (mae * 0.7) + (angle_loss * 0.3) \n",
    "#     # DEBUG\n",
    "#     if torch.isnan(angle_loss).any():\n",
    "#         print('going to nan')\n",
    "        \n",
    "#     print('mae loss:',mae.item(), '    angle_loss:', angle_loss.item(), '     total_loss:', loss.item())\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tensor_angle(input_tensor):\n",
    "    \"\"\"\n",
    "    Compute angles between vectors defined in input_tensor, returning angles in [0, 2pi).\n",
    "\n",
    "    Parameters:\n",
    "    - input_tensor: Tensor with 6 columns and multiple rows.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of angles in radians with the same number of rows as input_tensor.\n",
    "    \"\"\"\n",
    "    # Step 3: Compute vectors BA and BC\n",
    "    vector_tensor = torch.zeros((input_tensor.size(0), 4), device=input_tensor.device)\n",
    "    vector_tensor[:, 0] = input_tensor[:, 0] - input_tensor[:, 2]  # BA_x\n",
    "    vector_tensor[:, 1] = input_tensor[:, 1] - input_tensor[:, 3]  # BA_y\n",
    "    vector_tensor[:, 2] = input_tensor[:, 4] - input_tensor[:, 2]  # BC_x\n",
    "    vector_tensor[:, 3] = input_tensor[:, 5] - input_tensor[:, 3]  # BC_y\n",
    "\n",
    "    # Step 4: Compute dot product, magnitudes, and cross product\n",
    "    dot_prod_tensor = (vector_tensor[:, 0] * vector_tensor[:, 2] + \n",
    "                       vector_tensor[:, 1] * vector_tensor[:, 3]).unsqueeze(1)\n",
    "    # magnitude_tensor_BA = torch.linalg.norm(vector_tensor[:, :2], dim=1, keepdim=True)\n",
    "    # magnitude_tensor_BC = torch.linalg.norm(vector_tensor[:, 2:], dim=1, keepdim=True)\n",
    "    cross_prod_tensor = (vector_tensor[:, 0] * vector_tensor[:, 3] - \n",
    "                         vector_tensor[:, 1] * vector_tensor[:, 2]).unsqueeze(1)\n",
    "\n",
    "    # Compute angle in radians\n",
    "    angle_tensor = torch.atan2(cross_prod_tensor, dot_prod_tensor)\n",
    "\n",
    "    # Convert angle to [0, 2pi)\n",
    "    angle_tensor = torch.where(angle_tensor < 0, angle_tensor + 2 * torch.pi, angle_tensor)\n",
    "\n",
    "    return angle_tensor\n",
    "\n",
    "def tensor_angle_difference(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Compute the angular difference between two tensors (in [0, 2pi))\n",
    "\n",
    "    Parameters:\n",
    "    - tensor1: A PyTorch tensor with angles in radians.\n",
    "    - tensor2: A PyTorch tensor with angles in radians.\n",
    "\n",
    "    Returns:\n",
    "    - A PyTorch tensor with angular differences in [0, pi].\n",
    "    \"\"\"\n",
    "    angle_diff = torch.abs(tensor1 - tensor2)\n",
    "    return torch.where(angle_diff > torch.pi, 2 * torch.pi - angle_diff, angle_diff)\n",
    "\n",
    "def find_single_angle_loss(y_true_subset, y_pred_subset):\n",
    "    \"\"\"\n",
    "    Compute the mean angular loss between y_true_subset and y_pred_subset.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true_subset: Tensor of true coordinates.\n",
    "    - y_pred_subset: Tensor of predicted coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - The mean angular loss.\n",
    "    \"\"\"\n",
    "    y_true_angle = find_tensor_angle(y_true_subset)\n",
    "    y_pred_angle = find_tensor_angle(y_pred_subset)\n",
    "\n",
    "    # Compute angular difference\n",
    "    angle_abs_difference = tensor_angle_difference(y_true_angle, y_pred_angle)\n",
    "\n",
    "    # Create a mask for valid rows (without -10)\n",
    "    mask = (y_true_subset != -10).all(dim=1).unsqueeze(1) # Mask rows with -10\n",
    "    angle_abs_difference_masked = angle_abs_difference * mask # mask angles where kp are missing\n",
    "\n",
    "    # Calculate mean angular loss\n",
    "    num_angles = torch.sum(mask)\n",
    "    if num_angles == 0:\n",
    "        num_angles = 1\n",
    "\n",
    "    single_angle_loss = torch.sum(angle_abs_difference_masked) / num_angles\n",
    "    return single_angle_loss\n",
    "\n",
    "def create_angle_tensor(subset, indices):\n",
    "    \"\"\"\n",
    "    Helper function to generate a tensor containing the coordinates of keypoints needed to compute angles.\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros((subset.size(0), 6), device=subset.device)\n",
    "    \n",
    "    for i, index in enumerate(indices):\n",
    "        tensor[:, i] = subset[:, index] \n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def find_angle_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the average angular loss across all sets of angles.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Tensor of true keypoint coordinates.\n",
    "    - y_pred: Tensor of predicted keypoint coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - The average angular loss.\n",
    "    \"\"\"\n",
    "\n",
    "    indices_list = [\n",
    "        (2, 3, 0, 1, 4, 5),  # Beak\n",
    "        (0, 1, 4, 5, 10, 11),  # Head\n",
    "        (6, 7, 4, 5, 10, 11),  # Right wing\n",
    "        (8, 9, 4, 5, 10, 11),  # Left wing\n",
    "        (12, 13, 10, 11, 4, 5),  # Right foot\n",
    "        (14, 15, 10, 11, 4, 5)  # Left foot\n",
    "    ]\n",
    "\n",
    "    total_loss = 0\n",
    "    # DEBUG\n",
    "    i = 1\n",
    "    total_loss_old = 0\n",
    "    \n",
    "    for indices in indices_list:\n",
    "        true_tensor = create_angle_tensor(y_true, indices)\n",
    "        pred_tensor = create_angle_tensor(y_pred, indices)\n",
    "        total_loss += find_single_angle_loss(true_tensor, pred_tensor)\n",
    "\n",
    "        #DEBUG\n",
    "        print(\"this is loss for\", i, \":\", (total_loss-total_loss_old)/i)\n",
    "        i+=1\n",
    "        total_loss_old = total_loss.clone()\n",
    "        #print(id(total_loss), id(total_loss_old))\n",
    "\n",
    "\n",
    "    return total_loss / len(indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_simpleAngles(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error, ignoring the invisible keypoints.\n",
    "    Assuming that -10.0 indicates an invisible keypoint.\n",
    "    \"\"\"\n",
    "    # DEBUG \n",
    "    if torch.isnan(y_true).any():\n",
    "        print('going to nan')\n",
    "    if torch.isnan(y_pred).any():\n",
    "        print('going to nan')\n",
    "\n",
    "    # find the angle loss \n",
    "    angle_loss = find_angle_loss(y_true, y_pred)\n",
    "\n",
    "    # normalise the loss\n",
    "    angle_loss = angle_loss/torch.pi\n",
    "\n",
    "    # adjust angle to be same order of magnitude as mae\n",
    "    angle_loss = angle_loss/10\n",
    "\n",
    "    # Create a mask where keypoints are visible\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints from both\n",
    "    # the predictions and the true values\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # Compute the Mean Squared Error only on the visible keypoints\n",
    "    mae =  F.l1_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    \n",
    "\n",
    "    loss = (mae * 0.7) + (angle_loss * 0.3) \n",
    "    # DEBUG\n",
    "    if torch.isnan(angle_loss).any():\n",
    "        print('going to nan')\n",
    "        \n",
    "    print('mae loss:',mae.item(), '    angle_loss:', angle_loss.item(), '     total_loss:', loss.item())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.2. Directional bone vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boneVector_tensor(input_tensor):\n",
    "\n",
    "    boneVector_tensor = torch.zeros((input_tensor.size(0),2), device=input_tensor.device)\n",
    "\n",
    "    # beak (0,1) -> head(2,3) we need (head - beak). Vector pointing from A->B = B-A\n",
    "    boneVector_tensor[:, 0] = input_tensor[:, 2] - input_tensor[:, 0]  \n",
    "    boneVector_tensor[:, 1] = input_tensor[:, 3] - input_tensor[:, 1]  # beak_head\n",
    "\n",
    "    return boneVector_tensor\n",
    "    # boneVector_tensor = torch.zeros((input_tensor.size(0),14), device=input_tensor.device)\n",
    "\n",
    "    # boneVector_tensor[:, 0] = input_tensor[:, 0] - input_tensor[:, 2]  # beak_head_x (head -beak)\n",
    "    # boneVector_tensor[:, 1] = input_tensor[:, 1] - input_tensor[:, 3]  # beak_head_y\n",
    "    # boneVector_tensor[:, 2] = input_tensor[:, 4] - input_tensor[:, 0]  # head_btop_x (btop -head)\n",
    "    # boneVector_tensor[:, 3] = input_tensor[:, 5] - input_tensor[:, 1]  # head_btop_y (btop -head)\n",
    "    # boneVector_tensor[:, 4] = input_tensor[:, 4] - input_tensor[:, 6]  # rwing_btop_x (btop -rwing)\n",
    "    # boneVector_tensor[:, 5] = input_tensor[:, 5] - input_tensor[:, 7]  # rwing_btop_y (btop -rwing)\n",
    "    # boneVector_tensor[:, 6] = input_tensor[:, 4] - input_tensor[:, 8]  # lwing_btop_x (btop -lwing)\n",
    "    # boneVector_tensor[:, 7] = input_tensor[:, 5] - input_tensor[:, 9]  # lwing_btop_y (btop -lwing)\n",
    "    # boneVector_tensor[:, 8] = input_tensor[:, 4] - input_tensor[:, 10]  # bbot_btop_x (btop -bbot)\n",
    "    # boneVector_tensor[:, 9] = input_tensor[:, 5] - input_tensor[:, 11]  # bbot_btop_y (btop -bbot)\n",
    "    # boneVector_tensor[:, 10] = input_tensor[:, 10] - input_tensor[:, 12]  # rfoot_bbot_x (bbot - rfoot)\n",
    "    # boneVector_tensor[:, 11] = input_tensor[:, 11] - input_tensor[:, 13]  # rfoot_bbot_y (bbot - rfoot)\n",
    "    # boneVector_tensor[:, 12] = input_tensor[:, 10] - input_tensor[:, 14]  # lfoot_bbot_x (bbot - lfoot)\n",
    "    # boneVector_tensor[:, 13] = input_tensor[:, 11] - input_tensor[:, 15]  # lfoot_bbot_y (bbot - lfoot)\n",
    "\n",
    "\n",
    "def find_single_boneVector_loss(y_true_subset, y_pred_subset):\n",
    "    \"\"\"\n",
    "    Compute the mean bonevector loss between y_true_subset and y_pred_subset.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true_subset: Tensor of true coordinates.\n",
    "    - y_pred_subset: Tensor of predicted coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - The mean angular loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # create the bone vector for true and pred\n",
    "    y_true_boneVector = create_boneVector_tensor(y_true_subset)\n",
    "    y_pred_boneVector = create_boneVector_tensor(y_pred_subset)\n",
    "\n",
    "    # Create a mask for valid rows (without -10)\n",
    "    mask = (y_true_subset != -10).all(dim=1).unsqueeze(1) # Mask rows with -10\n",
    "    y_true_boneVector_masked = y_true_boneVector * mask # mask boneVectors where kp are missing\n",
    "    y_pred_boneVector_masked = y_pred_boneVector * mask # mask boneVectors where kp are missing\n",
    "\n",
    "    # find the number of visible bone vectors\n",
    "    num_boneVectors = torch.sum(mask)\n",
    "    if num_boneVectors == 0:\n",
    "        num_boneVectors = 1\n",
    "\n",
    "    # Compute the mae for the masked bone vectors\n",
    "    single_boneVector_loss = F.l1_loss(y_pred_boneVector_masked, y_true_boneVector_masked, reduction = 'sum')/num_boneVectors\n",
    "\n",
    "    return single_boneVector_loss\n",
    "\n",
    "\n",
    "def create_empty_tensor(subset, indices):\n",
    "    \"\"\"\n",
    "    Helper function to generate a tensor containing the coordinates of keypoints needed to compute angles.\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros((subset.size(0), 4), device=subset.device)\n",
    "    \n",
    "    for i, index in enumerate(indices):\n",
    "        tensor[:, i] = subset[:, index] \n",
    "\n",
    "    return tensor\n",
    "\n",
    "def find_boneVector_loss(y_true, y_pred):\n",
    "    # masking is included in here\n",
    "\n",
    "    # indices list (list of bone vectors to step through)\n",
    "    indices_list = [\n",
    "        (2,3,0,1), # beak_head\n",
    "        (0,1,4,5), # head_btop\n",
    "        (6,7,4,5), # rwing_btop\n",
    "        (8,9,4,5), # lwing_btop\n",
    "        (10,11,4,5), # bbot_btop\n",
    "        (12,13,10,11), # rfoot_bbot\n",
    "        (14,15,10,11), # lfoot_bbot\n",
    "    ]\n",
    "\n",
    "    total_loss = 0\n",
    "    # DEBUG\n",
    "    # i = 1\n",
    "    # total_loss_old = 0\n",
    "\n",
    "    for indices in indices_list:\n",
    "        true_tensor = create_empty_tensor(y_true, indices)\n",
    "        pred_tensor = create_empty_tensor(y_pred, indices)\n",
    "        total_loss += find_single_boneVector_loss(true_tensor, pred_tensor)\n",
    "\n",
    "                #DEBUG\n",
    "        # print(\"this is loss for\", i, \":\", (total_loss-total_loss_old)/i)\n",
    "        # i+=1\n",
    "        # total_loss_old = total_loss.clone()\n",
    "        #print(id(total_loss), id(total_loss_old))\n",
    "    # DEBUG \n",
    "    #print(total_loss)\n",
    "\n",
    "    return total_loss / len(indices_list)\n",
    "\n",
    "    # # create the bone vectors\n",
    "    # boneVector_tensor_true = create_boneVector_tensor(y_true)\n",
    "    # boneVector_tensor_pred = create_boneVector_tensor(y_pred)\n",
    "\n",
    "    # # find the L1 loss (difference between the true and predicted bone vectors)\n",
    "    # boneVector_loss = F.l1_loss(boneVector_tensor_pred, boneVector_tensor_true, reduction = 'sum')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def masked_boneVectors(y_true, y_pred):\n",
    "    \n",
    "#     # find the bone vector loss\n",
    "#     bone_vector_loss = find_boneVector_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.0. create a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with 1 example\n",
    "def create_single_test_pred_tensor():\n",
    "    \"\"\"\n",
    "    Create a test tensor with predefined values to check the loss function.\n",
    "\n",
    "    Returns:\n",
    "    - A PyTorch tensor with specified values.\n",
    "    \"\"\"\n",
    "    test_tensor = torch.tensor([\n",
    "        [ -0.040506199, -0.4795353711,  -0.1504004747, -0.3714727163, -0.0808008015,\n",
    "         -0.2102946043, -0.1742108762, 0.0699356273, 0.1866084188, 0.1029038727,\n",
    "         0.0455775224, 0.2878924012, -0.0331799872, 0.3556605279, 0.1316612512,\n",
    "        0.3684815466]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    return test_tensor\n",
    "single_tensor = create_single_test_pred_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the print precision\n",
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with 4 values\n",
    "def create_test_true_tensor():\n",
    "    \"\"\"\n",
    "    Create a test tensor with predefined values to check the loss function.\n",
    "\n",
    "    Returns:\n",
    "    - A PyTorch tensor with specified values.\n",
    "    \"\"\"\n",
    "    test_tensor = torch.tensor([\n",
    "        [-2.8862e-01, -3.9661e-01, -3.6642e-01, -2.0897e-01, -1.1928e-01,\n",
    "         -2.4787e-01, -7.5804e-02,  1.9866e-02,  1.3014e-01,  3.5885e-02,\n",
    "          8.8955e-02,  2.3726e-01,  5.6918e-02,  2.8760e-01,  2.0337e-01,\n",
    "          3.5396e-01],\n",
    "        [ 9.4359e-02, -4.5702e-01,  2.6858e-01, -3.9250e-01, -6.9108e-02,\n",
    "         -2.3763e-01,  1.3953e-01,  1.1941e-01, -1.0000e+01, -1.0000e+01,\n",
    "         -2.5193e-01,  2.9794e-01,  2.3380e-02,  4.2699e-01, -1.4654e-01,\n",
    "          4.3559e-01],\n",
    "        [ 3.4336e-01, -4.2581e-01,  4.8167e-01, -3.4162e-01,  1.8624e-02,\n",
    "         -2.7848e-01,  9.6801e-02,  6.4300e-02, -1.0000e+01, -1.0000e+01,\n",
    "         -3.5121e-01,  2.2667e-01, -1.6178e-01,  4.1309e-01, -1.0000e+01,\n",
    "         -1.0000e+01],\n",
    "        [-3.4921e-01, -3.5317e-01, -3.9229e-01, -1.9699e-01, -4.5485e-03,\n",
    "         -3.7740e-01, -1.0000e+01, -1.0000e+01, -9.3406e-02,  1.0339e-02,\n",
    "          3.4280e-01,  1.5305e-01,  2.0009e-01,  3.6308e-01,  4.1226e-02,\n",
    "          4.0077e-01]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    return test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with 4 values\n",
    "def create_test_pred_tensor():\n",
    "    \"\"\"\n",
    "    Create a test tensor with predefined values to check the loss function.\n",
    "\n",
    "    Returns:\n",
    "    - A PyTorch tensor with specified values.\n",
    "    \"\"\"\n",
    "    test_tensor = torch.tensor([\n",
    "        [-2.0000e-01, -2.0000e-01, -3.6642e-01, -2.0897e-01, -1.1928e-01,\n",
    "         -2.4787e-01, -7.5804e-02,  1.9866e-02,  1.3014e-01,  3.5885e-02,\n",
    "          8.8955e-02,  2.3726e-01,  5.6918e-02,  2.8760e-01,  2.0337e-01,\n",
    "          3.5396e-01],\n",
    "        [ 9.4359e-02, -4.5702e-01,  2.6858e-01, -3.9250e-01, -6.9108e-02,\n",
    "         -2.3763e-01,  1.3953e-01,  1.1941e-01, -5.0000e-01, -6.9108e+01,\n",
    "         -2.5193e-01,  2.9794e-01,  2.3380e-02,  4.2699e-01, -1.4654e-01,\n",
    "          4.3559e-01],\n",
    "        [ 3.4336e-01, -4.2581e-01,  4.8167e-01, -3.4162e-01,  1.8624e-02,\n",
    "         -2.7848e-01,  9.6801e-02,  6.4300e-02, -1.0000e+01, -1.0000e+01,\n",
    "         -3.5121e-01,  2.2667e-01, -1.6178e-01,  4.1309e-01, -1.0000e+01,\n",
    "         -1.0000e+01],\n",
    "        [-3.4921e-01, -3.5317e-01, -3.9229e-01, -1.9699e-01, -4.5485e-03,\n",
    "         -3.7740e-01, -1.0000e+01, -1.0000e+01, -9.3406e-02,  1.0339e-02,\n",
    "          3.4280e-01,  1.5305e-01,  2.0009e-01,  3.6308e-01,  4.1226e-02,\n",
    "          4.0077e-01]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    return test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test and pred tensors and make them the same\n",
    "y_pred = create_test_pred_tensor()\n",
    "y_true = create_test_true_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with 1 example\n",
    "def create_single_test_pred_tensor():\n",
    "    \"\"\"\n",
    "    Create a test tensor with predefined values to check the loss function.\n",
    "\n",
    "    Returns:\n",
    "    - A PyTorch tensor with specified values.\n",
    "    \"\"\"\n",
    "    test_tensor = torch.tensor([\n",
    "        [ -0.040506199, -0.4795353711,  -0.1504004747, -0.3714727163, -0.0808008015,\n",
    "         -0.2102946043, -0.1742108762, 0.0699356273, 0.1866084188, 0.1029038727,\n",
    "         0.0455775224, 0.2878924012, -0.0331799872, 0.3556605279, 0.1316612512,\n",
    "        0.3684815466]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    return test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tensor = create_single_test_pred_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Check that the maths is correct - MATHS IS CORRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1. create a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the print precision\n",
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with 4 values\n",
    "def create_test_true_tensor():\n",
    "    \"\"\"\n",
    "    Create a test tensor with predefined values to check the loss function.\n",
    "\n",
    "    Returns:\n",
    "    - A PyTorch tensor with specified values.\n",
    "    \"\"\"\n",
    "    test_tensor = torch.tensor([\n",
    "        [-2.8862e-01, -3.9661e-01, -3.6642e-01, -2.0897e-01, -1.1928e-01,\n",
    "         -2.4787e-01, -7.5804e-02,  1.9866e-02,  1.3014e-01,  3.5885e-02,\n",
    "          8.8955e-02,  2.3726e-01,  5.6918e-02,  2.8760e-01,  2.0337e-01,\n",
    "          3.5396e-01],\n",
    "        [ 9.4359e-02, -4.5702e-01,  2.6858e-01, -3.9250e-01, -6.9108e-02,\n",
    "         -2.3763e-01,  1.3953e-01,  1.1941e-01, -1.0000e+01, -1.0000e+01,\n",
    "         -2.5193e-01,  2.9794e-01,  2.3380e-02,  4.2699e-01, -1.4654e-01,\n",
    "          4.3559e-01],\n",
    "        [ 3.4336e-01, -4.2581e-01,  4.8167e-01, -3.4162e-01,  1.8624e-02,\n",
    "         -2.7848e-01,  9.6801e-02,  6.4300e-02, -1.0000e+01, -1.0000e+01,\n",
    "         -3.5121e-01,  2.2667e-01, -1.6178e-01,  4.1309e-01, -1.0000e+01,\n",
    "         -1.0000e+01],\n",
    "        [-3.4921e-01, -3.5317e-01, -3.9229e-01, -1.9699e-01, -4.5485e-03,\n",
    "         -3.7740e-01, -1.0000e+01, -1.0000e+01, -9.3406e-02,  1.0339e-02,\n",
    "          3.4280e-01,  1.5305e-01,  2.0009e-01,  3.6308e-01,  4.1226e-02,\n",
    "          4.0077e-01]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    return test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with 4 values\n",
    "def create_test_pred_tensor():\n",
    "    \"\"\"\n",
    "    Create a test tensor with predefined values to check the loss function.\n",
    "\n",
    "    Returns:\n",
    "    - A PyTorch tensor with specified values.\n",
    "    \"\"\"\n",
    "    test_tensor = torch.tensor([\n",
    "        [-2.0000e-01, -3.9661e-01, -3.6642e-01, -2.0897e-01, -1.1928e-01,\n",
    "         -2.4787e-01, -7.5804e-02,  1.9866e-02,  1.3014e-01,  3.5885e-02,\n",
    "          8.8955e-02,  2.3726e-01,  5.6918e-02,  2.8760e-01,  2.0337e-01,\n",
    "          3.5396e-01],\n",
    "        [ 9.4359e-02, -4.5702e-01,  2.6858e-01, -3.9250e-01, -6.9108e-02,\n",
    "         -2.3763e-01,  1.3953e-01,  1.1941e-01, -5.0000e-01, -6.9108e+01,\n",
    "         -2.5193e-01,  2.9794e-01,  2.3380e-02,  4.2699e-01, -1.4654e-01,\n",
    "          4.3559e-01],\n",
    "        [ 3.4336e-01, -4.2581e-01,  4.8167e-01, -3.4162e-01,  1.8624e-02,\n",
    "         -2.7848e-01,  9.6801e-02,  6.4300e-02, -1.0000e+01, -1.0000e+01,\n",
    "         -3.5121e-01,  2.2667e-01, -1.6178e-01,  4.1309e-01, -1.0000e+01,\n",
    "         -1.0000e+01],\n",
    "        [-3.4921e-01, -3.5317e-01, -3.9229e-01, -1.9699e-01, -4.5485e-03,\n",
    "         -3.7740e-01, -1.0000e+01, -1.0000e+01, -9.3406e-02,  1.0339e-02,\n",
    "          3.4280e-01,  1.5305e-01,  2.0009e-01,  3.6308e-01,  4.1226e-02,\n",
    "          4.0077e-01]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    return test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test and pred tensors and make them the same\n",
    "y_pred = create_test_pred_tensor()\n",
    "y_true = create_test_true_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_true.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2. run through the loss function and check that the maths is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0274)\n"
     ]
    }
   ],
   "source": [
    "print(masked_simpleAngles(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae loss: 0.005093392450362444     angle_loss: 0.004965120926499367      total_loss: 0.005054911132901907\n",
      "This is the loss, expected value is 0 tensor(0.0051)\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the loss, expected value is 0\",masked_simpleAngles(y_true, y_pred))\n",
    "# I have now added print statements within the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Plot the tesor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor_points(tensor):\n",
    "    \"\"\"\n",
    "    Plot the points from the given tensor with labeled keypoints and specific connections.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: PyTorch tensor of shape (N, 16), where each row represents 8 points (x, y pairs).\n",
    "    \"\"\"\n",
    "    labels = [\"head\", \"beak\", \"body_top\", \"right_wing\", \"left_wing\", \"body_bottom\", \"right_foot\", \"left_foot\"]\n",
    "    connections = [(0, 1), (0, 2), (2, 3), (2, 4), (2, 5), (5, 6), (5, 7)]\n",
    "    \n",
    "    for i, row in enumerate(tensor):\n",
    "        \n",
    "        # for plotting with an image you need to uncomment this and comment out plt.figure() and the axis limits and remove the - from the y values\n",
    "        #fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "        #plt.imshow(img)\n",
    "\n",
    "        x_values = row[::2].numpy()\n",
    "        y_values = row[1::2].numpy()\n",
    "        \n",
    "        # Replace -10 values with NaN\n",
    "        x_values[x_values == -10] = np.nan\n",
    "        y_values[y_values == -10] = np.nan\n",
    "        \n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.scatter(x_values, -y_values)\n",
    "\n",
    "        # Set axis limits to always be 220x220\n",
    "        plt.xlim(-0.6, 0.6)\n",
    "        plt.ylim(-0.6, 0.6)  # Inverted y-axis to maintain origin at top-left\n",
    "        \n",
    "        # Label keypoints\n",
    "        for j, label in enumerate(labels):\n",
    "            plt.annotate(label, (x_values[j], -y_values[j]), textcoords=\"offset points\", xytext=(5,5), ha='right')\n",
    "        \n",
    "        # Draw connections\n",
    "        for p1, p2 in connections:\n",
    "            if not (np.isnan(x_values[p1]) or np.isnan(x_values[p2]) or np.isnan(y_values[p1]) or np.isnan(y_values[p2])):\n",
    "                plt.plot([x_values[p1], x_values[p2]], [-y_values[p1], -y_values[p2]], 'k-', lw=1)\n",
    "        \n",
    "        plt.xlabel('X Coordinate')\n",
    "        plt.ylabel('Y Coordinate')\n",
    "        plt.title(f'Plot of Row {i+1}')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor_points_with_img(tensor):\n",
    "    \"\"\"\n",
    "    Plot the points from the given tensor with labeled keypoints and specific connections.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: PyTorch tensor of shape (N, 16), where each row represents 8 points (x, y pairs).\n",
    "    \"\"\"\n",
    "    labels = [\"head\", \"beak\", \"body_top\", \"right_wing\", \"left_wing\", \"body_bottom\", \"right_foot\", \"left_foot\"]\n",
    "    connections = [(0, 1), (0, 2), (2, 3), (2, 4), (2, 5), (5, 6), (5, 7)]\n",
    "    \n",
    "    for i, row in enumerate(tensor):\n",
    "        \n",
    "        # for plotting with an image you need to uncomment this and comment out plt.figure() and the axis limits and remove the - from the y values\n",
    "        #fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "        plt.imshow(img)\n",
    "\n",
    "        x_values = row[::2].numpy()\n",
    "        y_values = row[1::2].numpy()\n",
    "        \n",
    "        # Replace -10 values with NaN\n",
    "        x_values[x_values == -10] = np.nan\n",
    "        y_values[y_values == -10] = np.nan\n",
    "        \n",
    "        #plt.figure()#figsize=(8,8))\n",
    "        plt.scatter(x_values, y_values)\n",
    "\n",
    "        # Set axis limits to always be 220x220\n",
    "        # plt.xlim(-0.5, 0.5)\n",
    "        # plt.ylim(-0.5, 0.5)  # Inverted y-axis to maintain origin at top-left\n",
    "        \n",
    "        # Label keypoints\n",
    "        for j, label in enumerate(labels):\n",
    "            plt.annotate(label, (x_values[j], y_values[j]), textcoords=\"offset points\", xytext=(5,5), ha='right')\n",
    "        \n",
    "        # Draw connections\n",
    "        for p1, p2 in connections:\n",
    "            if not (np.isnan(x_values[p1]) or np.isnan(x_values[p2]) or np.isnan(y_values[p1]) or np.isnan(y_values[p2])):\n",
    "                plt.plot([x_values[p1], x_values[p2]], [y_values[p1], y_values[p2]], 'k-', lw=1)\n",
    "        \n",
    "        plt.xlabel('X Coordinate')\n",
    "        plt.ylabel('Y Coordinate')\n",
    "        plt.title(f'Plot of Row {i+1}')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#IF YOU WANT TO PLOT WITH AN IMAGE \n",
    "# load the image\n",
    "img_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/train/stand72_2_0_crop_220x220.jpg'\n",
    "#print(img_path)\n",
    "img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "#print(img)\n",
    "\n",
    "# change the img to RGB from BGR as plt uses RGB colour scale\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# scaling the pixel values to [0, 1] (you don't need to scal them back)\n",
    "img = img/255\n",
    "\n",
    "# make it a batch of images with a single image in it \n",
    "img_batch = np.expand_dims(img, axis=0)\n",
    "\n",
    "\n",
    "lst = single_tensor.tolist()\n",
    "\n",
    "new_kp = unnorm_keypoints_arr(lst, img_batch)\n",
    "\n",
    "unnorm_kp_tensor = torch.tensor(new_kp, dtype=torch.float32)\n",
    "\n",
    "plot_tensor_points_with_img(unnorm_kp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tensor_points(single_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. plot, move, save tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InteractivePointMover:\n",
    "#     def __init__(self, tensor, labels):\n",
    "#         \"\"\"\n",
    "#         Initialize the interactive plot with the original tensor and labels.\n",
    "#         \"\"\"\n",
    "#         self.original_tensor = tensor.clone()\n",
    "#         self.new_tensor = tensor.clone()\n",
    "#         self.labels = labels\n",
    "#         self.selected_index = None  # For tracking the selected point\n",
    "\n",
    "#     def on_click(self, event):\n",
    "#         \"\"\"Handle mouse click to select a point.\"\"\"\n",
    "#         if event.inaxes:\n",
    "#             x_click, y_click = event.xdata, event.ydata\n",
    "#             distances = np.sqrt((self.new_tensor[:, ::2] - x_click)**2 + (self.new_tensor[:, 1::2] - y_click)**2)\n",
    "#             nearest_point = np.unravel_index(np.argmin(distances), distances.shape)\n",
    "#             self.selected_index = nearest_point\n",
    "#             print(f\"Selected point: {self.labels[nearest_point[1]]}\")\n",
    "\n",
    "#     def on_drag(self, event):\n",
    "#         \"\"\"Handle mouse drag to move the selected point.\"\"\"\n",
    "#         if self.selected_index is not None and event.inaxes:\n",
    "#             x_drag, y_drag = event.xdata, event.ydata\n",
    "#             self.new_tensor[self.selected_index[0], self.selected_index[1]*2] = x_drag\n",
    "#             self.new_tensor[self.selected_index[0], self.selected_index[1]*2+1] = y_drag\n",
    "#             self.update_plot()\n",
    "\n",
    "#     def update_plot(self):\n",
    "#         \"\"\"Update the plot with the new positions.\"\"\"\n",
    "#         self.ax.clear()\n",
    "#         self.ax.scatter(self.original_tensor[0, ::2], -self.original_tensor[0, 1::2], label=\"Original Points\", c=\"blue\")\n",
    "#         self.ax.scatter(self.new_tensor[0, ::2], -self.new_tensor[0, 1::2], label=\"New Points\", c=\"red\")\n",
    "#         for i, label in enumerate(self.labels):\n",
    "#             self.ax.annotate(label, (self.new_tensor[0, i*2], -self.new_tensor[0, i*2+1]), textcoords=\"offset points\", xytext=(5, 5))\n",
    "#         self.ax.set_xlim(-0.5, 0.5)\n",
    "#         self.ax.set_ylim(-0.5, 0.5)\n",
    "#         self.ax.invert_yaxis()\n",
    "#         self.ax.legend()\n",
    "#         plt.draw()\n",
    "\n",
    "#     def save_points(self, file_path):\n",
    "#         \"\"\"Save the new points to a .txt file.\"\"\"\n",
    "#         np.savetxt(file_path, self.new_tensor.numpy(), fmt=\"%.6f\")\n",
    "#         print(f\"Saved new points to {file_path}\")\n",
    "\n",
    "#     def load_points(self, file_path):\n",
    "#         \"\"\"Load points from a .txt file into a tensor.\"\"\"\n",
    "#         loaded_array = np.loadtxt(file_path)\n",
    "#         self.new_tensor = torch.tensor(loaded_array, dtype=torch.float32)\n",
    "#         print(f\"Loaded points from {file_path}\")\n",
    "\n",
    "#     def start(self):\n",
    "#         \"\"\"Start the interactive session.\"\"\"\n",
    "#         fig, self.ax = plt.subplots()\n",
    "#         self.update_plot()\n",
    "\n",
    "#         fig.canvas.mpl_connect(\"button_press_event\", self.on_click)\n",
    "#         fig.canvas.mpl_connect(\"motion_notify_event\", self.on_drag)\n",
    "#         plt.show()\n",
    "\n",
    "# tensor_save_path_parent = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/loss_function_test/test_tensors'\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "# %matplotlib widget\n",
    "# # %matplotlib notebook\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot([0, 1, 2], [0, 1, 4])\n",
    "# plt.title(\"Test Plot\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# #tensor = torch.tensor([[-0.36642, -0.20897, -0.28862, -0.39661, -0.11928, -0.24787, -0.1, -0.1, -0.4, -0.3, -0.3, -0.2, -0.35, -0.1, -0.2, -0.15]], dtype=torch.float32)te\n",
    "# tensor = single_tensor\n",
    "# labels = [\"head\", \"beak\", \"body_top\", \"right_wing\", \"left_wing\", \"body_bottom\", \"right_foot\", \"left_foot\"]\n",
    "\n",
    "# mover = InteractivePointMover(tensor, labels)\n",
    "# mover.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# class PlotlyInteractivePointMover:\n",
    "#     def __init__(self, tensor, labels):\n",
    "#         \"\"\"\n",
    "#         Initialize the interactive plot with the original tensor and labels.\n",
    "#         \"\"\"\n",
    "#         self.original_tensor = tensor.clone()\n",
    "#         self.new_tensor = tensor.clone()\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def plot_points(self):\n",
    "#         \"\"\"Create the interactive plot.\"\"\"\n",
    "#         # Extract x and y coordinates\n",
    "#         x_original = self.original_tensor[0, ::2].numpy()\n",
    "#         y_original = self.original_tensor[0, 1::2].numpy()\n",
    "#         x_new = self.new_tensor[0, ::2].numpy()\n",
    "#         y_new = self.new_tensor[0, 1::2].numpy()\n",
    "\n",
    "#         # Create the interactive scatter plot\n",
    "#         fig = go.Figure()\n",
    "\n",
    "#         # Add original points\n",
    "#         fig.add_trace(go.Scatter(x=x_original, y=y_original, mode='markers+text',\n",
    "#                                  text=self.labels, textposition=\"top center\",\n",
    "#                                  marker=dict(color='blue', size=10), name='Original Points'))\n",
    "\n",
    "#         # Add draggable new points\n",
    "#         fig.add_trace(go.Scatter(x=x_new, y=y_new, mode='markers+text',\n",
    "#                                  text=self.labels, textposition=\"top center\",\n",
    "#                                  marker=dict(color='red', size=10), name='New Points'))\n",
    "\n",
    "#         # Set layout\n",
    "#         fig.update_layout(title=\"Interactive Point Editor\",\n",
    "#                           xaxis_title=\"X Coordinate\",\n",
    "#                           yaxis_title=\"Y Coordinate\",\n",
    "#                           xaxis=dict(range=[-0.5, 0.5]),\n",
    "#                           yaxis=dict(range=[-0.5, 0.5]),\n",
    "#                           dragmode=\"pan\")\n",
    "\n",
    "#         # Display the figure\n",
    "#         fig.show()\n",
    "\n",
    "#     def save_points(self, file_path):\n",
    "#         \"\"\"Save the new points to a .txt file.\"\"\"\n",
    "#         np.savetxt(file_path, self.new_tensor.numpy(), fmt=\"%.6f\")\n",
    "#         print(f\"Saved new points to {file_path}\")\n",
    "\n",
    "#     def load_points(self, file_path):\n",
    "#         \"\"\"Load points from a .txt file into the tensor.\"\"\"\n",
    "#         loaded_array = np.loadtxt(file_path)\n",
    "#         self.new_tensor = torch.tensor(loaded_array, dtype=torch.float32)\n",
    "#         print(f\"Loaded points from {file_path}\")\n",
    "\n",
    "# # Example Usage\n",
    "# tensor = torch.tensor([[-0.36642, -0.20897, -0.28862, -0.39661, -0.11928, -0.24787,\n",
    "#                         -0.1, -0.1, -0.4, -0.3, -0.3, -0.2, -0.35, -0.1, -0.2, -0.15]],\n",
    "#                       dtype=torch.float32)\n",
    "# labels = [\"head\", \"beak\", \"body_top\", \"right_wing\", \"left_wing\", \"body_bottom\", \"right_foot\", \"left_foot\"]\n",
    "\n",
    "# mover = PlotlyInteractivePointMover(tensor, labels)\n",
    "# mover.plot_points()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotlyInteractivePointMover:\n",
    "    def __init__(self, tensor, labels):\n",
    "        \"\"\"\n",
    "        Initialize the interactive plot with the original tensor and labels.\n",
    "        \"\"\"\n",
    "        self.original_tensor = tensor.clone()\n",
    "        self.new_tensor = tensor.clone()\n",
    "        self.labels = labels\n",
    "\n",
    "    def plot_points(self):\n",
    "        \"\"\"Create the interactive plot.\"\"\"\n",
    "        # Extract x and y coordinates\n",
    "        x_original = self.original_tensor[0, ::2].numpy()\n",
    "        y_original = self.original_tensor[0, 1::2].numpy()\n",
    "        x_new = self.new_tensor[0, ::2].numpy()\n",
    "        y_new = self.new_tensor[0, 1::2].numpy()\n",
    "\n",
    "        # Create the interactive scatter plot\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add original points\n",
    "        fig.add_trace(go.Scatter(x=x_original, y=y_original, mode='markers+text',\n",
    "                                 text=self.labels, textposition=\"top center\",\n",
    "                                 marker=dict(color='blue', size=10), name='Original Points'))\n",
    "\n",
    "        # Add draggable new points\n",
    "        fig.add_trace(go.Scatter(x=x_new, y=y_new, mode='markers+text',\n",
    "                                 text=self.labels, textposition=\"top center\",\n",
    "                                 marker=dict(color='red', size=10), name='New Points'))\n",
    "\n",
    "        # Set layout\n",
    "        fig.update_layout(title=\"Interactive Point Editor\",\n",
    "                          xaxis_title=\"X Coordinate\",\n",
    "                          yaxis_title=\"Y Coordinate\",\n",
    "                          xaxis=dict(scaleanchor=\"y\", range=[-0.6, 0.6]),  # Ensures square scale\n",
    "                          yaxis=dict(range=[-0.6, 0.6]),\n",
    "                          dragmode=\"pan\")\n",
    "\n",
    "        # Display the figure\n",
    "        fig.show()\n",
    "\n",
    "    def adjust_points(self, index, new_x, new_y):\n",
    "        \"\"\"Adjust a specific point's coordinates in the new tensor.\"\"\"\n",
    "        self.new_tensor[0, index * 2] = new_x\n",
    "        self.new_tensor[0, index * 2 + 1] = new_y\n",
    "        print(f\"Adjusted {self.labels[index]} to new coordinates: ({new_x}, {new_y})\")\n",
    "\n",
    "    def save_points(self, file_path):\n",
    "        \"\"\"Save the new points to a .txt file.\"\"\"\n",
    "        np.savetxt(file_path, self.new_tensor.numpy(), fmt=\"%.6f\")\n",
    "        print(f\"Saved new points to {file_path}\")\n",
    "\n",
    "    def load_points(self, file_path):\n",
    "        \"\"\"Load points from a .txt file into the tensor.\"\"\"\n",
    "        loaded_array = np.loadtxt(file_path)\n",
    "        self.new_tensor = torch.tensor(loaded_array, dtype=torch.float32)\n",
    "        print(f\"Loaded points from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"head\", \"beak\", \"body_top\", \"right_wing\", \"left_wing\", \"body_bottom\", \"right_foot\", \"left_foot\"]\n",
    "tensor_save_path_parent = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/loss_function_test/test_tensors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mover = PlotlyInteractivePointMover(single_tensor, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = 0.03\n",
    "new_y = -0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "size": 10
         },
         "mode": "markers+text",
         "name": "Original Points",
         "text": [
          "head",
          "beak",
          "body_top",
          "right_wing",
          "left_wing",
          "body_bottom",
          "right_foot",
          "left_foot"
         ],
         "textposition": "top center",
         "type": "scatter",
         "x": [
          -0.04050619900226593,
          -0.15040047466754913,
          -0.08080080151557922,
          -0.17421087622642517,
          0.1866084188222885,
          0.045577522367239,
          -0.03317998722195625,
          0.13166125118732452
         ],
         "y": [
          -0.47953537106513977,
          -0.37147271633148193,
          -0.21029460430145264,
          0.0699356272816658,
          0.10290387272834778,
          0.2878924012184143,
          0.3556605279445648,
          0.3684815466403961
         ]
        },
        {
         "marker": {
          "color": "red",
          "size": 10
         },
         "mode": "markers+text",
         "name": "New Points",
         "text": [
          "head",
          "beak",
          "body_top",
          "right_wing",
          "left_wing",
          "body_bottom",
          "right_foot",
          "left_foot"
         ],
         "textposition": "top center",
         "type": "scatter",
         "x": [
          -0.04050619900226593,
          -0.15040047466754913,
          -0.08080080151557922,
          -0.17421087622642517,
          0.1866084188222885,
          0.045577522367239,
          -0.03317998722195625,
          0.13166125118732452
         ],
         "y": [
          -0.47953537106513977,
          -0.37147271633148193,
          -0.21029460430145264,
          0.0699356272816658,
          0.10290387272834778,
          0.2878924012184143,
          0.3556605279445648,
          0.3684815466403961
         ]
        }
       ],
       "layout": {
        "dragmode": "pan",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Interactive Point Editor"
        },
        "xaxis": {
         "range": [
          -0.6,
          0.6
         ],
         "scaleanchor": "y",
         "title": {
          "text": "X Coordinate"
         }
        },
        "yaxis": {
         "range": [
          -0.6,
          0.6
         ],
         "title": {
          "text": "Y Coordinate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mover.plot_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mover.adjust_points(index=1, new_x=new_x, new_y=new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mover.save_points(tensor_save_path_parent+\"/beak_adjust_sameAngleLeftSide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOU NEED JUST RUN THE BELOW TO LOAD AN ADJUSTED POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded points from /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/loss_function_test/test_tensors/beak_adjust_sameAngleLeftSide\n"
     ]
    }
   ],
   "source": [
    "mover.load_points(tensor_save_path_parent+\"/beak_adjust_sameAngleLeftSide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_tensor = mover.new_tensor.unsqueeze(0)\n",
    "# use the below for if created a new tensor and above for the when you load a tensor \n",
    "#adjusted_tensor = mover.new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_tensor_points(adjusted_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_tensors = torch.cat((single_tensor,adjusted_tensor), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tensor_points(both_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. View the loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = single_tensor\n",
    "y_pred = adjusted_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: tensor(0.0117)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total loss:\",masked_simpleAngles(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Implement pretrained encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Original DeepPose Encoder (AlexNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEEPPOSE ORIGINAL (no pretrained encoder)\n",
    "# class DeepPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "#         super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "#         # The feature extractor part of the model, composed of several convolutional layers.\n",
    "#         self.features = nn.Sequential(\n",
    "#             # Conv2d: Input channels = 3 (RGB image), Output channels = 48, kernel size = 11x11,\n",
    "#             # stride = 4, padding = 4. \n",
    "#             # Input: (batch_size, 3, 220, 220)\n",
    "#             # Output: (batch_size, 48, 55, 55)\n",
    "#             nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 48, 55, 55)\n",
    "#             # Output: (batch_size, 48, 27, 27)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 48, Output channels = 128, kernel size = 5x5,\n",
    "#             # stride = 1, padding = 2.\n",
    "#             # Input: (batch_size, 48, 27, 27)\n",
    "#             # Output: (batch_size, 128, 27, 27)\n",
    "#             nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 128, 27, 27)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 128, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 192, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 192, Output channels = 128, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 192, 13, 13)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         # The classifier part of the model, composed of fully connected layers.\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten the input tensor\n",
    "#             # Input: (batch_size, 128, 6, 6)\n",
    "#             # Output: (batch_size, 128 * 6 * 6) = (batch_size, 4608)\n",
    "#             nn.Flatten(),\n",
    "            \n",
    "#             # Linear layer with input size 4608 and output size 4096\n",
    "#             # Input: (batch_size, 4608)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             nn.Linear(128 * 6 * 6, 4096),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             nn.Dropout(0.6),\n",
    "            \n",
    "#             # Linear layer with input size 4096 and output size 4096\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             nn.Linear(4096, 4096),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             nn.Dropout(0.6),\n",
    "            \n",
    "#             # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "#             # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, nkeypoints * 2)\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Define the forward pass through the network.\n",
    "#         # Pass input `x` through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEEPPOSE (Encoder test 6 version - with GAP)\n",
    "# class DeepPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "#         super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "#         # The feature extractor part of the model, composed of several convolutional layers.\n",
    "#         self.features = nn.Sequential(\n",
    "#             # Conv2d: Input channels = 3 (RGB image), Output channels = 48, kernel size = 11x11,\n",
    "#             # stride = 4, padding = 4. \n",
    "#             # Input: (batch_size, 3, 220, 220)\n",
    "#             # Output: (batch_size, 48, 55, 55)\n",
    "#             nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 48, 55, 55)\n",
    "#             # Output: (batch_size, 48, 27, 27)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 48, Output channels = 128, kernel size = 5x5,\n",
    "#             # stride = 1, padding = 2.\n",
    "#             # Input: (batch_size, 48, 27, 27)\n",
    "#             # Output: (batch_size, 128, 27, 27)\n",
    "#             nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 128, 27, 27)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 128, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 192, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             #------------CHANGE----------------\n",
    "#             # Here I have also changed to 192 output units instead of the 128 \n",
    "#             # Conv2d: Input channels = 192, Output channels = 128, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 192, 13, 13)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             #nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#             #-----------CHANGE------------------\n",
    "#             # Replace maxpool2d with a global pooling to reduce size\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             nn.AdaptiveAvgPool2d(1)\n",
    "#         )\n",
    "        \n",
    "#         # The classifier part of the model, composed of fully connected layers.\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten the input tensor\n",
    "#             # Input: (batch_size, 128, 6, 6)\n",
    "#             # Output: (batch_size, 128 * 6 * 6) = (batch_size, 4608)\n",
    "#             nn.Flatten(),\n",
    "            \n",
    "#             #------------CHANGE----------------\n",
    "#             # Here I have also changed to 192 units \n",
    "#             # Linear layer with input size 4608 and output size 4096\n",
    "#             # Input: (batch_size, 4608)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             nn.Linear(192, 4096),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             nn.Dropout(0.6),\n",
    "            \n",
    "#             # --------CHANGE---------------\n",
    "#             # Removed this FC layer and subsequent activation and dropout as only 7 layers are supposed to be present\n",
    "#             # # Linear layer with input size 4096 and output size 4096\n",
    "#             # # Input: (batch_size, 4096)\n",
    "#             # # Output: (batch_size, 4096)\n",
    "#             # nn.Linear(4096, 4096),\n",
    "            \n",
    "#             # # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             # nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # # Dropout layer with 60% dropout rate\n",
    "#             # nn.Dropout(0.6),\n",
    "            \n",
    "#             # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "#             # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, nkeypoints * 2)\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Define the forward pass through the network.\n",
    "#         # Pass input `x` through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEEPPOSE (Encoder test 12 version - with GAP - remove some layers to make smaller encoder) - removed the 2 x 192 layers\n",
    "# class DeepPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "#         super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "#         # The feature extractor part of the model, composed of several convolutional layers.\n",
    "#         self.features = nn.Sequential(\n",
    "#             # Conv2d: Input channels = 3 (RGB image), Output channels = 48, kernel size = 11x11,\n",
    "#             # stride = 4, padding = 4. \n",
    "#             # Input: (batch_size, 3, 220, 220)\n",
    "#             # Output: (batch_size, 48, 55, 55)\n",
    "#             nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 48, 55, 55)\n",
    "#             # Output: (batch_size, 48, 27, 27)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 48, Output channels = 128, kernel size = 5x5,\n",
    "#             # stride = 1, padding = 2.\n",
    "#             # Input: (batch_size, 48, 27, 27)\n",
    "#             # Output: (batch_size, 128, 27, 27)\n",
    "#             nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # Input: (batch_size, 128, 27, 27)\n",
    "#             # Output: (batch_size, 128, 13, 13)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 128, Output channels = 192, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 192, 13, 13)\n",
    "#             nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # -------- REMOVE THE NEXT TWO CONV LAYERS ------\n",
    "#             # # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "#             # # stride = 1, padding = 1.\n",
    "#             # # Input: (batch_size, 192, 13, 13)\n",
    "#             # # Output: (batch_size, 192, 13, 13)\n",
    "#             # nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # # # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             # nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # #------------CHANGE----------------\n",
    "#             # # Here I have also changed to 192 output units instead of the 128 \n",
    "#             # # Conv2d: Input channels = 192, Output channels = 128, kernel size = 3x3,\n",
    "#             # # stride = 1, padding = 1.\n",
    "#             # # Input: (batch_size, 192, 13, 13)\n",
    "#             # # Output: (batch_size, 128, 13, 13)\n",
    "#             # nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # -------- REMOVE THE NEXT TWO CONV LAYERS ------\n",
    "\n",
    "#             # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             #nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#             #-----------CHANGE------------------\n",
    "#             # Replace maxpool2d with a global pooling to reduce size\n",
    "#             # Input: (batch_size, 128, 13, 13)\n",
    "#             # Output: (batch_size, 128, 6, 6)\n",
    "#             nn.AdaptiveAvgPool2d(1)\n",
    "#         )\n",
    "        \n",
    "#         # The classifier part of the model, composed of fully connected layers.\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten the input tensor\n",
    "#             # Input: (batch_size, 128, 6, 6)\n",
    "#             # Output: (batch_size, 128 * 6 * 6) = (batch_size, 4608)\n",
    "#             nn.Flatten(),\n",
    "            \n",
    "#             #------------CHANGE----------------\n",
    "#             # Here I have also changed to 192 units \n",
    "#             # Linear layer with input size 4608 and output size 4096\n",
    "#             # Input: (batch_size, 4608)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             nn.Linear(192, 4096),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             nn.Dropout(0.6),\n",
    "            \n",
    "#             # --------CHANGE---------------\n",
    "#             # Removed this FC layer and subsequent activation and dropout as only 7 layers are supposed to be present\n",
    "#             # # Linear layer with input size 4096 and output size 4096\n",
    "#             # # Input: (batch_size, 4096)\n",
    "#             # # Output: (batch_size, 4096)\n",
    "#             # nn.Linear(4096, 4096),\n",
    "            \n",
    "#             # # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             # nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # # Dropout layer with 60% dropout rate\n",
    "#             # nn.Dropout(0.6),\n",
    "            \n",
    "#             # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "#             # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, nkeypoints * 2)\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Define the forward pass through the network.\n",
    "#         # Pass input `x` through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0.1. Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.0.1.1. Learning rate adjustment test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - DEEPPOSE (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class DeepPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "        super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "        # The feature extractor part of the model, composed of several convolutional layers.\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv2d: Input channels = 3 (RGB image), Output channels = 48, kernel size = 11x11,\n",
    "            # stride = 4, padding = 4. \n",
    "            # Input: (batch_size, 3, 220, 220)\n",
    "            # Output: (batch_size, 48, 55, 55)\n",
    "            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "            # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "            nn.LocalResponseNorm(5),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            # Input: (batch_size, 48, 55, 55)\n",
    "            # Output: (batch_size, 48, 27, 27)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2d: Input channels = 48, Output channels = 128, kernel size = 5x5,\n",
    "            # stride = 1, padding = 2.\n",
    "            # Input: (batch_size, 48, 27, 27)\n",
    "            # Output: (batch_size, 128, 27, 27)\n",
    "            nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "            # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "            nn.LocalResponseNorm(5),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            # Input: (batch_size, 128, 27, 27)\n",
    "            # Output: (batch_size, 128, 13, 13)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2d: Input channels = 128, Output channels = 192, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 128, 13, 13)\n",
    "            # Output: (batch_size, 192, 13, 13)\n",
    "            nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 192, 13, 13)\n",
    "            # Output: (batch_size, 192, 13, 13)\n",
    "            nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            #------------CHANGE----------------\n",
    "            # Here I have also changed to 192 output units instead of the 128 \n",
    "            # Conv2d: Input channels = 192, Output channels = 192, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 192, 13, 13)\n",
    "            # Output: (batch_size, 128, 13, 13)\n",
    "            nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Replace maxpool2d with a global pooling to reduce size\n",
    "            # Input: (batch_size, 128, 13, 13)\n",
    "            # Output: (batch_size, 128, 6, 6)\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        # The classifier part of the model, composed of fully connected layers.\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Flatten the input tensor\n",
    "            # Input: (batch_size, 128, 6, 6)\n",
    "            # Output: (batch_size, 128 * 6 * 6) = (batch_size, 4608)\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            #------------CHANGE----------------\n",
    "            # Here I have also changed to 192 units \n",
    "            # Linear layer with input size 4608 and output size 4096\n",
    "            # Input: (batch_size, 4608)\n",
    "            # Output: (batch_size, 4096)\n",
    "            nn.Linear(192, 4096),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Dropout layer with 60% dropout rate\n",
    "            nn.Dropout(0.6),\n",
    "                       \n",
    "            # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "            # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "            # Input: (batch_size, 4096)\n",
    "            # Output: (batch_size, nkeypoints * 2)\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass through the network.\n",
    "        # Pass input `x` through the feature extractor\n",
    "        x = self.features(x)\n",
    "        # Pass the result through the classifier to get the final output\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepPose Model Summary\n",
    "model = DeepPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Implement pretrained AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - ResNet50 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class AlexNetEncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(AlexNetEncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained ResNet-50 model\n",
    "        #resnet50 = models.resnet50(pretrained=True)\n",
    "        #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "        # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "        # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "        # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "        self.encoder = nn.Sequential(*list(alexnet.children())[:-2])\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        \n",
    "        # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the decoder part of the model (fully connected layers)\n",
    "        # The input size depends on the output size of the ResNet encoder\n",
    "        # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6), # tried 0.3 - didnt work\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        # Pass the input through the encoder (ResNet-50)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Global average pooling to decrease output size\n",
    "        x = self.GAP(x)\n",
    "\n",
    "        # Flatten the output from the encoder\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass the flattened tensor through the decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 54, 54]          23,296\n",
      "              ReLU-2           [-1, 64, 54, 54]               0\n",
      "         MaxPool2d-3           [-1, 64, 26, 26]               0\n",
      "            Conv2d-4          [-1, 192, 26, 26]         307,392\n",
      "              ReLU-5          [-1, 192, 26, 26]               0\n",
      "         MaxPool2d-6          [-1, 192, 12, 12]               0\n",
      "            Conv2d-7          [-1, 384, 12, 12]         663,936\n",
      "              ReLU-8          [-1, 384, 12, 12]               0\n",
      "            Conv2d-9          [-1, 256, 12, 12]         884,992\n",
      "             ReLU-10          [-1, 256, 12, 12]               0\n",
      "           Conv2d-11          [-1, 256, 12, 12]         590,080\n",
      "             ReLU-12          [-1, 256, 12, 12]               0\n",
      "        MaxPool2d-13            [-1, 256, 5, 5]               0\n",
      "AdaptiveAvgPool2d-14            [-1, 256, 1, 1]               0\n",
      "          Flatten-15                  [-1, 256]               0\n",
      "           Linear-16                 [-1, 4096]       1,052,672\n",
      "             ReLU-17                 [-1, 4096]               0\n",
      "          Dropout-18                 [-1, 4096]               0\n",
      "           Linear-19                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 3,587,920\n",
      "Trainable params: 3,587,920\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.55\n",
      "Forward/backward pass size (MB): 7.48\n",
      "Params size (MB): 13.69\n",
      "Estimated Total Size (MB): 21.73\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# DeepPose Model Summary\n",
    "model = AlexNetEncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 ResNet Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1. ResNet-50\n",
    "this was too big and GPU ran out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-50 (Encoder test 5 - pool, flattern, FC)\n",
    "# class ResNet50EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "#         # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "#         self.encoder = nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # Using the original output\n",
    "#             nn.Linear(2048, nkeypoints * 2)\n",
    "\n",
    "#             # # Fully connected layer 1\n",
    "#             # nn.Linear(2048 * 7 * 7, 64),  # Input size: 2048*7*7, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 2\n",
    "#             # nn.Linear(64, 64),  # Input size: 4096, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 3 (final output layer)\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-50 (Encoder test 6 - original decoder FC-4096 with GAP) - tried decreasing dropout - didn't work - increasing drop out to 0.85 did help\n",
    "# # this suggests that the issue is overfitting. But the increase in dropout causes model val loss and pck to jump around alot)\n",
    "# class ResNet50EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "#         # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "#         self.encoder = nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(2048, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6), # tried 0.3 - didnt work\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "\n",
    "#             # # Fully connected layer 1\n",
    "#             # nn.Linear(2048 * 7 * 7, 64),  # Input size: 2048*7*7, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 2\n",
    "#             # nn.Linear(64, 64),  # Input size: 4096, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 3 (final output layer)\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-50 (Encoder test 7 - original decoder FC-4096 with GAP and frozen layers - first 3 blocks frozen, last 2 are not=[-3] ) - prevent overfitting?\n",
    "# # !!!!! I have also adjusted the adam optimiser so check there in the train loop\n",
    "# class ResNet50EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "#         # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "#         self.encoder = nn.Sequential(*list(resnet50.children())[:-1])\n",
    "\n",
    "#         # Freeze shallower layers (everything before layer3) - see GPT for layers of ResNet\n",
    "#         for param in list(self.encoder.children())[:-3]:  # Freeze everything before `layer3` (Conv2d-79) - the final two blocks are unfrozen\n",
    "#             for sub_param in param.parameters():\n",
    "#                 sub_param.requires_grad = False\n",
    "\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(2048, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-50 (Encoder test 9 - original decoder FC-4096 with GAP) - tried with 0.85 dropout\n",
    "# class ResNet50EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "#         # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "#         self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(2048, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.85), # tried 0.3 - didnt work\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "\n",
    "#             # # Fully connected layer 1\n",
    "#             # nn.Linear(2048 * 7 * 7, 64),  # Input size: 2048*7*7, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 2\n",
    "#             # nn.Linear(64, 64),  # Input size: 4096, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 3 (final output layer)\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Global average pooling to decrease output size\n",
    "#         x = self.GAP(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check that the last 2 \n",
    "# model = ResNet50EncoderPoseModel()\n",
    "# for name, param in model.encoder.named_parameters():\n",
    "#     print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet50EncoderPoseModel()\n",
    "# # check frozen layers\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)  # Move model to GPU\n",
    "# summary(model.frozen_layers, input_size=(3, 220, 220), device=str(device))\n",
    "\n",
    "# checking that all layers are trainable (they are)\n",
    "# for param in model.encoder.parameters():\n",
    "#     print(param.requires_grad)  # Should be True\n",
    "# # checking the model encoder output is right\n",
    "# # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# # model = model.to(device)  # Move model to GPU\n",
    "# dummy_input = torch.randn(1, 3, 220, 220)#.to('cuda')\n",
    "# output = model.encoder(dummy_input)\n",
    "# print(output.shape)  # Should match expected size\n",
    "# output2 = model.flatten(output)\n",
    "# print(output2.shape)\n",
    "# output3 = model.decoder(output2)\n",
    "# print(output3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = resnet50.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1. Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1.2. Learning rate adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - ResNet50 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class ResNet50EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(ResNet50EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained ResNet-50 model\n",
    "        #resnet50 = models.resnet50(pretrained=True)\n",
    "        #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "        # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "        # self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "        # only removing the final fully connected layer and leaving the pooling beforehand\n",
    "        self.encoder = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        \n",
    "        # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the decoder part of the model (fully connected layers)\n",
    "        # The input size depends on the output size of the ResNet encoder\n",
    "        # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6), # tried 0.3 - didnt work\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        # Pass the input through the encoder (ResNet-50)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Global average pooling to decrease output size\n",
    "        x = self.GAP(x)\n",
    "\n",
    "        # Flatten the output from the encoder\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass the flattened tensor through the decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 110, 110]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 110, 110]             128\n",
      "              ReLU-3         [-1, 64, 110, 110]               0\n",
      "         MaxPool2d-4           [-1, 64, 55, 55]               0\n",
      "            Conv2d-5           [-1, 64, 55, 55]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 55, 55]             128\n",
      "              ReLU-7           [-1, 64, 55, 55]               0\n",
      "            Conv2d-8           [-1, 64, 55, 55]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 55, 55]             128\n",
      "             ReLU-10           [-1, 64, 55, 55]               0\n",
      "           Conv2d-11          [-1, 256, 55, 55]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 55, 55]             512\n",
      "           Conv2d-13          [-1, 256, 55, 55]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 55, 55]             512\n",
      "             ReLU-15          [-1, 256, 55, 55]               0\n",
      "       Bottleneck-16          [-1, 256, 55, 55]               0\n",
      "           Conv2d-17           [-1, 64, 55, 55]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 55, 55]             128\n",
      "             ReLU-19           [-1, 64, 55, 55]               0\n",
      "           Conv2d-20           [-1, 64, 55, 55]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 55, 55]             128\n",
      "             ReLU-22           [-1, 64, 55, 55]               0\n",
      "           Conv2d-23          [-1, 256, 55, 55]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 55, 55]             512\n",
      "             ReLU-25          [-1, 256, 55, 55]               0\n",
      "       Bottleneck-26          [-1, 256, 55, 55]               0\n",
      "           Conv2d-27           [-1, 64, 55, 55]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 55, 55]             128\n",
      "             ReLU-29           [-1, 64, 55, 55]               0\n",
      "           Conv2d-30           [-1, 64, 55, 55]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 55, 55]             128\n",
      "             ReLU-32           [-1, 64, 55, 55]               0\n",
      "           Conv2d-33          [-1, 256, 55, 55]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 55, 55]             512\n",
      "             ReLU-35          [-1, 256, 55, 55]               0\n",
      "       Bottleneck-36          [-1, 256, 55, 55]               0\n",
      "           Conv2d-37          [-1, 128, 55, 55]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 55, 55]             256\n",
      "             ReLU-39          [-1, 128, 55, 55]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "         Flatten-174                 [-1, 2048]               0\n",
      "          Linear-175                 [-1, 4096]       8,392,704\n",
      "            ReLU-176                 [-1, 4096]               0\n",
      "         Dropout-177                 [-1, 4096]               0\n",
      "          Linear-178                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 31,966,288\n",
      "Trainable params: 31,966,288\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.55\n",
      "Forward/backward pass size (MB): 281.62\n",
      "Params size (MB): 121.94\n",
      "Estimated Total Size (MB): 404.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# DeepPose Model Summary\n",
    "model = ResNet50EncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2. ResNet-34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-34 (Encoder test 5 - pool, flattern, FC)\n",
    "# class ResNet34EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet34EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet34 = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "#         # adjusting to only use the current output\n",
    "#         self.encoder = nn.Sequential(*list(resnet34.children())[:-1])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-34 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 512, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(512, nkeypoints*2)\n",
    "#             # # Fully connected layer 1\n",
    "#             # nn.Linear(512 * 7 * 7, 64),  # Input size: 512*7*7, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 2\n",
    "#             # nn.Linear(64, 64),  # Input size: 4096, Output size: 4096\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # # Fully connected layer 3 (final output layer)\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-34 (Encoder test 6 - original decoder FC-4096 with GAP)\n",
    "# class ResNet34EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet34EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet34 = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "#         # adjusting to only use the current output\n",
    "#         self.encoder = nn.Sequential(*list(resnet34.children())[:-1])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-34 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 512, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(512, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-34 (Encoder test 9 - original decoder FC-4096 with own GAP, 0.3 dropout)\n",
    "# class ResNet34EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet34EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet34 = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         # self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "#         # adjusting to only use the current output\n",
    "#         self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-34 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 512, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(512, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "    \n",
    "#         # Global average pooling to decrease output size\n",
    "#         x = self.GAP(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet34 = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = resnet34.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet34EncoderPoseModel()\n",
    "# # checking that all layers are trainable (they are)\n",
    "# for param in model.encoder.parameters():\n",
    "#     print(param.requires_grad)  # Should be True\n",
    "# # checking the model encoder output is right\n",
    "# # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# # model = model.to(device)  # Move model to GPU\n",
    "# dummy_input = torch.randn(1, 3, 220, 220)#.to('cuda')\n",
    "# output = model.encoder(dummy_input)\n",
    "# print(output.shape)  # Should match expected size\n",
    "# output2 = model.flatten(output)\n",
    "# print(output2.shape)\n",
    "# output3 = model.decoder(output2)\n",
    "# print(output3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2. Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.2.1. Learning rate adjustment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - ResNet34 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class ResNet34EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(ResNet34EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained ResNet-50 model\n",
    "        #resnet50 = models.resnet50(pretrained=True)\n",
    "        #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        resnet34 = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "        # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "        # self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "        # adjusting to only use the current output\n",
    "        self.encoder = nn.Sequential(*list(resnet34.children())[:-2])\n",
    "        \n",
    "        # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Define the decoder part of the model (fully connected layers)\n",
    "        # The input size depends on the output size of the ResNet encoder\n",
    "        # ResNet-34 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 512, 7, 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        # Pass the input through the encoder (ResNet-50)\n",
    "        x = self.encoder(x)\n",
    "    \n",
    "        # Global average pooling to decrease output size\n",
    "        x = self.GAP(x)\n",
    "\n",
    "        # Flatten the output from the encoder\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass the flattened tensor through the decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 110, 110]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 110, 110]             128\n",
      "              ReLU-3         [-1, 64, 110, 110]               0\n",
      "         MaxPool2d-4           [-1, 64, 55, 55]               0\n",
      "            Conv2d-5           [-1, 64, 55, 55]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 55, 55]             128\n",
      "              ReLU-7           [-1, 64, 55, 55]               0\n",
      "            Conv2d-8           [-1, 64, 55, 55]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 55, 55]             128\n",
      "             ReLU-10           [-1, 64, 55, 55]               0\n",
      "       BasicBlock-11           [-1, 64, 55, 55]               0\n",
      "           Conv2d-12           [-1, 64, 55, 55]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 55, 55]             128\n",
      "             ReLU-14           [-1, 64, 55, 55]               0\n",
      "           Conv2d-15           [-1, 64, 55, 55]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 55, 55]             128\n",
      "             ReLU-17           [-1, 64, 55, 55]               0\n",
      "       BasicBlock-18           [-1, 64, 55, 55]               0\n",
      "           Conv2d-19           [-1, 64, 55, 55]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 55, 55]             128\n",
      "             ReLU-21           [-1, 64, 55, 55]               0\n",
      "           Conv2d-22           [-1, 64, 55, 55]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 55, 55]             128\n",
      "             ReLU-24           [-1, 64, 55, 55]               0\n",
      "       BasicBlock-25           [-1, 64, 55, 55]               0\n",
      "           Conv2d-26          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 28, 28]             256\n",
      "             ReLU-28          [-1, 128, 28, 28]               0\n",
      "           Conv2d-29          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 28, 28]             256\n",
      "           Conv2d-31          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "             ReLU-37          [-1, 128, 28, 28]               0\n",
      "           Conv2d-38          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 28, 28]             256\n",
      "             ReLU-40          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-41          [-1, 128, 28, 28]               0\n",
      "           Conv2d-42          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "             ReLU-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-57          [-1, 256, 14, 14]             512\n",
      "             ReLU-58          [-1, 256, 14, 14]               0\n",
      "           Conv2d-59          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 14, 14]             512\n",
      "           Conv2d-61          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-62          [-1, 256, 14, 14]             512\n",
      "             ReLU-63          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-64          [-1, 256, 14, 14]               0\n",
      "           Conv2d-65          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-66          [-1, 256, 14, 14]             512\n",
      "             ReLU-67          [-1, 256, 14, 14]               0\n",
      "           Conv2d-68          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 14, 14]             512\n",
      "             ReLU-70          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-71          [-1, 256, 14, 14]               0\n",
      "           Conv2d-72          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "             ReLU-74          [-1, 256, 14, 14]               0\n",
      "           Conv2d-75          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
      "             ReLU-77          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
      "           Conv2d-79          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-80          [-1, 256, 14, 14]             512\n",
      "             ReLU-81          [-1, 256, 14, 14]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-85          [-1, 256, 14, 14]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "           Conv2d-89          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 14, 14]             512\n",
      "             ReLU-91          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
      "           Conv2d-93          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-94          [-1, 256, 14, 14]             512\n",
      "             ReLU-95          [-1, 256, 14, 14]               0\n",
      "           Conv2d-96          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 14, 14]             512\n",
      "             ReLU-98          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-99          [-1, 256, 14, 14]               0\n",
      "          Conv2d-100            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-101            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-102            [-1, 512, 7, 7]               0\n",
      "          Conv2d-103            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-104            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-107            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
      "          Conv2d-109            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-110            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-111            [-1, 512, 7, 7]               0\n",
      "          Conv2d-112            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-114            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-115            [-1, 512, 7, 7]               0\n",
      "          Conv2d-116            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-118            [-1, 512, 7, 7]               0\n",
      "          Conv2d-119            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-120            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-121            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
      "         Flatten-124                  [-1, 512]               0\n",
      "          Linear-125                 [-1, 4096]       2,101,248\n",
      "            ReLU-126                 [-1, 4096]               0\n",
      "         Dropout-127                 [-1, 4096]               0\n",
      "          Linear-128                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 23,451,472\n",
      "Trainable params: 23,451,472\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.55\n",
      "Forward/backward pass size (MB): 94.54\n",
      "Params size (MB): 89.46\n",
      "Estimated Total Size (MB): 184.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ResNet-34 Model Summary\n",
    "model = ResNet34EncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3. ResNet-18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-18\n",
    "# class ResNet18EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet18EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # Fully connected layer 1\n",
    "#             nn.Linear(512 * 7 * 7, 4096),  # Input size: 512*7*7, Output size: 4096\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # Fully connected layer 2\n",
    "#             nn.Linear(4096, 4096),  # Input size: 4096, Output size: 4096\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),  # Dropout to reduce overfitting\n",
    "\n",
    "#             # Fully connected layer 3 (final output layer)\n",
    "#             nn.Linear(4096, nkeypoints * 2)  # Output size: nkeypoints * 2 (x, y coordinates for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-18 (Encoder test 8 - original 4096 decoder with pooling layer, smaller encoder)\n",
    "# class ResNet18EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet18EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(512, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-18 (Encoder test 9 - original 4096 decoder with OWN GAP layer, smaller encoder, no dropout)\n",
    "# class ResNet18EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet18EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(512, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Global average pooling to decrease output size\n",
    "#         x = self.GAP(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResNet-18 (Encoder test 11 - original 4096 decoder with OWN GAP layer, smaller encoder, no dropout, train from scratch)\n",
    "# class ResNet18EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(ResNet18EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained ResNet-50 model\n",
    "#         #resnet50 = models.resnet50(pretrained=True)\n",
    "#         #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#         resnet18 = models.resnet18() # default is none weights\n",
    "        \n",
    "\n",
    "#         # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "#         self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "#         # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Define the decoder part of the model (fully connected layers)\n",
    "#         # The input size depends on the output size of the ResNet encoder\n",
    "#         # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(512, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.6),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defines the forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         # Pass the input through the encoder (ResNet-50)\n",
    "#         x = self.encoder(x)\n",
    "\n",
    "#         # Global average pooling to decrease output size\n",
    "#         x = self.GAP(x)\n",
    "\n",
    "#         # Flatten the output from the encoder\n",
    "#         x = self.flatten(x)\n",
    "\n",
    "#         # Pass the flattened tensor through the decoder\n",
    "#         x = self.decoder(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3. Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.3.1. Learning rate adjustment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - ResNet18 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class ResNet18EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model with ResNet-50 as the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(ResNet18EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained ResNet-50 model\n",
    "        #resnet50 = models.resnet50(pretrained=True)\n",
    "        #resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT) # default is none weights\n",
    "        \n",
    "\n",
    "        # Remove the final fully connected layer of ResNet-50 (we'll add our own decoder)\n",
    "        self.encoder = nn.Sequential(*list(resnet18.children())[:-2])\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Flatten layer: Prepares the output from the encoder for the fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the decoder part of the model (fully connected layers)\n",
    "        # The input size depends on the output size of the ResNet encoder\n",
    "        # ResNet-50 with an input image size of (3, 220, 220) produces an output of shape (batch_size, 2048, 7, 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        # Pass the input through the encoder (ResNet-50)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Global average pooling to decrease output size\n",
    "        x = self.GAP(x)\n",
    "\n",
    "        # Flatten the output from the encoder\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass the flattened tensor through the decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 110, 110]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 110, 110]             128\n",
      "              ReLU-3         [-1, 64, 110, 110]               0\n",
      "         MaxPool2d-4           [-1, 64, 55, 55]               0\n",
      "            Conv2d-5           [-1, 64, 55, 55]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 55, 55]             128\n",
      "              ReLU-7           [-1, 64, 55, 55]               0\n",
      "            Conv2d-8           [-1, 64, 55, 55]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 55, 55]             128\n",
      "             ReLU-10           [-1, 64, 55, 55]               0\n",
      "       BasicBlock-11           [-1, 64, 55, 55]               0\n",
      "           Conv2d-12           [-1, 64, 55, 55]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 55, 55]             128\n",
      "             ReLU-14           [-1, 64, 55, 55]               0\n",
      "           Conv2d-15           [-1, 64, 55, 55]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 55, 55]             128\n",
      "             ReLU-17           [-1, 64, 55, 55]               0\n",
      "       BasicBlock-18           [-1, 64, 55, 55]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "          Flatten-68                  [-1, 512]               0\n",
      "           Linear-69                 [-1, 4096]       2,101,248\n",
      "             ReLU-70                 [-1, 4096]               0\n",
      "           Linear-71                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 13,343,312\n",
      "Trainable params: 13,343,312\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.55\n",
      "Forward/backward pass size (MB): 61.39\n",
      "Params size (MB): 50.90\n",
      "Estimated Total Size (MB): 112.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ResNet-18 Model Summary\n",
    "model = ResNet18EncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 EfficientNet Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 EfficientNet B0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientNetB0EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB0EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B0 model\n",
    "#         efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT) #default is imgnet1k v1\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "    \n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(1280 * 7 * 7, 4096),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(4096, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "# model = efficientnet #nn.Sequential(*list(efficientnet.features)) #.children())[:])\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientNetB0EncoderPoseModel_HalfFC(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB0EncoderPoseModel_HalfFC, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B0 model\n",
    "#         efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT) #default is imgnet1k v1\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(1280 * 7 * 7, 2048),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(2048, 2048),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(2048, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EfficientNetB0EncoderPoseModel (Encoder test 11) train from scratch\n",
    "# class EfficientNetB0EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB0EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B0 model\n",
    "#         efficientnet = models.efficientnet_b0()# default is no pretrained weights\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # nn.Linear(1280 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, 64),\n",
    "#             # nn.ReLU(inplace=True),\n",
    "#             # nn.Dropout(0.3),\n",
    "\n",
    "#             # nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "#             nn.Linear(1280, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.3),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EfficientNet-BO Model Summary\n",
    "# model = EfficientNetB0EncoderPoseModel()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1. Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.1.1. Learning rate adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - EfficientNetB0 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class EfficientNetB0EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(EfficientNetB0EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained EfficientNet-B0 model\n",
    "        efficientnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "\n",
    "        # Remove the classification head (i.e., avgpool and fc layers)\n",
    "        # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "        self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            nn.Linear(1280, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 110, 110]             864\n",
      "       BatchNorm2d-2         [-1, 32, 110, 110]              64\n",
      "              SiLU-3         [-1, 32, 110, 110]               0\n",
      "            Conv2d-4         [-1, 32, 110, 110]             288\n",
      "       BatchNorm2d-5         [-1, 32, 110, 110]              64\n",
      "              SiLU-6         [-1, 32, 110, 110]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             264\n",
      "              SiLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-11             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-12         [-1, 32, 110, 110]               0\n",
      "           Conv2d-13         [-1, 16, 110, 110]             512\n",
      "      BatchNorm2d-14         [-1, 16, 110, 110]              32\n",
      "           MBConv-15         [-1, 16, 110, 110]               0\n",
      "           Conv2d-16         [-1, 96, 110, 110]           1,536\n",
      "      BatchNorm2d-17         [-1, 96, 110, 110]             192\n",
      "             SiLU-18         [-1, 96, 110, 110]               0\n",
      "           Conv2d-19           [-1, 96, 55, 55]             864\n",
      "      BatchNorm2d-20           [-1, 96, 55, 55]             192\n",
      "             SiLU-21           [-1, 96, 55, 55]               0\n",
      "AdaptiveAvgPool2d-22             [-1, 96, 1, 1]               0\n",
      "           Conv2d-23              [-1, 4, 1, 1]             388\n",
      "             SiLU-24              [-1, 4, 1, 1]               0\n",
      "           Conv2d-25             [-1, 96, 1, 1]             480\n",
      "          Sigmoid-26             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-27           [-1, 96, 55, 55]               0\n",
      "           Conv2d-28           [-1, 24, 55, 55]           2,304\n",
      "      BatchNorm2d-29           [-1, 24, 55, 55]              48\n",
      "           MBConv-30           [-1, 24, 55, 55]               0\n",
      "           Conv2d-31          [-1, 144, 55, 55]           3,456\n",
      "      BatchNorm2d-32          [-1, 144, 55, 55]             288\n",
      "             SiLU-33          [-1, 144, 55, 55]               0\n",
      "           Conv2d-34          [-1, 144, 55, 55]           1,296\n",
      "      BatchNorm2d-35          [-1, 144, 55, 55]             288\n",
      "             SiLU-36          [-1, 144, 55, 55]               0\n",
      "AdaptiveAvgPool2d-37            [-1, 144, 1, 1]               0\n",
      "           Conv2d-38              [-1, 6, 1, 1]             870\n",
      "             SiLU-39              [-1, 6, 1, 1]               0\n",
      "           Conv2d-40            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-41            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-42          [-1, 144, 55, 55]               0\n",
      "           Conv2d-43           [-1, 24, 55, 55]           3,456\n",
      "      BatchNorm2d-44           [-1, 24, 55, 55]              48\n",
      "  StochasticDepth-45           [-1, 24, 55, 55]               0\n",
      "           MBConv-46           [-1, 24, 55, 55]               0\n",
      "           Conv2d-47          [-1, 144, 55, 55]           3,456\n",
      "      BatchNorm2d-48          [-1, 144, 55, 55]             288\n",
      "             SiLU-49          [-1, 144, 55, 55]               0\n",
      "           Conv2d-50          [-1, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-51          [-1, 144, 28, 28]             288\n",
      "             SiLU-52          [-1, 144, 28, 28]               0\n",
      "AdaptiveAvgPool2d-53            [-1, 144, 1, 1]               0\n",
      "           Conv2d-54              [-1, 6, 1, 1]             870\n",
      "             SiLU-55              [-1, 6, 1, 1]               0\n",
      "           Conv2d-56            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-57            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-58          [-1, 144, 28, 28]               0\n",
      "           Conv2d-59           [-1, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-60           [-1, 40, 28, 28]              80\n",
      "           MBConv-61           [-1, 40, 28, 28]               0\n",
      "           Conv2d-62          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-63          [-1, 240, 28, 28]             480\n",
      "             SiLU-64          [-1, 240, 28, 28]               0\n",
      "           Conv2d-65          [-1, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-66          [-1, 240, 28, 28]             480\n",
      "             SiLU-67          [-1, 240, 28, 28]               0\n",
      "AdaptiveAvgPool2d-68            [-1, 240, 1, 1]               0\n",
      "           Conv2d-69             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-70             [-1, 10, 1, 1]               0\n",
      "           Conv2d-71            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-72            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-73          [-1, 240, 28, 28]               0\n",
      "           Conv2d-74           [-1, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-75           [-1, 40, 28, 28]              80\n",
      "  StochasticDepth-76           [-1, 40, 28, 28]               0\n",
      "           MBConv-77           [-1, 40, 28, 28]               0\n",
      "           Conv2d-78          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-79          [-1, 240, 28, 28]             480\n",
      "             SiLU-80          [-1, 240, 28, 28]               0\n",
      "           Conv2d-81          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-82          [-1, 240, 14, 14]             480\n",
      "             SiLU-83          [-1, 240, 14, 14]               0\n",
      "AdaptiveAvgPool2d-84            [-1, 240, 1, 1]               0\n",
      "           Conv2d-85             [-1, 10, 1, 1]           2,410\n",
      "             SiLU-86             [-1, 10, 1, 1]               0\n",
      "           Conv2d-87            [-1, 240, 1, 1]           2,640\n",
      "          Sigmoid-88            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-89          [-1, 240, 14, 14]               0\n",
      "           Conv2d-90           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-91           [-1, 80, 14, 14]             160\n",
      "           MBConv-92           [-1, 80, 14, 14]               0\n",
      "           Conv2d-93          [-1, 480, 14, 14]          38,400\n",
      "      BatchNorm2d-94          [-1, 480, 14, 14]             960\n",
      "             SiLU-95          [-1, 480, 14, 14]               0\n",
      "           Conv2d-96          [-1, 480, 14, 14]           4,320\n",
      "      BatchNorm2d-97          [-1, 480, 14, 14]             960\n",
      "             SiLU-98          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-99            [-1, 480, 1, 1]               0\n",
      "          Conv2d-100             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-101             [-1, 20, 1, 1]               0\n",
      "          Conv2d-102            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-103            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-104          [-1, 480, 14, 14]               0\n",
      "          Conv2d-105           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-106           [-1, 80, 14, 14]             160\n",
      " StochasticDepth-107           [-1, 80, 14, 14]               0\n",
      "          MBConv-108           [-1, 80, 14, 14]               0\n",
      "          Conv2d-109          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-110          [-1, 480, 14, 14]             960\n",
      "            SiLU-111          [-1, 480, 14, 14]               0\n",
      "          Conv2d-112          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-113          [-1, 480, 14, 14]             960\n",
      "            SiLU-114          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 480, 1, 1]               0\n",
      "          Conv2d-116             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-117             [-1, 20, 1, 1]               0\n",
      "          Conv2d-118            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-119            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-120          [-1, 480, 14, 14]               0\n",
      "          Conv2d-121           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-122           [-1, 80, 14, 14]             160\n",
      " StochasticDepth-123           [-1, 80, 14, 14]               0\n",
      "          MBConv-124           [-1, 80, 14, 14]               0\n",
      "          Conv2d-125          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-126          [-1, 480, 14, 14]             960\n",
      "            SiLU-127          [-1, 480, 14, 14]               0\n",
      "          Conv2d-128          [-1, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-129          [-1, 480, 14, 14]             960\n",
      "            SiLU-130          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-131            [-1, 480, 1, 1]               0\n",
      "          Conv2d-132             [-1, 20, 1, 1]           9,620\n",
      "            SiLU-133             [-1, 20, 1, 1]               0\n",
      "          Conv2d-134            [-1, 480, 1, 1]          10,080\n",
      "         Sigmoid-135            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-136          [-1, 480, 14, 14]               0\n",
      "          Conv2d-137          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-138          [-1, 112, 14, 14]             224\n",
      "          MBConv-139          [-1, 112, 14, 14]               0\n",
      "          Conv2d-140          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-141          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-142          [-1, 672, 14, 14]               0\n",
      "          Conv2d-143          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-144          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-145          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-146            [-1, 672, 1, 1]               0\n",
      "          Conv2d-147             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-148             [-1, 28, 1, 1]               0\n",
      "          Conv2d-149            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-150            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-151          [-1, 672, 14, 14]               0\n",
      "          Conv2d-152          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-153          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-154          [-1, 112, 14, 14]               0\n",
      "          MBConv-155          [-1, 112, 14, 14]               0\n",
      "          Conv2d-156          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-157          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-158          [-1, 672, 14, 14]               0\n",
      "          Conv2d-159          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-160          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-161          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-162            [-1, 672, 1, 1]               0\n",
      "          Conv2d-163             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-164             [-1, 28, 1, 1]               0\n",
      "          Conv2d-165            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-166            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-167          [-1, 672, 14, 14]               0\n",
      "          Conv2d-168          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-169          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-170          [-1, 112, 14, 14]               0\n",
      "          MBConv-171          [-1, 112, 14, 14]               0\n",
      "          Conv2d-172          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-173          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-174          [-1, 672, 14, 14]               0\n",
      "          Conv2d-175            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-176            [-1, 672, 7, 7]           1,344\n",
      "            SiLU-177            [-1, 672, 7, 7]               0\n",
      "AdaptiveAvgPool2d-178            [-1, 672, 1, 1]               0\n",
      "          Conv2d-179             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-180             [-1, 28, 1, 1]               0\n",
      "          Conv2d-181            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-182            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-183            [-1, 672, 7, 7]               0\n",
      "          Conv2d-184            [-1, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-185            [-1, 192, 7, 7]             384\n",
      "          MBConv-186            [-1, 192, 7, 7]               0\n",
      "          Conv2d-187           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-188           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-189           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-190           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-191           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-192           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-193           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-194             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-195             [-1, 48, 1, 1]               0\n",
      "          Conv2d-196           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-197           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-198           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-199            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-200            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-201            [-1, 192, 7, 7]               0\n",
      "          MBConv-202            [-1, 192, 7, 7]               0\n",
      "          Conv2d-203           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-204           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-205           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-206           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-207           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-208           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-209           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-210             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-211             [-1, 48, 1, 1]               0\n",
      "          Conv2d-212           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-213           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-214           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-215            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-216            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-217            [-1, 192, 7, 7]               0\n",
      "          MBConv-218            [-1, 192, 7, 7]               0\n",
      "          Conv2d-219           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-220           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-221           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-222           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-223           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-224           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-225           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-226             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-227             [-1, 48, 1, 1]               0\n",
      "          Conv2d-228           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-229           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-230           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-231            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-232            [-1, 192, 7, 7]             384\n",
      " StochasticDepth-233            [-1, 192, 7, 7]               0\n",
      "          MBConv-234            [-1, 192, 7, 7]               0\n",
      "          Conv2d-235           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-236           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-237           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-238           [-1, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-239           [-1, 1152, 7, 7]           2,304\n",
      "            SiLU-240           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-241           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-242             [-1, 48, 1, 1]          55,344\n",
      "            SiLU-243             [-1, 48, 1, 1]               0\n",
      "          Conv2d-244           [-1, 1152, 1, 1]          56,448\n",
      "         Sigmoid-245           [-1, 1152, 1, 1]               0\n",
      "SqueezeExcitation-246           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-247            [-1, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-248            [-1, 320, 7, 7]             640\n",
      "          MBConv-249            [-1, 320, 7, 7]               0\n",
      "          Conv2d-250           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-251           [-1, 1280, 7, 7]           2,560\n",
      "            SiLU-252           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-253           [-1, 1280, 1, 1]               0\n",
      "         Flatten-254                 [-1, 1280]               0\n",
      "          Linear-255                 [-1, 4096]       5,246,976\n",
      "            ReLU-256                 [-1, 4096]               0\n",
      "         Dropout-257                 [-1, 4096]               0\n",
      "          Linear-258                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 9,320,076\n",
      "Trainable params: 9,320,076\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.55\n",
      "Forward/backward pass size (MB): 170.15\n",
      "Params size (MB): 35.55\n",
      "Estimated Total Size (MB): 206.26\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# EfficientNet-BO Model Summary\n",
    "model = EfficientNetB0EncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 EfficientNet B2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2. Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2.1. Learning rate adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - EfficientNetB2 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class EfficientNetB2EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained EfficientNet-B2 encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(EfficientNetB2EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained EfficientNet-B0 model\n",
    "        efficientnet = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT) #default is imgnet1k v1\n",
    "\n",
    "        # Remove the classification head (i.e., avgpool and fc layers)\n",
    "        # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "        self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            nn.Linear(1408, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 \n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 110, 110]             864\n",
      "       BatchNorm2d-2         [-1, 32, 110, 110]              64\n",
      "              SiLU-3         [-1, 32, 110, 110]               0\n",
      "            Conv2d-4         [-1, 32, 110, 110]             288\n",
      "       BatchNorm2d-5         [-1, 32, 110, 110]              64\n",
      "              SiLU-6         [-1, 32, 110, 110]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             264\n",
      "              SiLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 32, 1, 1]             288\n",
      "          Sigmoid-11             [-1, 32, 1, 1]               0\n",
      "SqueezeExcitation-12         [-1, 32, 110, 110]               0\n",
      "           Conv2d-13         [-1, 16, 110, 110]             512\n",
      "      BatchNorm2d-14         [-1, 16, 110, 110]              32\n",
      "           MBConv-15         [-1, 16, 110, 110]               0\n",
      "           Conv2d-16         [-1, 16, 110, 110]             144\n",
      "      BatchNorm2d-17         [-1, 16, 110, 110]              32\n",
      "             SiLU-18         [-1, 16, 110, 110]               0\n",
      "AdaptiveAvgPool2d-19             [-1, 16, 1, 1]               0\n",
      "           Conv2d-20              [-1, 4, 1, 1]              68\n",
      "             SiLU-21              [-1, 4, 1, 1]               0\n",
      "           Conv2d-22             [-1, 16, 1, 1]              80\n",
      "          Sigmoid-23             [-1, 16, 1, 1]               0\n",
      "SqueezeExcitation-24         [-1, 16, 110, 110]               0\n",
      "           Conv2d-25         [-1, 16, 110, 110]             256\n",
      "      BatchNorm2d-26         [-1, 16, 110, 110]              32\n",
      "  StochasticDepth-27         [-1, 16, 110, 110]               0\n",
      "           MBConv-28         [-1, 16, 110, 110]               0\n",
      "           Conv2d-29         [-1, 96, 110, 110]           1,536\n",
      "      BatchNorm2d-30         [-1, 96, 110, 110]             192\n",
      "             SiLU-31         [-1, 96, 110, 110]               0\n",
      "           Conv2d-32           [-1, 96, 55, 55]             864\n",
      "      BatchNorm2d-33           [-1, 96, 55, 55]             192\n",
      "             SiLU-34           [-1, 96, 55, 55]               0\n",
      "AdaptiveAvgPool2d-35             [-1, 96, 1, 1]               0\n",
      "           Conv2d-36              [-1, 4, 1, 1]             388\n",
      "             SiLU-37              [-1, 4, 1, 1]               0\n",
      "           Conv2d-38             [-1, 96, 1, 1]             480\n",
      "          Sigmoid-39             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-40           [-1, 96, 55, 55]               0\n",
      "           Conv2d-41           [-1, 24, 55, 55]           2,304\n",
      "      BatchNorm2d-42           [-1, 24, 55, 55]              48\n",
      "           MBConv-43           [-1, 24, 55, 55]               0\n",
      "           Conv2d-44          [-1, 144, 55, 55]           3,456\n",
      "      BatchNorm2d-45          [-1, 144, 55, 55]             288\n",
      "             SiLU-46          [-1, 144, 55, 55]               0\n",
      "           Conv2d-47          [-1, 144, 55, 55]           1,296\n",
      "      BatchNorm2d-48          [-1, 144, 55, 55]             288\n",
      "             SiLU-49          [-1, 144, 55, 55]               0\n",
      "AdaptiveAvgPool2d-50            [-1, 144, 1, 1]               0\n",
      "           Conv2d-51              [-1, 6, 1, 1]             870\n",
      "             SiLU-52              [-1, 6, 1, 1]               0\n",
      "           Conv2d-53            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-54            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-55          [-1, 144, 55, 55]               0\n",
      "           Conv2d-56           [-1, 24, 55, 55]           3,456\n",
      "      BatchNorm2d-57           [-1, 24, 55, 55]              48\n",
      "  StochasticDepth-58           [-1, 24, 55, 55]               0\n",
      "           MBConv-59           [-1, 24, 55, 55]               0\n",
      "           Conv2d-60          [-1, 144, 55, 55]           3,456\n",
      "      BatchNorm2d-61          [-1, 144, 55, 55]             288\n",
      "             SiLU-62          [-1, 144, 55, 55]               0\n",
      "           Conv2d-63          [-1, 144, 55, 55]           1,296\n",
      "      BatchNorm2d-64          [-1, 144, 55, 55]             288\n",
      "             SiLU-65          [-1, 144, 55, 55]               0\n",
      "AdaptiveAvgPool2d-66            [-1, 144, 1, 1]               0\n",
      "           Conv2d-67              [-1, 6, 1, 1]             870\n",
      "             SiLU-68              [-1, 6, 1, 1]               0\n",
      "           Conv2d-69            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-70            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-71          [-1, 144, 55, 55]               0\n",
      "           Conv2d-72           [-1, 24, 55, 55]           3,456\n",
      "      BatchNorm2d-73           [-1, 24, 55, 55]              48\n",
      "  StochasticDepth-74           [-1, 24, 55, 55]               0\n",
      "           MBConv-75           [-1, 24, 55, 55]               0\n",
      "           Conv2d-76          [-1, 144, 55, 55]           3,456\n",
      "      BatchNorm2d-77          [-1, 144, 55, 55]             288\n",
      "             SiLU-78          [-1, 144, 55, 55]               0\n",
      "           Conv2d-79          [-1, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-80          [-1, 144, 28, 28]             288\n",
      "             SiLU-81          [-1, 144, 28, 28]               0\n",
      "AdaptiveAvgPool2d-82            [-1, 144, 1, 1]               0\n",
      "           Conv2d-83              [-1, 6, 1, 1]             870\n",
      "             SiLU-84              [-1, 6, 1, 1]               0\n",
      "           Conv2d-85            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-86            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-87          [-1, 144, 28, 28]               0\n",
      "           Conv2d-88           [-1, 48, 28, 28]           6,912\n",
      "      BatchNorm2d-89           [-1, 48, 28, 28]              96\n",
      "           MBConv-90           [-1, 48, 28, 28]               0\n",
      "           Conv2d-91          [-1, 288, 28, 28]          13,824\n",
      "      BatchNorm2d-92          [-1, 288, 28, 28]             576\n",
      "             SiLU-93          [-1, 288, 28, 28]               0\n",
      "           Conv2d-94          [-1, 288, 28, 28]           7,200\n",
      "      BatchNorm2d-95          [-1, 288, 28, 28]             576\n",
      "             SiLU-96          [-1, 288, 28, 28]               0\n",
      "AdaptiveAvgPool2d-97            [-1, 288, 1, 1]               0\n",
      "           Conv2d-98             [-1, 12, 1, 1]           3,468\n",
      "             SiLU-99             [-1, 12, 1, 1]               0\n",
      "          Conv2d-100            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-101            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-102          [-1, 288, 28, 28]               0\n",
      "          Conv2d-103           [-1, 48, 28, 28]          13,824\n",
      "     BatchNorm2d-104           [-1, 48, 28, 28]              96\n",
      " StochasticDepth-105           [-1, 48, 28, 28]               0\n",
      "          MBConv-106           [-1, 48, 28, 28]               0\n",
      "          Conv2d-107          [-1, 288, 28, 28]          13,824\n",
      "     BatchNorm2d-108          [-1, 288, 28, 28]             576\n",
      "            SiLU-109          [-1, 288, 28, 28]               0\n",
      "          Conv2d-110          [-1, 288, 28, 28]           7,200\n",
      "     BatchNorm2d-111          [-1, 288, 28, 28]             576\n",
      "            SiLU-112          [-1, 288, 28, 28]               0\n",
      "AdaptiveAvgPool2d-113            [-1, 288, 1, 1]               0\n",
      "          Conv2d-114             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-115             [-1, 12, 1, 1]               0\n",
      "          Conv2d-116            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-117            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-118          [-1, 288, 28, 28]               0\n",
      "          Conv2d-119           [-1, 48, 28, 28]          13,824\n",
      "     BatchNorm2d-120           [-1, 48, 28, 28]              96\n",
      " StochasticDepth-121           [-1, 48, 28, 28]               0\n",
      "          MBConv-122           [-1, 48, 28, 28]               0\n",
      "          Conv2d-123          [-1, 288, 28, 28]          13,824\n",
      "     BatchNorm2d-124          [-1, 288, 28, 28]             576\n",
      "            SiLU-125          [-1, 288, 28, 28]               0\n",
      "          Conv2d-126          [-1, 288, 14, 14]           2,592\n",
      "     BatchNorm2d-127          [-1, 288, 14, 14]             576\n",
      "            SiLU-128          [-1, 288, 14, 14]               0\n",
      "AdaptiveAvgPool2d-129            [-1, 288, 1, 1]               0\n",
      "          Conv2d-130             [-1, 12, 1, 1]           3,468\n",
      "            SiLU-131             [-1, 12, 1, 1]               0\n",
      "          Conv2d-132            [-1, 288, 1, 1]           3,744\n",
      "         Sigmoid-133            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-134          [-1, 288, 14, 14]               0\n",
      "          Conv2d-135           [-1, 88, 14, 14]          25,344\n",
      "     BatchNorm2d-136           [-1, 88, 14, 14]             176\n",
      "          MBConv-137           [-1, 88, 14, 14]               0\n",
      "          Conv2d-138          [-1, 528, 14, 14]          46,464\n",
      "     BatchNorm2d-139          [-1, 528, 14, 14]           1,056\n",
      "            SiLU-140          [-1, 528, 14, 14]               0\n",
      "          Conv2d-141          [-1, 528, 14, 14]           4,752\n",
      "     BatchNorm2d-142          [-1, 528, 14, 14]           1,056\n",
      "            SiLU-143          [-1, 528, 14, 14]               0\n",
      "AdaptiveAvgPool2d-144            [-1, 528, 1, 1]               0\n",
      "          Conv2d-145             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-146             [-1, 22, 1, 1]               0\n",
      "          Conv2d-147            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-148            [-1, 528, 1, 1]               0\n",
      "SqueezeExcitation-149          [-1, 528, 14, 14]               0\n",
      "          Conv2d-150           [-1, 88, 14, 14]          46,464\n",
      "     BatchNorm2d-151           [-1, 88, 14, 14]             176\n",
      " StochasticDepth-152           [-1, 88, 14, 14]               0\n",
      "          MBConv-153           [-1, 88, 14, 14]               0\n",
      "          Conv2d-154          [-1, 528, 14, 14]          46,464\n",
      "     BatchNorm2d-155          [-1, 528, 14, 14]           1,056\n",
      "            SiLU-156          [-1, 528, 14, 14]               0\n",
      "          Conv2d-157          [-1, 528, 14, 14]           4,752\n",
      "     BatchNorm2d-158          [-1, 528, 14, 14]           1,056\n",
      "            SiLU-159          [-1, 528, 14, 14]               0\n",
      "AdaptiveAvgPool2d-160            [-1, 528, 1, 1]               0\n",
      "          Conv2d-161             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-162             [-1, 22, 1, 1]               0\n",
      "          Conv2d-163            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-164            [-1, 528, 1, 1]               0\n",
      "SqueezeExcitation-165          [-1, 528, 14, 14]               0\n",
      "          Conv2d-166           [-1, 88, 14, 14]          46,464\n",
      "     BatchNorm2d-167           [-1, 88, 14, 14]             176\n",
      " StochasticDepth-168           [-1, 88, 14, 14]               0\n",
      "          MBConv-169           [-1, 88, 14, 14]               0\n",
      "          Conv2d-170          [-1, 528, 14, 14]          46,464\n",
      "     BatchNorm2d-171          [-1, 528, 14, 14]           1,056\n",
      "            SiLU-172          [-1, 528, 14, 14]               0\n",
      "          Conv2d-173          [-1, 528, 14, 14]           4,752\n",
      "     BatchNorm2d-174          [-1, 528, 14, 14]           1,056\n",
      "            SiLU-175          [-1, 528, 14, 14]               0\n",
      "AdaptiveAvgPool2d-176            [-1, 528, 1, 1]               0\n",
      "          Conv2d-177             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-178             [-1, 22, 1, 1]               0\n",
      "          Conv2d-179            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-180            [-1, 528, 1, 1]               0\n",
      "SqueezeExcitation-181          [-1, 528, 14, 14]               0\n",
      "          Conv2d-182           [-1, 88, 14, 14]          46,464\n",
      "     BatchNorm2d-183           [-1, 88, 14, 14]             176\n",
      " StochasticDepth-184           [-1, 88, 14, 14]               0\n",
      "          MBConv-185           [-1, 88, 14, 14]               0\n",
      "          Conv2d-186          [-1, 528, 14, 14]          46,464\n",
      "     BatchNorm2d-187          [-1, 528, 14, 14]           1,056\n",
      "            SiLU-188          [-1, 528, 14, 14]               0\n",
      "          Conv2d-189          [-1, 528, 14, 14]          13,200\n",
      "     BatchNorm2d-190          [-1, 528, 14, 14]           1,056\n",
      "            SiLU-191          [-1, 528, 14, 14]               0\n",
      "AdaptiveAvgPool2d-192            [-1, 528, 1, 1]               0\n",
      "          Conv2d-193             [-1, 22, 1, 1]          11,638\n",
      "            SiLU-194             [-1, 22, 1, 1]               0\n",
      "          Conv2d-195            [-1, 528, 1, 1]          12,144\n",
      "         Sigmoid-196            [-1, 528, 1, 1]               0\n",
      "SqueezeExcitation-197          [-1, 528, 14, 14]               0\n",
      "          Conv2d-198          [-1, 120, 14, 14]          63,360\n",
      "     BatchNorm2d-199          [-1, 120, 14, 14]             240\n",
      "          MBConv-200          [-1, 120, 14, 14]               0\n",
      "          Conv2d-201          [-1, 720, 14, 14]          86,400\n",
      "     BatchNorm2d-202          [-1, 720, 14, 14]           1,440\n",
      "            SiLU-203          [-1, 720, 14, 14]               0\n",
      "          Conv2d-204          [-1, 720, 14, 14]          18,000\n",
      "     BatchNorm2d-205          [-1, 720, 14, 14]           1,440\n",
      "            SiLU-206          [-1, 720, 14, 14]               0\n",
      "AdaptiveAvgPool2d-207            [-1, 720, 1, 1]               0\n",
      "          Conv2d-208             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-209             [-1, 30, 1, 1]               0\n",
      "          Conv2d-210            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-211            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-212          [-1, 720, 14, 14]               0\n",
      "          Conv2d-213          [-1, 120, 14, 14]          86,400\n",
      "     BatchNorm2d-214          [-1, 120, 14, 14]             240\n",
      " StochasticDepth-215          [-1, 120, 14, 14]               0\n",
      "          MBConv-216          [-1, 120, 14, 14]               0\n",
      "          Conv2d-217          [-1, 720, 14, 14]          86,400\n",
      "     BatchNorm2d-218          [-1, 720, 14, 14]           1,440\n",
      "            SiLU-219          [-1, 720, 14, 14]               0\n",
      "          Conv2d-220          [-1, 720, 14, 14]          18,000\n",
      "     BatchNorm2d-221          [-1, 720, 14, 14]           1,440\n",
      "            SiLU-222          [-1, 720, 14, 14]               0\n",
      "AdaptiveAvgPool2d-223            [-1, 720, 1, 1]               0\n",
      "          Conv2d-224             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-225             [-1, 30, 1, 1]               0\n",
      "          Conv2d-226            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-227            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-228          [-1, 720, 14, 14]               0\n",
      "          Conv2d-229          [-1, 120, 14, 14]          86,400\n",
      "     BatchNorm2d-230          [-1, 120, 14, 14]             240\n",
      " StochasticDepth-231          [-1, 120, 14, 14]               0\n",
      "          MBConv-232          [-1, 120, 14, 14]               0\n",
      "          Conv2d-233          [-1, 720, 14, 14]          86,400\n",
      "     BatchNorm2d-234          [-1, 720, 14, 14]           1,440\n",
      "            SiLU-235          [-1, 720, 14, 14]               0\n",
      "          Conv2d-236          [-1, 720, 14, 14]          18,000\n",
      "     BatchNorm2d-237          [-1, 720, 14, 14]           1,440\n",
      "            SiLU-238          [-1, 720, 14, 14]               0\n",
      "AdaptiveAvgPool2d-239            [-1, 720, 1, 1]               0\n",
      "          Conv2d-240             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-241             [-1, 30, 1, 1]               0\n",
      "          Conv2d-242            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-243            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-244          [-1, 720, 14, 14]               0\n",
      "          Conv2d-245          [-1, 120, 14, 14]          86,400\n",
      "     BatchNorm2d-246          [-1, 120, 14, 14]             240\n",
      " StochasticDepth-247          [-1, 120, 14, 14]               0\n",
      "          MBConv-248          [-1, 120, 14, 14]               0\n",
      "          Conv2d-249          [-1, 720, 14, 14]          86,400\n",
      "     BatchNorm2d-250          [-1, 720, 14, 14]           1,440\n",
      "            SiLU-251          [-1, 720, 14, 14]               0\n",
      "          Conv2d-252            [-1, 720, 7, 7]          18,000\n",
      "     BatchNorm2d-253            [-1, 720, 7, 7]           1,440\n",
      "            SiLU-254            [-1, 720, 7, 7]               0\n",
      "AdaptiveAvgPool2d-255            [-1, 720, 1, 1]               0\n",
      "          Conv2d-256             [-1, 30, 1, 1]          21,630\n",
      "            SiLU-257             [-1, 30, 1, 1]               0\n",
      "          Conv2d-258            [-1, 720, 1, 1]          22,320\n",
      "         Sigmoid-259            [-1, 720, 1, 1]               0\n",
      "SqueezeExcitation-260            [-1, 720, 7, 7]               0\n",
      "          Conv2d-261            [-1, 208, 7, 7]         149,760\n",
      "     BatchNorm2d-262            [-1, 208, 7, 7]             416\n",
      "          MBConv-263            [-1, 208, 7, 7]               0\n",
      "          Conv2d-264           [-1, 1248, 7, 7]         259,584\n",
      "     BatchNorm2d-265           [-1, 1248, 7, 7]           2,496\n",
      "            SiLU-266           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-267           [-1, 1248, 7, 7]          31,200\n",
      "     BatchNorm2d-268           [-1, 1248, 7, 7]           2,496\n",
      "            SiLU-269           [-1, 1248, 7, 7]               0\n",
      "AdaptiveAvgPool2d-270           [-1, 1248, 1, 1]               0\n",
      "          Conv2d-271             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-272             [-1, 52, 1, 1]               0\n",
      "          Conv2d-273           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-274           [-1, 1248, 1, 1]               0\n",
      "SqueezeExcitation-275           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-276            [-1, 208, 7, 7]         259,584\n",
      "     BatchNorm2d-277            [-1, 208, 7, 7]             416\n",
      " StochasticDepth-278            [-1, 208, 7, 7]               0\n",
      "          MBConv-279            [-1, 208, 7, 7]               0\n",
      "          Conv2d-280           [-1, 1248, 7, 7]         259,584\n",
      "     BatchNorm2d-281           [-1, 1248, 7, 7]           2,496\n",
      "            SiLU-282           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-283           [-1, 1248, 7, 7]          31,200\n",
      "     BatchNorm2d-284           [-1, 1248, 7, 7]           2,496\n",
      "            SiLU-285           [-1, 1248, 7, 7]               0\n",
      "AdaptiveAvgPool2d-286           [-1, 1248, 1, 1]               0\n",
      "          Conv2d-287             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-288             [-1, 52, 1, 1]               0\n",
      "          Conv2d-289           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-290           [-1, 1248, 1, 1]               0\n",
      "SqueezeExcitation-291           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-292            [-1, 208, 7, 7]         259,584\n",
      "     BatchNorm2d-293            [-1, 208, 7, 7]             416\n",
      " StochasticDepth-294            [-1, 208, 7, 7]               0\n",
      "          MBConv-295            [-1, 208, 7, 7]               0\n",
      "          Conv2d-296           [-1, 1248, 7, 7]         259,584\n",
      "     BatchNorm2d-297           [-1, 1248, 7, 7]           2,496\n",
      "            SiLU-298           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-299           [-1, 1248, 7, 7]          31,200\n",
      "     BatchNorm2d-300           [-1, 1248, 7, 7]           2,496\n",
      "            SiLU-301           [-1, 1248, 7, 7]               0\n",
      "AdaptiveAvgPool2d-302           [-1, 1248, 1, 1]               0\n",
      "          Conv2d-303             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-304             [-1, 52, 1, 1]               0\n",
      "          Conv2d-305           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-306           [-1, 1248, 1, 1]               0\n",
      "SqueezeExcitation-307           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-308            [-1, 208, 7, 7]         259,584\n",
      "     BatchNorm2d-309            [-1, 208, 7, 7]             416\n",
      " StochasticDepth-310            [-1, 208, 7, 7]               0\n",
      "          MBConv-311            [-1, 208, 7, 7]               0\n",
      "          Conv2d-312           [-1, 1248, 7, 7]         259,584\n",
      "     BatchNorm2d-313           [-1, 1248, 7, 7]           2,496\n",
      "            SiLU-314           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-315           [-1, 1248, 7, 7]          31,200\n",
      "     BatchNorm2d-316           [-1, 1248, 7, 7]           2,496\n",
      "            SiLU-317           [-1, 1248, 7, 7]               0\n",
      "AdaptiveAvgPool2d-318           [-1, 1248, 1, 1]               0\n",
      "          Conv2d-319             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-320             [-1, 52, 1, 1]               0\n",
      "          Conv2d-321           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-322           [-1, 1248, 1, 1]               0\n",
      "SqueezeExcitation-323           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-324            [-1, 208, 7, 7]         259,584\n",
      "     BatchNorm2d-325            [-1, 208, 7, 7]             416\n",
      " StochasticDepth-326            [-1, 208, 7, 7]               0\n",
      "          MBConv-327            [-1, 208, 7, 7]               0\n",
      "          Conv2d-328           [-1, 1248, 7, 7]         259,584\n",
      "     BatchNorm2d-329           [-1, 1248, 7, 7]           2,496\n",
      "            SiLU-330           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-331           [-1, 1248, 7, 7]          11,232\n",
      "     BatchNorm2d-332           [-1, 1248, 7, 7]           2,496\n",
      "            SiLU-333           [-1, 1248, 7, 7]               0\n",
      "AdaptiveAvgPool2d-334           [-1, 1248, 1, 1]               0\n",
      "          Conv2d-335             [-1, 52, 1, 1]          64,948\n",
      "            SiLU-336             [-1, 52, 1, 1]               0\n",
      "          Conv2d-337           [-1, 1248, 1, 1]          66,144\n",
      "         Sigmoid-338           [-1, 1248, 1, 1]               0\n",
      "SqueezeExcitation-339           [-1, 1248, 7, 7]               0\n",
      "          Conv2d-340            [-1, 352, 7, 7]         439,296\n",
      "     BatchNorm2d-341            [-1, 352, 7, 7]             704\n",
      "          MBConv-342            [-1, 352, 7, 7]               0\n",
      "          Conv2d-343           [-1, 2112, 7, 7]         743,424\n",
      "     BatchNorm2d-344           [-1, 2112, 7, 7]           4,224\n",
      "            SiLU-345           [-1, 2112, 7, 7]               0\n",
      "          Conv2d-346           [-1, 2112, 7, 7]          19,008\n",
      "     BatchNorm2d-347           [-1, 2112, 7, 7]           4,224\n",
      "            SiLU-348           [-1, 2112, 7, 7]               0\n",
      "AdaptiveAvgPool2d-349           [-1, 2112, 1, 1]               0\n",
      "          Conv2d-350             [-1, 88, 1, 1]         185,944\n",
      "            SiLU-351             [-1, 88, 1, 1]               0\n",
      "          Conv2d-352           [-1, 2112, 1, 1]         187,968\n",
      "         Sigmoid-353           [-1, 2112, 1, 1]               0\n",
      "SqueezeExcitation-354           [-1, 2112, 7, 7]               0\n",
      "          Conv2d-355            [-1, 352, 7, 7]         743,424\n",
      "     BatchNorm2d-356            [-1, 352, 7, 7]             704\n",
      " StochasticDepth-357            [-1, 352, 7, 7]               0\n",
      "          MBConv-358            [-1, 352, 7, 7]               0\n",
      "          Conv2d-359           [-1, 1408, 7, 7]         495,616\n",
      "     BatchNorm2d-360           [-1, 1408, 7, 7]           2,816\n",
      "            SiLU-361           [-1, 1408, 7, 7]               0\n",
      "AdaptiveAvgPool2d-362           [-1, 1408, 1, 1]               0\n",
      "         Flatten-363                 [-1, 1408]               0\n",
      "          Linear-364                 [-1, 4096]       5,771,264\n",
      "            ReLU-365                 [-1, 4096]               0\n",
      "         Dropout-366                 [-1, 4096]               0\n",
      "          Linear-367                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 13,537,810\n",
      "Trainable params: 13,537,810\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.55\n",
      "Forward/backward pass size (MB): 252.56\n",
      "Params size (MB): 51.64\n",
      "Estimated Total Size (MB): 304.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# EfficientNet-BO Model Summary\n",
    "model = EfficientNetB2EncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 EfficientNet-B4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientNetB4EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB4EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B4 model\n",
    "#         efficientnet = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT) #default is imgnet1k v2\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(1792 * 7 * 7, 4096),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(4096, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EfficientNet-B4 Model Summary\n",
    "# model = EfficientNetB4EncoderPoseModel()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EfficientNetB4EncoderPoseModel_HalfFC(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(EfficientNetB4EncoderPoseModel_HalfFC, self).__init__()\n",
    "\n",
    "#         # Load a pretrained EfficientNet-B4 model\n",
    "#         efficientnet = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT) #default is imgnet1k v2\n",
    "\n",
    "#         # Remove the classification head (i.e., avgpool and fc layers)\n",
    "#         # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "#         self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(1792 * 7 * 7, 2048),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(2048, 2048),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(2048, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1. Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1.2. Learning rate adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - EfficientNetB4 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class EfficientNetB4EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained EfficientNet-B0 encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(EfficientNetB4EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained EfficientNet-B0 model\n",
    "        efficientnet = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n",
    "\n",
    "        # Remove the classification head (i.e., avgpool and fc layers)\n",
    "        # EfficientNet-B0's feature extractor ends with a Conv2d layer (output: 1280 channels)\n",
    "        self.encoder = nn.Sequential(*list(efficientnet.features))\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (usually 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            nn.Linear(1792, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 48, 110, 110]           1,296\n",
      "       BatchNorm2d-2         [-1, 48, 110, 110]              96\n",
      "              SiLU-3         [-1, 48, 110, 110]               0\n",
      "            Conv2d-4         [-1, 48, 110, 110]             432\n",
      "       BatchNorm2d-5         [-1, 48, 110, 110]              96\n",
      "              SiLU-6         [-1, 48, 110, 110]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 48, 1, 1]               0\n",
      "            Conv2d-8             [-1, 12, 1, 1]             588\n",
      "              SiLU-9             [-1, 12, 1, 1]               0\n",
      "           Conv2d-10             [-1, 48, 1, 1]             624\n",
      "          Sigmoid-11             [-1, 48, 1, 1]               0\n",
      "SqueezeExcitation-12         [-1, 48, 110, 110]               0\n",
      "           Conv2d-13         [-1, 24, 110, 110]           1,152\n",
      "      BatchNorm2d-14         [-1, 24, 110, 110]              48\n",
      "           MBConv-15         [-1, 24, 110, 110]               0\n",
      "           Conv2d-16         [-1, 24, 110, 110]             216\n",
      "      BatchNorm2d-17         [-1, 24, 110, 110]              48\n",
      "             SiLU-18         [-1, 24, 110, 110]               0\n",
      "AdaptiveAvgPool2d-19             [-1, 24, 1, 1]               0\n",
      "           Conv2d-20              [-1, 6, 1, 1]             150\n",
      "             SiLU-21              [-1, 6, 1, 1]               0\n",
      "           Conv2d-22             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-23             [-1, 24, 1, 1]               0\n",
      "SqueezeExcitation-24         [-1, 24, 110, 110]               0\n",
      "           Conv2d-25         [-1, 24, 110, 110]             576\n",
      "      BatchNorm2d-26         [-1, 24, 110, 110]              48\n",
      "  StochasticDepth-27         [-1, 24, 110, 110]               0\n",
      "           MBConv-28         [-1, 24, 110, 110]               0\n",
      "           Conv2d-29        [-1, 144, 110, 110]           3,456\n",
      "      BatchNorm2d-30        [-1, 144, 110, 110]             288\n",
      "             SiLU-31        [-1, 144, 110, 110]               0\n",
      "           Conv2d-32          [-1, 144, 55, 55]           1,296\n",
      "      BatchNorm2d-33          [-1, 144, 55, 55]             288\n",
      "             SiLU-34          [-1, 144, 55, 55]               0\n",
      "AdaptiveAvgPool2d-35            [-1, 144, 1, 1]               0\n",
      "           Conv2d-36              [-1, 6, 1, 1]             870\n",
      "             SiLU-37              [-1, 6, 1, 1]               0\n",
      "           Conv2d-38            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-39            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-40          [-1, 144, 55, 55]               0\n",
      "           Conv2d-41           [-1, 32, 55, 55]           4,608\n",
      "      BatchNorm2d-42           [-1, 32, 55, 55]              64\n",
      "           MBConv-43           [-1, 32, 55, 55]               0\n",
      "           Conv2d-44          [-1, 192, 55, 55]           6,144\n",
      "      BatchNorm2d-45          [-1, 192, 55, 55]             384\n",
      "             SiLU-46          [-1, 192, 55, 55]               0\n",
      "           Conv2d-47          [-1, 192, 55, 55]           1,728\n",
      "      BatchNorm2d-48          [-1, 192, 55, 55]             384\n",
      "             SiLU-49          [-1, 192, 55, 55]               0\n",
      "AdaptiveAvgPool2d-50            [-1, 192, 1, 1]               0\n",
      "           Conv2d-51              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-52              [-1, 8, 1, 1]               0\n",
      "           Conv2d-53            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-54            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-55          [-1, 192, 55, 55]               0\n",
      "           Conv2d-56           [-1, 32, 55, 55]           6,144\n",
      "      BatchNorm2d-57           [-1, 32, 55, 55]              64\n",
      "  StochasticDepth-58           [-1, 32, 55, 55]               0\n",
      "           MBConv-59           [-1, 32, 55, 55]               0\n",
      "           Conv2d-60          [-1, 192, 55, 55]           6,144\n",
      "      BatchNorm2d-61          [-1, 192, 55, 55]             384\n",
      "             SiLU-62          [-1, 192, 55, 55]               0\n",
      "           Conv2d-63          [-1, 192, 55, 55]           1,728\n",
      "      BatchNorm2d-64          [-1, 192, 55, 55]             384\n",
      "             SiLU-65          [-1, 192, 55, 55]               0\n",
      "AdaptiveAvgPool2d-66            [-1, 192, 1, 1]               0\n",
      "           Conv2d-67              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-68              [-1, 8, 1, 1]               0\n",
      "           Conv2d-69            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-70            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-71          [-1, 192, 55, 55]               0\n",
      "           Conv2d-72           [-1, 32, 55, 55]           6,144\n",
      "      BatchNorm2d-73           [-1, 32, 55, 55]              64\n",
      "  StochasticDepth-74           [-1, 32, 55, 55]               0\n",
      "           MBConv-75           [-1, 32, 55, 55]               0\n",
      "           Conv2d-76          [-1, 192, 55, 55]           6,144\n",
      "      BatchNorm2d-77          [-1, 192, 55, 55]             384\n",
      "             SiLU-78          [-1, 192, 55, 55]               0\n",
      "           Conv2d-79          [-1, 192, 55, 55]           1,728\n",
      "      BatchNorm2d-80          [-1, 192, 55, 55]             384\n",
      "             SiLU-81          [-1, 192, 55, 55]               0\n",
      "AdaptiveAvgPool2d-82            [-1, 192, 1, 1]               0\n",
      "           Conv2d-83              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-84              [-1, 8, 1, 1]               0\n",
      "           Conv2d-85            [-1, 192, 1, 1]           1,728\n",
      "          Sigmoid-86            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-87          [-1, 192, 55, 55]               0\n",
      "           Conv2d-88           [-1, 32, 55, 55]           6,144\n",
      "      BatchNorm2d-89           [-1, 32, 55, 55]              64\n",
      "  StochasticDepth-90           [-1, 32, 55, 55]               0\n",
      "           MBConv-91           [-1, 32, 55, 55]               0\n",
      "           Conv2d-92          [-1, 192, 55, 55]           6,144\n",
      "      BatchNorm2d-93          [-1, 192, 55, 55]             384\n",
      "             SiLU-94          [-1, 192, 55, 55]               0\n",
      "           Conv2d-95          [-1, 192, 28, 28]           4,800\n",
      "      BatchNorm2d-96          [-1, 192, 28, 28]             384\n",
      "             SiLU-97          [-1, 192, 28, 28]               0\n",
      "AdaptiveAvgPool2d-98            [-1, 192, 1, 1]               0\n",
      "           Conv2d-99              [-1, 8, 1, 1]           1,544\n",
      "            SiLU-100              [-1, 8, 1, 1]               0\n",
      "          Conv2d-101            [-1, 192, 1, 1]           1,728\n",
      "         Sigmoid-102            [-1, 192, 1, 1]               0\n",
      "SqueezeExcitation-103          [-1, 192, 28, 28]               0\n",
      "          Conv2d-104           [-1, 56, 28, 28]          10,752\n",
      "     BatchNorm2d-105           [-1, 56, 28, 28]             112\n",
      "          MBConv-106           [-1, 56, 28, 28]               0\n",
      "          Conv2d-107          [-1, 336, 28, 28]          18,816\n",
      "     BatchNorm2d-108          [-1, 336, 28, 28]             672\n",
      "            SiLU-109          [-1, 336, 28, 28]               0\n",
      "          Conv2d-110          [-1, 336, 28, 28]           8,400\n",
      "     BatchNorm2d-111          [-1, 336, 28, 28]             672\n",
      "            SiLU-112          [-1, 336, 28, 28]               0\n",
      "AdaptiveAvgPool2d-113            [-1, 336, 1, 1]               0\n",
      "          Conv2d-114             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-115             [-1, 14, 1, 1]               0\n",
      "          Conv2d-116            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-117            [-1, 336, 1, 1]               0\n",
      "SqueezeExcitation-118          [-1, 336, 28, 28]               0\n",
      "          Conv2d-119           [-1, 56, 28, 28]          18,816\n",
      "     BatchNorm2d-120           [-1, 56, 28, 28]             112\n",
      " StochasticDepth-121           [-1, 56, 28, 28]               0\n",
      "          MBConv-122           [-1, 56, 28, 28]               0\n",
      "          Conv2d-123          [-1, 336, 28, 28]          18,816\n",
      "     BatchNorm2d-124          [-1, 336, 28, 28]             672\n",
      "            SiLU-125          [-1, 336, 28, 28]               0\n",
      "          Conv2d-126          [-1, 336, 28, 28]           8,400\n",
      "     BatchNorm2d-127          [-1, 336, 28, 28]             672\n",
      "            SiLU-128          [-1, 336, 28, 28]               0\n",
      "AdaptiveAvgPool2d-129            [-1, 336, 1, 1]               0\n",
      "          Conv2d-130             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-131             [-1, 14, 1, 1]               0\n",
      "          Conv2d-132            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-133            [-1, 336, 1, 1]               0\n",
      "SqueezeExcitation-134          [-1, 336, 28, 28]               0\n",
      "          Conv2d-135           [-1, 56, 28, 28]          18,816\n",
      "     BatchNorm2d-136           [-1, 56, 28, 28]             112\n",
      " StochasticDepth-137           [-1, 56, 28, 28]               0\n",
      "          MBConv-138           [-1, 56, 28, 28]               0\n",
      "          Conv2d-139          [-1, 336, 28, 28]          18,816\n",
      "     BatchNorm2d-140          [-1, 336, 28, 28]             672\n",
      "            SiLU-141          [-1, 336, 28, 28]               0\n",
      "          Conv2d-142          [-1, 336, 28, 28]           8,400\n",
      "     BatchNorm2d-143          [-1, 336, 28, 28]             672\n",
      "            SiLU-144          [-1, 336, 28, 28]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 336, 1, 1]               0\n",
      "          Conv2d-146             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-147             [-1, 14, 1, 1]               0\n",
      "          Conv2d-148            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-149            [-1, 336, 1, 1]               0\n",
      "SqueezeExcitation-150          [-1, 336, 28, 28]               0\n",
      "          Conv2d-151           [-1, 56, 28, 28]          18,816\n",
      "     BatchNorm2d-152           [-1, 56, 28, 28]             112\n",
      " StochasticDepth-153           [-1, 56, 28, 28]               0\n",
      "          MBConv-154           [-1, 56, 28, 28]               0\n",
      "          Conv2d-155          [-1, 336, 28, 28]          18,816\n",
      "     BatchNorm2d-156          [-1, 336, 28, 28]             672\n",
      "            SiLU-157          [-1, 336, 28, 28]               0\n",
      "          Conv2d-158          [-1, 336, 14, 14]           3,024\n",
      "     BatchNorm2d-159          [-1, 336, 14, 14]             672\n",
      "            SiLU-160          [-1, 336, 14, 14]               0\n",
      "AdaptiveAvgPool2d-161            [-1, 336, 1, 1]               0\n",
      "          Conv2d-162             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-163             [-1, 14, 1, 1]               0\n",
      "          Conv2d-164            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-165            [-1, 336, 1, 1]               0\n",
      "SqueezeExcitation-166          [-1, 336, 14, 14]               0\n",
      "          Conv2d-167          [-1, 112, 14, 14]          37,632\n",
      "     BatchNorm2d-168          [-1, 112, 14, 14]             224\n",
      "          MBConv-169          [-1, 112, 14, 14]               0\n",
      "          Conv2d-170          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-171          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-172          [-1, 672, 14, 14]               0\n",
      "          Conv2d-173          [-1, 672, 14, 14]           6,048\n",
      "     BatchNorm2d-174          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-175          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-176            [-1, 672, 1, 1]               0\n",
      "          Conv2d-177             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-178             [-1, 28, 1, 1]               0\n",
      "          Conv2d-179            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-180            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-181          [-1, 672, 14, 14]               0\n",
      "          Conv2d-182          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-183          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-184          [-1, 112, 14, 14]               0\n",
      "          MBConv-185          [-1, 112, 14, 14]               0\n",
      "          Conv2d-186          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-187          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-188          [-1, 672, 14, 14]               0\n",
      "          Conv2d-189          [-1, 672, 14, 14]           6,048\n",
      "     BatchNorm2d-190          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-191          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-192            [-1, 672, 1, 1]               0\n",
      "          Conv2d-193             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-194             [-1, 28, 1, 1]               0\n",
      "          Conv2d-195            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-196            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-197          [-1, 672, 14, 14]               0\n",
      "          Conv2d-198          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-199          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-200          [-1, 112, 14, 14]               0\n",
      "          MBConv-201          [-1, 112, 14, 14]               0\n",
      "          Conv2d-202          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-203          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-204          [-1, 672, 14, 14]               0\n",
      "          Conv2d-205          [-1, 672, 14, 14]           6,048\n",
      "     BatchNorm2d-206          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-207          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-208            [-1, 672, 1, 1]               0\n",
      "          Conv2d-209             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-210             [-1, 28, 1, 1]               0\n",
      "          Conv2d-211            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-212            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-213          [-1, 672, 14, 14]               0\n",
      "          Conv2d-214          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-215          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-216          [-1, 112, 14, 14]               0\n",
      "          MBConv-217          [-1, 112, 14, 14]               0\n",
      "          Conv2d-218          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-219          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-220          [-1, 672, 14, 14]               0\n",
      "          Conv2d-221          [-1, 672, 14, 14]           6,048\n",
      "     BatchNorm2d-222          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-223          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-224            [-1, 672, 1, 1]               0\n",
      "          Conv2d-225             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-226             [-1, 28, 1, 1]               0\n",
      "          Conv2d-227            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-228            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-229          [-1, 672, 14, 14]               0\n",
      "          Conv2d-230          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-231          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-232          [-1, 112, 14, 14]               0\n",
      "          MBConv-233          [-1, 112, 14, 14]               0\n",
      "          Conv2d-234          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-235          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-236          [-1, 672, 14, 14]               0\n",
      "          Conv2d-237          [-1, 672, 14, 14]           6,048\n",
      "     BatchNorm2d-238          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-239          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-240            [-1, 672, 1, 1]               0\n",
      "          Conv2d-241             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-242             [-1, 28, 1, 1]               0\n",
      "          Conv2d-243            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-244            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-245          [-1, 672, 14, 14]               0\n",
      "          Conv2d-246          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-247          [-1, 112, 14, 14]             224\n",
      " StochasticDepth-248          [-1, 112, 14, 14]               0\n",
      "          MBConv-249          [-1, 112, 14, 14]               0\n",
      "          Conv2d-250          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-251          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-252          [-1, 672, 14, 14]               0\n",
      "          Conv2d-253          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-254          [-1, 672, 14, 14]           1,344\n",
      "            SiLU-255          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-256            [-1, 672, 1, 1]               0\n",
      "          Conv2d-257             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-258             [-1, 28, 1, 1]               0\n",
      "          Conv2d-259            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-260            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-261          [-1, 672, 14, 14]               0\n",
      "          Conv2d-262          [-1, 160, 14, 14]         107,520\n",
      "     BatchNorm2d-263          [-1, 160, 14, 14]             320\n",
      "          MBConv-264          [-1, 160, 14, 14]               0\n",
      "          Conv2d-265          [-1, 960, 14, 14]         153,600\n",
      "     BatchNorm2d-266          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-267          [-1, 960, 14, 14]               0\n",
      "          Conv2d-268          [-1, 960, 14, 14]          24,000\n",
      "     BatchNorm2d-269          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-270          [-1, 960, 14, 14]               0\n",
      "AdaptiveAvgPool2d-271            [-1, 960, 1, 1]               0\n",
      "          Conv2d-272             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-273             [-1, 40, 1, 1]               0\n",
      "          Conv2d-274            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-275            [-1, 960, 1, 1]               0\n",
      "SqueezeExcitation-276          [-1, 960, 14, 14]               0\n",
      "          Conv2d-277          [-1, 160, 14, 14]         153,600\n",
      "     BatchNorm2d-278          [-1, 160, 14, 14]             320\n",
      " StochasticDepth-279          [-1, 160, 14, 14]               0\n",
      "          MBConv-280          [-1, 160, 14, 14]               0\n",
      "          Conv2d-281          [-1, 960, 14, 14]         153,600\n",
      "     BatchNorm2d-282          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-283          [-1, 960, 14, 14]               0\n",
      "          Conv2d-284          [-1, 960, 14, 14]          24,000\n",
      "     BatchNorm2d-285          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-286          [-1, 960, 14, 14]               0\n",
      "AdaptiveAvgPool2d-287            [-1, 960, 1, 1]               0\n",
      "          Conv2d-288             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-289             [-1, 40, 1, 1]               0\n",
      "          Conv2d-290            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-291            [-1, 960, 1, 1]               0\n",
      "SqueezeExcitation-292          [-1, 960, 14, 14]               0\n",
      "          Conv2d-293          [-1, 160, 14, 14]         153,600\n",
      "     BatchNorm2d-294          [-1, 160, 14, 14]             320\n",
      " StochasticDepth-295          [-1, 160, 14, 14]               0\n",
      "          MBConv-296          [-1, 160, 14, 14]               0\n",
      "          Conv2d-297          [-1, 960, 14, 14]         153,600\n",
      "     BatchNorm2d-298          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-299          [-1, 960, 14, 14]               0\n",
      "          Conv2d-300          [-1, 960, 14, 14]          24,000\n",
      "     BatchNorm2d-301          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-302          [-1, 960, 14, 14]               0\n",
      "AdaptiveAvgPool2d-303            [-1, 960, 1, 1]               0\n",
      "          Conv2d-304             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-305             [-1, 40, 1, 1]               0\n",
      "          Conv2d-306            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-307            [-1, 960, 1, 1]               0\n",
      "SqueezeExcitation-308          [-1, 960, 14, 14]               0\n",
      "          Conv2d-309          [-1, 160, 14, 14]         153,600\n",
      "     BatchNorm2d-310          [-1, 160, 14, 14]             320\n",
      " StochasticDepth-311          [-1, 160, 14, 14]               0\n",
      "          MBConv-312          [-1, 160, 14, 14]               0\n",
      "          Conv2d-313          [-1, 960, 14, 14]         153,600\n",
      "     BatchNorm2d-314          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-315          [-1, 960, 14, 14]               0\n",
      "          Conv2d-316          [-1, 960, 14, 14]          24,000\n",
      "     BatchNorm2d-317          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-318          [-1, 960, 14, 14]               0\n",
      "AdaptiveAvgPool2d-319            [-1, 960, 1, 1]               0\n",
      "          Conv2d-320             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-321             [-1, 40, 1, 1]               0\n",
      "          Conv2d-322            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-323            [-1, 960, 1, 1]               0\n",
      "SqueezeExcitation-324          [-1, 960, 14, 14]               0\n",
      "          Conv2d-325          [-1, 160, 14, 14]         153,600\n",
      "     BatchNorm2d-326          [-1, 160, 14, 14]             320\n",
      " StochasticDepth-327          [-1, 160, 14, 14]               0\n",
      "          MBConv-328          [-1, 160, 14, 14]               0\n",
      "          Conv2d-329          [-1, 960, 14, 14]         153,600\n",
      "     BatchNorm2d-330          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-331          [-1, 960, 14, 14]               0\n",
      "          Conv2d-332          [-1, 960, 14, 14]          24,000\n",
      "     BatchNorm2d-333          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-334          [-1, 960, 14, 14]               0\n",
      "AdaptiveAvgPool2d-335            [-1, 960, 1, 1]               0\n",
      "          Conv2d-336             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-337             [-1, 40, 1, 1]               0\n",
      "          Conv2d-338            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-339            [-1, 960, 1, 1]               0\n",
      "SqueezeExcitation-340          [-1, 960, 14, 14]               0\n",
      "          Conv2d-341          [-1, 160, 14, 14]         153,600\n",
      "     BatchNorm2d-342          [-1, 160, 14, 14]             320\n",
      " StochasticDepth-343          [-1, 160, 14, 14]               0\n",
      "          MBConv-344          [-1, 160, 14, 14]               0\n",
      "          Conv2d-345          [-1, 960, 14, 14]         153,600\n",
      "     BatchNorm2d-346          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-347          [-1, 960, 14, 14]               0\n",
      "          Conv2d-348            [-1, 960, 7, 7]          24,000\n",
      "     BatchNorm2d-349            [-1, 960, 7, 7]           1,920\n",
      "            SiLU-350            [-1, 960, 7, 7]               0\n",
      "AdaptiveAvgPool2d-351            [-1, 960, 1, 1]               0\n",
      "          Conv2d-352             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-353             [-1, 40, 1, 1]               0\n",
      "          Conv2d-354            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-355            [-1, 960, 1, 1]               0\n",
      "SqueezeExcitation-356            [-1, 960, 7, 7]               0\n",
      "          Conv2d-357            [-1, 272, 7, 7]         261,120\n",
      "     BatchNorm2d-358            [-1, 272, 7, 7]             544\n",
      "          MBConv-359            [-1, 272, 7, 7]               0\n",
      "          Conv2d-360           [-1, 1632, 7, 7]         443,904\n",
      "     BatchNorm2d-361           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-362           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-363           [-1, 1632, 7, 7]          40,800\n",
      "     BatchNorm2d-364           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-365           [-1, 1632, 7, 7]               0\n",
      "AdaptiveAvgPool2d-366           [-1, 1632, 1, 1]               0\n",
      "          Conv2d-367             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-368             [-1, 68, 1, 1]               0\n",
      "          Conv2d-369           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-370           [-1, 1632, 1, 1]               0\n",
      "SqueezeExcitation-371           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-372            [-1, 272, 7, 7]         443,904\n",
      "     BatchNorm2d-373            [-1, 272, 7, 7]             544\n",
      " StochasticDepth-374            [-1, 272, 7, 7]               0\n",
      "          MBConv-375            [-1, 272, 7, 7]               0\n",
      "          Conv2d-376           [-1, 1632, 7, 7]         443,904\n",
      "     BatchNorm2d-377           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-378           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-379           [-1, 1632, 7, 7]          40,800\n",
      "     BatchNorm2d-380           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-381           [-1, 1632, 7, 7]               0\n",
      "AdaptiveAvgPool2d-382           [-1, 1632, 1, 1]               0\n",
      "          Conv2d-383             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-384             [-1, 68, 1, 1]               0\n",
      "          Conv2d-385           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-386           [-1, 1632, 1, 1]               0\n",
      "SqueezeExcitation-387           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-388            [-1, 272, 7, 7]         443,904\n",
      "     BatchNorm2d-389            [-1, 272, 7, 7]             544\n",
      " StochasticDepth-390            [-1, 272, 7, 7]               0\n",
      "          MBConv-391            [-1, 272, 7, 7]               0\n",
      "          Conv2d-392           [-1, 1632, 7, 7]         443,904\n",
      "     BatchNorm2d-393           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-394           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-395           [-1, 1632, 7, 7]          40,800\n",
      "     BatchNorm2d-396           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-397           [-1, 1632, 7, 7]               0\n",
      "AdaptiveAvgPool2d-398           [-1, 1632, 1, 1]               0\n",
      "          Conv2d-399             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-400             [-1, 68, 1, 1]               0\n",
      "          Conv2d-401           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-402           [-1, 1632, 1, 1]               0\n",
      "SqueezeExcitation-403           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-404            [-1, 272, 7, 7]         443,904\n",
      "     BatchNorm2d-405            [-1, 272, 7, 7]             544\n",
      " StochasticDepth-406            [-1, 272, 7, 7]               0\n",
      "          MBConv-407            [-1, 272, 7, 7]               0\n",
      "          Conv2d-408           [-1, 1632, 7, 7]         443,904\n",
      "     BatchNorm2d-409           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-410           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-411           [-1, 1632, 7, 7]          40,800\n",
      "     BatchNorm2d-412           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-413           [-1, 1632, 7, 7]               0\n",
      "AdaptiveAvgPool2d-414           [-1, 1632, 1, 1]               0\n",
      "          Conv2d-415             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-416             [-1, 68, 1, 1]               0\n",
      "          Conv2d-417           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-418           [-1, 1632, 1, 1]               0\n",
      "SqueezeExcitation-419           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-420            [-1, 272, 7, 7]         443,904\n",
      "     BatchNorm2d-421            [-1, 272, 7, 7]             544\n",
      " StochasticDepth-422            [-1, 272, 7, 7]               0\n",
      "          MBConv-423            [-1, 272, 7, 7]               0\n",
      "          Conv2d-424           [-1, 1632, 7, 7]         443,904\n",
      "     BatchNorm2d-425           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-426           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-427           [-1, 1632, 7, 7]          40,800\n",
      "     BatchNorm2d-428           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-429           [-1, 1632, 7, 7]               0\n",
      "AdaptiveAvgPool2d-430           [-1, 1632, 1, 1]               0\n",
      "          Conv2d-431             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-432             [-1, 68, 1, 1]               0\n",
      "          Conv2d-433           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-434           [-1, 1632, 1, 1]               0\n",
      "SqueezeExcitation-435           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-436            [-1, 272, 7, 7]         443,904\n",
      "     BatchNorm2d-437            [-1, 272, 7, 7]             544\n",
      " StochasticDepth-438            [-1, 272, 7, 7]               0\n",
      "          MBConv-439            [-1, 272, 7, 7]               0\n",
      "          Conv2d-440           [-1, 1632, 7, 7]         443,904\n",
      "     BatchNorm2d-441           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-442           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-443           [-1, 1632, 7, 7]          40,800\n",
      "     BatchNorm2d-444           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-445           [-1, 1632, 7, 7]               0\n",
      "AdaptiveAvgPool2d-446           [-1, 1632, 1, 1]               0\n",
      "          Conv2d-447             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-448             [-1, 68, 1, 1]               0\n",
      "          Conv2d-449           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-450           [-1, 1632, 1, 1]               0\n",
      "SqueezeExcitation-451           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-452            [-1, 272, 7, 7]         443,904\n",
      "     BatchNorm2d-453            [-1, 272, 7, 7]             544\n",
      " StochasticDepth-454            [-1, 272, 7, 7]               0\n",
      "          MBConv-455            [-1, 272, 7, 7]               0\n",
      "          Conv2d-456           [-1, 1632, 7, 7]         443,904\n",
      "     BatchNorm2d-457           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-458           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-459           [-1, 1632, 7, 7]          40,800\n",
      "     BatchNorm2d-460           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-461           [-1, 1632, 7, 7]               0\n",
      "AdaptiveAvgPool2d-462           [-1, 1632, 1, 1]               0\n",
      "          Conv2d-463             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-464             [-1, 68, 1, 1]               0\n",
      "          Conv2d-465           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-466           [-1, 1632, 1, 1]               0\n",
      "SqueezeExcitation-467           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-468            [-1, 272, 7, 7]         443,904\n",
      "     BatchNorm2d-469            [-1, 272, 7, 7]             544\n",
      " StochasticDepth-470            [-1, 272, 7, 7]               0\n",
      "          MBConv-471            [-1, 272, 7, 7]               0\n",
      "          Conv2d-472           [-1, 1632, 7, 7]         443,904\n",
      "     BatchNorm2d-473           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-474           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-475           [-1, 1632, 7, 7]          14,688\n",
      "     BatchNorm2d-476           [-1, 1632, 7, 7]           3,264\n",
      "            SiLU-477           [-1, 1632, 7, 7]               0\n",
      "AdaptiveAvgPool2d-478           [-1, 1632, 1, 1]               0\n",
      "          Conv2d-479             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-480             [-1, 68, 1, 1]               0\n",
      "          Conv2d-481           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-482           [-1, 1632, 1, 1]               0\n",
      "SqueezeExcitation-483           [-1, 1632, 7, 7]               0\n",
      "          Conv2d-484            [-1, 448, 7, 7]         731,136\n",
      "     BatchNorm2d-485            [-1, 448, 7, 7]             896\n",
      "          MBConv-486            [-1, 448, 7, 7]               0\n",
      "          Conv2d-487           [-1, 2688, 7, 7]       1,204,224\n",
      "     BatchNorm2d-488           [-1, 2688, 7, 7]           5,376\n",
      "            SiLU-489           [-1, 2688, 7, 7]               0\n",
      "          Conv2d-490           [-1, 2688, 7, 7]          24,192\n",
      "     BatchNorm2d-491           [-1, 2688, 7, 7]           5,376\n",
      "            SiLU-492           [-1, 2688, 7, 7]               0\n",
      "AdaptiveAvgPool2d-493           [-1, 2688, 1, 1]               0\n",
      "          Conv2d-494            [-1, 112, 1, 1]         301,168\n",
      "            SiLU-495            [-1, 112, 1, 1]               0\n",
      "          Conv2d-496           [-1, 2688, 1, 1]         303,744\n",
      "         Sigmoid-497           [-1, 2688, 1, 1]               0\n",
      "SqueezeExcitation-498           [-1, 2688, 7, 7]               0\n",
      "          Conv2d-499            [-1, 448, 7, 7]       1,204,224\n",
      "     BatchNorm2d-500            [-1, 448, 7, 7]             896\n",
      " StochasticDepth-501            [-1, 448, 7, 7]               0\n",
      "          MBConv-502            [-1, 448, 7, 7]               0\n",
      "          Conv2d-503           [-1, 1792, 7, 7]         802,816\n",
      "     BatchNorm2d-504           [-1, 1792, 7, 7]           3,584\n",
      "            SiLU-505           [-1, 1792, 7, 7]               0\n",
      "AdaptiveAvgPool2d-506           [-1, 1792, 1, 1]               0\n",
      "         Flatten-507                 [-1, 1792]               0\n",
      "          Linear-508                 [-1, 4096]       7,344,128\n",
      "            ReLU-509                 [-1, 4096]               0\n",
      "         Dropout-510                 [-1, 4096]               0\n",
      "          Linear-511                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 24,958,296\n",
      "Trainable params: 24,958,296\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.55\n",
      "Forward/backward pass size (MB): 441.71\n",
      "Params size (MB): 95.21\n",
      "Estimated Total Size (MB): 537.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# EfficientNet-B4 Model Summary\n",
    "model = EfficientNetB4EncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MobileNetV2EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV2EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(1280 * 7 * 7, 512),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(512, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MobileNetV2EncoderPoseModel Model Summary\n",
    "# model = MobileNetV2EncoderPoseModel()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MobileNetV2EncoderPoseModel_HalfFC(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV2EncoderPoseModel_HalfFC, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(1280 * 7 * 7, 2048),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(2048, 2048),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.6),\n",
    "\n",
    "#             nn.Linear(2048, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MobileNetV2EncoderPoseModel (encoder test 11) - train from scratch \n",
    "# class MobileNetV2EncoderPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         \"\"\"\n",
    "#         Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "#         Parameters:\n",
    "#         - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "#         \"\"\"\n",
    "#         super(MobileNetV2EncoderPoseModel, self).__init__()\n",
    "\n",
    "#         # Load a pretrained MobileNetV2 model\n",
    "#         mobilenet = models.mobilenet_v2() # removed weights=models.MobileNet_V2_Weights.IMAGENET1K_V1 as default is from scratch\n",
    "\n",
    "#         # Remove the classification head (classifier block)\n",
    "#         # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "#         self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "#         # GAP\n",
    "#         self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#         # Flatten the output of the encoder\n",
    "#         self.flatten = nn.Flatten()\n",
    "\n",
    "#         # Decoder: fully connected layers\n",
    "#         # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "#         self.decoder = nn.Sequential(\n",
    "#         #     nn.Linear(1280 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(0.3),\n",
    "\n",
    "#         #     nn.Linear(64, 64),\n",
    "#         #     nn.ReLU(inplace=True),\n",
    "#         #     nn.Dropout(0.3),\n",
    "\n",
    "#         #     nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "\n",
    "#             nn.Linear(1280, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             #nn.Dropout(0.3),\n",
    "#             # Using the original output\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass of the model.\n",
    "\n",
    "#         Parameters:\n",
    "#         - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "#         Returns:\n",
    "#         - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "#         \"\"\"\n",
    "#         x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "#         x = self.GAP(x)\n",
    "#         x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "#         x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1. Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3.1.1. Learning rate adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - MobileNetV2 (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class MobileNetV2EncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained MobileNetV2 encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(MobileNetV2EncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained MobileNetV2 model\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT) # removed weights=models.MobileNet_V2_Weights.IMAGENET1K_V2 as default is from scratch\n",
    "\n",
    "        # Remove the classification head (classifier block)\n",
    "        # MobileNetV2's feature extractor ends with Conv2d outputting 1280 channels\n",
    "        self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "        #     nn.Linear(1280 * 7 * 7, 64),  # 1280 channels * 7 * 7 spatial dimensions\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(0.3),\n",
    "\n",
    "        #     nn.Linear(64, 64),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(0.3),\n",
    "\n",
    "        #     nn.Linear(64, nkeypoints * 2)  # Output: nkeypoints * 2 (x, y for each keypoint)\n",
    "\n",
    "\n",
    "            nn.Linear(1280, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /home/matthew/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n",
      "100%|██████████| 13.6M/13.6M [00:12<00:00, 1.16MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 110, 110]             864\n",
      "       BatchNorm2d-2         [-1, 32, 110, 110]              64\n",
      "             ReLU6-3         [-1, 32, 110, 110]               0\n",
      "            Conv2d-4         [-1, 32, 110, 110]             288\n",
      "       BatchNorm2d-5         [-1, 32, 110, 110]              64\n",
      "             ReLU6-6         [-1, 32, 110, 110]               0\n",
      "            Conv2d-7         [-1, 16, 110, 110]             512\n",
      "       BatchNorm2d-8         [-1, 16, 110, 110]              32\n",
      "  InvertedResidual-9         [-1, 16, 110, 110]               0\n",
      "           Conv2d-10         [-1, 96, 110, 110]           1,536\n",
      "      BatchNorm2d-11         [-1, 96, 110, 110]             192\n",
      "            ReLU6-12         [-1, 96, 110, 110]               0\n",
      "           Conv2d-13           [-1, 96, 55, 55]             864\n",
      "      BatchNorm2d-14           [-1, 96, 55, 55]             192\n",
      "            ReLU6-15           [-1, 96, 55, 55]               0\n",
      "           Conv2d-16           [-1, 24, 55, 55]           2,304\n",
      "      BatchNorm2d-17           [-1, 24, 55, 55]              48\n",
      " InvertedResidual-18           [-1, 24, 55, 55]               0\n",
      "           Conv2d-19          [-1, 144, 55, 55]           3,456\n",
      "      BatchNorm2d-20          [-1, 144, 55, 55]             288\n",
      "            ReLU6-21          [-1, 144, 55, 55]               0\n",
      "           Conv2d-22          [-1, 144, 55, 55]           1,296\n",
      "      BatchNorm2d-23          [-1, 144, 55, 55]             288\n",
      "            ReLU6-24          [-1, 144, 55, 55]               0\n",
      "           Conv2d-25           [-1, 24, 55, 55]           3,456\n",
      "      BatchNorm2d-26           [-1, 24, 55, 55]              48\n",
      " InvertedResidual-27           [-1, 24, 55, 55]               0\n",
      "           Conv2d-28          [-1, 144, 55, 55]           3,456\n",
      "      BatchNorm2d-29          [-1, 144, 55, 55]             288\n",
      "            ReLU6-30          [-1, 144, 55, 55]               0\n",
      "           Conv2d-31          [-1, 144, 28, 28]           1,296\n",
      "      BatchNorm2d-32          [-1, 144, 28, 28]             288\n",
      "            ReLU6-33          [-1, 144, 28, 28]               0\n",
      "           Conv2d-34           [-1, 32, 28, 28]           4,608\n",
      "      BatchNorm2d-35           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-36           [-1, 32, 28, 28]               0\n",
      "           Conv2d-37          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-38          [-1, 192, 28, 28]             384\n",
      "            ReLU6-39          [-1, 192, 28, 28]               0\n",
      "           Conv2d-40          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-41          [-1, 192, 28, 28]             384\n",
      "            ReLU6-42          [-1, 192, 28, 28]               0\n",
      "           Conv2d-43           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-44           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-45           [-1, 32, 28, 28]               0\n",
      "           Conv2d-46          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-47          [-1, 192, 28, 28]             384\n",
      "            ReLU6-48          [-1, 192, 28, 28]               0\n",
      "           Conv2d-49          [-1, 192, 28, 28]           1,728\n",
      "      BatchNorm2d-50          [-1, 192, 28, 28]             384\n",
      "            ReLU6-51          [-1, 192, 28, 28]               0\n",
      "           Conv2d-52           [-1, 32, 28, 28]           6,144\n",
      "      BatchNorm2d-53           [-1, 32, 28, 28]              64\n",
      " InvertedResidual-54           [-1, 32, 28, 28]               0\n",
      "           Conv2d-55          [-1, 192, 28, 28]           6,144\n",
      "      BatchNorm2d-56          [-1, 192, 28, 28]             384\n",
      "            ReLU6-57          [-1, 192, 28, 28]               0\n",
      "           Conv2d-58          [-1, 192, 14, 14]           1,728\n",
      "      BatchNorm2d-59          [-1, 192, 14, 14]             384\n",
      "            ReLU6-60          [-1, 192, 14, 14]               0\n",
      "           Conv2d-61           [-1, 64, 14, 14]          12,288\n",
      "      BatchNorm2d-62           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-63           [-1, 64, 14, 14]               0\n",
      "           Conv2d-64          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-65          [-1, 384, 14, 14]             768\n",
      "            ReLU6-66          [-1, 384, 14, 14]               0\n",
      "           Conv2d-67          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-68          [-1, 384, 14, 14]             768\n",
      "            ReLU6-69          [-1, 384, 14, 14]               0\n",
      "           Conv2d-70           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-71           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-72           [-1, 64, 14, 14]               0\n",
      "           Conv2d-73          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-74          [-1, 384, 14, 14]             768\n",
      "            ReLU6-75          [-1, 384, 14, 14]               0\n",
      "           Conv2d-76          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-77          [-1, 384, 14, 14]             768\n",
      "            ReLU6-78          [-1, 384, 14, 14]               0\n",
      "           Conv2d-79           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-80           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-81           [-1, 64, 14, 14]               0\n",
      "           Conv2d-82          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-83          [-1, 384, 14, 14]             768\n",
      "            ReLU6-84          [-1, 384, 14, 14]               0\n",
      "           Conv2d-85          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-86          [-1, 384, 14, 14]             768\n",
      "            ReLU6-87          [-1, 384, 14, 14]               0\n",
      "           Conv2d-88           [-1, 64, 14, 14]          24,576\n",
      "      BatchNorm2d-89           [-1, 64, 14, 14]             128\n",
      " InvertedResidual-90           [-1, 64, 14, 14]               0\n",
      "           Conv2d-91          [-1, 384, 14, 14]          24,576\n",
      "      BatchNorm2d-92          [-1, 384, 14, 14]             768\n",
      "            ReLU6-93          [-1, 384, 14, 14]               0\n",
      "           Conv2d-94          [-1, 384, 14, 14]           3,456\n",
      "      BatchNorm2d-95          [-1, 384, 14, 14]             768\n",
      "            ReLU6-96          [-1, 384, 14, 14]               0\n",
      "           Conv2d-97           [-1, 96, 14, 14]          36,864\n",
      "      BatchNorm2d-98           [-1, 96, 14, 14]             192\n",
      " InvertedResidual-99           [-1, 96, 14, 14]               0\n",
      "          Conv2d-100          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-102          [-1, 576, 14, 14]               0\n",
      "          Conv2d-103          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-105          [-1, 576, 14, 14]               0\n",
      "          Conv2d-106           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-107           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-108           [-1, 96, 14, 14]               0\n",
      "          Conv2d-109          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-111          [-1, 576, 14, 14]               0\n",
      "          Conv2d-112          [-1, 576, 14, 14]           5,184\n",
      "     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-114          [-1, 576, 14, 14]               0\n",
      "          Conv2d-115           [-1, 96, 14, 14]          55,296\n",
      "     BatchNorm2d-116           [-1, 96, 14, 14]             192\n",
      "InvertedResidual-117           [-1, 96, 14, 14]               0\n",
      "          Conv2d-118          [-1, 576, 14, 14]          55,296\n",
      "     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n",
      "           ReLU6-120          [-1, 576, 14, 14]               0\n",
      "          Conv2d-121            [-1, 576, 7, 7]           5,184\n",
      "     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n",
      "           ReLU6-123            [-1, 576, 7, 7]               0\n",
      "          Conv2d-124            [-1, 160, 7, 7]          92,160\n",
      "     BatchNorm2d-125            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-126            [-1, 160, 7, 7]               0\n",
      "          Conv2d-127            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-129            [-1, 960, 7, 7]               0\n",
      "          Conv2d-130            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-132            [-1, 960, 7, 7]               0\n",
      "          Conv2d-133            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-134            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-135            [-1, 160, 7, 7]               0\n",
      "          Conv2d-136            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-138            [-1, 960, 7, 7]               0\n",
      "          Conv2d-139            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-141            [-1, 960, 7, 7]               0\n",
      "          Conv2d-142            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-143            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-144            [-1, 160, 7, 7]               0\n",
      "          Conv2d-145            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-147            [-1, 960, 7, 7]               0\n",
      "          Conv2d-148            [-1, 960, 7, 7]           8,640\n",
      "     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n",
      "           ReLU6-150            [-1, 960, 7, 7]               0\n",
      "          Conv2d-151            [-1, 320, 7, 7]         307,200\n",
      "     BatchNorm2d-152            [-1, 320, 7, 7]             640\n",
      "InvertedResidual-153            [-1, 320, 7, 7]               0\n",
      "          Conv2d-154           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n",
      "           ReLU6-156           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157           [-1, 1280, 1, 1]               0\n",
      "         Flatten-158                 [-1, 1280]               0\n",
      "          Linear-159                 [-1, 4096]       5,246,976\n",
      "            ReLU-160                 [-1, 4096]               0\n",
      "         Dropout-161                 [-1, 4096]               0\n",
      "          Linear-162                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 7,536,400\n",
      "Trainable params: 7,536,400\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.55\n",
      "Forward/backward pass size (MB): 149.71\n",
      "Params size (MB): 28.75\n",
      "Estimated Total Size (MB): 179.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# MobileNetV2EncoderPoseModel Model Summary\n",
    "model = MobileNetV2EncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2. MobileNetV3Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2. Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3.2.1. Learning rate adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - MobileNetV3Large (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class MobileNetV3LargeEncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained MobileNetV3-Large encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(MobileNetV3LargeEncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained MobileNetV3-Large model\n",
    "        mobilenet = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
    "\n",
    "        # Remove the classification head (classifier block)\n",
    "        # MobileNetV3-Large's feature extractor ends with Conv2d outputting 1280 channels\n",
    "        self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(960, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 110, 110]             432\n",
      "       BatchNorm2d-2         [-1, 16, 110, 110]              32\n",
      "         Hardswish-3         [-1, 16, 110, 110]               0\n",
      "            Conv2d-4         [-1, 16, 110, 110]             144\n",
      "       BatchNorm2d-5         [-1, 16, 110, 110]              32\n",
      "              ReLU-6         [-1, 16, 110, 110]               0\n",
      "            Conv2d-7         [-1, 16, 110, 110]             256\n",
      "       BatchNorm2d-8         [-1, 16, 110, 110]              32\n",
      "  InvertedResidual-9         [-1, 16, 110, 110]               0\n",
      "           Conv2d-10         [-1, 64, 110, 110]           1,024\n",
      "      BatchNorm2d-11         [-1, 64, 110, 110]             128\n",
      "             ReLU-12         [-1, 64, 110, 110]               0\n",
      "           Conv2d-13           [-1, 64, 55, 55]             576\n",
      "      BatchNorm2d-14           [-1, 64, 55, 55]             128\n",
      "             ReLU-15           [-1, 64, 55, 55]               0\n",
      "           Conv2d-16           [-1, 24, 55, 55]           1,536\n",
      "      BatchNorm2d-17           [-1, 24, 55, 55]              48\n",
      " InvertedResidual-18           [-1, 24, 55, 55]               0\n",
      "           Conv2d-19           [-1, 72, 55, 55]           1,728\n",
      "      BatchNorm2d-20           [-1, 72, 55, 55]             144\n",
      "             ReLU-21           [-1, 72, 55, 55]               0\n",
      "           Conv2d-22           [-1, 72, 55, 55]             648\n",
      "      BatchNorm2d-23           [-1, 72, 55, 55]             144\n",
      "             ReLU-24           [-1, 72, 55, 55]               0\n",
      "           Conv2d-25           [-1, 24, 55, 55]           1,728\n",
      "      BatchNorm2d-26           [-1, 24, 55, 55]              48\n",
      " InvertedResidual-27           [-1, 24, 55, 55]               0\n",
      "           Conv2d-28           [-1, 72, 55, 55]           1,728\n",
      "      BatchNorm2d-29           [-1, 72, 55, 55]             144\n",
      "             ReLU-30           [-1, 72, 55, 55]               0\n",
      "           Conv2d-31           [-1, 72, 28, 28]           1,800\n",
      "      BatchNorm2d-32           [-1, 72, 28, 28]             144\n",
      "             ReLU-33           [-1, 72, 28, 28]               0\n",
      "AdaptiveAvgPool2d-34             [-1, 72, 1, 1]               0\n",
      "           Conv2d-35             [-1, 24, 1, 1]           1,752\n",
      "             ReLU-36             [-1, 24, 1, 1]               0\n",
      "           Conv2d-37             [-1, 72, 1, 1]           1,800\n",
      "      Hardsigmoid-38             [-1, 72, 1, 1]               0\n",
      "SqueezeExcitation-39           [-1, 72, 28, 28]               0\n",
      "           Conv2d-40           [-1, 40, 28, 28]           2,880\n",
      "      BatchNorm2d-41           [-1, 40, 28, 28]              80\n",
      " InvertedResidual-42           [-1, 40, 28, 28]               0\n",
      "           Conv2d-43          [-1, 120, 28, 28]           4,800\n",
      "      BatchNorm2d-44          [-1, 120, 28, 28]             240\n",
      "             ReLU-45          [-1, 120, 28, 28]               0\n",
      "           Conv2d-46          [-1, 120, 28, 28]           3,000\n",
      "      BatchNorm2d-47          [-1, 120, 28, 28]             240\n",
      "             ReLU-48          [-1, 120, 28, 28]               0\n",
      "AdaptiveAvgPool2d-49            [-1, 120, 1, 1]               0\n",
      "           Conv2d-50             [-1, 32, 1, 1]           3,872\n",
      "             ReLU-51             [-1, 32, 1, 1]               0\n",
      "           Conv2d-52            [-1, 120, 1, 1]           3,960\n",
      "      Hardsigmoid-53            [-1, 120, 1, 1]               0\n",
      "SqueezeExcitation-54          [-1, 120, 28, 28]               0\n",
      "           Conv2d-55           [-1, 40, 28, 28]           4,800\n",
      "      BatchNorm2d-56           [-1, 40, 28, 28]              80\n",
      " InvertedResidual-57           [-1, 40, 28, 28]               0\n",
      "           Conv2d-58          [-1, 120, 28, 28]           4,800\n",
      "      BatchNorm2d-59          [-1, 120, 28, 28]             240\n",
      "             ReLU-60          [-1, 120, 28, 28]               0\n",
      "           Conv2d-61          [-1, 120, 28, 28]           3,000\n",
      "      BatchNorm2d-62          [-1, 120, 28, 28]             240\n",
      "             ReLU-63          [-1, 120, 28, 28]               0\n",
      "AdaptiveAvgPool2d-64            [-1, 120, 1, 1]               0\n",
      "           Conv2d-65             [-1, 32, 1, 1]           3,872\n",
      "             ReLU-66             [-1, 32, 1, 1]               0\n",
      "           Conv2d-67            [-1, 120, 1, 1]           3,960\n",
      "      Hardsigmoid-68            [-1, 120, 1, 1]               0\n",
      "SqueezeExcitation-69          [-1, 120, 28, 28]               0\n",
      "           Conv2d-70           [-1, 40, 28, 28]           4,800\n",
      "      BatchNorm2d-71           [-1, 40, 28, 28]              80\n",
      " InvertedResidual-72           [-1, 40, 28, 28]               0\n",
      "           Conv2d-73          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-74          [-1, 240, 28, 28]             480\n",
      "        Hardswish-75          [-1, 240, 28, 28]               0\n",
      "           Conv2d-76          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-77          [-1, 240, 14, 14]             480\n",
      "        Hardswish-78          [-1, 240, 14, 14]               0\n",
      "           Conv2d-79           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-80           [-1, 80, 14, 14]             160\n",
      " InvertedResidual-81           [-1, 80, 14, 14]               0\n",
      "           Conv2d-82          [-1, 200, 14, 14]          16,000\n",
      "      BatchNorm2d-83          [-1, 200, 14, 14]             400\n",
      "        Hardswish-84          [-1, 200, 14, 14]               0\n",
      "           Conv2d-85          [-1, 200, 14, 14]           1,800\n",
      "      BatchNorm2d-86          [-1, 200, 14, 14]             400\n",
      "        Hardswish-87          [-1, 200, 14, 14]               0\n",
      "           Conv2d-88           [-1, 80, 14, 14]          16,000\n",
      "      BatchNorm2d-89           [-1, 80, 14, 14]             160\n",
      " InvertedResidual-90           [-1, 80, 14, 14]               0\n",
      "           Conv2d-91          [-1, 184, 14, 14]          14,720\n",
      "      BatchNorm2d-92          [-1, 184, 14, 14]             368\n",
      "        Hardswish-93          [-1, 184, 14, 14]               0\n",
      "           Conv2d-94          [-1, 184, 14, 14]           1,656\n",
      "      BatchNorm2d-95          [-1, 184, 14, 14]             368\n",
      "        Hardswish-96          [-1, 184, 14, 14]               0\n",
      "           Conv2d-97           [-1, 80, 14, 14]          14,720\n",
      "      BatchNorm2d-98           [-1, 80, 14, 14]             160\n",
      " InvertedResidual-99           [-1, 80, 14, 14]               0\n",
      "          Conv2d-100          [-1, 184, 14, 14]          14,720\n",
      "     BatchNorm2d-101          [-1, 184, 14, 14]             368\n",
      "       Hardswish-102          [-1, 184, 14, 14]               0\n",
      "          Conv2d-103          [-1, 184, 14, 14]           1,656\n",
      "     BatchNorm2d-104          [-1, 184, 14, 14]             368\n",
      "       Hardswish-105          [-1, 184, 14, 14]               0\n",
      "          Conv2d-106           [-1, 80, 14, 14]          14,720\n",
      "     BatchNorm2d-107           [-1, 80, 14, 14]             160\n",
      "InvertedResidual-108           [-1, 80, 14, 14]               0\n",
      "          Conv2d-109          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-110          [-1, 480, 14, 14]             960\n",
      "       Hardswish-111          [-1, 480, 14, 14]               0\n",
      "          Conv2d-112          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-113          [-1, 480, 14, 14]             960\n",
      "       Hardswish-114          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 480, 1, 1]               0\n",
      "          Conv2d-116            [-1, 120, 1, 1]          57,720\n",
      "            ReLU-117            [-1, 120, 1, 1]               0\n",
      "          Conv2d-118            [-1, 480, 1, 1]          58,080\n",
      "     Hardsigmoid-119            [-1, 480, 1, 1]               0\n",
      "SqueezeExcitation-120          [-1, 480, 14, 14]               0\n",
      "          Conv2d-121          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-122          [-1, 112, 14, 14]             224\n",
      "InvertedResidual-123          [-1, 112, 14, 14]               0\n",
      "          Conv2d-124          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-125          [-1, 672, 14, 14]           1,344\n",
      "       Hardswish-126          [-1, 672, 14, 14]               0\n",
      "          Conv2d-127          [-1, 672, 14, 14]           6,048\n",
      "     BatchNorm2d-128          [-1, 672, 14, 14]           1,344\n",
      "       Hardswish-129          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 672, 1, 1]               0\n",
      "          Conv2d-131            [-1, 168, 1, 1]         113,064\n",
      "            ReLU-132            [-1, 168, 1, 1]               0\n",
      "          Conv2d-133            [-1, 672, 1, 1]         113,568\n",
      "     Hardsigmoid-134            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-135          [-1, 672, 14, 14]               0\n",
      "          Conv2d-136          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-137          [-1, 112, 14, 14]             224\n",
      "InvertedResidual-138          [-1, 112, 14, 14]               0\n",
      "          Conv2d-139          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-140          [-1, 672, 14, 14]           1,344\n",
      "       Hardswish-141          [-1, 672, 14, 14]               0\n",
      "          Conv2d-142            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-143            [-1, 672, 7, 7]           1,344\n",
      "       Hardswish-144            [-1, 672, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 672, 1, 1]               0\n",
      "          Conv2d-146            [-1, 168, 1, 1]         113,064\n",
      "            ReLU-147            [-1, 168, 1, 1]               0\n",
      "          Conv2d-148            [-1, 672, 1, 1]         113,568\n",
      "     Hardsigmoid-149            [-1, 672, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 672, 7, 7]               0\n",
      "          Conv2d-151            [-1, 160, 7, 7]         107,520\n",
      "     BatchNorm2d-152            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-153            [-1, 160, 7, 7]               0\n",
      "          Conv2d-154            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-155            [-1, 960, 7, 7]           1,920\n",
      "       Hardswish-156            [-1, 960, 7, 7]               0\n",
      "          Conv2d-157            [-1, 960, 7, 7]          24,000\n",
      "     BatchNorm2d-158            [-1, 960, 7, 7]           1,920\n",
      "       Hardswish-159            [-1, 960, 7, 7]               0\n",
      "AdaptiveAvgPool2d-160            [-1, 960, 1, 1]               0\n",
      "          Conv2d-161            [-1, 240, 1, 1]         230,640\n",
      "            ReLU-162            [-1, 240, 1, 1]               0\n",
      "          Conv2d-163            [-1, 960, 1, 1]         231,360\n",
      "     Hardsigmoid-164            [-1, 960, 1, 1]               0\n",
      "SqueezeExcitation-165            [-1, 960, 7, 7]               0\n",
      "          Conv2d-166            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-167            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-168            [-1, 160, 7, 7]               0\n",
      "          Conv2d-169            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-170            [-1, 960, 7, 7]           1,920\n",
      "       Hardswish-171            [-1, 960, 7, 7]               0\n",
      "          Conv2d-172            [-1, 960, 7, 7]          24,000\n",
      "     BatchNorm2d-173            [-1, 960, 7, 7]           1,920\n",
      "       Hardswish-174            [-1, 960, 7, 7]               0\n",
      "AdaptiveAvgPool2d-175            [-1, 960, 1, 1]               0\n",
      "          Conv2d-176            [-1, 240, 1, 1]         230,640\n",
      "            ReLU-177            [-1, 240, 1, 1]               0\n",
      "          Conv2d-178            [-1, 960, 1, 1]         231,360\n",
      "     Hardsigmoid-179            [-1, 960, 1, 1]               0\n",
      "SqueezeExcitation-180            [-1, 960, 7, 7]               0\n",
      "          Conv2d-181            [-1, 160, 7, 7]         153,600\n",
      "     BatchNorm2d-182            [-1, 160, 7, 7]             320\n",
      "InvertedResidual-183            [-1, 160, 7, 7]               0\n",
      "          Conv2d-184            [-1, 960, 7, 7]         153,600\n",
      "     BatchNorm2d-185            [-1, 960, 7, 7]           1,920\n",
      "       Hardswish-186            [-1, 960, 7, 7]               0\n",
      "AdaptiveAvgPool2d-187            [-1, 960, 1, 1]               0\n",
      "         Flatten-188                  [-1, 960]               0\n",
      "          Linear-189                 [-1, 4096]       3,936,256\n",
      "            ReLU-190                 [-1, 4096]               0\n",
      "         Dropout-191                 [-1, 4096]               0\n",
      "          Linear-192                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 6,973,760\n",
      "Trainable params: 6,973,760\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.55\n",
      "Forward/backward pass size (MB): 103.51\n",
      "Params size (MB): 26.60\n",
      "Estimated Total Size (MB): 130.66\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# MobileNetV2EncoderPoseModel Model Summary\n",
    "model = MobileNetV3LargeEncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3. MobileNetV3Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.3. Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3.3.1. Learning rate adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate test - MobileNetV3Large (Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 8)\n",
    "class MobileNetV3SmallEncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model using a pretrained MobileNetV3-Large encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(MobileNetV3SmallEncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained MobileNetV3-Large model\n",
    "        mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "        # Remove the classification head (classifier block)\n",
    "        # MobileNetV3-Large's feature extractor ends with Conv2d outputting 1280 channels\n",
    "        self.encoder = nn.Sequential(*list(mobilenet.features))\n",
    "\n",
    "        # GAP\n",
    "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Decoder: fully connected layers\n",
    "        # Input dimension depends on the spatial size of the encoder's output (typically 7x7 for ~224x224 inputs)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(576, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            # Using the original output\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 224, 224).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (batch_size, 1280, 7, 7)\n",
    "        x = self.GAP(x)\n",
    "        x = self.flatten(x)  # (batch_size, 1280 * 7 * 7)\n",
    "        x = self.decoder(x)  # (batch_size, nkeypoints * 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 110, 110]             432\n",
      "       BatchNorm2d-2         [-1, 16, 110, 110]              32\n",
      "         Hardswish-3         [-1, 16, 110, 110]               0\n",
      "            Conv2d-4           [-1, 16, 55, 55]             144\n",
      "       BatchNorm2d-5           [-1, 16, 55, 55]              32\n",
      "              ReLU-6           [-1, 16, 55, 55]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
      "            Conv2d-8              [-1, 8, 1, 1]             136\n",
      "              ReLU-9              [-1, 8, 1, 1]               0\n",
      "           Conv2d-10             [-1, 16, 1, 1]             144\n",
      "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
      "SqueezeExcitation-12           [-1, 16, 55, 55]               0\n",
      "           Conv2d-13           [-1, 16, 55, 55]             256\n",
      "      BatchNorm2d-14           [-1, 16, 55, 55]              32\n",
      " InvertedResidual-15           [-1, 16, 55, 55]               0\n",
      "           Conv2d-16           [-1, 72, 55, 55]           1,152\n",
      "      BatchNorm2d-17           [-1, 72, 55, 55]             144\n",
      "             ReLU-18           [-1, 72, 55, 55]               0\n",
      "           Conv2d-19           [-1, 72, 28, 28]             648\n",
      "      BatchNorm2d-20           [-1, 72, 28, 28]             144\n",
      "             ReLU-21           [-1, 72, 28, 28]               0\n",
      "           Conv2d-22           [-1, 24, 28, 28]           1,728\n",
      "      BatchNorm2d-23           [-1, 24, 28, 28]              48\n",
      " InvertedResidual-24           [-1, 24, 28, 28]               0\n",
      "           Conv2d-25           [-1, 88, 28, 28]           2,112\n",
      "      BatchNorm2d-26           [-1, 88, 28, 28]             176\n",
      "             ReLU-27           [-1, 88, 28, 28]               0\n",
      "           Conv2d-28           [-1, 88, 28, 28]             792\n",
      "      BatchNorm2d-29           [-1, 88, 28, 28]             176\n",
      "             ReLU-30           [-1, 88, 28, 28]               0\n",
      "           Conv2d-31           [-1, 24, 28, 28]           2,112\n",
      "      BatchNorm2d-32           [-1, 24, 28, 28]              48\n",
      " InvertedResidual-33           [-1, 24, 28, 28]               0\n",
      "           Conv2d-34           [-1, 96, 28, 28]           2,304\n",
      "      BatchNorm2d-35           [-1, 96, 28, 28]             192\n",
      "        Hardswish-36           [-1, 96, 28, 28]               0\n",
      "           Conv2d-37           [-1, 96, 14, 14]           2,400\n",
      "      BatchNorm2d-38           [-1, 96, 14, 14]             192\n",
      "        Hardswish-39           [-1, 96, 14, 14]               0\n",
      "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
      "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
      "             ReLU-42             [-1, 24, 1, 1]               0\n",
      "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
      "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
      "SqueezeExcitation-45           [-1, 96, 14, 14]               0\n",
      "           Conv2d-46           [-1, 40, 14, 14]           3,840\n",
      "      BatchNorm2d-47           [-1, 40, 14, 14]              80\n",
      " InvertedResidual-48           [-1, 40, 14, 14]               0\n",
      "           Conv2d-49          [-1, 240, 14, 14]           9,600\n",
      "      BatchNorm2d-50          [-1, 240, 14, 14]             480\n",
      "        Hardswish-51          [-1, 240, 14, 14]               0\n",
      "           Conv2d-52          [-1, 240, 14, 14]           6,000\n",
      "      BatchNorm2d-53          [-1, 240, 14, 14]             480\n",
      "        Hardswish-54          [-1, 240, 14, 14]               0\n",
      "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
      "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-57             [-1, 64, 1, 1]               0\n",
      "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-60          [-1, 240, 14, 14]               0\n",
      "           Conv2d-61           [-1, 40, 14, 14]           9,600\n",
      "      BatchNorm2d-62           [-1, 40, 14, 14]              80\n",
      " InvertedResidual-63           [-1, 40, 14, 14]               0\n",
      "           Conv2d-64          [-1, 240, 14, 14]           9,600\n",
      "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
      "        Hardswish-66          [-1, 240, 14, 14]               0\n",
      "           Conv2d-67          [-1, 240, 14, 14]           6,000\n",
      "      BatchNorm2d-68          [-1, 240, 14, 14]             480\n",
      "        Hardswish-69          [-1, 240, 14, 14]               0\n",
      "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
      "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
      "             ReLU-72             [-1, 64, 1, 1]               0\n",
      "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
      "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
      "SqueezeExcitation-75          [-1, 240, 14, 14]               0\n",
      "           Conv2d-76           [-1, 40, 14, 14]           9,600\n",
      "      BatchNorm2d-77           [-1, 40, 14, 14]              80\n",
      " InvertedResidual-78           [-1, 40, 14, 14]               0\n",
      "           Conv2d-79          [-1, 120, 14, 14]           4,800\n",
      "      BatchNorm2d-80          [-1, 120, 14, 14]             240\n",
      "        Hardswish-81          [-1, 120, 14, 14]               0\n",
      "           Conv2d-82          [-1, 120, 14, 14]           3,000\n",
      "      BatchNorm2d-83          [-1, 120, 14, 14]             240\n",
      "        Hardswish-84          [-1, 120, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
      "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
      "             ReLU-87             [-1, 32, 1, 1]               0\n",
      "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
      "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
      "SqueezeExcitation-90          [-1, 120, 14, 14]               0\n",
      "           Conv2d-91           [-1, 48, 14, 14]           5,760\n",
      "      BatchNorm2d-92           [-1, 48, 14, 14]              96\n",
      " InvertedResidual-93           [-1, 48, 14, 14]               0\n",
      "           Conv2d-94          [-1, 144, 14, 14]           6,912\n",
      "      BatchNorm2d-95          [-1, 144, 14, 14]             288\n",
      "        Hardswish-96          [-1, 144, 14, 14]               0\n",
      "           Conv2d-97          [-1, 144, 14, 14]           3,600\n",
      "      BatchNorm2d-98          [-1, 144, 14, 14]             288\n",
      "        Hardswish-99          [-1, 144, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
      "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
      "            ReLU-102             [-1, 40, 1, 1]               0\n",
      "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
      "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
      "SqueezeExcitation-105          [-1, 144, 14, 14]               0\n",
      "          Conv2d-106           [-1, 48, 14, 14]           6,912\n",
      "     BatchNorm2d-107           [-1, 48, 14, 14]              96\n",
      "InvertedResidual-108           [-1, 48, 14, 14]               0\n",
      "          Conv2d-109          [-1, 288, 14, 14]          13,824\n",
      "     BatchNorm2d-110          [-1, 288, 14, 14]             576\n",
      "       Hardswish-111          [-1, 288, 14, 14]               0\n",
      "          Conv2d-112            [-1, 288, 7, 7]           7,200\n",
      "     BatchNorm2d-113            [-1, 288, 7, 7]             576\n",
      "       Hardswish-114            [-1, 288, 7, 7]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
      "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
      "            ReLU-117             [-1, 72, 1, 1]               0\n",
      "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
      "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
      "SqueezeExcitation-120            [-1, 288, 7, 7]               0\n",
      "          Conv2d-121             [-1, 96, 7, 7]          27,648\n",
      "     BatchNorm2d-122             [-1, 96, 7, 7]             192\n",
      "InvertedResidual-123             [-1, 96, 7, 7]               0\n",
      "          Conv2d-124            [-1, 576, 7, 7]          55,296\n",
      "     BatchNorm2d-125            [-1, 576, 7, 7]           1,152\n",
      "       Hardswish-126            [-1, 576, 7, 7]               0\n",
      "          Conv2d-127            [-1, 576, 7, 7]          14,400\n",
      "     BatchNorm2d-128            [-1, 576, 7, 7]           1,152\n",
      "       Hardswish-129            [-1, 576, 7, 7]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
      "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-132            [-1, 144, 1, 1]               0\n",
      "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-135            [-1, 576, 7, 7]               0\n",
      "          Conv2d-136             [-1, 96, 7, 7]          55,296\n",
      "     BatchNorm2d-137             [-1, 96, 7, 7]             192\n",
      "InvertedResidual-138             [-1, 96, 7, 7]               0\n",
      "          Conv2d-139            [-1, 576, 7, 7]          55,296\n",
      "     BatchNorm2d-140            [-1, 576, 7, 7]           1,152\n",
      "       Hardswish-141            [-1, 576, 7, 7]               0\n",
      "          Conv2d-142            [-1, 576, 7, 7]          14,400\n",
      "     BatchNorm2d-143            [-1, 576, 7, 7]           1,152\n",
      "       Hardswish-144            [-1, 576, 7, 7]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
      "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
      "            ReLU-147            [-1, 144, 1, 1]               0\n",
      "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
      "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
      "SqueezeExcitation-150            [-1, 576, 7, 7]               0\n",
      "          Conv2d-151             [-1, 96, 7, 7]          55,296\n",
      "     BatchNorm2d-152             [-1, 96, 7, 7]             192\n",
      "InvertedResidual-153             [-1, 96, 7, 7]               0\n",
      "          Conv2d-154            [-1, 576, 7, 7]          55,296\n",
      "     BatchNorm2d-155            [-1, 576, 7, 7]           1,152\n",
      "       Hardswish-156            [-1, 576, 7, 7]               0\n",
      "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
      "         Flatten-158                  [-1, 576]               0\n",
      "          Linear-159                 [-1, 4096]       2,363,392\n",
      "            ReLU-160                 [-1, 4096]               0\n",
      "         Dropout-161                 [-1, 4096]               0\n",
      "          Linear-162                   [-1, 16]          65,552\n",
      "================================================================\n",
      "Total params: 3,355,952\n",
      "Trainable params: 3,355,952\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.55\n",
      "Forward/backward pass size (MB): 34.24\n",
      "Params size (MB): 12.80\n",
      "Estimated Total Size (MB): 47.59\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# MobileNetV2EncoderPoseModel Model Summary\n",
    "model = MobileNetV3SmallEncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoderPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        \"\"\"\n",
    "        Initializes the pose estimation model with a Vision Transformer (ViT-B/16) as the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        - nkeypoints: Number of keypoints to predict, each with (x, y) coordinates.\n",
    "        \"\"\"\n",
    "        super(ViTEncoderPoseModel, self).__init__()\n",
    "\n",
    "        # Load a pretrained Vision Transformer (ViT-B/16)\n",
    "        vit = models.vit_b_16(weights=models.vision_transformer.ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "        # Remove the classification head (we'll use a custom decoder)\n",
    "        self.encoder = nn.Sequential(\n",
    "            vit.conv_proj,  # Initial projection layer\n",
    "            vit.encoder  # Transformer Encoder layers\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling (GAP) to get a fixed-size feature vector\n",
    "        self.GAP = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Flatten the output of the encoder\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the decoder (Fully Connected layers)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(768, 4096),  # ViT-B/16 has an embedding dimension of 768\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.85),  # Keeping dropout same as your ResNet model\n",
    "            nn.Linear(4096, nkeypoints * 2)  # Output (nkeypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, 3, 220, 220).\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, nkeypoints * 2).\n",
    "        \"\"\"\n",
    "        # Pass through ViT encoder\n",
    "        x = self.encoder[0](x)  # Patch Embedding Layer\n",
    "        x = x.flatten(2).transpose(1, 2)  # Reshape to fit Transformer input format\n",
    "        x = self.encoder[1](x)  # Transformer Encoder\n",
    "\n",
    "        # Extract CLS token (position [0]) and apply GAP\n",
    "        x = x[:, 0, :].unsqueeze(-1)  # Extract CLS token\n",
    "        x = self.GAP(x).squeeze(-1)  # Apply GAP\n",
    "\n",
    "        # Pass through the decoder\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (169) must match the size of tensor b (197) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move model to GPU\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m220\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m220\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_pe/lib/python3.12/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_pe/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_pe/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m, in \u001b[0;36mViTEncoderPoseModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;241m0\u001b[39m](x)  \u001b[38;5;66;03m# Patch Embedding Layer\u001b[39;00m\n\u001b[1;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Reshape to fit Transformer input format\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Transformer Encoder\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Extract CLS token (position [0]) and apply GAP\u001b[39;00m\n\u001b[1;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Extract CLS token\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_pe/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_pe/lib/python3.12/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_pe/lib/python3.12/site-packages/torchvision/models/vision_transformer.py:156\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    155\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_assert(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected (batch_size, seq_length, hidden_dim) got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embedding\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (169) must match the size of tensor b (197) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = ViTEncoderPoseModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)  # Move model to GPU\n",
    "summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Save results in a single csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def extract_results(parent_dir):\n",
    "    \"\"\"\n",
    "    Extracts evaluation results from JSON files located in nested directories and saves them as a CSV file.\n",
    "\n",
    "    This function scans through all model directories within a given parent directory, identifies\n",
    "    an evaluation subdirectory (whose name contains \"evaluation\" in any casing), and extracts \n",
    "    relevant performance metrics from a JSON file named \"results.json\". The extracted results \n",
    "    are written to a CSV file in the parent directory.\n",
    "\n",
    "    Parameters:\n",
    "    - parent_dir (str): The absolute or relative path to the parent directory containing \n",
    "      multiple model directories.\n",
    "\n",
    "    The CSV file is named \"results.csv\" and is saved in the parent directory. It contains the \n",
    "    following columns in order:\n",
    "      - description\n",
    "      - total_params\n",
    "      - GPU_inf(ms)\n",
    "      - CPU_inf(ms)\n",
    "      - GFLOPs\n",
    "      - val_pck005\n",
    "      - val_pck01\n",
    "      - pck005\n",
    "      - pck01\n",
    "\n",
    "    The function ensures model directories are processed in alphabetical order.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    extract_results(\"/path/to/parent/directory\")\n",
    "    ```\n",
    "\n",
    "    The function does not return any value but saves the extracted results as a CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all items in the parent directory and sort them alphabetically.\n",
    "    model_dirs = sorted([\n",
    "        d for d in os.listdir(parent_dir)\n",
    "        if os.path.isdir(os.path.join(parent_dir, d))\n",
    "    ])\n",
    "    \n",
    "    # This list will hold the rows to be written to the CSV.\n",
    "    csv_rows = []\n",
    "\n",
    "    # Iterate through each model directory.\n",
    "    for model in model_dirs:\n",
    "        model_path = os.path.join(parent_dir, model)\n",
    "        \n",
    "        # Find a subdirectory with \"evaluation\" in its name (case-insensitive).\n",
    "        eval_dirs = [d for d in os.listdir(model_path)\n",
    "                     if os.path.isdir(os.path.join(model_path, d)) and \"evaluation\" in d.lower()]\n",
    "        if not eval_dirs:\n",
    "            continue  # Skip if no evaluation directory is found\n",
    "        \n",
    "        # Assuming there's only one evaluation directory per model.\n",
    "        eval_dir = eval_dirs[0]\n",
    "        eval_path = os.path.join(model_path, eval_dir)\n",
    "        results_file = os.path.join(eval_path, \"results.json\")\n",
    "        \n",
    "        # Check if results.json exists.\n",
    "        if not os.path.exists(results_file):\n",
    "            continue\n",
    "        \n",
    "        # Open and load the JSON file.\n",
    "        with open(results_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract values in the specific order:\n",
    "        # description, total_params, GPU_inf(ms), CPU_inf(ms), GFLOPs, val_pck005, val_pck01, pck005, pck01.\n",
    "        row = [\n",
    "            data.get(\"description\", \"\"),\n",
    "            data.get(\"total_params\", \"\"),\n",
    "            data.get(\"GPU_inf(ms)\", \"\"),\n",
    "            data.get(\"CPU_inf(ms)\", \"\"),\n",
    "            data.get(\"GFLOPs\", \"\"),\n",
    "            data.get(\"val_pck005\", \"\"),\n",
    "            data.get(\"val_pck01\", \"\"),\n",
    "            data.get(\"pck005\", \"\"),\n",
    "            data.get(\"pck01\", \"\")\n",
    "        ]\n",
    "        csv_rows.append(row)\n",
    "    \n",
    "    # Define the CSV file path in the parent directory.\n",
    "    csv_file_path = os.path.join(parent_dir, \"results.csv\")\n",
    "    \n",
    "    # Write the header and rows to the CSV file.\n",
    "    header = [\"description\", \"total_params\", \"GPU_inf(ms)\", \"CPU_inf(ms)\", \"GFLOPs\",\n",
    "              \"val_pck005\", \"val_pck01\", \"pck005\", \"pck01\"]\n",
    "    with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(csv_rows)\n",
    "    \n",
    "    print(f\"CSV file saved at: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at: /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 16/results.csv\n"
     ]
    }
   ],
   "source": [
    "# directory path\n",
    "parent_directory = \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/Encoder test Final 1 - learning rate - GAP, flat, 4096, DropOut 0.6, batch 16\"  \n",
    "extract_results(parent_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_pe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
