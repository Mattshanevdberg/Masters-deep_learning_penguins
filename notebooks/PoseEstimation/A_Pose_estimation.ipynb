{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# get the FLOPs\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "from imgaug.augmentables.kps import Keypoint, KeypointsOnImage\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# get the FLOPs\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table, parameter_count, flop_count_str\n",
    "import torchprofile\n",
    "# # decrease Cuda memory usage\n",
    "# from torch.cuda.amp import GradScaler, autocast # use gradscaler amd mixed precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_info \u001b[38;5;28;01mas\u001b[39;00m tf_build_info\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "print(tf.__version__)\n",
    "#print(\"CUDA Version:\", tf_build_info.cuda_version)\n",
    "#print(\"cuDNN Version:\", tf_build_info.cudnn_version)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc4...\n",
      "3.0.0rc4\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "print(deeplabcut.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLC imports (used with DEEPLABCUT env)\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n",
      "12.1\n",
      "90100\n",
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "#print(torch.cuda.current_device())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  True\n",
      "Number of GPUs available:  1\n",
      "CUDA device name:  NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs available: \", torch.cuda.device_count())\n",
    "print(\"CUDA device name: \", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Important Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ids into list \n",
    "def load_file_to_list(file_path):\n",
    "    \"\"\"\n",
    "    loads a text file to a list with each entry on a new line becoming a new entry in the list.\n",
    "\n",
    "    :param file_path: Path to the file where the list should be saved.\n",
    "    :return list of data from file\n",
    "    \"\"\"\n",
    "    # Open the file for writing\n",
    "    lst = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Write each item on a new line\n",
    "        for line in file:\n",
    "            lst.append(line.strip())\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the image data into an arr\n",
    "# in the same order as the annotations and ids are stored (use id list for this)\n",
    "\n",
    "# The load image data function may take a while to run\n",
    "\n",
    "def load_image_data(ids_to_load, image_folder, crop_ext):\n",
    "\n",
    "  # list for loading image data\n",
    "  selected_imgs = []\n",
    "\n",
    "  # for loop for loading image data that is present in the list of ids\n",
    "  for i, img_id in enumerate(ids_to_load):\n",
    "\n",
    "    # load the image\n",
    "    img_path = os.path.join(image_folder, img_id+crop_ext)\n",
    "    #print(img_path)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    #print(img)\n",
    "\n",
    "    # change the img to RGB from BGR as plt uses RGB colour scale\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # scaling the pixel values to [0, 1] (you don't need to scal them back)\n",
    "    img = img/255\n",
    "\n",
    "    selected_imgs.append(img)\n",
    "\n",
    "  # Convert the list of images to a NumPy array\n",
    "  selected_imgs_array = np.array(selected_imgs)\n",
    "  \n",
    "  return selected_imgs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(df, path):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame to a .json file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be converted to JSON.\n",
    "    path (str): The path (including file name) where the .json file will be saved.\n",
    "    \"\"\"\n",
    "    df.to_json(path, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_to_df(path):\n",
    "    \"\"\"\n",
    "    Converts a .json file to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The path to the .json file that will be read.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame created from the JSON file.\n",
    "    \"\"\"\n",
    "    #print(path)\n",
    "    df = pd.read_json(path, orient='records')\n",
    "    #print(f\"JSON file has been successfully converted to DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lists(df_to_list, list_of_cols):\n",
    "\n",
    "  # create temp lists\n",
    "  keypoints_temp = []\n",
    "\n",
    "  # step through the rows and\n",
    "  for _, row in df_to_list.iterrows():\n",
    "\n",
    "    # extract the data arrays\n",
    "    keypoints_data = row[list_of_cols].values\n",
    "\n",
    "    # adding data to the list\n",
    "    keypoints_temp.append(keypoints_data)\n",
    "\n",
    "  # Convert the list to a NumPy array and make sure that they are float32\n",
    "  keypoints_array = np.array(keypoints_temp, dtype=np.float32)\n",
    "  \n",
    "  return keypoints_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dtypes_df_full_annotation_abs(df):\n",
    "    df['vid_id'] = df['vid_id'].astype(str)\n",
    "    df['img_id'] = df['img_id'].astype(str)\n",
    "    df['bbox_id'] = df['bbox_id'].astype(str)\n",
    "    df['bbox_c_x'] = df['bbox_c_x'].astype('float32')\n",
    "    df['bbox_c_y'] = df['bbox_c_y'].astype('float32')\n",
    "    df['bbox_w'] = df['bbox_w'].astype('float32')\n",
    "    df['bbox_h'] = df['bbox_h'].astype('float32')\n",
    "    df['Head_x'] = df['Head_x'].astype('float32')\n",
    "    df['Head_y'] = df['Head_y'].astype('float32')\n",
    "    df['Beak_x'] = df['Beak_x'].astype('float32')\n",
    "    df['Beak_y'] = df['Beak_y'].astype('float32')\n",
    "    df['Body_top_x'] = df['Body_top_x'].astype('float32')\n",
    "    df['Body_top_y'] = df['Body_top_y'].astype('float32')\n",
    "    df['RFlipper_mid_x'] = df['RFlipper_mid_x'].astype('float32')\n",
    "    df['RFlipper_mid_y'] = df['RFlipper_mid_y'].astype('float32')\n",
    "    df['LFlipper_mid_x'] = df['LFlipper_mid_x'].astype('float32')\n",
    "    df['LFlipper_mid_y'] = df['LFlipper_mid_y'].astype('float32')\n",
    "    df['Body_bottom_x'] = df['Body_bottom_x'].astype('float32')\n",
    "    df['Body_bottom_y'] = df['Body_bottom_y'].astype('float32')\n",
    "    df['RFoot_x'] = df['RFoot_x'].astype('float32')\n",
    "    df['RFoot_y'] = df['RFoot_y'].astype('float32')\n",
    "    df['LFoot_x'] = df['LFoot_x'].astype('float32')\n",
    "    df['LFoot_y'] = df['LFoot_y'].astype('float32')\n",
    "    df['kp_outside_best_bbox'] = df['kp_outside_best_bbox'].astype('float32')\n",
    "    df['kp_missing'] = df['kp_missing'].astype('float32')\n",
    "    df['kp_primary_missing'] = df['kp_primary_missing'].astype(bool)\n",
    "    df['img_width'] = df['img_width'].astype('float32')\n",
    "    df['img_height'] = df['img_height'].astype('float32')\n",
    "    df['bbox_max_h_w'] = df['bbox_max_h_w'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnorm_keypoints(img_size, keypoints, kp_to_null=None):\n",
    "    \"\"\"\n",
    "    De-normalizes keypoints based on image size and returns the de-normalized keypoints along with \n",
    "    the positions of any missing or nullified keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    - img_size: Tuple of the image dimensions (height, width).\n",
    "    - keypoints: List of normalized keypoints (with values between -1 and 1).\n",
    "    - kp_to_null: Optional. List of indices where the keypoints should be nulled (set to NaN).\n",
    "\n",
    "    Returns:\n",
    "    - new_keypoints: List of de-normalized keypoints where each coordinate is scaled back to the \n",
    "                     image's pixel dimensions.\n",
    "    - missing_kp: List of indices where the keypoints were either originally set to -10 (indicating \n",
    "                  missing keypoints) or explicitly nullified by the kp_to_null list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract image width and height\n",
    "    readjust_x = img_size[0]  # width of the image\n",
    "    readjust_y = img_size[1]  # height of the image\n",
    "\n",
    "    new_keypoints = []  # List to store the de-normalized keypoints\n",
    "    missing_kp = []     # List to store the indices of missing or nullified keypoints\n",
    "\n",
    "    # Iterate through each keypoint\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        # Null keypoints if they are -10 or if they are specified in kp_to_null\n",
    "        if keypoint == -10 or (kp_to_null and i in kp_to_null):\n",
    "            keypoint = np.nan  # Set keypoint to NaN\n",
    "            missing_kp.append(i)  # Record the index of the missing or nullified keypoint\n",
    "\n",
    "        # De-normalize the x-coordinates\n",
    "        if i % 2 == 0:  # Even indices are x-coordinates\n",
    "            keypoint = keypoint * readjust_x + readjust_x / 2\n",
    "        # De-normalize the y-coordinates\n",
    "        else:  # Odd indices are y-coordinates\n",
    "            keypoint = keypoint * readjust_y + readjust_y / 2\n",
    "\n",
    "        new_keypoints.append(keypoint)  # Append the de-normalized keypoint to the list\n",
    "\n",
    "    return new_keypoints, missing_kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_keypoints(img_size, keypoints):\n",
    "    \"\"\"\n",
    "    Normalizes keypoints based on image size and replaces any NaN values with -10.\n",
    "\n",
    "    Parameters:\n",
    "    - img_size: Tuple of the image dimensions (width, height).\n",
    "    - keypoints: List of de-normalized keypoints where each coordinate is in pixel dimensions.\n",
    "\n",
    "    Returns:\n",
    "    - norm_keypoints: List of normalized keypoints where each coordinate is scaled to the range \n",
    "                      [-1, 1] relative to the image size, with NaNs replaced by -10.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract image width and height\n",
    "    readjust_x = img_size[0]  # width of the image\n",
    "    readjust_y = img_size[1]  # height of the image\n",
    "\n",
    "    norm_keypoints = []  # List to store the normalized keypoints\n",
    "\n",
    "    # Iterate through each keypoint\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        # Replace NaN values with -10\n",
    "        if np.isnan(keypoint):\n",
    "            keypoint = -10.0\n",
    "        else:\n",
    "            # Normalize the x-coordinates\n",
    "            if i % 2 == 0:  # Even indices are x-coordinates\n",
    "                keypoint = (keypoint - readjust_x / 2) / readjust_x\n",
    "            # Normalize the y-coordinates\n",
    "            else:  # Odd indices are y-coordinates\n",
    "                keypoint = (keypoint - readjust_y / 2) / readjust_y\n",
    "\n",
    "        norm_keypoints.append(keypoint)  # Append the normalized keypoint to the list\n",
    "\n",
    "    return norm_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize keypoints for an array of images\n",
    "def unnorm_keypoints_arr(kp_arr, img_arr):\n",
    "    \"\"\"\n",
    "    Denormalizes keypoints for each image in the array based on the corresponding image size.\n",
    "    It converts normalized keypoints (range [-1, 1]) back to pixel coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - kp_arr: Array of normalized keypoints, where each entry is a list of keypoints for an image.\n",
    "              The keypoints are expected to be in the format [x1, y1, x2, y2, ...].\n",
    "    - img_arr: Array of images. The size of each image is used to scale the keypoints back \n",
    "               to their pixel coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - kp_abs_arr: Array of denormalized keypoints where each entry corresponds to the denormalized \n",
    "                  keypoints for the corresponding image in `img_arr`.\n",
    "    \"\"\"\n",
    "\n",
    "    kp_abs_list = []  # List to store the denormalized keypoints for each image\n",
    "\n",
    "    # Iterate through each set of keypoints and corresponding image\n",
    "    for i, kp in enumerate(kp_arr):\n",
    "        img_size = img_arr[i].shape  # Get the size of the current image (height, width, channels)\n",
    "\n",
    "        # Denormalize the keypoints based on the image size\n",
    "        kp_abs, missing_kp = unnorm_keypoints(img_size, kp_arr[i])\n",
    "\n",
    "        # Save the denormalized keypoints to the list\n",
    "        kp_abs_list.append(kp_abs)\n",
    "    \n",
    "    # Convert the list of denormalized keypoints to a NumPy array\n",
    "    kp_abs_arr = np.array(kp_abs_list)\n",
    "\n",
    "    return kp_abs_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize keypoints for an array of images\n",
    "def norm_keypoints_arr(kp_arr, img_arr):\n",
    "    \"\"\"\n",
    "    Normalizes keypoints for each image in the array based on the corresponding image size.\n",
    "    It converts keypoints from pixel coordinates back to normalized coordinates (range [-1, 1]).\n",
    "\n",
    "    Parameters:\n",
    "    - kp_arr: Array of keypoints, where each entry is a list of keypoints for an image.\n",
    "              The keypoints are expected to be in the format [x1, y1, x2, y2, ...] \n",
    "              with pixel coordinates.\n",
    "    - img_arr: Array of images. The size of each image is used to scale the keypoints \n",
    "               to normalized coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - kp_norm_arr: Array of normalized keypoints where each entry corresponds to the normalized \n",
    "                   keypoints for the corresponding image in `img_arr`.\n",
    "    \"\"\"\n",
    "        \n",
    "    kp_norm_list = []  # List to store the normalized keypoints for each image\n",
    "\n",
    "    # Iterate through each set of keypoints and corresponding image\n",
    "    for i, kp in enumerate(kp_arr):\n",
    "        img_size = img_arr[i].shape  # Get the size of the current image (height, width, channels)\n",
    "\n",
    "        # Normalize the keypoints based on the image size\n",
    "        kp_norm = norm_keypoints(img_size, kp_arr[i])\n",
    "\n",
    "        # Save the normalized keypoints to the list\n",
    "        kp_norm_list.append(kp_norm)\n",
    "    \n",
    "    # Convert the list of normalized keypoints to a NumPy array\n",
    "    kp_norm_arr = np.array(kp_norm_list)  \n",
    "\n",
    "    return kp_norm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation to images and keypoints\n",
    "def apply_aug(img_arr_orig, kp_arr_orig, aug, num_of_kp=8):\n",
    "    \"\"\"\n",
    "    Applies augmentation to a batch of images and their corresponding keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    - img_arr_orig: Original array of images. Shape should be (num_imgs, height, width, channels).\n",
    "    - kp_arr_orig: Original array of keypoints. Shape should be (num_imgs, num_of_kp*2), where each \n",
    "                   keypoint is represented by its x and y coordinates in pixel values.\n",
    "    - aug: An imgaug augmentation sequence or augmenter to apply to the images and keypoints.\n",
    "    - num_of_kp: Optional. Number of keypoints per image (default is 8).\n",
    "\n",
    "    Returns:\n",
    "    - img_arr_aug: Augmented array of images. Same shape as `img_arr_orig`.\n",
    "    - kp_arr_aug: Augmented array of keypoints. Same shape as `kp_arr_orig`.\n",
    "    \"\"\"\n",
    "    # print(img_arr_orig.shape)\n",
    "    #print(kp_arr_orig.shape)\n",
    "    \n",
    "    # Initialize lists to store augmented images and keypoints\n",
    "    aug_img = []  # List for augmented images\n",
    "    aug_kp = []   # List for augmented keypoints\n",
    "\n",
    "    # Get the number of images in the batch\n",
    "    num_imgs = img_arr_orig.shape[0]\n",
    "    #print(num_imgs)\n",
    "\n",
    "    # Loop over each image and its corresponding keypoints\n",
    "    for i in range(num_imgs):\n",
    "        image = img_arr_orig[i]  # Extract the i-th image\n",
    "        #print(image.shape)\n",
    "        \n",
    "        # Convert keypoints to KeypointsOnImage format for imgaug\n",
    "        keypoints = kp_arr_orig[i]\n",
    "        #print(keypoints)\n",
    "        kps = [Keypoint(x=keypoints[j*2], y=keypoints[j*2+1]) for j in range(num_of_kp)]\n",
    "        kps_on_image = KeypointsOnImage(kps, shape=image.shape)\n",
    "        \n",
    "        # Apply the augmentation to the image and keypoints\n",
    "        image_aug, kps_aug = aug(image=image, keypoints=kps_on_image)\n",
    "        \n",
    "        # Convert augmented keypoints back to the original flattened format [x1, y1, x2, y2, ...]\n",
    "        keypoints_aug = []\n",
    "        for kp in kps_aug.keypoints:\n",
    "            keypoints_aug.extend([kp.x, kp.y])\n",
    "        \n",
    "        # Append the augmented image and keypoints to their respective lists\n",
    "        aug_img.append(image_aug)\n",
    "        aug_kp.append(keypoints_aug)\n",
    "\n",
    "    # Convert the lists of augmented images and keypoints back to NumPy arrays\n",
    "    img_arr_aug = np.array(aug_img)\n",
    "    kp_arr_aug = np.array(aug_kp)\n",
    "\n",
    "    return img_arr_aug, kp_arr_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_padding(image):\n",
    "    \"\"\"\n",
    "    Detects if padding is on the x-axis (left and right) or y-axis (top and bottom)\n",
    "    of the image and calculates the padding size on one side.\n",
    "\n",
    "    Parameters:\n",
    "    - image: A NumPy array representing the image. The shape should be (width, height, channels).\n",
    "\n",
    "    Returns:\n",
    "    - is_padding_x: True if padding is on the x-axis, False if padding is on the y-axis.\n",
    "    - padding_size: The size of the padding on one side in pixels.\n",
    "    \"\"\"\n",
    "\n",
    "    width, height, _ = image.shape\n",
    "    \n",
    "    # Check for padding along the x-axis (left and right)\n",
    "    left_column = image[:, 0, :]#image[0, :, :]  # The first column (left side)\n",
    "    right_column = image[:, -1, :] #image[-1, :, :]  # The last column (right side)\n",
    "\n",
    "    # Check for padding along the y-axis (top and bottom)\n",
    "    top_row = image[:, 0, :]  # The first row (top side)\n",
    "    bottom_row = image[:, -1, :]  # The last row (bottom side)\n",
    "    #print(image[:, 5, :] *255)\n",
    "    #print(left_column*255)\n",
    "    \n",
    "    # Check if the columns are fully black (indicating padding)\n",
    "    if np.all(left_column*255 < 30) and np.all(right_column*255 < 30):\n",
    "        # Padding is along the x-axis\n",
    "        is_padding_x = True\n",
    "        #plot_img(image)\n",
    "        # Calculate padding size\n",
    "        #padding_size = np.sum(image[0, :, 0]*255 < 30) // 2  # Count black pixels on one side\n",
    "        # if padding_size > 60:\n",
    "        sum1 = np.sum(image[5, :, 0]*255 < 20) // 2\n",
    "        sum2 = np.sum(image[10, :, 0]*255 < 20) // 2\n",
    "        sum3 = np.sum(image[60, :, 0]*255 < 20) // 2\n",
    "        sum4 = np.sum(image[110, :, 0]*255 < 20) // 2\n",
    "        sum5 = np.sum(image[-60, :, 0]*255 < 20) // 2\n",
    "        sum6 = np.sum(image[-10, :, 0]*255 < 20) // 2\n",
    "        sum7 = np.sum(image[-5, :, 0]*255 < 20) // 2\n",
    "        padding_size = min(sum1, sum2, sum3, sum4, sum5, sum6, sum7)\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        # if padding_size > 60:\n",
    "        #     sum1 = np.sum(image[5, :, 0]*255 < 10) // 2\n",
    "        #     sum2 = np.sum(image[10, :, 0]*255 < 10) // 2\n",
    "        #     sum3 = np.sum(image[60, :, 0]*255 < 10) // 2\n",
    "        #     sum4 = np.sum(image[110, :, 0]*255 < 10) // 2\n",
    "        #     sum5 = np.sum(image[-60, :, 0]*255 < 10) // 2\n",
    "        #     sum6 = np.sum(image[-10, :, 0]*255 < 10) // 2\n",
    "        #     sum7 = np.sum(image[-5, :, 0]*255 < 10) // 2\n",
    "        #     average = (sum1 + sum2 + sum3 + sum4 + sum5 + sum6 + sum7) // 7  # Floor division for rounding down\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        if padding_size > 20: \n",
    "            padding_size = 20\n",
    "\n",
    "    else:\n",
    "        # Padding is along the y-axis (top and bottom)\n",
    "        is_padding_x = False\n",
    "        # Calculate padding size\n",
    "        padding_size = np.sum(image[:, 0, 0]*255 < 30) // 2  # Count black pixels on one side\n",
    "        # if padding_size > 60:\n",
    "        sum1 = np.sum(image[:, 5, 0]*255 < 20) // 2\n",
    "        sum2 = np.sum(image[:, 10, 0]*255 < 20) // 2\n",
    "        sum3 = np.sum(image[:, 60, 0]*255 < 20) // 2\n",
    "        sum4 = np.sum(image[:, 110, 0]*255 < 20) // 2\n",
    "        sum5 = np.sum(image[:, -60, 0]*255 < 20) // 2\n",
    "        sum6 = np.sum(image[:, -10, 0]*255 < 20) // 2\n",
    "        sum7 = np.sum(image[:, -5, 0]*255 < 20) // 2\n",
    "        padding_size = min(sum1, sum2, sum3, sum4, sum5, sum6, sum7)\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        # if padding_size > 60:\n",
    "        #     sum1 = np.sum(image[:, 5, 0]*255 < 10) // 2\n",
    "        #     sum2 = np.sum(image[:, 10, 0]*255 < 10) // 2\n",
    "        #     sum3 = np.sum(image[:, 60, 0]*255 < 10) // 2\n",
    "        #     sum4 = np.sum(image[:, 110, 0]*255 < 10) // 2\n",
    "        #     sum5 = np.sum(image[:, -60, 0]*255 < 10) // 2\n",
    "        #     sum6 = np.sum(image[:, -10, 0]*255 < 10) // 2\n",
    "        #     sum7 = np.sum(image[:, -5, 0]*255 < 10) // 2\n",
    "        #     average = (sum1 + sum2 + sum3 + sum4 + sum5 + sum6 + sum7) // 7  # Floor division for rounding down\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        if padding_size > 20: \n",
    "            padding_size = 20\n",
    "\n",
    "    return is_padding_x, padding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aug_translate(train_imgs_array, train_kp_array_abs):\n",
    "\n",
    "    \n",
    "    # Get the number of images in the batch\n",
    "    num_imgs = train_imgs_array.shape[0]\n",
    "    # print(num_imgs)\n",
    "\n",
    "    # creat empty arrays\n",
    "    train_imgs_array_aug_trans = np.empty((0, train_imgs_array.shape[1], train_imgs_array.shape[2], train_imgs_array.shape[3]), dtype=train_imgs_array.dtype)\n",
    "    train_kp_array_aug_trans = np.empty((0, train_kp_array_abs.shape[1]), dtype=train_kp_array_abs.dtype)\n",
    "\n",
    "    # print(train_imgs_array_aug_trans.shape)\n",
    "    # print(train_kp_array_aug_trans.shape)\n",
    "\n",
    "    # Loop over each image and its corresponding keypoints\n",
    "    for i in range(num_imgs):\n",
    "        image = train_imgs_array[i]  # Extract the i-th image\n",
    "        kp = train_kp_array_abs[i]\n",
    "        # print(i)\n",
    "        # print(image.shape)\n",
    "        # print(kp.shape)\n",
    "\n",
    "        is_padding_x, padding_size = detect_padding(image)\n",
    "        # print(f'this: {i}')\n",
    "        # print(is_padding_x)\n",
    "        # print(padding_size)\n",
    "\n",
    "        if is_padding_x:\n",
    "            seq_trans_x_left = iaa.Sequential([\n",
    "                iaa.TranslateX(px=(-padding_size, -padding_size)),\n",
    "            ])\n",
    "            seq_trans_x_right = iaa.Sequential([\n",
    "                iaa.TranslateX(px=(padding_size, padding_size)),\n",
    "            ])\n",
    "\n",
    "            # Convert to shape (1, 220, 220, 3) and (1, 16)\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            #print(is_padding_x)\n",
    "            #print(image.shape)\n",
    "            #print(i)\n",
    "            kp = np.expand_dims(kp, axis=0)\n",
    "\n",
    "            # apply augmentations\n",
    "            single_trans_x_left_img_arr, single_trans_x_left_kp_arr = apply_aug(image, kp, seq_trans_x_left)\n",
    "            single_trans_x_right_img_arr, single_trans_x_right_kp_arr = apply_aug(image, kp, seq_trans_x_right)\n",
    "\n",
    "            #save to image array\n",
    "            train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_x_left_img_arr), axis=0)\n",
    "            train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_x_right_img_arr), axis=0)\n",
    "            #save to kp array\n",
    "            train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_x_left_kp_arr), axis=0)\n",
    "            train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_x_right_kp_arr), axis=0)\n",
    "\n",
    "        else :\n",
    "            seq_trans_y_up = iaa.Sequential([\n",
    "                iaa.TranslateY(px=(-padding_size, -padding_size)),\n",
    "            ])\n",
    "            seq_trans_y_down = iaa.Sequential([\n",
    "                iaa.TranslateY(px=(padding_size, padding_size)),\n",
    "            ])\n",
    "\n",
    "            # Convert to shape (1, 220, 220, 3) and (1, 16)\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            #print(is_padding_x)\n",
    "            #print(image.shape)\n",
    "            #print(i)\n",
    "            kp = np.expand_dims(kp, axis=0)\n",
    "\n",
    "            # apply augmentations\n",
    "            single_trans_y_up_img_arr, single_trans_y_up_kp_arr = apply_aug(image, kp, seq_trans_y_up)\n",
    "            single_trans_y_down_img_arr, single_trans_y_down_kp_arr = apply_aug(image, kp, seq_trans_y_down)\n",
    "\n",
    "            #save to image array\n",
    "            train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_y_up_img_arr), axis=0)\n",
    "            train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_y_down_img_arr), axis=0)\n",
    "            #save to kp array\n",
    "            train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_y_up_kp_arr), axis=0)\n",
    "            train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_y_down_kp_arr), axis=0)\n",
    "\n",
    "\n",
    "    return train_imgs_array_aug_trans, train_kp_array_aug_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_out_of_img_kp(arr):\n",
    "    \"\"\"\n",
    "    Finds and replaces the elements in the array that are outside the frame but not the missing ones.\n",
    "    Specifically, elements greater than 0.5 are replaced with 0.5, and elements less than -0.5 but \n",
    "    greater than -9.0 are replaced with -0.5.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: A NumPy array of shape (n, 16).\n",
    "\n",
    "    Returns:\n",
    "    - modified_arr: The modified NumPy array with replaced values.\n",
    "    - count_replacements: The number of elements that were replaced.\n",
    "    \"\"\"\n",
    "    # Make a copy of the array to avoid modifying the original array\n",
    "    modified_arr = arr.copy()\n",
    "\n",
    "    # Replace elements greater than 0.5 with 0.5\n",
    "    count_pos_replacements = np.sum(modified_arr > 0.5)\n",
    "    modified_arr[modified_arr > 0.5] = 0.49\n",
    "\n",
    "    # Replace elements less than -0.5 but greater than -9.0 with -0.5\n",
    "    count_neg_replacements = np.sum((modified_arr < -0.5) & (modified_arr > -9.0))\n",
    "    modified_arr[(modified_arr < -0.5) & (modified_arr > -9.0)] = -0.49\n",
    "\n",
    "    # Total count of replacements\n",
    "    count_replacements = count_pos_replacements + count_neg_replacements\n",
    "\n",
    "    return modified_arr, count_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_out_of_img_kp(arr):\n",
    "    \"\"\"\n",
    "    Finds and replaces the elements in the array that are outside the frame but not the missing ones.\n",
    "    Specifically, elements greater than 0.5 are replaced with 0.5, and elements less than -0.5 but \n",
    "    greater than -9.0 are replaced with -0.5.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: A NumPy array of shape (n, 16).\n",
    "\n",
    "    Returns:\n",
    "    - modified_arr: The modified NumPy array with replaced values.\n",
    "    - count_replacements: The number of elements that were replaced.\n",
    "    \"\"\"\n",
    "    # Make a copy of the array to avoid modifying the original array\n",
    "    modified_arr = arr.copy()\n",
    "\n",
    "    # Replace elements greater than 0.5 with 0.5\n",
    "    count_pos_replacements = np.sum(modified_arr > 0.5)\n",
    "    modified_arr[modified_arr > 0.5] = 0.49\n",
    "\n",
    "    # Replace elements less than -0.5 but greater than -9.0 with -0.5\n",
    "    count_neg_replacements = np.sum((modified_arr < -0.5) & (modified_arr > -9.0))\n",
    "    modified_arr[(modified_arr < -0.5) & (modified_arr > -9.0)] = -0.49\n",
    "\n",
    "    # Total count of replacements\n",
    "    count_replacements = count_pos_replacements + count_neg_replacements\n",
    "\n",
    "    return modified_arr, count_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error, ignoring the invisible keypoints.\n",
    "    Assuming that -10.0 indicates an invisible keypoint.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints from both\n",
    "    # the predictions and the true values\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # Compute the Mean Squared Error only on the visible keypoints\n",
    "    mse = F.mse_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCK\n",
    "# put in a function that will use the max bbox if primary kp is missing\n",
    "def pck_metric(y_true, y_pred, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Computes the Percentage of Correct Keypoints (PCK) metric.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (torch.Tensor): The ground truth keypoints (batch_size, num_keypoints*2).\n",
    "    y_pred (torch.Tensor): The predicted keypoints (batch_size, num_keypoints*2).\n",
    "    threshold (float): The distance threshold for a keypoint to be considered correct.\n",
    "                       Typically set relative to the size of the bounding box (e.g., 0.2).\n",
    "    \n",
    "    Returns:\n",
    "    float: The percentage of correct keypoints.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible (not equal to -10)\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "    #print(mask)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # print(y_true_masked)\n",
    "    # print(y_pred_masked)\n",
    "\n",
    "    # Compute the Euclidean distance between the predicted and true keypoints\n",
    "    distances = torch.sqrt((y_pred_masked[:, ::2] - y_true_masked[:, ::2]) ** 2 +\n",
    "                           (y_pred_masked[:, 1::2] - y_true_masked[:, 1::2]) ** 2)\n",
    "    \n",
    "    #print(distances)\n",
    "    \n",
    "    # Normalize the distances (relative to the max and min y coord)\n",
    "    Norm_max_min_kp = torch.max(y_true_masked[:, 1::2], dim=1)[0] - torch.min(y_true_masked[:, 1::2], dim=1)[0]\n",
    "    # Normalise based on the distance between the head and the bottom of the body (position 0, 1 and )\n",
    "    #print(y_true[:, 0],y_true[:,10],y_true[:, 1],y_true[:, 11])\n",
    "    #print((y_true[:, 0] - y_true[:,10]) ** 2)\n",
    "    #print((y_true[:, 1] - y_true[:, 11]) ** 2)\n",
    "    Norm_head_lowerbody = torch.sqrt((y_true[:, 0] - y_true[:,10]) ** 2 +\n",
    "                        (y_true[:, 1] - y_true[:, 11]) ** 2)\n",
    "    #print(Norm_head_lowerbody)\n",
    "    normalized_distances = distances / Norm_head_lowerbody[:, None]\n",
    "    #print(distances)\n",
    "    #print(normalized_distances)\n",
    "\n",
    "    # Count the correct keypoints (distance <= threshold)\n",
    "    correct_keypoints = (normalized_distances <= threshold).float() * mask[:, ::2]\n",
    "    #print(correct_keypoints)\n",
    "\n",
    "    # Calculate the PCK as the percentage of correct keypoints\n",
    "    pck = correct_keypoints.sum() / mask[:, ::2].sum()\n",
    "    return pck#.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "        super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "        # The feature extractor part of the model, composed of several convolutional layers.\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv2d: Input channels = 3 (RGB image), Output channels = 96, kernel size = 11x11,\n",
    "            # stride = 4, padding = 4. \n",
    "            # Input: (batch_size, 3, 220, 220)\n",
    "            # Output: (batch_size, 96, 55, 55)\n",
    "            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "            # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "            nn.LocalResponseNorm(5),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            # output size: (batch_size, 96, 27, 27)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2d: Input channels = 96, Output channels = 256, kernel size = 5x5,\n",
    "            # stride = 2, padding = 2.\n",
    "            # Input: (batch_size, 96, 27, 27)\n",
    "            # Output: (batch_size, 256, 27, 27)\n",
    "            nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "            # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "            nn.LocalResponseNorm(5),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            # output size: (batch_size, 96, 13, 13)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2d: Input channels = 256, Output channels = 384, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 256, 13, 13)\n",
    "            # Output: (batch_size, 384, 13, 13)\n",
    "            nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv2d: Input channels = 384, Output channels = 384, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 384, 13, 13)\n",
    "            # Output: (batch_size, 384, 13, 13)\n",
    "            nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv2d: Input channels = 384, Output channels = 256, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 384, 13, 13)\n",
    "            # Output: (batch_size, 256, 13, 13)\n",
    "            nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "            # Input: (batch_size, 256, 13, 13)\n",
    "            # Output: (batch_size, 256, 6, 6)\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        # The classifier part of the model, composed of fully connected layers.\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Flatten the input tensor\n",
    "            # Input: (batch_size, 256, 6, 6)\n",
    "            # Output: (batch_size, 256 * 6 * 6) = (batch_size, 9216)\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # Linear layer with input size 6400 and output size 4096\n",
    "            # Input: (batch_size, 6400)\n",
    "            # Output: (batch_size, 4096)\n",
    "            nn.Linear(128 * 6 * 6, 4096),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Dropout layer with 60% dropout rate\n",
    "            nn.Dropout(0.6),\n",
    "            \n",
    "            # Linear layer with input size 4096 and output size 4096\n",
    "            # Input: (batch_size, 4096)\n",
    "            # Output: (batch_size, 4096)\n",
    "            nn.Linear(4096, 4096),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Dropout layer with 60% dropout rate\n",
    "            nn.Dropout(0.6),\n",
    "            \n",
    "            # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "            # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "            # Input: (batch_size, 4096)\n",
    "            # Output: (batch_size, nkeypoints * 2)\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass through the network.\n",
    "        # Pass input `x` through the feature extractor\n",
    "        x = self.features(x)\n",
    "        # Pass the result through the classifier to get the final output\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data_PyTorch(img_arr, kp_arr, batch_size, train_flag=True):\n",
    "#     '''\n",
    "#     conLoad data into PT dataLoader in specified batch size\n",
    "    \n",
    "#     Params\n",
    "#     img_arr: images loaded into an array (i,255,255,3) and are converted to (i,3,255,255)\n",
    "#     kp_arr: array of keypoints (i, num_kp*2)\n",
    "#     batch_size: batch size \n",
    "\n",
    "#     Return:\n",
    "#     PT_Dataset: containing input (x) and groundtruth (y)\n",
    "#     PT_DataLoader: Dataloader containing dataset and batch size\n",
    "\n",
    "#     '''\n",
    "\n",
    "#     # create tensors from arrays and load them to the GPU\n",
    "#     img_tensor = torch.tensor(img_arr, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "#     kp_tensor = torch.tensor(kp_arr, dtype=torch.float32)#.to('cuda')\n",
    "\n",
    "#     # Create a TensorDataset and DataLoader for training data\n",
    "#     dataset = TensorDataset(img_tensor, kp_tensor)\n",
    "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train_flag)\n",
    "\n",
    "#     return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timestamped_dir(descriptor, base_dir='/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/'):\n",
    "    \"\"\"\n",
    "    Creates a directory with a timestamp appended to the base directory name.\n",
    "    Returns the path to the created directory.\n",
    "    \n",
    "    Parameters:\n",
    "    descriptor: string describing the run generally model_dataDescriptor\n",
    "    base_dir (str): The base directory name. Default is './training_results'.\n",
    "    \n",
    "    Returns:\n",
    "    str: The path to the created directory.\n",
    "    \"\"\"\n",
    "    # Get the current datetime and format it as YYYY-MM-DD_HH-MM-SS\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    base_dir_descriptor = f\"{base_dir}{descriptor}\"\n",
    "    \n",
    "    # Create the final directory name with the timestamp\n",
    "    final_dir = f\"{base_dir_descriptor}_{timestamp}\"\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    \n",
    "    return final_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_data, val_data, save_dir, data_descriptor='Loss', show_plot=False):\n",
    "    # Plot the loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_data, label=f'Training {data_descriptor}')\n",
    "    plt.plot(val_data, label=f'Validation {data_descriptor}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(f'{data_descriptor}')\n",
    "    plt.title(f'Training and Validation {data_descriptor} Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, f'{data_descriptor}_plot.png')\n",
    "    plt.savefig(plot_path)\n",
    "    #print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "    # Optionally, display the plot\n",
    "    if show_plot == True:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_stats_and_models(model, epoch, val_loss, val_pck, save_dir, \n",
    "                     best_val_loss=None, best_val_pck=None, \n",
    "                     final_model=False, train_loss_list=None, val_loss_list=None, train_pck_list=None, val_pck_list=None):\n",
    "    \"\"\"\n",
    "    Saves the best models based on validation loss, PCK value, and final model.\n",
    "    Saves the train and val curves and results for training\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to be saved.\n",
    "    - epoch (int): The current epoch number.\n",
    "    - val_loss (float): The current validation loss.\n",
    "    - val_pck (float): The current validation PCK value.\n",
    "    - save_dir (str): The directory where the models will be saved.\n",
    "    - best_val_loss (float): The best validation loss seen so far.\n",
    "    - best_val_pck (float): The best validation PCK value seen so far.\n",
    "    - final_model (bool): If True, saves the final model after all epochs.\n",
    "    - train_loss_list (list): List of all the loss values from each epoch\n",
    "    \n",
    "    Returns:\n",
    "    - best_val_loss (float): Updated best validation loss.\n",
    "    - best_val_pck (float): Updated best validation PCK value.\n",
    "    - model_save_path_best_val_loss\n",
    "    - model_save_path_best_val_pck\n",
    "    - final_model_path\n",
    "    \"\"\"\n",
    "    model_save_path_best_val_loss = None\n",
    "    model_save_path_best_val_pck = None\n",
    "    \n",
    "    # Check if the current model has the lowest validation loss\n",
    "    if best_val_loss is None or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_name = f'best_val_loss_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth'\n",
    "        model_save_path_best_val_loss = os.path.join(save_dir, model_name)\n",
    "        torch.save(model.state_dict(), model_save_path_best_val_loss)\n",
    "        print(f'New best model saved with lowest validation loss to {model_save_path_best_val_loss}')\n",
    "    \n",
    "    # Check if the current model has the highest validation PCK\n",
    "    if best_val_pck is None or val_pck > best_val_pck:\n",
    "        best_val_pck = val_pck\n",
    "        model_save_path_best_val_pck = os.path.join(save_dir, f'best_val_pck_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth')\n",
    "        torch.save(model.state_dict(), model_save_path_best_val_pck)\n",
    "        print(f'New best model saved with highest validation PCK to {model_save_path_best_val_pck}')\n",
    "    \n",
    "    # Save the final model and perform final stats evaluation and save\n",
    "    if final_model:\n",
    "        final_model_path = os.path.join(save_dir, f'final_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth')\n",
    "        torch.save(model.state_dict(), final_model_path)\n",
    "        print(f'Final model saved to {final_model_path}')\n",
    "        plot_training_curves(train_loss_list, val_loss_list, save_dir, 'Loss', show_plot=True)\n",
    "        plot_training_curves(train_pck_list, val_pck_list, save_dir, data_descriptor='PCK@0.1', show_plot=True)\n",
    "        return best_val_loss, best_val_pck, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path\n",
    "    \n",
    "    return best_val_loss, best_val_pck, model_save_path_best_val_loss, model_save_path_best_val_pck, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(aug, train_imgs_array, train_kp_array): \n",
    "    #\n",
    "    print('augmenting data...')\n",
    "    \n",
    "    # unnorm kp\n",
    "    train_kp_array_abs = unnorm_keypoints_arr(train_kp_array, train_imgs_array)\n",
    "\n",
    "    # specify augmentations\n",
    "    # lrflip\n",
    "    seq_lrflip = iaa.Sequential([\n",
    "        iaa.Fliplr(1.0)\n",
    "    ])\n",
    "    # rotate clock\n",
    "    seq_rotate_clock = iaa.Sequential([\n",
    "        iaa.Affine(rotate=(5, 20)),\n",
    "    ])\n",
    "    #rotate anticlock\n",
    "    seq_rotate_anticlock = iaa.Sequential([\n",
    "        iaa.Affine(rotate=(-20, -5)),\n",
    "    ])\n",
    "\n",
    "    # apply augmentation\n",
    "    #lrflip\n",
    "    train_imgs_array_aug_lrflip, train_kp_array_aug_lrflip_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_lrflip)\n",
    "    # rotate clock\n",
    "    train_imgs_array_aug_rclock, train_kp_array_aug_rclock_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_rotate_clock)\n",
    "    #rotate anticlock\n",
    "    train_imgs_array_aug_ranticlock, train_kp_array_aug_ranticlock_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_rotate_anticlock)\n",
    "    #translat\n",
    "    train_imgs_array_aug_trans, train_kp_array_aug_trans = apply_aug_translate(train_imgs_array, train_kp_array_abs)\n",
    "\n",
    "    # norm the aug kp\n",
    "    #lrflip\n",
    "    train_kp_array_aug_lrflip_norm = norm_keypoints_arr(train_kp_array_aug_lrflip_abs, train_imgs_array_aug_lrflip)  \n",
    "    # rotate clock\n",
    "    train_kp_array_aug_rclock_norm = norm_keypoints_arr(train_kp_array_aug_rclock_abs, train_imgs_array_aug_rclock)\n",
    "    #rotate anticlock\n",
    "    train_kp_array_aug_ranticlock_norm = norm_keypoints_arr(train_kp_array_aug_ranticlock_abs, train_imgs_array_aug_ranticlock)\n",
    "    #translat\n",
    "    train_kp_array_aug_trans_norm = norm_keypoints_arr(train_kp_array_aug_trans, train_imgs_array_aug_trans)\n",
    "\n",
    "    # combine augmented arrays to original array\n",
    "    #save to image array\n",
    "    train_imgs_array_aug = np.concatenate((train_imgs_array, train_imgs_array_aug_lrflip), axis=0)\n",
    "    train_imgs_array_aug = np.concatenate((train_imgs_array_aug, train_imgs_array_aug_rclock), axis=0)\n",
    "    train_imgs_array_aug = np.concatenate((train_imgs_array_aug, train_imgs_array_aug_ranticlock), axis=0)\n",
    "    train_imgs_array_aug = np.concatenate((train_imgs_array_aug, train_imgs_array_aug_trans), axis=0)\n",
    "    #save to kp array\n",
    "    train_kp_array_aug = np.concatenate((train_kp_array, train_kp_array_aug_lrflip_norm), axis=0)\n",
    "    train_kp_array_aug = np.concatenate((train_kp_array_aug, train_kp_array_aug_rclock_norm), axis=0)\n",
    "    train_kp_array_aug = np.concatenate((train_kp_array_aug, train_kp_array_aug_ranticlock_norm), axis=0)\n",
    "    train_kp_array_aug = np.concatenate((train_kp_array_aug, train_kp_array_aug_trans_norm), axis=0)\n",
    "\n",
    "    if aug == 2:\n",
    "        #put additional augmentations here and then concat the arrays\n",
    "        pass\n",
    "\n",
    "    return train_imgs_array_aug, train_kp_array_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset, augmentation, crop_extension):\n",
    "\n",
    "    print('laoding data ...')\n",
    "\n",
    "    DATA_PARENT_PATH = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/'\n",
    "\n",
    "    if dataset == 1: # Simple dataset \n",
    "\n",
    "        # variables\n",
    "        dataset_name = 'PE_Simple'\n",
    "        crop_extension = '_crop_220x220.jpg'# cropsize extension\n",
    "\n",
    "        # loading ids to a list\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_test_bbox.txt'\n",
    "        ids_test_bbox = load_file_to_list(path)\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_val_bbox.txt'\n",
    "        ids_val_bbox=load_file_to_list(path)\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_train_bbox.txt'\n",
    "        ids_train_bbox=load_file_to_list(path)\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_test.txt'\n",
    "        ids_test=load_file_to_list(path)\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_val.txt'\n",
    "        ids_val=load_file_to_list(path)\n",
    "        path = DATA_PARENT_PATH + dataset_name + '/ids_train.txt'\n",
    "        ids_train=load_file_to_list(path)\n",
    "\n",
    "        # load image data to array \n",
    "        img_dir = '/images'\n",
    "        path = DATA_PARENT_PATH + dataset_name + img_dir + '/test'\n",
    "        test_imgs_array = load_image_data(ids_test_bbox, path, crop_extension)\n",
    "        path = DATA_PARENT_PATH + dataset_name + img_dir + '/val'\n",
    "        val_imgs_array = load_image_data(ids_val_bbox, path, crop_extension)\n",
    "        path = DATA_PARENT_PATH + dataset_name + img_dir + '/train'\n",
    "        train_imgs_array = load_image_data(ids_train_bbox, path, crop_extension)\n",
    "\n",
    "        # load annoation to df and set datatyoes\n",
    "        anno_dir = '/annotation'\n",
    "        path = DATA_PARENT_PATH + dataset_name + anno_dir + '/test_annotation_simple.json'\n",
    "        df_full_annotation_norm_test = json_to_df(path)\n",
    "        df_full_annotation_norm_test = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_test)\n",
    "        path = DATA_PARENT_PATH + dataset_name + anno_dir + '/val_annotation_simple.json'\n",
    "        df_full_annotation_norm_val = json_to_df(path)\n",
    "        df_full_annotation_norm_val = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_val)\n",
    "        path = DATA_PARENT_PATH + dataset_name + anno_dir + '/train_annotation_simple.json'\n",
    "        df_full_annotation_norm_train = json_to_df(path)\n",
    "        df_full_annotation_norm_train = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_train)\n",
    "\n",
    "        # create lists with col names (potentially remove)\n",
    "        id_cols = df_full_annotation_norm_test.iloc[:, :3].columns.to_list()\n",
    "        bbox_cols = df_full_annotation_norm_test.iloc[:, 3:7].columns.to_list()\n",
    "        kp_cols = df_full_annotation_norm_test.iloc[:, 7:23].columns.to_list()\n",
    "\n",
    "        # load the kps to arrays\n",
    "        test_kp_array = create_data_lists(df_full_annotation_norm_test, kp_cols)\n",
    "        val_kp_array = create_data_lists(df_full_annotation_norm_val, kp_cols)\n",
    "        train_kp_array = create_data_lists(df_full_annotation_norm_train, kp_cols)\n",
    "\n",
    "        if augmentation > 1: \n",
    "            train_imgs_array, train_kp_array = augment_data(augmentation, train_imgs_array, train_kp_array)\n",
    "        \n",
    "        train_kp_array, _ = replace_out_of_img_kp(train_kp_array)\n",
    "\n",
    "        return train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model, dataset, aug, optimizer, lr, batch_size, num_epochs, crop_extension):\n",
    "\n",
    "    print('training ...')\n",
    "\n",
    "    # load data\n",
    "    train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array = load_data(dataset, aug, crop_extension)\n",
    "\n",
    "    # define model\n",
    "    if model == 1:\n",
    "        model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "        descriptor = 'DeepPose'\n",
    "    \n",
    "    if dataset == 1:\n",
    "        descriptor = descriptor + '_Simple'\n",
    "    \n",
    "    if aug == 1:\n",
    "        descriptor = descriptor + '_noAug'\n",
    "    if aug == 2:\n",
    "        descriptor = descriptor + '_simpleAug'\n",
    "    if aug == 3:\n",
    "        descriptor = descriptor + '_largeAug'\n",
    "\n",
    "    if optimizer == 1:\n",
    "        # Define your optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        descriptor = descriptor + '_Adam'\n",
    "    \n",
    "    # get naming convention\n",
    "    descriptor = descriptor + '_' + str(lr) + '_' + str(batch_size) + '_' + str(num_epochs)\n",
    "    # create save dir\n",
    "    save_dir = create_timestamped_dir(descriptor)\n",
    "    \n",
    "    # create tensors from arrays and load to a PT dataloader\n",
    "    #train\n",
    "    train_imgs_tensor = torch.tensor(train_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "    train_kp_tensor = torch.tensor(train_kp_array, dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(train_imgs_tensor, train_kp_tensor)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    #val\n",
    "    val_imgs_tensor = torch.tensor(val_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "    val_kp_tensor = torch.tensor(val_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "    val_dataset = TensorDataset(val_imgs_tensor, val_kp_tensor)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "    #test\n",
    "    test_imgs_tensor = torch.tensor(test_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "    test_kp_tensor = torch.tensor(test_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "    test_dataset = TensorDataset(test_imgs_tensor, test_kp_tensor)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "        \n",
    "    # Lists to store the training and validation loss for each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_pck_list = []\n",
    "    val_pck_list = []\n",
    "    best_val_loss = None\n",
    "    best_val_pck = None\n",
    "    model_save_path_best_val_loss = None\n",
    "    model_save_path_best_val_pck = None\n",
    "\n",
    "    print('start training loop ...')\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        running_pck_01 = 0.0\n",
    "        running_pck_val_01 = 0.0\n",
    "\n",
    "        # loop for a single batch\n",
    "        for batch_images, batch_keypoints in train_dataloader:\n",
    "\n",
    "            # Move the data to the GPU\n",
    "            batch_images = batch_images.to('cuda')\n",
    "            batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_images)\n",
    "            # Compute the loss\n",
    "            loss = masked_mse(batch_keypoints, outputs)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # compute metrics\n",
    "            pck_01 = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "\n",
    "            # accumulate metrics\n",
    "            running_pck_01 += pck_01.item()\n",
    "\n",
    "        # calculate average loss for epoch\n",
    "        avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "        # calculate average pck for epoch\n",
    "        avg_pck_01 = running_pck_01 / len(train_dataloader)\n",
    "    \n",
    "        # populate train losses list for evaluation\n",
    "        train_losses.append(avg_train_loss)\n",
    "        # populate train pck list for evaluation\n",
    "        train_pck_list.append(avg_pck_01)\n",
    "\n",
    "        # evalution for training phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad(): # dont update weights\n",
    "\n",
    "            # evaluation loop for a single batch\n",
    "            for batch_images, batch_keypoints in val_dataloader:\n",
    "\n",
    "                # Move the data to the GPU\n",
    "                batch_images = batch_images.to('cuda')\n",
    "                batch_keypoints = batch_keypoints.to('cuda')\n",
    "                \n",
    "                # forward pass\n",
    "                outputs = model(batch_images)\n",
    "                # Compute the loss\n",
    "                loss = masked_mse(batch_keypoints, outputs)\n",
    "\n",
    "                # Accumulate the loss\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # compute metrics\n",
    "                pck_01_val = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "\n",
    "                # accumulate metrics\n",
    "                running_pck_val_01 += pck_01_val.item()\n",
    "\n",
    "        # calculate average loss for epoch\n",
    "        avg_val_loss = running_val_loss / len(val_dataloader)\n",
    "        # calculate average pck for epoch\n",
    "        avg_val_pck_01 = running_pck_val_01 / len(val_dataloader)\n",
    "\n",
    "        # populate train losses list for evaluation\n",
    "        val_losses.append(avg_val_loss)\n",
    "        # populate train pck list for evaluation\n",
    "        val_pck_list.append(avg_val_pck_01)\n",
    "\n",
    "        # save best performing models based on the PCK and loss as well as the stats\n",
    "        best_val_loss, best_val_pck, model_save_path_best_val_loss_temp, model_save_path_best_val_pck_temp, _ = save_stats_and_models(\n",
    "        model, epoch + 1, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "        best_val_loss, best_val_pck)\n",
    "\n",
    "        if model_save_path_best_val_loss_temp:\n",
    "            model_save_path_best_val_loss = model_save_path_best_val_loss_temp\n",
    "        \n",
    "        if model_save_path_best_val_pck_temp:\n",
    "            model_save_path_best_val_pck = model_save_path_best_val_pck_temp\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train PCK0.1: {avg_pck_01:.4f}, Val PCK0.1: {avg_val_pck_01:.4f}')\n",
    "        \n",
    "    best_val_loss, best_val_pck, _, _, final_model_path = save_stats_and_models(model, num_epochs, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "                    best_val_loss, best_val_pck, final_model=True, train_loss_list=train_losses, \n",
    "                    val_loss_list=val_losses, train_pck_list=train_pck_list, val_pck_list=val_pck_list)\n",
    "    \n",
    "    return save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader,\\\n",
    "        train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_eval(model_path, model_class, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a PyTorch model from a .pth file.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path (str): The path to the .pth model file.\n",
    "    - model_class (torch.nn.Module): The class of the model to instantiate.\n",
    "    - device (str): The device to load the model onto ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module): The loaded PyTorch model.\n",
    "    \"\"\"\n",
    "    # Instantiate the model class\n",
    "    model = model_class().to(device)\n",
    "    \n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pck(model, dataloader, threshold=0.2, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluates the average PCK over an entire dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model to evaluate.\n",
    "    - dataloader: A DataLoader providing the data to evaluate on.\n",
    "    - threshold: The PCK threshold distance (default is 0.2).\n",
    "    - device: The device to perform computations on (default is 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - average_pck: The average PCK over the entire dataset.\n",
    "    \"\"\"\n",
    "    total_pck = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_images, batch_keypoints in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_keypoints = batch_keypoints.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(batch_images)\n",
    "\n",
    "            # Compute PCK for the current batch\n",
    "            pck = pck_metric(batch_keypoints, outputs, threshold)\n",
    "            total_pck += pck.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    average_pck = total_pck / num_batches\n",
    "    \n",
    "    return average_pck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pck_per_keypoint(model, dataloader, num_keypoints=8, threshold=0.2, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluates the average PCK for each keypoint individually.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model to evaluate.\n",
    "    - dataloader: A DataLoader providing the data to evaluate on.\n",
    "    - num_keypoints: The number of keypoints in the dataset.\n",
    "    - threshold: The PCK threshold distance (default is 0.2).\n",
    "    - device: The device to perform computations on (default is 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - keypoint_pcks: A list of average PCK values for each keypoint.\n",
    "    \"\"\"\n",
    "    #model.eval()  # Set the model to evaluation mode\n",
    "    total_pck_per_keypoint = torch.zeros(num_keypoints, device=device)\n",
    "    total_visable_kp = torch.zeros(num_keypoints, device=device)\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_images, batch_keypoints in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_keypoints = batch_keypoints.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(batch_images)\n",
    "\n",
    "            # Create a mask for visible keypoints\n",
    "            mask = (batch_keypoints != -10.0).float().to(device)\n",
    "\n",
    "            # Compute the Euclidean distances for each keypoint\n",
    "            distances = torch.sqrt((outputs[:, ::2] - batch_keypoints[:, ::2]) ** 2 +\n",
    "                                   (outputs[:, 1::2] - batch_keypoints[:, 1::2]) ** 2)\n",
    "\n",
    "            # Normalize the distances\n",
    "            Norm_head_lowerbody = torch.sqrt((batch_keypoints[:, 0] - batch_keypoints[:,10]) ** 2 +\n",
    "                                             (batch_keypoints[:, 1] - batch_keypoints[:, 11]) ** 2)\n",
    "            normalized_distances = distances / Norm_head_lowerbody[:, None]\n",
    "\n",
    "            # Compute correct keypoints (distance <= threshold) for each keypoint\n",
    "            correct_keypoints_per_keypoint = (normalized_distances <= threshold).float() * mask[:, ::2]\n",
    "\n",
    "            # Accumulate PCK per keypoint\n",
    "            total_pck_per_keypoint += correct_keypoints_per_keypoint.sum(dim=0)\n",
    "            total_visable_kp += mask[:, ::2].sum(dim=0)\n",
    "            num_batches += 1\n",
    "\n",
    "    # Average PCK per keypoint\n",
    "    #keypoint_pcks = (total_pck_per_keypoint / mask[:, ::2].sum(dim=0)).cpu().numpy()\n",
    "    keypoint_pcks = (total_pck_per_keypoint / total_visable_kp).cpu().numpy()\n",
    "    return keypoint_pcks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pck_evaluation(model, val_dataloader, test_dataloader):\n",
    "\n",
    "    print('calculating PCK ...')\n",
    "\n",
    "    # create lists for pck at different thresholds\n",
    "    avg_pck_test_list = []\n",
    "    avg_pck_val_list = []\n",
    "    avg_pck_per_kp_val_list = []\n",
    "    avg_pck_per_kp_test_list = []\n",
    "    \n",
    "    # create a for loop to get PCK at 0.01 to 0.2\n",
    "    for i in range (1, 21):\n",
    "\n",
    "        # get pck threshold\n",
    "        pck_threshold = (i/100)\n",
    "\n",
    "        # calculate average pck\n",
    "        avg_pck_val = evaluate_pck(model, val_dataloader, threshold=pck_threshold)\n",
    "        avg_pck_test = evaluate_pck(model, test_dataloader, threshold=pck_threshold)\n",
    "\n",
    "        # calculate average pck per kp\n",
    "        avg_pck_per_kp_val = evaluate_pck_per_keypoint(model, val_dataloader, threshold=pck_threshold)\n",
    "        avg_pck_per_kp_test = evaluate_pck_per_keypoint(model, test_dataloader, threshold=pck_threshold)\n",
    "\n",
    "        if i == 5:\n",
    "            # capture pck@0.05\n",
    "            avg_pck_val_005 = avg_pck_val\n",
    "            avg_pck_test_005 = avg_pck_test\n",
    "\n",
    "        if i == 10:\n",
    "            # capture pck@0.1\n",
    "            avg_pck_val_01 = avg_pck_val\n",
    "            avg_pck_test_01 = avg_pck_test\n",
    "\n",
    "        if i == 20:\n",
    "            # capture pck@0.2\n",
    "            avg_pck_val_02 = avg_pck_val\n",
    "            avg_pck_test_02 = avg_pck_test\n",
    "\n",
    "        # save to lists\n",
    "        avg_pck_test_list.append(avg_pck_test)\n",
    "        avg_pck_val_list.append(avg_pck_val)\n",
    "        avg_pck_per_kp_val_list.append(avg_pck_per_kp_val)\n",
    "        avg_pck_per_kp_test_list.append(avg_pck_per_kp_test)\n",
    "\n",
    "    return avg_pck_val_list, avg_pck_test_list, avg_pck_per_kp_val_list, avg_pck_per_kp_test_list, \\\n",
    "        avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_inference_time(model, dummy_input):\n",
    "        \n",
    "    # Warm up GPU to avoid initial overheads\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "\n",
    "    # Synchronize GPU and measure the time\n",
    "    torch.cuda.synchronize()  # Ensure all previous CUDA operations are complete\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(dummy_input)\n",
    "    torch.cuda.synchronize()  # Wait for all CUDA operations to finish\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    return (end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_inference_time(model, dummy_input):\n",
    "\n",
    "    # move model and dummy data to cpu\n",
    "    model.to('cpu')\n",
    "    dummy_input = dummy_input.to('cpu')\n",
    "\n",
    "    # Warm up to avoid initial overheads affecting the time\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "\n",
    "    # Time the forward pass\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(dummy_input)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # move model and dummy iput back to gpu\n",
    "    model.to('cuda')\n",
    "    dummy_input.to('cuda')\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    return (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pck_to_dict(arr_list):\n",
    "        \n",
    "    # Define the keys for the dictionary\n",
    "    keys = ['head', 'beak', 'body_top', 'rflipper', 'lflipper', 'body_bottom', 'rfoot', 'lfoot']\n",
    "\n",
    "    # Initialize the dictionary with empty lists for each key\n",
    "    results_dict = {key: [] for key in keys}\n",
    "\n",
    "    # Populate the dictionary with values from the arrays\n",
    "    for array in arr_list:\n",
    "        for i, key in enumerate(keys):\n",
    "            results_dict[key].append(array[i])\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results_to_dict(save_dir, avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02,\n",
    "                         total_params, total_flops, gpu_inf_time, cpu_inf_time, param_dict, flops_extend, avg_pck_test_list, \n",
    "                         avg_pck_per_kp_test_list, avg_pck_val_list, avg_pck_per_kp_val_list, num_train_imgs, num_val_imgs, num_test_imgs):\n",
    "\n",
    "    description = save_dir.split('/')[-1]\n",
    "    #print(description)\n",
    "\n",
    "    #avg_pck_test_dict = load_pck_to_dict(avg_pck_test_list)\n",
    "    avg_pck_per_kp_test_dict = load_pck_to_dict(avg_pck_per_kp_test_list)\n",
    "    #avg_pck_val_dict = load_pck_to_dict(avg_pck_val_list)\n",
    "    avg_pck_per_kp_val_dict = load_pck_to_dict(avg_pck_per_kp_val_list)\n",
    "\n",
    "    results_dict = {\n",
    "    'description': '',  # Placeholder for a string description\n",
    "    'pck005': None,  # Placeholder for PCK@0.05 variable\n",
    "    'pck01': None,  # Placeholder for PCK@0.1 variable\n",
    "    'pck02': None,  # Placeholder for PCK@0.2 variable\n",
    "    'total_params': None,  # Placeholder for total parameters variable\n",
    "    'GFLOPs': None,  # Placeholder for GFLOPs variable\n",
    "    'GPU_inf(ms)': None,  # Placeholder for GPU inference time variable\n",
    "    'CPU_inf(ms)': None,  # Placeholder for CPU inference time variable\n",
    "    'param_dict': {},  # Placeholder for parameter dictionary\n",
    "    'flops_dict': {},  # Placeholder for FLOPs dictionary\n",
    "    'PCK001-02': [],  # Placeholder for PCK@0.01-0.2 list\n",
    "    'PCK001-02_per_kp': {},  # Placeholder for PCK per joint dictionary\n",
    "    'val_PCK001-02': [],  # Placeholder for PCK@0.01-0.2 list\n",
    "    'val_PCK001-02_per_kp': {},  # Placeholder for PCK per joint dictionary\n",
    "    'val_pck005': None,  # Placeholder for PCK@0.05 variable\n",
    "    'val_pck01': None,  # Placeholder for PCK@0.1 variable\n",
    "    'val_pck02': None,  # Placeholder for PCK@0.2 variable\n",
    "    'num_train_imgs': None, # number of train imgs\n",
    "    'num_val_imgs': None, # number of train imgs\n",
    "    'num_test_imgs': None, # number of train imgs\n",
    "}\n",
    "    \n",
    "    results_dict['description'] = description\n",
    "    results_dict['pck005'] = avg_pck_test_005 \n",
    "    results_dict['pck01'] = avg_pck_test_01  \n",
    "    results_dict['pck02'] = avg_pck_test_02 \n",
    "    results_dict['total_params'] = total_params  \n",
    "    results_dict['GFLOPs'] = (total_flops/1e9)\n",
    "    results_dict['GPU_inf(ms)'] = gpu_inf_time*1000  \n",
    "    results_dict['CPU_inf(ms)'] = cpu_inf_time*1000  \n",
    "    results_dict['param_dict'] = param_dict  \n",
    "    results_dict['flops_dict'] = flops_extend \n",
    "    results_dict['PCK001-02'] = avg_pck_test_list\n",
    "    results_dict['PCK001-02_per_kp'] = avg_pck_per_kp_test_dict\n",
    "    results_dict['val_PCK001-02'] = avg_pck_val_list\n",
    "    results_dict['val_PCK001-02_per_kp'] = avg_pck_per_kp_val_dict\n",
    "    results_dict['val_pck005'] = avg_pck_val_005 \n",
    "    results_dict['val_pck01'] = avg_pck_val_01  \n",
    "    results_dict['val_pck02'] = avg_pck_val_02\n",
    "    results_dict['num_train_imgs'] = num_train_imgs\n",
    "    results_dict['num_val_imgs'] = num_val_imgs\n",
    "    results_dict['num_test_imgs'] = num_test_imgs\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numpy_types(obj):\n",
    "    \"\"\"\n",
    "    Convert numpy types in an object to their native Python equivalents.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()  # Convert numpy arrays to lists\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()  # Convert numpy scalars to native Python types\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(element) for element in obj]\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_json(data_dict, save_dir):\n",
    "    \"\"\"\n",
    "    Saves a dictionary to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict: The dictionary to save.\n",
    "    - save_dir: The directory path where the JSON file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Convert any numpy types in the dictionary to native Python types\n",
    "    data_dict = convert_numpy_types(data_dict)\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "    # json name\n",
    "    results_json = save_dir+'/results.json'\n",
    "    \n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(results_json, 'w') as json_file:\n",
    "        json.dump(data_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_comparison(img, pred_keypoints, true_keypoints, save_dir, img_num, nkeypoints=8, keypoint_labels=None, connections = [(0, 1), (0, 2), (2, 3), (2, 4), (2, 5), (5, 6), (5, 7)]):\n",
    "#     \"\"\"\n",
    "#     Plots predicted keypoints vs. ground truth keypoints on the same image.\n",
    "\n",
    "#     Parameters:\n",
    "#     - img: The image on which to plot the keypoints.\n",
    "#     - pred_keypoints: The predicted keypoints (flattened x, y coordinates).\n",
    "#     - true_keypoints: The ground truth keypoints (flattened x, y coordinates).\n",
    "#     - save_dir: Directory to save the result to\n",
    "#     - img_num: image number that is getting compared\n",
    "#     - nkeypoints:  Optional The number of keypoints (default=8).\n",
    "#     - keypoint_labels: Optional list of keypoint labels to display next to the keypoints.\n",
    "#     - connections: OPtional list of tupels defining the connections between kps\n",
    "#     \"\"\"\n",
    "\n",
    "#     fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "#     plt.imshow(img)\n",
    "    \n",
    "#     # Extract x and y coordinates for predicted keypoints\n",
    "#     pred_x_keypoints = pred_keypoints[::2]\n",
    "#     pred_y_keypoints = pred_keypoints[1::2]\n",
    "    \n",
    "#     # Extract x and y coordinates for ground truth keypoints\n",
    "#     true_x_keypoints = true_keypoints[::2]\n",
    "#     true_y_keypoints = true_keypoints[1::2]\n",
    "\n",
    "#     # Plot skeleton for true keypoints\n",
    "#     for (i, j) in connections:\n",
    "#         plt.plot([true_x_keypoints[i], true_x_keypoints[j]], \n",
    "#                  [true_y_keypoints[i], true_y_keypoints[j]], \n",
    "#                  'r-', linewidth=1)\n",
    "\n",
    "#     # Plot skeleton for predicted keypoints\n",
    "#     for (i, j) in connections:\n",
    "#         plt.plot([pred_x_keypoints[i], pred_x_keypoints[j]], \n",
    "#                  [pred_y_keypoints[i], pred_y_keypoints[j]], \n",
    "#                  'g-', linewidth=1)\n",
    "    \n",
    "#     # Plot predicted keypoints\n",
    "#     plt.scatter(pred_x_keypoints, pred_y_keypoints, marker='o', c='g', s=100, label='Predicted', edgecolor='black')\n",
    "    \n",
    "#     # Plot ground truth keypoints\n",
    "#     plt.scatter(true_x_keypoints, true_y_keypoints, marker='x', c='r', s=100, label='Ground Truth')\n",
    "    \n",
    "#     # If labels are provided, add them to the plot\n",
    "#     if keypoint_labels is not None:\n",
    "#         for i, (x, y) in enumerate(zip(true_x_keypoints, true_y_keypoints)):\n",
    "#             plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "#                      bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "#     # If labels are provided, add them to the plot\n",
    "#     if keypoint_labels is not None:\n",
    "#         for i, (x, y) in enumerate(zip(pred_x_keypoints, pred_y_keypoints)):\n",
    "#             plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "#                      bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "#     # Add a legend to differentiate between predicted and ground truth keypoints\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Save the plot\n",
    "#     plot_path = os.path.join(save_dir, f'Comparison of predicted and ground truth for img {img_num}.png')\n",
    "#     plt.savefig(plot_path)\n",
    "#     #print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_and_plot(model_path, start_img, end_img, model_class=DeepPoseModel, device='cuda'):\n",
    "#     \"\"\"\n",
    "#     Loads a model, predicts keypoints for a range of images, and plots the predicted keypoints \n",
    "#     versus ground truth keypoints on the same image. The images with plotted keypoints are then \n",
    "#     saved to a specified directory.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model_path: The file path to the saved model's .pth file.\n",
    "#     - start_img: The starting index of the images in the validation set to process.\n",
    "#     - end_img: The ending index of the images in the validation set to process (exclusive).\n",
    "#     - model_class: Optional. The class of the model architecture to instantiate and load \n",
    "#                    with the saved weights (default=DeepPoseModel).\n",
    "#     - device: Optional. The device to run the model on ('cuda' for GPU, 'cpu' for CPU; default='cuda').\n",
    "    \n",
    "#     Returns:\n",
    "#     - None. The function saves the images with plotted keypoints to the directory derived from the \n",
    "#             model path.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # get img lists\n",
    "#     img_arr = val_imgs_array[start_img:end_img,:,:,:]\n",
    "#     true_kp_arr = val_kp_array[start_img:end_img,:]\n",
    "\n",
    "#     # Load the model\n",
    "#     model = load_model(model_path, model_class, device=device)\n",
    "\n",
    "#     # Get predictions\n",
    "#     predictions = predict(model, img_arr, device=device)\n",
    "#     #print(predictions)\n",
    "\n",
    "#     # DeNorm predictions \n",
    "#     predictions_abs = []\n",
    "#     true_kp_arr_abs = []\n",
    "#     for i, kp in enumerate(predictions):\n",
    "\n",
    "#         img_size = img_arr[i].shape\n",
    "#         #print(img_size)\n",
    "\n",
    "#         #unNorm each prediction\n",
    "#         true_kp_abs, missing_kp = unnorm_keypoints(img_size, true_kp_arr[i])\n",
    "#         #print(missing_kp)\n",
    "#         kp_abs, missing_kp = unnorm_keypoints(img_size, kp, kp_to_null=missing_kp)\n",
    "#         #print(missing_kp)\n",
    "        \n",
    "\n",
    "#         # save result to new list\n",
    "#         predictions_abs.append(kp_abs)\n",
    "#         true_kp_arr_abs.append(true_kp_abs)\n",
    "\n",
    "#     #print(predictions_abs)\n",
    "\n",
    "#     # get the save directory parent (where the images will be saved)\n",
    "#     save_dir = model_path.rsplit('/',1)[0]\n",
    "\n",
    "#     # labels\n",
    "#     labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "\n",
    "#     for i, kp in enumerate(predictions_abs):\n",
    "\n",
    "#         plot_comparison(img_arr[i], predictions_abs[i], true_kp_arr_abs[i], save_dir, img_num=i+start_img)#, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(model, images, img_is_tensor=False, device='cuda'):\n",
    "#     \"\"\"\n",
    "#     Generates predictions from a PyTorch model given an array of images.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model (torch.nn.Module): The PyTorch model to use for predictions.\n",
    "#     - images (np.array): Array of images (e.g., shape: (num_images, 220, 220, 3)).\n",
    "#     - device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "#     Returns:\n",
    "#     - predictions (np.array): Array of predictions (e.g., keypoints for each image).\n",
    "#     \"\"\"\n",
    "#     # Convert images to PyTorch tensor and move to the specified device\n",
    "#     if not img_is_tensor:\n",
    "#         images_tensor = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "#     # Forward pass through the model to get predictions\n",
    "#     with torch.no_grad():\n",
    "#         predictions = model(images_tensor)\n",
    "    \n",
    "#     # Convert predictions back to a NumPy array and move to CPU if necessary\n",
    "#     predictions = predictions.cpu().numpy() if device == 'cuda' else predictions.numpy()\n",
    "    \n",
    "#     return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, images, img_is_tensor=False, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generates predictions from a PyTorch model given an array of images.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to use for predictions.\n",
    "    - images (np.array): Array of images (e.g., shape: (num_images, 220, 220, 3)).\n",
    "    - device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - predictions (np.array): Array of predictions (e.g., keypoints for each image).\n",
    "    \"\"\"\n",
    "    # Convert images to PyTorch tensor and move to the specified device\n",
    "    if not img_is_tensor:\n",
    "        images_tensor = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "    # Forward pass through the model to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images_tensor)\n",
    "    \n",
    "    # Convert predictions back to a NumPy array and move to CPU if necessary\n",
    "    predictions = predictions.cpu().numpy() if device == 'cuda' else predictions.numpy()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(img, pred_keypoints, true_keypoints, save_dir, img_num, nkeypoints=8, keypoint_labels=None, connections = [(0, 1), (0, 2), (2, 3), (2, 4), (2, 5), (5, 6), (5, 7)]):\n",
    "    \"\"\"\n",
    "    Plots predicted keypoints vs. ground truth keypoints on the same image.\n",
    "\n",
    "    Parameters:\n",
    "    - img: The image on which to plot the keypoints.\n",
    "    - pred_keypoints: The predicted keypoints (flattened x, y coordinates).\n",
    "    - true_keypoints: The ground truth keypoints (flattened x, y coordinates).\n",
    "    - save_dir: Directory to save the result to\n",
    "    - img_num: image number that is getting compared\n",
    "    - nkeypoints:  Optional The number of keypoints (default=8).\n",
    "    - keypoint_labels: Optional list of keypoint labels to display next to the keypoints.\n",
    "    - connections: OPtional list of tupels defining the connections between kps\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Extract x and y coordinates for predicted keypoints\n",
    "    pred_x_keypoints = pred_keypoints[::2]\n",
    "    pred_y_keypoints = pred_keypoints[1::2]\n",
    "    \n",
    "    # Extract x and y coordinates for ground truth keypoints\n",
    "    true_x_keypoints = true_keypoints[::2]\n",
    "    true_y_keypoints = true_keypoints[1::2]\n",
    "\n",
    "    # Plot skeleton for true keypoints\n",
    "    for (i, j) in connections:\n",
    "        plt.plot([true_x_keypoints[i], true_x_keypoints[j]], \n",
    "                 [true_y_keypoints[i], true_y_keypoints[j]], \n",
    "                 'r-', linewidth=1)\n",
    "\n",
    "    # Plot skeleton for predicted keypoints\n",
    "    for (i, j) in connections:\n",
    "        plt.plot([pred_x_keypoints[i], pred_x_keypoints[j]], \n",
    "                 [pred_y_keypoints[i], pred_y_keypoints[j]], \n",
    "                 'g-', linewidth=1)\n",
    "    \n",
    "    # Plot predicted keypoints\n",
    "    plt.scatter(pred_x_keypoints, pred_y_keypoints, marker='o', c='g', s=100, label='Predicted', edgecolor='black')\n",
    "    \n",
    "    # Plot ground truth keypoints\n",
    "    plt.scatter(true_x_keypoints, true_y_keypoints, marker='x', c='r', s=100, label='Ground Truth')\n",
    "    \n",
    "    # If labels are provided, add them to the plot\n",
    "    if keypoint_labels is not None:\n",
    "        for i, (x, y) in enumerate(zip(true_x_keypoints, true_y_keypoints)):\n",
    "            plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "                     bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "    if keypoint_labels is not None:\n",
    "        for i, (x, y) in enumerate(zip(pred_x_keypoints, pred_y_keypoints)):\n",
    "            plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "                     bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # Add a legend to differentiate between predicted and ground truth keypoints\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the plot\n",
    "    part = save_dir.split('/')[-1]\n",
    "    plot_path = os.path.join(save_dir, f'{part}_vs_GT_img_{img_num}.png')\n",
    "    plt.savefig(plot_path)\n",
    "    #print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(model, img_arr, kp_arr, start_img, end_img, save_dir, keypoint_display=False, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a model, predicts keypoints for a range of images, and plots the predicted keypoints \n",
    "    versus ground truth keypoints on the same image. The images with plotted keypoints are then \n",
    "    saved to a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path: The file path to the saved model's .pth file.\n",
    "    - start_img: The starting index of the images in the validation set to process.\n",
    "    - end_img: The ending index of the images in the validation set to process (exclusive).\n",
    "    - model_class: Optional. The class of the model architecture to instantiate and load \n",
    "                   with the saved weights (default=DeepPoseModel).\n",
    "    - device: Optional. The device to run the model on ('cuda' for GPU, 'cpu' for CPU; default='cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - None. The function saves the images with plotted keypoints to the directory derived from the \n",
    "            model path.\n",
    "    \"\"\"\n",
    "\n",
    "    # get img lists\n",
    "    img_arr = img_arr[start_img:end_img,:,:,:]\n",
    "    true_kp_arr = kp_arr[start_img:end_img,:]\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = predict(model, img_arr, device=device)\n",
    "    #print(predictions)\n",
    "\n",
    "    # DeNorm predictions \n",
    "    predictions_abs = []\n",
    "    true_kp_arr_abs = []\n",
    "    for i, kp in enumerate(predictions):\n",
    "\n",
    "        img_size = img_arr[i].shape\n",
    "        #print(img_size)\n",
    "\n",
    "        #unNorm each prediction\n",
    "        true_kp_abs, missing_kp = unnorm_keypoints(img_size, true_kp_arr[i])\n",
    "        #print(missing_kp)\n",
    "        kp_abs, missing_kp = unnorm_keypoints(img_size, kp, kp_to_null=missing_kp)\n",
    "        #print(missing_kp)\n",
    "        \n",
    "\n",
    "        # save result to new list\n",
    "        predictions_abs.append(kp_abs)\n",
    "        true_kp_arr_abs.append(true_kp_abs)\n",
    "\n",
    "    # labels\n",
    "    labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "\n",
    "    for i, kp in enumerate(predictions_abs):\n",
    "\n",
    "        if keypoint_display:\n",
    "            plot_comparison(img_arr[i], predictions_abs[i], true_kp_arr_abs[i], save_dir, img_num=i+start_img, keypoint_labels=labels)\n",
    "        else:\n",
    "            plot_comparison(img_arr[i], predictions_abs[i], true_kp_arr_abs[i], save_dir, img_num=i+start_img)#, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not running directly off the back of training then set this to true\n",
    "# straight_eval = True\n",
    "def run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=None, val_dataloader=None, \\\n",
    "             test_dataloader=None, train_imgs_array=None, val_imgs_array=None, test_imgs_array=None, val_kp_array=None, \\\n",
    "                test_kp_array=None, straight_eval=True):\n",
    "    if straight_eval:\n",
    "        # run just evalutation\n",
    "\n",
    "        # set model paths\n",
    "        save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02'\n",
    "        model_save_path_best_val_loss = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02/best_val_loss_model_epoch_22_PCK_0.6433_loss_0.0053.pth'\n",
    "        model_save_path_best_val_pck = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02/best_val_loss_model_epoch_22_PCK_0.6433_loss_0.0053.pth'\n",
    "        final_model_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_batch16_2024-08-22_16-34-02/final_model_epoch_30_PCK_0.5835_loss_0.0076.pth'\n",
    "\n",
    "        # load the data\n",
    "        train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array = load_data(1, 1, crop_extension)\n",
    "        #val\n",
    "        val_imgs_tensor = torch.tensor(val_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "        val_kp_tensor = torch.tensor(val_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "        val_dataset = TensorDataset(val_imgs_tensor, val_kp_tensor)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "        #test\n",
    "        test_imgs_tensor = torch.tensor(test_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "        test_kp_tensor = torch.tensor(test_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "        test_dataset = TensorDataset(test_imgs_tensor, test_kp_tensor)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)# shuffle omly needs to be true for traing\n",
    "\n",
    "    # create evaluation dir\n",
    "    save_dir_eval = create_timestamped_dir('/evaluation', save_dir)\n",
    "\n",
    "    if model == 1:\n",
    "        model_class = DeepPoseModel\n",
    "        input_size = (1, 3, 220, 220)#(torch.randn((1,3,220,220)),)\n",
    "\n",
    "    # load model for evaluation\n",
    "    model = load_model_eval(model_save_path_best_val_pck, model_class)\n",
    "\n",
    "    avg_pck_val_list, avg_pck_test_list, avg_pck_per_kp_val_list, avg_pck_per_kp_test_list,\\\n",
    "        avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02 \\\n",
    "        = full_pck_evaluation(model, val_dataloader, test_dataloader)\n",
    "\n",
    "    print('Finding other metrics ...')\n",
    "\n",
    "    # Calculate number of FLOPs\n",
    "    dummy_input = torch.randn(input_size).to('cuda') # move the dummy input to GPU\n",
    "    flops = FlopCountAnalysis(model, dummy_input)\n",
    "    total_flops = flops.total()\n",
    "    flops_extend = flops.by_module_and_operator()\n",
    "    flops_2 = torchprofile.profile_macs(model, dummy_input)\n",
    "    print(f'Total FLOPs: {flops_2}')\n",
    "    print(flop_count_table(flops))\n",
    "\n",
    "    # Calculate the number of params\n",
    "    param_dict = parameter_count(model)\n",
    "    total_params = param_dict['']\n",
    "    # print(total_params)\n",
    "\n",
    "    # Calculate inference time GPU\n",
    "    gpu_inf_time = gpu_inference_time(model, dummy_input)\n",
    "    # print(gpu_inf_time)\n",
    "    # print(gpu_inf_time*1e3)\n",
    "\n",
    "    # Calculate inference time CPU\n",
    "    cpu_inf_time = cpu_inference_time(model, dummy_input)\n",
    "    # print(cpu_inf_time)\n",
    "    # print(cpu_inf_time*1e3)\n",
    "\n",
    "    # load results to a dict\n",
    "    results = load_results_to_dict(save_dir, avg_pck_val_005, avg_pck_test_005, avg_pck_val_01, avg_pck_test_01, avg_pck_val_02, avg_pck_test_02,\\\n",
    "                        total_params, total_flops, gpu_inf_time, cpu_inf_time, param_dict, flops_extend, avg_pck_test_list, \\\n",
    "                        avg_pck_per_kp_test_list, avg_pck_val_list, avg_pck_per_kp_val_list, train_imgs_array.shape[0], \\\n",
    "                        val_imgs_array.shape[0], test_imgs_array.shape[0])\n",
    "\n",
    "    # save \n",
    "    print('saving metrics ...')\n",
    "    save_dict_to_json(results, save_dir_eval)\n",
    "\n",
    "    # plot and save images\n",
    "    print('plotting and saving some result images ...')\n",
    "    # get random images with seed so that it is consistant\n",
    "    # Set a fixed seed for reproducibility\n",
    "    fixed_seed = 42\n",
    "    random.seed(fixed_seed)\n",
    "\n",
    "    # val - create loop to produce and save 5 random images to the save dir\n",
    "    # Generate unique random numbers for validation\n",
    "    val_random_nums = random.sample(range(val_imgs_array.shape[0]), 5)\n",
    "\n",
    "    for i, random_num in enumerate(val_random_nums):\n",
    "\n",
    "        print('VALIDATION', i)\n",
    "        predict_and_plot(model, val_imgs_array, val_kp_array, random_num, random_num+1, save_dir_eval+'/val_predictions')\n",
    "\n",
    "    # test - create loop to produce and save 5 random images to the save dir\n",
    "    # Generate unique random numbers for testing\n",
    "    test_random_nums = random.sample(range(test_imgs_array.shape[0]), 15)\n",
    "    for i, random_num in enumerate(test_random_nums):\n",
    "        print(f'TEST {i}')\n",
    "        # get a random image in the list\n",
    "        predict_and_plot(model, test_imgs_array, test_kp_array, random_num, random_num+1, save_dir_eval+'/test_predictions')\n",
    "\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_and_keypoint(img, keypoints, nkeypoints=8, keypoint_labels=None):\n",
    "  fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "  plt.imshow(img)\n",
    "  #print(keypoints)\n",
    "  x_keypoints = keypoints[::2]\n",
    "  y_keypoints = keypoints[1::2]\n",
    "  #print(x_keypoints)\n",
    "  #print(y_keypoints)\n",
    "  plt.scatter(x_keypoints, y_keypoints, marker='.', c=np.arange(nkeypoints), cmap='jet')\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "  if keypoint_labels is not None:\n",
    "      for i, (x, y) in enumerate(zip(x_keypoints, y_keypoints)):\n",
    "          plt.text(x, y, keypoint_labels[i], fontsize=12, color='white', \n",
    "                    bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_cols(df):\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1. Model input and run train and evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a model\n",
    "model = 1 # 1. DeepPose\n",
    "dataset = 1 # 1. Simple - need to think how to handle crop image size - this should be moved to the model rather maybe\n",
    "augmentation = 2 # 1. no aug, 2. simple aug, 3. large aug*\n",
    "batch_size = 16\n",
    "num_epochs = 300\n",
    "learning_rate = 0.00005\n",
    "optimizer = 1 # 1. Adam\n",
    "crop_extension = '_crop_220x220.jpg'# cropsize extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run just train\n",
    "save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader, \\\n",
    "    train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\\\n",
    "        = run_train(model, dataset, augmentation, optimizer, learning_rate, batch_size, num_epochs, crop_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run just evaluation\n",
    "run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run both train and evaluation\n",
    "\n",
    "# train\n",
    "save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, val_dataloader, test_dataloader, \\\n",
    "    train_imgs_array, val_imgs_array, test_imgs_array, train_kp_array, val_kp_array, test_kp_array\\\n",
    "        = run_train(model, dataset, augmentation, optimizer, learning_rate, batch_size, num_epochs, crop_extension)\n",
    "\n",
    "# evaluate\n",
    "run_eval(save_dir, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path, model=model, val_dataloader=val_dataloader, \\\n",
    "             test_dataloader=test_dataloader, train_imgs_array=train_imgs_array, val_imgs_array=val_imgs_array, test_imgs_array=test_imgs_array,\\\n",
    "                 val_kp_array=val_kp_array, test_kp_array=test_kp_array, straight_eval=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create a DLC model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. create project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/videos\"\n",
      "Created \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data\"\n",
      "Created \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/training-datasets\"\n",
      "Created \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/dlc-models\"\n",
      "Attempting to create a symbolic link of the video ...\n",
      "Created the symlink of /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_videos/flap1.mp4 to /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/videos/flap1.mp4\n",
      "Created the symlink of /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_videos/flap2.mp4 to /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/videos/flap2.mp4\n",
      "/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/videos/flap1.mp4\n",
      "/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/videos/flap2.mp4\n",
      "Generated \"/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/config.yaml\"\n",
      "\n",
      "A new project with name DLC_simple_dataset-model1-2024-09-02 is created at /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    }
   ],
   "source": [
    "# # create new project\n",
    "# config_path = deeplabcut.create_new_project('DLC_simple_dataset','model1', ['/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_videos/flap1.mp4', '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_videos/flap2.mp4'],\n",
    "#               copy_videos=False, multianimal=True, working_directory = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# print(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. load the annotations previously annotated to the labelled data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/CollectedData_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_hdf('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/Filtered_CollectedData_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scorer                                  Ro                           \\\n",
      "individuals                            ID1                            \n",
      "bodyparts                             Head                     Beak   \n",
      "coords                                   x           y            x   \n",
      "labeled-data flap1 img010.png  1232.965220   84.758200  1326.238992   \n",
      "                   img014.png  1232.965220   78.325526  1331.063497   \n",
      "                   img024.png  1232.965220   75.109190  1331.063497   \n",
      "                   img048.png  1226.532546   84.758200  1326.238992   \n",
      "                   img071.png  1183.111997  116.921570  1200.801851   \n",
      "\n",
      "scorer                                                              \\\n",
      "individuals                                                          \n",
      "bodyparts                                     Body_top               \n",
      "coords                                  y            x           y   \n",
      "labeled-data flap1 img010.png  144.260434  1184.720166  287.387428   \n",
      "                   img014.png  134.611423  1189.544671  282.562923   \n",
      "                   img024.png  133.003255  1189.544671  269.697575   \n",
      "                   img048.png  147.476771  1184.720166  269.697575   \n",
      "                   img071.png  197.329994  1163.813976  272.913912   \n",
      "\n",
      "scorer                                                               \\\n",
      "individuals                                                           \n",
      "bodyparts                     RFlipper_mid             LFlipper_mid   \n",
      "coords                                   x           y            x   \n",
      "labeled-data flap1 img010.png  1031.944161  461.069623  1374.484046   \n",
      "                   img014.png  1036.768666  443.379770  1379.308551   \n",
      "                   img024.png   988.523612  401.567390  1419.512763   \n",
      "                   img048.png  1123.609764  166.774793  1332.671666   \n",
      "                   img071.png   966.009253  319.550798  1429.161774   \n",
      "\n",
      "scorer                                                              \\\n",
      "individuals                                                          \n",
      "bodyparts                                  Body_bottom               \n",
      "coords                                  y            x           y   \n",
      "labeled-data flap1 img010.png  465.894129  1226.532546  583.290428   \n",
      "                   img014.png  457.853287  1224.924378  578.465922   \n",
      "                   img024.png  427.298086  1223.316209  581.682259   \n",
      "                   img048.png  190.897320  1220.099872  572.033248   \n",
      "                   img071.png  324.375303  1223.316209  570.425080   \n",
      "\n",
      "scorer                                                               \\\n",
      "individuals                                                           \n",
      "bodyparts                            RFoot                    LFoot   \n",
      "coords                                   x           y            x   \n",
      "labeled-data flap1 img010.png  1144.515954  626.710976  1305.332802   \n",
      "                   img014.png  1144.515954  633.143650  1308.549138   \n",
      "                   img024.png  1146.124123  633.143650  1305.332802   \n",
      "                   img048.png  1147.732291  626.710976  1303.724633   \n",
      "                   img071.png  1158.989470  626.710976  1298.900128   \n",
      "\n",
      "scorer                                     \n",
      "individuals                                \n",
      "bodyparts                                  \n",
      "coords                                  y  \n",
      "labeled-data flap1 img010.png  636.359987  \n",
      "                   img014.png  646.008998  \n",
      "                   img024.png  642.792661  \n",
      "                   img048.png  636.359987  \n",
      "                   img071.png  634.751819  \n"
     ]
    }
   ],
   "source": [
    "# display_all_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scorer                                  Ro                           \\\n",
      "individuals                            ID1                            \n",
      "bodyparts                             Head                     Beak   \n",
      "coords                                   x           y            x   \n",
      "labeled-data flap1 img010.png  1232.965220   84.758200  1326.238992   \n",
      "                   img014.png  1232.965220   78.325526  1331.063497   \n",
      "                   img024.png  1232.965220   75.109190  1331.063497   \n",
      "                   img048.png  1226.532546   84.758200  1326.238992   \n",
      "                   img071.png  1183.111997  116.921570  1200.801851   \n",
      "                   img082.png  1160.597639   96.015380  1167.030313   \n",
      "                   img086.png  1168.638481   79.933695  1179.895660   \n",
      "                   img097.png          NaN         NaN  1247.438736   \n",
      "                   img120.png  1339.104340    4.349777  1461.325144   \n",
      "                   img136.png  1329.455329   73.501021  1388.957562   \n",
      "\n",
      "scorer                                                              \\\n",
      "individuals                                                          \n",
      "bodyparts                                     Body_top               \n",
      "coords                                  y            x           y   \n",
      "labeled-data flap1 img010.png  144.260434  1184.720166  287.387428   \n",
      "                   img014.png  134.611423  1189.544671  282.562923   \n",
      "                   img024.png  133.003255  1189.544671  269.697575   \n",
      "                   img048.png  147.476771  1184.720166  269.697575   \n",
      "                   img071.png  197.329994  1163.813976  272.913912   \n",
      "                   img082.png  165.166624  1162.205807  268.089407   \n",
      "                   img086.png  144.260434  1160.597639  258.440396   \n",
      "                   img097.png    7.566114  1178.287492  206.979004   \n",
      "                   img120.png   76.717358  1290.859285  192.505488   \n",
      "                   img136.png  176.423803  1277.993937  231.101532   \n",
      "\n",
      "scorer                                                               \\\n",
      "individuals                                                           \n",
      "bodyparts                     RFlipper_mid             LFlipper_mid   \n",
      "coords                                   x           y            x   \n",
      "labeled-data flap1 img010.png  1031.944161  461.069623  1374.484046   \n",
      "                   img014.png  1036.768666  443.379770  1379.308551   \n",
      "                   img024.png   988.523612  401.567390  1419.512763   \n",
      "                   img048.png  1123.609764  166.774793  1332.671666   \n",
      "                   img071.png   966.009253  319.550798  1429.161774   \n",
      "                   img082.png  1036.768666  412.824569  1355.186024   \n",
      "                   img086.png  1052.850351  401.567390  1329.455329   \n",
      "                   img097.png  1093.054563  428.906254  1303.724633   \n",
      "                   img120.png  1014.254308  301.860944  1479.014997   \n",
      "                   img136.png  1019.078813  271.305743  1451.676133   \n",
      "\n",
      "scorer                                                              \\\n",
      "individuals                                                          \n",
      "bodyparts                                  Body_bottom               \n",
      "coords                                  y            x           y   \n",
      "labeled-data flap1 img010.png  465.894129  1226.532546  583.290428   \n",
      "                   img014.png  457.853287  1224.924378  578.465922   \n",
      "                   img024.png  427.298086  1223.316209  581.682259   \n",
      "                   img048.png  190.897320  1220.099872  572.033248   \n",
      "                   img071.png  324.375303  1223.316209  570.425080   \n",
      "                   img082.png  422.473580  1218.491704  560.776069   \n",
      "                   img086.png  408.000064  1218.491704  557.559732   \n",
      "                   img097.png  409.608232  1220.099872  576.857754   \n",
      "                   img120.png  330.807977  1220.099872  536.653542   \n",
      "                   img136.png  319.550798  1228.140715  565.600574   \n",
      "\n",
      "scorer                                                               \\\n",
      "individuals                                                           \n",
      "bodyparts                            RFoot                    LFoot   \n",
      "coords                                   x           y            x   \n",
      "labeled-data flap1 img010.png  1144.515954  626.710976  1305.332802   \n",
      "                   img014.png  1144.515954  633.143650  1308.549138   \n",
      "                   img024.png  1146.124123  633.143650  1305.332802   \n",
      "                   img048.png  1147.732291  626.710976  1303.724633   \n",
      "                   img071.png  1158.989470  626.710976  1298.900128   \n",
      "                   img082.png  1157.381302  633.143650  1298.900128   \n",
      "                   img086.png  1155.773133  633.143650  1295.683791   \n",
      "                   img097.png  1160.597639  631.535482  1294.075622   \n",
      "                   img120.png  1157.381302  623.494639  1284.426611   \n",
      "                   img136.png  1187.936503  620.278302  1295.683791   \n",
      "\n",
      "scorer                                     \n",
      "individuals                                \n",
      "bodyparts                                  \n",
      "coords                                  y  \n",
      "labeled-data flap1 img010.png  636.359987  \n",
      "                   img014.png  646.008998  \n",
      "                   img024.png  642.792661  \n",
      "                   img048.png  636.359987  \n",
      "                   img071.png  634.751819  \n",
      "                   img082.png  641.184493  \n",
      "                   img086.png  636.359987  \n",
      "                   img097.png  636.359987  \n",
      "                   img120.png  628.319145  \n",
      "                   img136.png  639.576324  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the body parts to retain\n",
    "# body_parts_to_keep = ['Head', 'Beak', 'Body_top', 'RFlipper_mid', 'LFlipper_mid', 'Body_bottom', 'RFoot', 'LFoot']\n",
    "\n",
    "# # Filter the dataframe by selecting only the relevant body parts\n",
    "# filtered_df = df.loc[:, (slice(None), slice(None), body_parts_to_keep)]\n",
    "\n",
    "# # Save the filtered dataframe back to an HDF5 file\n",
    "# filtered_file_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/CollectedData_model1_edit.h5'\n",
    "# filtered_df.to_hdf(filtered_file_path, key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_hdf('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/CollectedData_model1_edit.h5',key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data/flap1/CollectedData_model1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>scorer</th>\n",
       "      <th colspan=\"16\" halign=\"left\">model1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>individuals</th>\n",
       "      <th colspan=\"16\" halign=\"left\">ID1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bodyparts</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Head</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Beak</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Body_top</th>\n",
       "      <th colspan=\"2\" halign=\"left\">RFlipper_mid</th>\n",
       "      <th colspan=\"2\" halign=\"left\">LFlipper_mid</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Body_bottom</th>\n",
       "      <th colspan=\"2\" halign=\"left\">RFoot</th>\n",
       "      <th colspan=\"2\" halign=\"left\">LFoot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>coords</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">labeled-data</th>\n",
       "      <th rowspan=\"10\" valign=\"top\">flap1</th>\n",
       "      <th>img010.png</th>\n",
       "      <td>1232.965220</td>\n",
       "      <td>84.758200</td>\n",
       "      <td>1326.238992</td>\n",
       "      <td>144.260434</td>\n",
       "      <td>1184.720166</td>\n",
       "      <td>287.387428</td>\n",
       "      <td>1031.944161</td>\n",
       "      <td>461.069623</td>\n",
       "      <td>1374.484046</td>\n",
       "      <td>465.894129</td>\n",
       "      <td>1226.532546</td>\n",
       "      <td>583.290428</td>\n",
       "      <td>1144.515954</td>\n",
       "      <td>626.710976</td>\n",
       "      <td>1305.332802</td>\n",
       "      <td>636.359987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img014.png</th>\n",
       "      <td>1232.965220</td>\n",
       "      <td>78.325526</td>\n",
       "      <td>1331.063497</td>\n",
       "      <td>134.611423</td>\n",
       "      <td>1189.544671</td>\n",
       "      <td>282.562923</td>\n",
       "      <td>1036.768666</td>\n",
       "      <td>443.379770</td>\n",
       "      <td>1379.308551</td>\n",
       "      <td>457.853287</td>\n",
       "      <td>1224.924378</td>\n",
       "      <td>578.465922</td>\n",
       "      <td>1144.515954</td>\n",
       "      <td>633.143650</td>\n",
       "      <td>1308.549138</td>\n",
       "      <td>646.008998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img024.png</th>\n",
       "      <td>1232.965220</td>\n",
       "      <td>75.109190</td>\n",
       "      <td>1331.063497</td>\n",
       "      <td>133.003255</td>\n",
       "      <td>1189.544671</td>\n",
       "      <td>269.697575</td>\n",
       "      <td>988.523612</td>\n",
       "      <td>401.567390</td>\n",
       "      <td>1419.512763</td>\n",
       "      <td>427.298086</td>\n",
       "      <td>1223.316209</td>\n",
       "      <td>581.682259</td>\n",
       "      <td>1146.124123</td>\n",
       "      <td>633.143650</td>\n",
       "      <td>1305.332802</td>\n",
       "      <td>642.792661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img048.png</th>\n",
       "      <td>1226.532546</td>\n",
       "      <td>84.758200</td>\n",
       "      <td>1326.238992</td>\n",
       "      <td>147.476771</td>\n",
       "      <td>1184.720166</td>\n",
       "      <td>269.697575</td>\n",
       "      <td>1123.609764</td>\n",
       "      <td>166.774793</td>\n",
       "      <td>1332.671666</td>\n",
       "      <td>190.897320</td>\n",
       "      <td>1220.099872</td>\n",
       "      <td>572.033248</td>\n",
       "      <td>1147.732291</td>\n",
       "      <td>626.710976</td>\n",
       "      <td>1303.724633</td>\n",
       "      <td>636.359987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img071.png</th>\n",
       "      <td>1183.111997</td>\n",
       "      <td>116.921570</td>\n",
       "      <td>1200.801851</td>\n",
       "      <td>197.329994</td>\n",
       "      <td>1163.813976</td>\n",
       "      <td>272.913912</td>\n",
       "      <td>966.009253</td>\n",
       "      <td>319.550798</td>\n",
       "      <td>1429.161774</td>\n",
       "      <td>324.375303</td>\n",
       "      <td>1223.316209</td>\n",
       "      <td>570.425080</td>\n",
       "      <td>1158.989470</td>\n",
       "      <td>626.710976</td>\n",
       "      <td>1298.900128</td>\n",
       "      <td>634.751819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img082.png</th>\n",
       "      <td>1160.597639</td>\n",
       "      <td>96.015380</td>\n",
       "      <td>1167.030313</td>\n",
       "      <td>165.166624</td>\n",
       "      <td>1162.205807</td>\n",
       "      <td>268.089407</td>\n",
       "      <td>1036.768666</td>\n",
       "      <td>412.824569</td>\n",
       "      <td>1355.186024</td>\n",
       "      <td>422.473580</td>\n",
       "      <td>1218.491704</td>\n",
       "      <td>560.776069</td>\n",
       "      <td>1157.381302</td>\n",
       "      <td>633.143650</td>\n",
       "      <td>1298.900128</td>\n",
       "      <td>641.184493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img086.png</th>\n",
       "      <td>1168.638481</td>\n",
       "      <td>79.933695</td>\n",
       "      <td>1179.895660</td>\n",
       "      <td>144.260434</td>\n",
       "      <td>1160.597639</td>\n",
       "      <td>258.440396</td>\n",
       "      <td>1052.850351</td>\n",
       "      <td>401.567390</td>\n",
       "      <td>1329.455329</td>\n",
       "      <td>408.000064</td>\n",
       "      <td>1218.491704</td>\n",
       "      <td>557.559732</td>\n",
       "      <td>1155.773133</td>\n",
       "      <td>633.143650</td>\n",
       "      <td>1295.683791</td>\n",
       "      <td>636.359987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img097.png</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1247.438736</td>\n",
       "      <td>7.566114</td>\n",
       "      <td>1178.287492</td>\n",
       "      <td>206.979004</td>\n",
       "      <td>1093.054563</td>\n",
       "      <td>428.906254</td>\n",
       "      <td>1303.724633</td>\n",
       "      <td>409.608232</td>\n",
       "      <td>1220.099872</td>\n",
       "      <td>576.857754</td>\n",
       "      <td>1160.597639</td>\n",
       "      <td>631.535482</td>\n",
       "      <td>1294.075622</td>\n",
       "      <td>636.359987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img120.png</th>\n",
       "      <td>1339.104340</td>\n",
       "      <td>4.349777</td>\n",
       "      <td>1461.325144</td>\n",
       "      <td>76.717358</td>\n",
       "      <td>1290.859285</td>\n",
       "      <td>192.505488</td>\n",
       "      <td>1014.254308</td>\n",
       "      <td>301.860944</td>\n",
       "      <td>1479.014997</td>\n",
       "      <td>330.807977</td>\n",
       "      <td>1220.099872</td>\n",
       "      <td>536.653542</td>\n",
       "      <td>1157.381302</td>\n",
       "      <td>623.494639</td>\n",
       "      <td>1284.426611</td>\n",
       "      <td>628.319145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img136.png</th>\n",
       "      <td>1329.455329</td>\n",
       "      <td>73.501021</td>\n",
       "      <td>1388.957562</td>\n",
       "      <td>176.423803</td>\n",
       "      <td>1277.993937</td>\n",
       "      <td>231.101532</td>\n",
       "      <td>1019.078813</td>\n",
       "      <td>271.305743</td>\n",
       "      <td>1451.676133</td>\n",
       "      <td>319.550798</td>\n",
       "      <td>1228.140715</td>\n",
       "      <td>565.600574</td>\n",
       "      <td>1187.936503</td>\n",
       "      <td>620.278302</td>\n",
       "      <td>1295.683791</td>\n",
       "      <td>639.576324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "scorer                              model1                           \\\n",
       "individuals                            ID1                            \n",
       "bodyparts                             Head                     Beak   \n",
       "coords                                   x           y            x   \n",
       "labeled-data flap1 img010.png  1232.965220   84.758200  1326.238992   \n",
       "                   img014.png  1232.965220   78.325526  1331.063497   \n",
       "                   img024.png  1232.965220   75.109190  1331.063497   \n",
       "                   img048.png  1226.532546   84.758200  1326.238992   \n",
       "                   img071.png  1183.111997  116.921570  1200.801851   \n",
       "                   img082.png  1160.597639   96.015380  1167.030313   \n",
       "                   img086.png  1168.638481   79.933695  1179.895660   \n",
       "                   img097.png          NaN         NaN  1247.438736   \n",
       "                   img120.png  1339.104340    4.349777  1461.325144   \n",
       "                   img136.png  1329.455329   73.501021  1388.957562   \n",
       "\n",
       "scorer                                                              \\\n",
       "individuals                                                          \n",
       "bodyparts                                     Body_top               \n",
       "coords                                  y            x           y   \n",
       "labeled-data flap1 img010.png  144.260434  1184.720166  287.387428   \n",
       "                   img014.png  134.611423  1189.544671  282.562923   \n",
       "                   img024.png  133.003255  1189.544671  269.697575   \n",
       "                   img048.png  147.476771  1184.720166  269.697575   \n",
       "                   img071.png  197.329994  1163.813976  272.913912   \n",
       "                   img082.png  165.166624  1162.205807  268.089407   \n",
       "                   img086.png  144.260434  1160.597639  258.440396   \n",
       "                   img097.png    7.566114  1178.287492  206.979004   \n",
       "                   img120.png   76.717358  1290.859285  192.505488   \n",
       "                   img136.png  176.423803  1277.993937  231.101532   \n",
       "\n",
       "scorer                                                               \\\n",
       "individuals                                                           \n",
       "bodyparts                     RFlipper_mid             LFlipper_mid   \n",
       "coords                                   x           y            x   \n",
       "labeled-data flap1 img010.png  1031.944161  461.069623  1374.484046   \n",
       "                   img014.png  1036.768666  443.379770  1379.308551   \n",
       "                   img024.png   988.523612  401.567390  1419.512763   \n",
       "                   img048.png  1123.609764  166.774793  1332.671666   \n",
       "                   img071.png   966.009253  319.550798  1429.161774   \n",
       "                   img082.png  1036.768666  412.824569  1355.186024   \n",
       "                   img086.png  1052.850351  401.567390  1329.455329   \n",
       "                   img097.png  1093.054563  428.906254  1303.724633   \n",
       "                   img120.png  1014.254308  301.860944  1479.014997   \n",
       "                   img136.png  1019.078813  271.305743  1451.676133   \n",
       "\n",
       "scorer                                                              \\\n",
       "individuals                                                          \n",
       "bodyparts                                  Body_bottom               \n",
       "coords                                  y            x           y   \n",
       "labeled-data flap1 img010.png  465.894129  1226.532546  583.290428   \n",
       "                   img014.png  457.853287  1224.924378  578.465922   \n",
       "                   img024.png  427.298086  1223.316209  581.682259   \n",
       "                   img048.png  190.897320  1220.099872  572.033248   \n",
       "                   img071.png  324.375303  1223.316209  570.425080   \n",
       "                   img082.png  422.473580  1218.491704  560.776069   \n",
       "                   img086.png  408.000064  1218.491704  557.559732   \n",
       "                   img097.png  409.608232  1220.099872  576.857754   \n",
       "                   img120.png  330.807977  1220.099872  536.653542   \n",
       "                   img136.png  319.550798  1228.140715  565.600574   \n",
       "\n",
       "scorer                                                               \\\n",
       "individuals                                                           \n",
       "bodyparts                            RFoot                    LFoot   \n",
       "coords                                   x           y            x   \n",
       "labeled-data flap1 img010.png  1144.515954  626.710976  1305.332802   \n",
       "                   img014.png  1144.515954  633.143650  1308.549138   \n",
       "                   img024.png  1146.124123  633.143650  1305.332802   \n",
       "                   img048.png  1147.732291  626.710976  1303.724633   \n",
       "                   img071.png  1158.989470  626.710976  1298.900128   \n",
       "                   img082.png  1157.381302  633.143650  1298.900128   \n",
       "                   img086.png  1155.773133  633.143650  1295.683791   \n",
       "                   img097.png  1160.597639  631.535482  1294.075622   \n",
       "                   img120.png  1157.381302  623.494639  1284.426611   \n",
       "                   img136.png  1187.936503  620.278302  1295.683791   \n",
       "\n",
       "scorer                                     \n",
       "individuals                                \n",
       "bodyparts                                  \n",
       "coords                                  y  \n",
       "labeled-data flap1 img010.png  636.359987  \n",
       "                   img014.png  646.008998  \n",
       "                   img024.png  642.792661  \n",
       "                   img048.png  636.359987  \n",
       "                   img071.png  634.751819  \n",
       "                   img082.png  641.184493  \n",
       "                   img086.png  636.359987  \n",
       "                   img097.png  636.359987  \n",
       "                   img120.png  628.319145  \n",
       "                   img136.png  639.576324  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_scorer(df, old_scorer, new_scorer):\n",
    "    \"\"\"\n",
    "    Replaces the scorer in the MultiIndex of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame with a MultiIndex.\n",
    "    - old_scorer: The scorer value to be replaced.\n",
    "    - new_scorer: The new scorer value.\n",
    "\n",
    "    Returns:\n",
    "    - df: The DataFrame with the scorer replaced.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rename the 'scorer' level of the index\n",
    "    df.columns = df.columns.set_levels(\n",
    "        [new_scorer if scorer == old_scorer else scorer for scorer in df.columns.levels[0]], level=0\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labelled_images(parent_dir, model_name, base_dir, kp_to_keep):\n",
    "    \"\"\"\n",
    "    adjust data have correct naming convention and to have only relevant keypoints, load the first .h5 file from each, and save as new .h5 and .csv.\n",
    "    Also, generate a list of video paths with crop information and save it to a .txt file in base_dir. (to be put in the config file later).\n",
    "    \n",
    "    NOTES: Adjusted csvs are not it the same format... But that's not really an issue, just something to note \n",
    "    \n",
    "    Parameters:\n",
    "    - parent_dir (str): Path to the parent directory containing nested directories.\n",
    "    - model_name (str): The model name to use for renaming the saved files.\n",
    "    - base_dir (str): Path to the base directory where the video_paths.txt will be saved.\n",
    "    - kp_to_keep (list): list of kp names that need to be kept (filter others out)\n",
    "    \"\"\"\n",
    "    # Create a list to store video paths and crop information\n",
    "    video_paths_list = []\n",
    "    \n",
    "    # Walk through the parent directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(parent_dir):\n",
    "        # If there are any .h5 files in the current directory\n",
    "        h5_files = [f for f in files if f.endswith('.h5')]\n",
    "        if h5_files:\n",
    "            # Load the first .h5 file\n",
    "            h5_file_path = os.path.join(root, h5_files[0])\n",
    "            \n",
    "            # Load .h5 into DataFrame\n",
    "            df = pd.read_hdf(h5_file_path)\n",
    "\n",
    "            # Filter the dataframe by selecting only the relevant body parts\n",
    "            filtered_df = df.loc[:, (slice(None), slice(None), kp_to_keep)]\n",
    "\n",
    "            # replace scorer with new scorer\n",
    "            filtered_df = replace_scorer(filtered_df, old_scorer='Ro', new_scorer='model1')\n",
    "            #filtered_df = replace_scorer(df, old_scorer='Ro', new_scorer='model1')\n",
    "            \n",
    "            # Save to new .h5 file\n",
    "            new_h5_filename = f\"CollectedData_{model_name}.h5\"\n",
    "            new_h5_path = os.path.join(root, new_h5_filename)\n",
    "            filtered_df.to_hdf(new_h5_path, key='df', mode='w')\n",
    "\n",
    "            # Save to .csv file\n",
    "            new_csv_filename = f\"CollectedData_{model_name}.csv\"\n",
    "            new_csv_path = os.path.join(root, new_csv_filename)\n",
    "            filtered_df.to_csv(new_csv_path, index=False)\n",
    "\n",
    "            # Delete the old .h5 file\n",
    "            if os.path.exists(h5_file_path):\n",
    "                os.remove(h5_file_path)\n",
    "                #print(f\"Deleted old file: {h5_file_path}\")\n",
    "\n",
    "            # Delete the old .csv file with the same name, if it exists\n",
    "            old_csv_file = h5_file_path.replace('.h5', '.csv')\n",
    "            if os.path.exists(old_csv_file):\n",
    "                os.remove(old_csv_file)\n",
    "                #print(f\"Deleted old file: {old_csv_file}\")\n",
    "            \n",
    "            # Get the directory name \n",
    "            dir_name = os.path.basename(root)\n",
    "            \n",
    "            # Create video path string\n",
    "            video_path = f\"  /home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_videos/{dir_name}.mp4\"\n",
    "            crop_info = \"    crop: 0, 1920, 0, 1080\"\n",
    "            video_paths_list.append(f\"{video_path}:\\n{crop_info}\")\n",
    "\n",
    "    # Save video paths to a .txt file in the base directory\n",
    "    txt_file_path = os.path.join(base_dir, \"video_paths.txt\")\n",
    "    with open(txt_file_path, 'w') as txt_file:\n",
    "        txt_file.write(\"\\n\".join(video_paths_list))\n",
    "\n",
    "    print(\"Processing complete. Files saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/labeled-data'\n",
    "model_name = 'model1'\n",
    "base_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02'\n",
    "kp_to_keep = ['Head', 'Beak', 'Body_top', 'RFlipper_mid', 'LFlipper_mid', 'Body_bottom', 'RFoot', 'LFoot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Files saved.\n"
     ]
    }
   ],
   "source": [
    "# process_labelled_images(parent_dir, model_name, base_dir, kp_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. check the labels and relabel where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by model1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:07<00:00,  1.36it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.56it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.33it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.61it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.36it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.58it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.38it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.51it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.52it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.28it/s]\n",
      "100%|| 10/10 [00:08<00:00,  1.14it/s]\n",
      "100%|| 10/10 [00:08<00:00,  1.17it/s]\n",
      "100%|| 10/10 [00:08<00:00,  1.20it/s]\n",
      "100%|| 10/10 [00:08<00:00,  1.24it/s]\n",
      "100%|| 10/10 [00:08<00:00,  1.20it/s]\n",
      "100%|| 10/10 [00:08<00:00,  1.22it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.28it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.26it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.26it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.39it/s]\n",
      "100%|| 10/10 [00:05<00:00,  1.79it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.52it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.52it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.54it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.56it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.59it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.60it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.27it/s]\n",
      "100%|| 10/10 [00:05<00:00,  1.77it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.30it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.66it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.43it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.55it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.48it/s]\n",
      "100%|| 10/10 [00:08<00:00,  1.24it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.31it/s]\n",
      "100%|| 10/10 [00:08<00:00,  1.21it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.29it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.29it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.56it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.56it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.59it/s]\n",
      "100%|| 10/10 [00:08<00:00,  1.24it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.42it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.36it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.53it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.35it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.65it/s]\n",
      "100%|| 10/10 [00:07<00:00,  1.37it/s]\n",
      "100%|| 10/10 [00:06<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.check_labels(config_path, visualizeindividuals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.label_frames(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Using `skeleton` from the config file as a paf_graph. Data-driven skeleton will not be computed.\n",
      "Utilizing the following graph: [[0, 1], [0, 2], [2, 3], [2, 4], [2, 5], [5, 6], [5, 7]]\n",
      "Creating training data for: Shuffle: 5 TrainFraction:  0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 475/475 [00:00<00:00, 3179.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    }
   ],
   "source": [
    "# paf_graph='config' use the skeleton defined\n",
    "print('hello')\n",
    "deeplabcut.create_multianimaltraining_dataset(config_path, paf_graph='config')#, augmenter_type='imgaug', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_pickle(file_path):\n",
    "    \"\"\"\n",
    "    Load a pickle file and return its content.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the pickle file.\n",
    "    \n",
    "    Returns:\n",
    "    - data: The data loaded from the pickle file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_pickle(data, file_path):\n",
    "    \"\"\"\n",
    "    Save data back to a pickle file.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The data to save.\n",
    "    - file_path (str): Path to save the pickle file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    \n",
    "    print(f\"Data saved to {file_path}\")\n",
    "\n",
    "# # Load the pickle file\n",
    "# file_path = 'path_to_your_pickle_file.pickle'\n",
    "# data = load_pickle(file_path)\n",
    "\n",
    "# # View the data\n",
    "# print(\"Data loaded from pickle file:\", data)\n",
    "\n",
    "# # Modify the data (example: if it's a dictionary, you can update a key-value pair)\n",
    "# if isinstance(data, dict):\n",
    "#     data['new_key'] = 'new_value'\n",
    "#     print(\"Updated data:\", data)\n",
    "\n",
    "# # Save the modified data back to the pickle file\n",
    "# save_pickle(data, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'image': ('labeled-data', 'preen31', 'img000.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17929e+03, 1.67090e+02],\n",
      "       [1.00000e+00, 1.14687e+03, 3.63730e+02],\n",
      "       [2.00000e+00, 1.09285e+03, 3.63730e+02],\n",
      "       [3.00000e+00, 8.98380e+02, 6.35990e+02],\n",
      "       [4.00000e+00, 1.23979e+03, 6.33830e+02],\n",
      "       [5.00000e+00, 1.08421e+03, 8.04540e+02],\n",
      "       [6.00000e+00, 9.69690e+02, 9.86040e+02],\n",
      "       [7.00000e+00, 1.14039e+03, 9.88210e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest47', 'img020.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.34430e+02, 3.88700e+02],\n",
      "       [1.00000e+00, 6.10600e+02, 5.28610e+02],\n",
      "       [2.00000e+00, 9.22590e+02, 3.79050e+02],\n",
      "       [4.00000e+00, 1.06411e+03, 6.46010e+02],\n",
      "       [5.00000e+00, 1.47419e+03, 6.12240e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand11', 'img066.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.06141e+03, 3.54120e+02],\n",
      "       [1.00000e+00, 1.10071e+03, 3.53590e+02],\n",
      "       [2.00000e+00, 1.03167e+03, 4.34320e+02],\n",
      "       [3.00000e+00, 1.11027e+03, 5.32040e+02],\n",
      "       [5.00000e+00, 1.02955e+03, 6.24980e+02],\n",
      "       [6.00000e+00, 1.07681e+03, 6.40910e+02],\n",
      "       [7.00000e+00, 1.01096e+03, 6.46220e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk66', 'img079.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08222e+03, 2.19470e+02],\n",
      "       [1.00000e+00, 9.95610e+02, 2.46770e+02],\n",
      "       [2.00000e+00, 1.05680e+03, 3.42790e+02],\n",
      "       [4.00000e+00, 1.20460e+03, 5.09410e+02],\n",
      "       [5.00000e+00, 1.16506e+03, 6.08260e+02],\n",
      "       [6.00000e+00, 1.17165e+03, 6.47800e+02],\n",
      "       [7.00000e+00, 1.21683e+03, 6.54380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk80', 'img012.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 466.16, 493.66],\n",
      "       [  1.  , 503.92, 547.84],\n",
      "       [  2.  , 370.92, 539.64],\n",
      "       [  3.  , 359.43, 643.9 ],\n",
      "       [  5.  , 248.6 , 709.57],\n",
      "       [  6.  , 323.31, 781.  ]], dtype=float32)}}, {'image': ('labeled-data', 'flap12', 'img065.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.16864e+03, 4.06390e+02],\n",
      "       [1.00000e+00, 1.18954e+03, 4.35340e+02],\n",
      "       [2.00000e+00, 1.06250e+03, 4.90020e+02],\n",
      "       [3.00000e+00, 1.20563e+03, 6.10630e+02],\n",
      "       [4.00000e+00, 9.12940e+02, 5.86510e+02],\n",
      "       [5.00000e+00, 9.48320e+02, 7.89140e+02],\n",
      "       [6.00000e+00, 1.03677e+03, 8.56680e+02],\n",
      "       [7.00000e+00, 9.85310e+02, 8.69540e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand4', 'img104.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.03677e+03, 2.48790e+02],\n",
      "       [1.00000e+00, 9.75660e+02, 2.40750e+02],\n",
      "       [2.00000e+00, 1.09145e+03, 4.28910e+02],\n",
      "       [4.00000e+00, 9.25810e+02, 6.79780e+02],\n",
      "       [5.00000e+00, 1.21206e+03, 8.32560e+02],\n",
      "       [6.00000e+00, 1.16542e+03, 9.09750e+02],\n",
      "       [7.00000e+00, 1.06893e+03, 9.32260e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk86', 'img127.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 512.9 , 413.22],\n",
      "       [  2.  , 592.2 , 365.11],\n",
      "       [  3.  , 676.85, 476.49],\n",
      "       [  4.  , 520.92, 476.49],\n",
      "       [  5.  , 634.08, 522.82],\n",
      "       [  6.  , 623.39, 586.08],\n",
      "       [  7.  , 555.67, 568.26]], dtype=float32)}}, {'image': ('labeled-data', 'preen48', 'img145.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.39788e+03, 2.12320e+02],\n",
      "       [1.00000e+00, 1.34891e+03, 2.84170e+02],\n",
      "       [2.00000e+00, 1.31330e+03, 2.13590e+02],\n",
      "       [3.00000e+00, 1.23572e+03, 3.54760e+02],\n",
      "       [4.00000e+00, 1.43476e+03, 3.40130e+02],\n",
      "       [5.00000e+00, 1.31839e+03, 4.68580e+02],\n",
      "       [6.00000e+00, 1.28660e+03, 5.03560e+02],\n",
      "       [7.00000e+00, 1.38198e+03, 5.11190e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen48', 'img066.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.31966e+03, 3.30590e+02],\n",
      "       [1.00000e+00, 1.27197e+03, 3.74470e+02],\n",
      "       [2.00000e+00, 1.18676e+03, 2.47290e+02],\n",
      "       [3.00000e+00, 1.11872e+03, 4.04990e+02],\n",
      "       [4.00000e+00, 1.33747e+03, 3.82100e+02],\n",
      "       [5.00000e+00, 1.20711e+03, 5.11820e+02],\n",
      "       [6.00000e+00, 1.17722e+03, 5.48070e+02],\n",
      "       [7.00000e+00, 1.26497e+03, 5.55070e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest44', 'img010.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.69520e+02, 3.75840e+02],\n",
      "       [1.00000e+00, 7.29610e+02, 4.85190e+02],\n",
      "       [2.00000e+00, 1.02390e+03, 4.43380e+02],\n",
      "       [4.00000e+00, 1.09627e+03, 6.74960e+02],\n",
      "       [5.00000e+00, 1.42434e+03, 6.81390e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest47', 'img001.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.40870e+02, 3.77440e+02],\n",
      "       [1.00000e+00, 5.97740e+02, 5.09310e+02],\n",
      "       [2.00000e+00, 9.48320e+02, 3.74230e+02],\n",
      "       [4.00000e+00, 1.06572e+03, 6.47620e+02],\n",
      "       [5.00000e+00, 1.48545e+03, 6.09020e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand9', 'img006.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.07054e+03, 2.52010e+02],\n",
      "       [1.00000e+00, 1.19598e+03, 2.85780e+02],\n",
      "       [2.00000e+00, 1.02873e+03, 4.08000e+02],\n",
      "       [4.00000e+00, 9.08120e+02, 6.83000e+02],\n",
      "       [5.00000e+00, 1.18150e+03, 8.37380e+02],\n",
      "       [6.00000e+00, 1.14130e+03, 9.09750e+02],\n",
      "       [7.00000e+00, 1.04320e+03, 9.35480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand11', 'img149.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.04548e+03, 3.66340e+02],\n",
      "       [1.00000e+00, 1.02052e+03, 3.81740e+02],\n",
      "       [2.00000e+00, 1.05451e+03, 4.38570e+02],\n",
      "       [3.00000e+00, 1.13417e+03, 5.37880e+02],\n",
      "       [5.00000e+00, 1.05822e+03, 6.15420e+02],\n",
      "       [6.00000e+00, 1.10708e+03, 6.40380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen84', 'img004.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00139e+03, 5.30220e+02],\n",
      "       [1.00000e+00, 1.00139e+03, 6.49230e+02],\n",
      "       [2.00000e+00, 1.13808e+03, 4.94840e+02],\n",
      "       [3.00000e+00, 9.03290e+02, 6.20280e+02],\n",
      "       [4.00000e+00, 1.17185e+03, 6.58870e+02],\n",
      "       [5.00000e+00, 9.96560e+02, 8.24520e+02],\n",
      "       [6.00000e+00, 9.08120e+02, 8.45420e+02],\n",
      "       [7.00000e+00, 1.07858e+03, 8.18080e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand6', 'img144.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.62790e+02, 2.52010e+02],\n",
      "       [1.00000e+00, 8.34140e+02, 2.89000e+02],\n",
      "       [2.00000e+00, 1.07215e+03, 4.19260e+02],\n",
      "       [4.00000e+00, 8.98470e+02, 6.76560e+02],\n",
      "       [5.00000e+00, 1.17829e+03, 8.10040e+02],\n",
      "       [6.00000e+00, 1.11557e+03, 9.01710e+02],\n",
      "       [7.00000e+00, 1.01747e+03, 9.21010e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk80', 'img137.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 546.61, 515.83],\n",
      "       [  1.  , 508.85, 560.16],\n",
      "       [  2.  , 590.12, 566.73],\n",
      "       [  4.  , 504.74, 680.02],\n",
      "       [  5.  , 659.9 , 763.76],\n",
      "       [  6.  , 622.96, 797.42],\n",
      "       [  7.  , 540.86, 812.2 ]], dtype=float32)}}, {'image': ('labeled-data', 'stand72', 'img092.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.09145e+03, 2.02150e+02],\n",
      "       [1.00000e+00, 1.04481e+03, 3.27590e+02],\n",
      "       [2.00000e+00, 1.05446e+03, 4.09610e+02],\n",
      "       [3.00000e+00, 9.46710e+02, 5.96160e+02],\n",
      "       [4.00000e+00, 1.27960e+03, 6.37970e+02],\n",
      "       [5.00000e+00, 1.15416e+03, 7.74660e+02],\n",
      "       [6.00000e+00, 1.08823e+03, 8.16470e+02],\n",
      "       [7.00000e+00, 1.23618e+03, 8.40600e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen48', 'img071.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.34510e+03, 3.31230e+02],\n",
      "       [1.00000e+00, 1.30122e+03, 3.63660e+02],\n",
      "       [2.00000e+00, 1.22555e+03, 2.44750e+02],\n",
      "       [3.00000e+00, 1.14352e+03, 4.01810e+02],\n",
      "       [4.00000e+00, 1.35909e+03, 3.82100e+02],\n",
      "       [5.00000e+00, 1.23064e+03, 5.09920e+02],\n",
      "       [6.00000e+00, 1.20202e+03, 5.49980e+02],\n",
      "       [7.00000e+00, 1.29105e+03, 5.53790e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap14', 'img104.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 893.64, 113.71],\n",
      "       [  1.  , 874.34,  97.62],\n",
      "       [  2.  , 814.84, 317.94],\n",
      "       [  3.  , 953.14, 647.62],\n",
      "       [  5.  , 628.29, 782.7 ],\n",
      "       [  6.  , 837.36, 940.3 ],\n",
      "       [  7.  , 773.03, 940.3 ]], dtype=float32)}}, {'image': ('labeled-data', 'rest24', 'img072.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.87900e+02, 3.55580e+02],\n",
      "       [1.00000e+00, 5.55260e+02, 4.00800e+02],\n",
      "       [2.00000e+00, 8.74800e+02, 3.65630e+02],\n",
      "       [4.00000e+00, 1.05869e+03, 5.62580e+02],\n",
      "       [5.00000e+00, 1.30689e+03, 4.81190e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand75', 'img093.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00091e+03, 3.40920e+02],\n",
      "       [1.00000e+00, 1.04896e+03, 3.62910e+02],\n",
      "       [2.00000e+00, 9.43900e+02, 4.16660e+02],\n",
      "       [3.00000e+00, 1.02045e+03, 5.60810e+02],\n",
      "       [5.00000e+00, 8.86080e+02, 6.25150e+02],\n",
      "       [6.00000e+00, 9.78920e+02, 6.74010e+02],\n",
      "       [7.00000e+00, 9.05620e+02, 6.74820e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest44', 'img008.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.55050e+02, 3.72620e+02],\n",
      "       [1.00000e+00, 7.31220e+02, 4.83580e+02],\n",
      "       [2.00000e+00, 1.03034e+03, 4.36950e+02],\n",
      "       [4.00000e+00, 1.11235e+03, 6.78170e+02],\n",
      "       [5.00000e+00, 1.42273e+03, 6.73350e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen33', 'img109.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.40617e+03, 4.24230e+02],\n",
      "       [1.00000e+00, 1.23547e+03, 5.21470e+02],\n",
      "       [2.00000e+00, 1.16200e+03, 4.06940e+02],\n",
      "       [3.00000e+00, 8.91900e+02, 6.46800e+02],\n",
      "       [4.00000e+00, 1.27220e+03, 6.53280e+02],\n",
      "       [5.00000e+00, 1.08637e+03, 8.08860e+02],\n",
      "       [6.00000e+00, 9.95620e+02, 9.77400e+02],\n",
      "       [7.00000e+00, 1.15336e+03, 9.86040e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap2', 'img097.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.67039e+03, 8.63700e+01],\n",
      "       [1.00000e+00, 1.80547e+03, 1.47480e+02],\n",
      "       [2.00000e+00, 1.43881e+03, 1.71600e+02],\n",
      "       [3.00000e+00, 1.32785e+03, 2.32710e+02],\n",
      "       [5.00000e+00, 1.17185e+03, 4.98060e+02],\n",
      "       [6.00000e+00, 1.27478e+03, 6.00980e+02],\n",
      "       [7.00000e+00, 1.31016e+03, 6.39580e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk64', 'img104.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.49188e+03, 2.63260e+02],\n",
      "       [1.00000e+00, 1.49992e+03, 3.42070e+02],\n",
      "       [2.00000e+00, 1.47419e+03, 3.61360e+02],\n",
      "       [3.00000e+00, 1.32946e+03, 4.62680e+02],\n",
      "       [4.00000e+00, 1.44524e+03, 4.62680e+02],\n",
      "       [5.00000e+00, 1.28603e+03, 5.49520e+02],\n",
      "       [6.00000e+00, 1.26674e+03, 5.81680e+02],\n",
      "       [7.00000e+00, 1.30051e+03, 5.80070e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap13', 'img005.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1051.24,  387.09],\n",
      "       [   2.  , 1085.01,  510.92],\n",
      "       [   3.  , 1208.84,  588.11],\n",
      "       [   4.  ,  896.86,  596.16],\n",
      "       [   5.  , 1094.66,  792.35],\n",
      "       [   7.  , 1017.47,  850.25]], dtype=float32)}}, {'image': ('labeled-data', 'stand75', 'img112.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00091e+03, 3.40920e+02],\n",
      "       [1.00000e+00, 1.06362e+03, 3.63730e+02],\n",
      "       [2.00000e+00, 9.45530e+02, 4.14220e+02],\n",
      "       [3.00000e+00, 1.01801e+03, 5.58370e+02],\n",
      "       [5.00000e+00, 8.81190e+02, 6.25960e+02],\n",
      "       [6.00000e+00, 9.79730e+02, 6.71570e+02],\n",
      "       [7.00000e+00, 9.09700e+02, 6.70750e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen31', 'img108.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.23331e+03, 2.40560e+02],\n",
      "       [1.00000e+00, 1.17929e+03, 3.81010e+02],\n",
      "       [2.00000e+00, 1.09717e+03, 3.70210e+02],\n",
      "       [3.00000e+00, 8.87570e+02, 6.53280e+02],\n",
      "       [4.00000e+00, 1.23114e+03, 6.33830e+02],\n",
      "       [5.00000e+00, 1.08853e+03, 7.93730e+02],\n",
      "       [6.00000e+00, 9.74010e+02, 9.68760e+02],\n",
      "       [7.00000e+00, 1.14039e+03, 9.86040e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest42', 'img056.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.67910e+02, 3.42070e+02],\n",
      "       [1.00000e+00, 6.91010e+02, 3.83880e+02],\n",
      "       [2.00000e+00, 1.01908e+03, 4.35340e+02],\n",
      "       [4.00000e+00, 1.08501e+03, 6.55660e+02],\n",
      "       [5.00000e+00, 1.41308e+03, 6.63700e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk62', 'img112.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1110.74,   89.58],\n",
      "       [   2.  , 1097.88,  227.89],\n",
      "       [   3.  , 1255.48,  467.5 ],\n",
      "       [   4.  ,  949.93,  478.76],\n",
      "       [   5.  , 1081.8 ,  584.9 ],\n",
      "       [   6.  , 1176.68,  705.51],\n",
      "       [   7.  , 1025.51,  723.2 ]], dtype=float32)}}, {'image': ('labeled-data', 'preen35', 'img131.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.44360e+02, 3.16190e+02],\n",
      "       [1.00000e+00, 9.50240e+02, 4.65290e+02],\n",
      "       [2.00000e+00, 1.12743e+03, 3.63730e+02],\n",
      "       [3.00000e+00, 9.58880e+02, 6.53280e+02],\n",
      "       [4.00000e+00, 1.22466e+03, 6.66240e+02],\n",
      "       [5.00000e+00, 1.08421e+03, 7.93730e+02],\n",
      "       [6.00000e+00, 9.67520e+02, 9.77400e+02],\n",
      "       [7.00000e+00, 1.11662e+03, 9.86040e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen37', 'img069.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.68730e+02, 7.48350e+02],\n",
      "       [1.00000e+00, 9.22150e+02, 7.78610e+02],\n",
      "       [2.00000e+00, 8.05460e+02, 5.94940e+02],\n",
      "       [3.00000e+00, 9.82650e+02, 8.60720e+02],\n",
      "       [4.00000e+00, 1.22682e+03, 5.40920e+02],\n",
      "       [5.00000e+00, 1.11878e+03, 8.47750e+02],\n",
      "       [6.00000e+00, 9.63200e+02, 1.00765e+03],\n",
      "       [7.00000e+00, 1.12094e+03, 1.01197e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest19', 'img028.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17025e+03, 2.58440e+02],\n",
      "       [1.00000e+00, 1.27156e+03, 3.17940e+02],\n",
      "       [2.00000e+00, 1.06089e+03, 3.96740e+02],\n",
      "       [3.00000e+00, 1.08341e+03, 7.15160e+02],\n",
      "       [5.00000e+00, 7.10310e+02, 5.51130e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand79', 'img036.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.82090e+02, 2.90600e+02],\n",
      "       [1.00000e+00, 8.93640e+02, 3.37240e+02],\n",
      "       [2.00000e+00, 9.27410e+02, 4.69110e+02],\n",
      "       [3.00000e+00, 9.11330e+02, 6.17060e+02],\n",
      "       [4.00000e+00, 1.10914e+03, 6.62090e+02],\n",
      "       [5.00000e+00, 1.04320e+03, 7.69840e+02],\n",
      "       [6.00000e+00, 1.03516e+03, 8.06830e+02],\n",
      "       [7.00000e+00, 1.07215e+03, 8.18080e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk89', 'img041.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.22710e+02, 2.83290e+02],\n",
      "       [1.00000e+00, 7.51980e+02, 4.03360e+02],\n",
      "       [2.00000e+00, 9.72380e+02, 3.78690e+02],\n",
      "       [3.00000e+00, 9.04950e+02, 5.54680e+02],\n",
      "       [4.00000e+00, 1.16811e+03, 5.56330e+02],\n",
      "       [5.00000e+00, 1.05627e+03, 7.07650e+02],\n",
      "       [6.00000e+00, 9.88830e+02, 7.61920e+02],\n",
      "       [7.00000e+00, 1.14837e+03, 7.75080e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand4', 'img021.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.04481e+03, 2.35930e+02],\n",
      "       [1.00000e+00, 1.15899e+03, 2.53620e+02],\n",
      "       [2.00000e+00, 1.05124e+03, 3.88700e+02],\n",
      "       [4.00000e+00, 8.92030e+02, 7.05510e+02],\n",
      "       [5.00000e+00, 1.19919e+03, 8.03610e+02],\n",
      "       [6.00000e+00, 1.13487e+03, 9.04920e+02],\n",
      "       [7.00000e+00, 1.03677e+03, 9.27440e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk68', 'img012.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.15130e+02, 2.76130e+02],\n",
      "       [1.00000e+00, 6.15430e+02, 3.87090e+02],\n",
      "       [2.00000e+00, 7.56950e+02, 4.57850e+02],\n",
      "       [3.00000e+00, 7.11920e+02, 6.95860e+02],\n",
      "       [4.00000e+00, 1.02230e+03, 6.63700e+02],\n",
      "       [5.00000e+00, 9.11330e+02, 9.00100e+02],\n",
      "       [6.00000e+00, 8.45400e+02, 9.21010e+02],\n",
      "       [7.00000e+00, 1.10431e+03, 9.22610e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand72', 'img034.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.06572e+03, 9.28000e+01],\n",
      "       [1.00000e+00, 9.66010e+02, 1.94110e+02],\n",
      "       [2.00000e+00, 1.04803e+03, 3.29200e+02],\n",
      "       [3.00000e+00, 9.69230e+02, 5.67210e+02],\n",
      "       [4.00000e+00, 1.27156e+03, 5.72030e+02],\n",
      "       [5.00000e+00, 1.15738e+03, 7.52150e+02],\n",
      "       [6.00000e+00, 1.08984e+03, 8.16470e+02],\n",
      "       [7.00000e+00, 1.23136e+03, 8.34160e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest42', 'img142.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.30630e+02, 3.29200e+02],\n",
      "       [1.00000e+00, 9.32240e+02, 4.30510e+02],\n",
      "       [2.00000e+00, 1.02390e+03, 4.12820e+02],\n",
      "       [4.00000e+00, 1.11074e+03, 6.44400e+02],\n",
      "       [5.00000e+00, 1.42595e+03, 6.47620e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand6', 'img040.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  962.79,  255.22],\n",
      "       [   2.  , 1017.47,  408.  ],\n",
      "       [   4.  ,  880.78,  687.82],\n",
      "       [   5.  , 1163.81,  818.08],\n",
      "       [   6.  , 1110.74,  896.88],\n",
      "       [   7.  , 1014.25,  924.22]], dtype=float32)}}, {'image': ('labeled-data', 'flap2', 'img105.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.67682e+03, 8.79700e+01],\n",
      "       [1.00000e+00, 1.81030e+03, 1.45870e+02],\n",
      "       [2.00000e+00, 1.46937e+03, 1.69990e+02],\n",
      "       [3.00000e+00, 1.38092e+03, 2.69700e+02],\n",
      "       [5.00000e+00, 1.19759e+03, 5.28610e+02],\n",
      "       [6.00000e+00, 1.31498e+03, 6.60480e+02],\n",
      "       [7.00000e+00, 1.34554e+03, 6.37970e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk57', 'img079.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.53140e+02, 3.59760e+02],\n",
      "       [1.00000e+00, 8.30920e+02, 4.98060e+02],\n",
      "       [2.00000e+00, 1.18794e+03, 4.69110e+02],\n",
      "       [4.00000e+00, 1.21528e+03, 7.90740e+02],\n",
      "       [5.00000e+00, 1.48223e+03, 8.84020e+02],\n",
      "       [6.00000e+00, 1.21045e+03, 9.99810e+02],\n",
      "       [7.00000e+00, 1.36162e+03, 1.05448e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest39', 'img043.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00621e+03, 3.88700e+02],\n",
      "       [1.00000e+00, 1.17185e+03, 4.20870e+02],\n",
      "       [3.00000e+00, 7.24780e+02, 6.07410e+02],\n",
      "       [4.00000e+00, 1.29568e+03, 5.65600e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap7', 'img087.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.02069e+03, 4.53030e+02],\n",
      "       [1.00000e+00, 1.03516e+03, 5.07710e+02],\n",
      "       [2.00000e+00, 1.04481e+03, 6.54050e+02],\n",
      "       [3.00000e+00, 8.61480e+02, 8.50250e+02],\n",
      "       [4.00000e+00, 1.09949e+03, 7.93960e+02],\n",
      "       [5.00000e+00, 9.66010e+02, 9.57990e+02],\n",
      "       [6.00000e+00, 9.04900e+02, 1.00946e+03],\n",
      "       [7.00000e+00, 1.01908e+03, 9.82120e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap1', 'img014.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.23297e+03, 7.83300e+01],\n",
      "       [1.00000e+00, 1.33106e+03, 1.34610e+02],\n",
      "       [2.00000e+00, 1.18954e+03, 2.82560e+02],\n",
      "       [3.00000e+00, 1.03677e+03, 4.43380e+02],\n",
      "       [4.00000e+00, 1.37931e+03, 4.57850e+02],\n",
      "       [5.00000e+00, 1.22492e+03, 5.78470e+02],\n",
      "       [6.00000e+00, 1.14452e+03, 6.33140e+02],\n",
      "       [7.00000e+00, 1.30855e+03, 6.46010e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest30', 'img046.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.67110e+02, 3.93500e+02],\n",
      "       [1.00000e+00, 5.77360e+02, 4.41610e+02],\n",
      "       [2.00000e+00, 7.51850e+02, 4.00680e+02],\n",
      "       [4.00000e+00, 8.78950e+02, 5.53630e+02],\n",
      "       [5.00000e+00, 1.05344e+03, 4.96900e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand6', 'img070.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.93350e+02, 2.50400e+02],\n",
      "       [1.00000e+00, 1.06572e+03, 2.72910e+02],\n",
      "       [2.00000e+00, 9.85310e+02, 4.17650e+02],\n",
      "       [4.00000e+00, 8.83990e+02, 6.86210e+02],\n",
      "       [5.00000e+00, 1.16542e+03, 8.10040e+02],\n",
      "       [6.00000e+00, 1.11557e+03, 8.96880e+02],\n",
      "       [7.00000e+00, 1.00943e+03, 9.19400e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk6', 'img128.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 181.28, 207.36],\n",
      "       [  2.  , 181.28, 299.3 ],\n",
      "       [  3.  , 301.37, 473.8 ],\n",
      "       [  4.  ,  58.38, 485.05],\n",
      "       [  5.  , 210.36, 558.23],\n",
      "       [  6.  , 284.48, 622.96],\n",
      "       [  7.  , 156.89, 640.79]], dtype=float32)}}, {'image': ('labeled-data', 'preen33', 'img023.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.44291e+03, 4.48000e+02],\n",
      "       [1.00000e+00, 1.26140e+03, 5.43080e+02],\n",
      "       [2.00000e+00, 1.19657e+03, 4.35040e+02],\n",
      "       [3.00000e+00, 9.37270e+02, 6.29510e+02],\n",
      "       [4.00000e+00, 1.30893e+03, 6.57600e+02],\n",
      "       [5.00000e+00, 1.12526e+03, 8.30470e+02],\n",
      "       [6.00000e+00, 1.01938e+03, 1.00333e+03],\n",
      "       [7.00000e+00, 1.17929e+03, 1.00333e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand75', 'img054.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00417e+03, 3.38480e+02],\n",
      "       [1.00000e+00, 1.07013e+03, 3.62910e+02],\n",
      "       [2.00000e+00, 9.42270e+02, 4.21550e+02],\n",
      "       [3.00000e+00, 1.02127e+03, 5.56740e+02],\n",
      "       [5.00000e+00, 8.73050e+02, 6.24330e+02],\n",
      "       [6.00000e+00, 9.77290e+02, 6.73200e+02],\n",
      "       [7.00000e+00, 9.12950e+02, 6.76450e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen84', 'img000.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.96560e+02, 5.12530e+02],\n",
      "       [1.00000e+00, 9.86920e+02, 6.05800e+02],\n",
      "       [2.00000e+00, 1.13165e+03, 4.62680e+02],\n",
      "       [3.00000e+00, 9.06510e+02, 5.96160e+02],\n",
      "       [4.00000e+00, 1.17507e+03, 6.52440e+02],\n",
      "       [5.00000e+00, 9.96560e+02, 8.29340e+02],\n",
      "       [6.00000e+00, 8.77560e+02, 8.37380e+02],\n",
      "       [7.00000e+00, 1.08180e+03, 8.11650e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand69', 'img060.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.98170e+02, 2.00550e+02],\n",
      "       [1.00000e+00, 1.14452e+03, 2.40750e+02],\n",
      "       [2.00000e+00, 9.67620e+02, 4.36950e+02],\n",
      "       [3.00000e+00, 7.02270e+02, 6.65310e+02],\n",
      "       [4.00000e+00, 9.86920e+02, 6.78170e+02],\n",
      "       [5.00000e+00, 8.18060e+02, 8.63110e+02],\n",
      "       [6.00000e+00, 7.29610e+02, 9.09750e+02],\n",
      "       [7.00000e+00, 9.16160e+02, 9.12960e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen46', 'img055.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.02531e+03, 2.00030e+02],\n",
      "       [1.00000e+00, 9.85110e+02, 2.40820e+02],\n",
      "       [2.00000e+00, 8.98220e+02, 1.26730e+02],\n",
      "       [3.00000e+00, 8.26690e+02, 2.83380e+02],\n",
      "       [4.00000e+00, 1.02885e+03, 2.72740e+02],\n",
      "       [5.00000e+00, 9.09450e+02, 3.83280e+02],\n",
      "       [6.00000e+00, 8.88760e+02, 4.30570e+02],\n",
      "       [7.00000e+00, 9.75650e+02, 4.36480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen40', 'img011.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.98700e+02, 1.64870e+02],\n",
      "       [1.00000e+00, 9.97060e+02, 2.45460e+02],\n",
      "       [2.00000e+00, 1.05791e+03, 3.93490e+02],\n",
      "       [3.00000e+00, 9.23040e+02, 6.71460e+02],\n",
      "       [4.00000e+00, 1.26515e+03, 6.23760e+02],\n",
      "       [5.00000e+00, 1.10726e+03, 8.03040e+02],\n",
      "       [6.00000e+00, 9.95410e+02, 9.82320e+02],\n",
      "       [7.00000e+00, 1.15824e+03, 9.82320e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap12', 'img028.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.23618e+03, 4.24080e+02],\n",
      "       [1.00000e+00, 1.28282e+03, 4.70720e+02],\n",
      "       [2.00000e+00, 1.07215e+03, 5.04490e+02],\n",
      "       [3.00000e+00, 1.21206e+03, 6.60480e+02],\n",
      "       [4.00000e+00, 9.70830e+02, 5.75250e+02],\n",
      "       [5.00000e+00, 9.09720e+02, 7.79490e+02],\n",
      "       [6.00000e+00, 1.00621e+03, 8.77590e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand6', 'img108.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1006.21,  247.18],\n",
      "       [   2.  , 1030.34,  412.82],\n",
      "       [   4.  ,  903.29,  683.  ],\n",
      "       [   5.  , 1167.03,  822.91],\n",
      "       [   6.  , 1130.04,  903.32],\n",
      "       [   7.  , 1030.34,  925.83]], dtype=float32)}}, {'image': ('labeled-data', 'preen84', 'img044.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.01425e+03, 6.26710e+02],\n",
      "       [1.00000e+00, 1.06089e+03, 6.92650e+02],\n",
      "       [2.00000e+00, 1.13326e+03, 5.47910e+02],\n",
      "       [3.00000e+00, 8.77560e+02, 6.17060e+02],\n",
      "       [4.00000e+00, 1.16221e+03, 6.73350e+02],\n",
      "       [5.00000e+00, 9.75660e+02, 8.24520e+02],\n",
      "       [6.00000e+00, 9.00070e+02, 8.37380e+02],\n",
      "       [7.00000e+00, 1.08180e+03, 8.10040e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest39', 'img097.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.01747e+03, 4.06390e+02],\n",
      "       [1.00000e+00, 1.16060e+03, 4.57850e+02],\n",
      "       [3.00000e+00, 7.39260e+02, 6.33140e+02],\n",
      "       [4.00000e+00, 1.29247e+03, 5.70430e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand69', 'img031.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.94960e+02, 2.24670e+02],\n",
      "       [1.00000e+00, 1.13487e+03, 2.58440e+02],\n",
      "       [2.00000e+00, 9.54750e+02, 4.43380e+02],\n",
      "       [3.00000e+00, 7.07090e+02, 6.55660e+02],\n",
      "       [4.00000e+00, 9.82090e+02, 6.68520e+02],\n",
      "       [5.00000e+00, 8.38960e+02, 8.45420e+02],\n",
      "       [6.00000e+00, 7.28000e+02, 9.08140e+02],\n",
      "       [7.00000e+00, 9.09720e+02, 9.06530e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap7', 'img115.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.80480e+02, 5.97760e+02],\n",
      "       [1.00000e+00, 9.91740e+02, 7.13550e+02],\n",
      "       [2.00000e+00, 1.01586e+03, 7.32850e+02],\n",
      "       [3.00000e+00, 8.40570e+02, 8.66330e+02],\n",
      "       [4.00000e+00, 1.09145e+03, 8.05220e+02],\n",
      "       [5.00000e+00, 9.54750e+02, 9.77290e+02],\n",
      "       [6.00000e+00, 9.03290e+02, 1.02071e+03],\n",
      "       [7.00000e+00, 1.02230e+03, 1.00141e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest8', 'img097.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  830.92,  121.75],\n",
      "       [   2.  ,  956.36,  260.05],\n",
      "       [   4.  ,  711.92,  683.  ],\n",
      "       [   5.  , 1445.24,  620.28]], dtype=float32)}}, {'image': ('labeled-data', 'stand82', 'img077.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.43490e+02, 1.04060e+02],\n",
      "       [1.00000e+00, 7.76250e+02, 1.47480e+02],\n",
      "       [2.00000e+00, 1.07697e+03, 3.34020e+02],\n",
      "       [4.00000e+00, 1.06732e+03, 7.42500e+02],\n",
      "       [5.00000e+00, 1.31337e+03, 8.14870e+02],\n",
      "       [6.00000e+00, 1.08180e+03, 9.56390e+02],\n",
      "       [7.00000e+00, 1.11235e+03, 1.01267e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk62', 'img071.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.04320e+03, 1.60340e+02],\n",
      "       [1.00000e+00, 1.11396e+03, 2.27890e+02],\n",
      "       [2.00000e+00, 9.30630e+02, 2.93820e+02],\n",
      "       [3.00000e+00, 1.07215e+03, 5.36650e+02],\n",
      "       [5.00000e+00, 8.63090e+02, 6.46010e+02],\n",
      "       [6.00000e+00, 9.96560e+02, 7.47320e+02],\n",
      "       [7.00000e+00, 8.50220e+02, 8.13260e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand2', 'img023.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.23180e+02, 9.44100e+01],\n",
      "       [1.00000e+00, 5.59140e+02, 1.65170e+02],\n",
      "       [2.00000e+00, 8.96860e+02, 2.58440e+02],\n",
      "       [3.00000e+00, 7.31220e+02, 5.68820e+02],\n",
      "       [4.00000e+00, 1.03516e+03, 6.42790e+02],\n",
      "       [5.00000e+00, 1.13808e+03, 6.57270e+02],\n",
      "       [6.00000e+00, 8.50220e+02, 8.51850e+02],\n",
      "       [7.00000e+00, 9.59580e+02, 9.16180e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest19', 'img090.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.25066e+03, 2.90600e+02],\n",
      "       [1.00000e+00, 1.43559e+03, 3.21160e+02],\n",
      "       [2.00000e+00, 1.07697e+03, 3.93530e+02],\n",
      "       [3.00000e+00, 1.07376e+03, 7.13550e+02],\n",
      "       [5.00000e+00, 7.00660e+02, 5.46300e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap12', 'img002.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.23297e+03, 3.88700e+02],\n",
      "       [1.00000e+00, 1.31820e+03, 4.30510e+02],\n",
      "       [2.00000e+00, 1.08341e+03, 4.86800e+02],\n",
      "       [3.00000e+00, 1.07215e+03, 6.18670e+02],\n",
      "       [5.00000e+00, 8.75950e+02, 7.50540e+02],\n",
      "       [6.00000e+00, 9.46710e+02, 8.30950e+02],\n",
      "       [7.00000e+00, 9.72440e+02, 8.63110e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk80', 'img148.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 475.19, 507.62],\n",
      "       [  1.  , 442.35, 548.67],\n",
      "       [  2.  , 540.04, 558.52],\n",
      "       [  4.  , 472.72, 678.38],\n",
      "       [  5.  , 612.29, 735.85],\n",
      "       [  6.  , 571.24, 796.6 ],\n",
      "       [  7.  , 536.76, 813.84]], dtype=float32)}}, {'image': ('labeled-data', 'rest8', 'img027.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  814.84,  112.1 ],\n",
      "       [   2.  ,  959.58,  243.97],\n",
      "       [   4.  ,  721.57,  660.48],\n",
      "       [   5.  , 1433.99,  615.45]], dtype=float32)}}, {'image': ('labeled-data', 'flap7', 'img076.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.04159e+03, 4.17650e+02],\n",
      "       [1.00000e+00, 1.11557e+03, 4.75540e+02],\n",
      "       [2.00000e+00, 1.07215e+03, 6.41180e+02],\n",
      "       [3.00000e+00, 8.66300e+02, 8.58290e+02],\n",
      "       [4.00000e+00, 1.10592e+03, 8.10040e+02],\n",
      "       [5.00000e+00, 9.57970e+02, 9.56390e+02],\n",
      "       [6.00000e+00, 9.00070e+02, 1.02232e+03],\n",
      "       [7.00000e+00, 1.02551e+03, 9.96590e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen46', 'img027.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00225e+03, 2.11850e+02],\n",
      "       [1.00000e+00, 9.68560e+02, 2.50870e+02],\n",
      "       [2.00000e+00, 8.81670e+02, 1.35010e+02],\n",
      "       [3.00000e+00, 8.09550e+02, 2.96970e+02],\n",
      "       [4.00000e+00, 1.01585e+03, 2.81010e+02],\n",
      "       [5.00000e+00, 8.96440e+02, 3.98640e+02],\n",
      "       [6.00000e+00, 8.71030e+02, 4.38250e+02],\n",
      "       [7.00000e+00, 9.62650e+02, 4.42390e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap4', 'img098.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.36805e+03, 5.90300e+01],\n",
      "       [1.00000e+00, 1.56425e+03, 1.13710e+02],\n",
      "       [2.00000e+00, 1.06572e+03, 2.26280e+02],\n",
      "       [3.00000e+00, 8.35750e+02, 4.93230e+02],\n",
      "       [4.00000e+00, 1.24583e+03, 5.02880e+02],\n",
      "       [5.00000e+00, 7.42470e+02, 7.93960e+02],\n",
      "       [6.00000e+00, 9.29020e+02, 9.64430e+02],\n",
      "       [7.00000e+00, 1.00621e+03, 9.51560e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand69', 'img006.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.72440e+02, 2.34320e+02],\n",
      "       [1.00000e+00, 1.11074e+03, 2.92210e+02],\n",
      "       [2.00000e+00, 9.35450e+02, 4.51420e+02],\n",
      "       [3.00000e+00, 6.97450e+02, 6.65310e+02],\n",
      "       [4.00000e+00, 9.82090e+02, 6.65310e+02],\n",
      "       [5.00000e+00, 8.37360e+02, 8.55070e+02],\n",
      "       [6.00000e+00, 7.36040e+02, 9.16180e+02],\n",
      "       [7.00000e+00, 9.19370e+02, 9.09750e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk86', 'img062.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 677.74, 440.85],\n",
      "       [  1.  , 660.81, 495.2 ],\n",
      "       [  2.  , 802.49, 416.79],\n",
      "       [  4.  , 752.59, 547.77],\n",
      "       [  5.  , 888.91, 584.3 ],\n",
      "       [  6.  , 877.33, 664.49],\n",
      "       [  7.  , 790.9 , 652.91]], dtype=float32)}}, {'image': ('labeled-data', 'preen84', 'img125.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.03034e+03, 3.01860e+02],\n",
      "       [1.00000e+00, 1.00782e+03, 4.11220e+02],\n",
      "       [2.00000e+00, 1.07536e+03, 4.62680e+02],\n",
      "       [3.00000e+00, 8.71130e+02, 6.17060e+02],\n",
      "       [4.00000e+00, 1.15416e+03, 6.73350e+02],\n",
      "       [5.00000e+00, 9.69230e+02, 8.32560e+02],\n",
      "       [6.00000e+00, 9.11330e+02, 8.43810e+02],\n",
      "       [7.00000e+00, 1.07536e+03, 8.19690e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand75', 'img037.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.01882e+03, 3.35220e+02],\n",
      "       [1.00000e+00, 1.07746e+03, 3.62910e+02],\n",
      "       [2.00000e+00, 9.52860e+02, 4.16660e+02],\n",
      "       [3.00000e+00, 1.01964e+03, 5.55920e+02],\n",
      "       [5.00000e+00, 8.86080e+02, 6.16190e+02],\n",
      "       [6.00000e+00, 9.78100e+02, 6.70750e+02],\n",
      "       [7.00000e+00, 9.09700e+02, 6.71570e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest65', 'img095.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.30855e+03, 3.09900e+02],\n",
      "       [1.00000e+00, 1.48706e+03, 3.51710e+02],\n",
      "       [2.00000e+00, 1.11557e+03, 4.46600e+02],\n",
      "       [3.00000e+00, 1.26352e+03, 7.68230e+02],\n",
      "       [5.00000e+00, 5.75220e+02, 6.21890e+02],\n",
      "       [6.00000e+00, 8.55050e+02, 8.42210e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen33', 'img010.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.43858e+03, 4.30710e+02],\n",
      "       [1.00000e+00, 1.26356e+03, 5.32270e+02],\n",
      "       [2.00000e+00, 1.19009e+03, 4.30710e+02],\n",
      "       [3.00000e+00, 9.30790e+02, 6.46800e+02],\n",
      "       [4.00000e+00, 1.32190e+03, 6.70560e+02],\n",
      "       [5.00000e+00, 1.12743e+03, 8.36950e+02],\n",
      "       [6.00000e+00, 1.02587e+03, 1.01414e+03],\n",
      "       [7.00000e+00, 1.19441e+03, 1.02710e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk89', 'img031.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.87190e+02, 1.74740e+02],\n",
      "       [1.00000e+00, 8.83570e+02, 2.60270e+02],\n",
      "       [2.00000e+00, 1.05133e+03, 3.40860e+02],\n",
      "       [3.00000e+00, 9.41130e+02, 5.39880e+02],\n",
      "       [4.00000e+00, 1.21252e+03, 5.56330e+02],\n",
      "       [5.00000e+00, 1.07107e+03, 7.12580e+02],\n",
      "       [6.00000e+00, 1.02995e+03, 7.68500e+02],\n",
      "       [7.00000e+00, 1.15331e+03, 7.78370e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand19', 'img053.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.04481e+03, 1.87680e+02],\n",
      "       [1.00000e+00, 1.04642e+03, 2.45580e+02],\n",
      "       [2.00000e+00, 1.11235e+03, 4.17650e+02],\n",
      "       [3.00000e+00, 9.03290e+02, 7.07120e+02],\n",
      "       [4.00000e+00, 1.25066e+03, 6.58870e+02],\n",
      "       [5.00000e+00, 1.09627e+03, 8.47030e+02],\n",
      "       [6.00000e+00, 9.86920e+02, 1.00302e+03],\n",
      "       [7.00000e+00, 1.15577e+03, 1.00624e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand2', 'img000.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.19960e+02, 9.11900e+01],\n",
      "       [1.00000e+00, 5.35020e+02, 1.24960e+02],\n",
      "       [2.00000e+00, 9.38670e+02, 2.48790e+02],\n",
      "       [3.00000e+00, 7.82680e+02, 5.57560e+02],\n",
      "       [4.00000e+00, 1.07376e+03, 6.49230e+02],\n",
      "       [5.00000e+00, 1.18472e+03, 6.49230e+02],\n",
      "       [6.00000e+00, 8.92030e+02, 8.35770e+02],\n",
      "       [7.00000e+00, 9.96560e+02, 8.93670e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest42', 'img079.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.80780e+02, 3.54930e+02],\n",
      "       [1.00000e+00, 7.11920e+02, 3.96740e+02],\n",
      "       [2.00000e+00, 1.04320e+03, 4.46600e+02],\n",
      "       [4.00000e+00, 1.09949e+03, 6.73350e+02],\n",
      "       [5.00000e+00, 1.42916e+03, 6.78170e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk82', 'img060.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.57229e+03, 1.15310e+02],\n",
      "       [1.00000e+00, 1.66556e+03, 1.78030e+02],\n",
      "       [2.00000e+00, 1.43399e+03, 2.29490e+02],\n",
      "       [3.00000e+00, 1.39700e+03, 4.65890e+02],\n",
      "       [5.00000e+00, 1.25066e+03, 5.14140e+02],\n",
      "       [6.00000e+00, 1.34071e+03, 6.41180e+02],\n",
      "       [7.00000e+00, 1.38574e+03, 6.25100e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest30', 'img144.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.46290e+02, 4.03550e+02],\n",
      "       [1.00000e+00, 5.61560e+02, 4.48070e+02],\n",
      "       [2.00000e+00, 7.34610e+02, 4.10010e+02],\n",
      "       [4.00000e+00, 8.68170e+02, 5.62240e+02],\n",
      "       [5.00000e+00, 1.04913e+03, 5.22030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand67', 'img007.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.21528e+03, 1.12100e+02],\n",
      "       [1.00000e+00, 1.32463e+03, 1.28180e+02],\n",
      "       [2.00000e+00, 1.16864e+03, 2.60050e+02],\n",
      "       [3.00000e+00, 1.03194e+03, 4.80370e+02],\n",
      "       [4.00000e+00, 1.27799e+03, 4.73930e+02],\n",
      "       [5.00000e+00, 1.18150e+03, 6.52440e+02],\n",
      "       [6.00000e+00, 1.10753e+03, 6.65310e+02],\n",
      "       [7.00000e+00, 1.26030e+03, 6.60480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand6', 'img104.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1006.21,  248.79],\n",
      "       [   2.  , 1028.73,  409.61],\n",
      "       [   4.  ,  901.68,  686.21],\n",
      "       [   5.  , 1171.85,  819.69],\n",
      "       [   6.  , 1131.65,  901.71],\n",
      "       [   7.  , 1028.73,  925.83]], dtype=float32)}}, {'image': ('labeled-data', 'walk62', 'img012.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.07858e+03, 2.56830e+02],\n",
      "       [1.00000e+00, 1.14612e+03, 3.22770e+02],\n",
      "       [2.00000e+00, 9.01680e+02, 3.64580e+02],\n",
      "       [3.00000e+00, 9.03290e+02, 6.33140e+02],\n",
      "       [5.00000e+00, 6.71710e+02, 7.29630e+02],\n",
      "       [6.00000e+00, 7.87500e+02, 8.59900e+02],\n",
      "       [7.00000e+00, 8.43790e+02, 9.03320e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen48', 'img134.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.38579e+03, 2.81630e+02],\n",
      "       [1.00000e+00, 1.35654e+03, 3.22960e+02],\n",
      "       [2.00000e+00, 1.28278e+03, 2.07230e+02],\n",
      "       [3.00000e+00, 1.20711e+03, 3.73200e+02],\n",
      "       [4.00000e+00, 1.41250e+03, 3.54120e+02],\n",
      "       [5.00000e+00, 1.28914e+03, 4.81300e+02],\n",
      "       [6.00000e+00, 1.26052e+03, 5.22630e+02],\n",
      "       [7.00000e+00, 1.35018e+03, 5.26450e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand69', 'img140.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.45100e+02, 2.27890e+02],\n",
      "       [1.00000e+00, 1.03838e+03, 2.76130e+02],\n",
      "       [2.00000e+00, 9.62790e+02, 4.43380e+02],\n",
      "       [3.00000e+00, 6.99050e+02, 6.73350e+02],\n",
      "       [4.00000e+00, 9.88520e+02, 6.83000e+02],\n",
      "       [5.00000e+00, 8.29310e+02, 8.47030e+02],\n",
      "       [6.00000e+00, 7.26390e+02, 9.14570e+02],\n",
      "       [7.00000e+00, 9.14550e+02, 9.09750e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap3', 'img062.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.72346e+03, 1.28180e+02],\n",
      "       [1.00000e+00, 1.86498e+03, 3.14730e+02],\n",
      "       [2.00000e+00, 1.38413e+03, 1.74820e+02],\n",
      "       [3.00000e+00, 1.16703e+03, 2.45580e+02],\n",
      "       [5.00000e+00, 9.11330e+02, 7.45720e+02],\n",
      "       [6.00000e+00, 1.19437e+03, 9.62820e+02],\n",
      "       [7.00000e+00, 1.14452e+03, 9.54780e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest39', 'img003.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.67620e+02, 3.85490e+02],\n",
      "       [1.00000e+00, 1.12683e+03, 3.85490e+02],\n",
      "       [3.00000e+00, 7.36040e+02, 6.21890e+02],\n",
      "       [4.00000e+00, 1.29729e+03, 5.62380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk57', 'img068.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.13808e+03, 3.29200e+02],\n",
      "       [1.00000e+00, 1.02230e+03, 5.12530e+02],\n",
      "       [2.00000e+00, 1.39057e+03, 4.35340e+02],\n",
      "       [3.00000e+00, 1.27156e+03, 7.50540e+02],\n",
      "       [4.00000e+00, 1.41951e+03, 7.82700e+02],\n",
      "       [5.00000e+00, 1.66395e+03, 8.48640e+02],\n",
      "       [7.00000e+00, 1.45489e+03, 1.03036e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand79', 'img138.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.83700e+02, 2.92210e+02],\n",
      "       [1.00000e+00, 9.33850e+02, 3.35630e+02],\n",
      "       [2.00000e+00, 9.25810e+02, 4.72330e+02],\n",
      "       [3.00000e+00, 8.96860e+02, 6.20280e+02],\n",
      "       [4.00000e+00, 1.09627e+03, 6.44400e+02],\n",
      "       [5.00000e+00, 1.03516e+03, 7.65010e+02],\n",
      "       [6.00000e+00, 1.02390e+03, 8.11650e+02],\n",
      "       [7.00000e+00, 1.06572e+03, 8.18080e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest24', 'img003.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 5.69330e+02, 2.27970e+02],\n",
      "       [1.00000e+00, 4.39700e+02, 2.95290e+02],\n",
      "       [2.00000e+00, 7.54220e+02, 2.23950e+02],\n",
      "       [4.00000e+00, 9.55190e+02, 4.26930e+02],\n",
      "       [5.00000e+00, 1.20439e+03, 3.48550e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen84', 'img151.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08180e+03, 2.60050e+02],\n",
      "       [1.00000e+00, 1.11074e+03, 3.42070e+02],\n",
      "       [2.00000e+00, 1.10914e+03, 4.36950e+02],\n",
      "       [3.00000e+00, 8.95250e+02, 6.28320e+02],\n",
      "       [4.00000e+00, 1.16703e+03, 6.44400e+02],\n",
      "       [5.00000e+00, 9.82090e+02, 8.21300e+02],\n",
      "       [6.00000e+00, 8.93640e+02, 8.35770e+02],\n",
      "       [7.00000e+00, 1.07054e+03, 8.19690e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand79', 'img008.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.67620e+02, 2.95430e+02],\n",
      "       [1.00000e+00, 8.66300e+02, 3.29200e+02],\n",
      "       [2.00000e+00, 9.30630e+02, 4.69110e+02],\n",
      "       [3.00000e+00, 9.12940e+02, 6.28320e+02],\n",
      "       [4.00000e+00, 1.10592e+03, 6.66920e+02],\n",
      "       [5.00000e+00, 1.05124e+03, 7.73050e+02],\n",
      "       [6.00000e+00, 1.04159e+03, 8.02000e+02],\n",
      "       [7.00000e+00, 1.07054e+03, 8.16470e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap3', 'img101.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.63983e+03, 1.00840e+02],\n",
      "       [1.00000e+00, 1.79261e+03, 2.79350e+02],\n",
      "       [2.00000e+00, 1.33750e+03, 1.74820e+02],\n",
      "       [3.00000e+00, 1.08823e+03, 3.03470e+02],\n",
      "       [4.00000e+00, 1.26030e+03, 1.92510e+02],\n",
      "       [5.00000e+00, 8.98470e+02, 7.68230e+02],\n",
      "       [6.00000e+00, 1.12522e+03, 9.67640e+02],\n",
      "       [7.00000e+00, 1.18311e+03, 9.49950e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk66', 'img004.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.16788e+03, 2.41120e+02],\n",
      "       [1.00000e+00, 1.09916e+03, 2.77840e+02],\n",
      "       [2.00000e+00, 1.17447e+03, 3.63500e+02],\n",
      "       [3.00000e+00, 1.22154e+03, 5.19770e+02],\n",
      "       [4.00000e+00, 1.38346e+03, 5.05650e+02],\n",
      "       [5.00000e+00, 1.31756e+03, 6.08260e+02],\n",
      "       [6.00000e+00, 1.29685e+03, 6.53440e+02],\n",
      "       [7.00000e+00, 1.39005e+03, 6.53440e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap4', 'img025.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.21688e+03, 1.61950e+02],\n",
      "       [1.00000e+00, 1.31981e+03, 2.03760e+02],\n",
      "       [2.00000e+00, 1.19759e+03, 4.19260e+02],\n",
      "       [4.00000e+00, 6.33120e+02, 2.68090e+02],\n",
      "       [5.00000e+00, 8.34140e+02, 9.40300e+02],\n",
      "       [6.00000e+00, 1.08984e+03, 9.22610e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest24', 'img094.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.90400e+02, 3.94770e+02],\n",
      "       [1.00000e+00, 6.66800e+02, 4.24920e+02],\n",
      "       [2.00000e+00, 9.79310e+02, 4.07830e+02],\n",
      "       [4.00000e+00, 1.16119e+03, 6.07800e+02],\n",
      "       [5.00000e+00, 1.40838e+03, 5.30430e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen33', 'img075.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.38456e+03, 4.19910e+02],\n",
      "       [1.00000e+00, 1.22034e+03, 5.36590e+02],\n",
      "       [2.00000e+00, 1.15119e+03, 4.24230e+02],\n",
      "       [3.00000e+00, 8.98380e+02, 6.20870e+02],\n",
      "       [4.00000e+00, 1.28517e+03, 6.55440e+02],\n",
      "       [5.00000e+00, 1.08853e+03, 8.02370e+02],\n",
      "       [6.00000e+00, 9.93450e+02, 9.81720e+02],\n",
      "       [7.00000e+00, 1.15119e+03, 9.92530e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand67', 'img105.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17346e+03, 1.28180e+02],\n",
      "       [1.00000e+00, 1.26352e+03, 1.41040e+02],\n",
      "       [2.00000e+00, 1.15899e+03, 2.68090e+02],\n",
      "       [3.00000e+00, 1.01425e+03, 5.06100e+02],\n",
      "       [4.00000e+00, 1.27317e+03, 4.81980e+02],\n",
      "       [5.00000e+00, 1.18150e+03, 6.62090e+02],\n",
      "       [6.00000e+00, 1.09949e+03, 6.76560e+02],\n",
      "       [7.00000e+00, 1.25870e+03, 6.57270e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap7', 'img022.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.99780e+02, 4.49810e+02],\n",
      "       [1.00000e+00, 9.98170e+02, 5.38260e+02],\n",
      "       [2.00000e+00, 1.06089e+03, 6.57270e+02],\n",
      "       [3.00000e+00, 8.79170e+02, 8.29340e+02],\n",
      "       [4.00000e+00, 1.11235e+03, 7.81100e+02],\n",
      "       [5.00000e+00, 9.64400e+02, 9.38700e+02],\n",
      "       [6.00000e+00, 8.98470e+02, 1.00302e+03],\n",
      "       [7.00000e+00, 1.03034e+03, 9.77290e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk82', 'img058.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.53691e+03, 1.21750e+02],\n",
      "       [1.00000e+00, 1.63822e+03, 1.81250e+02],\n",
      "       [2.00000e+00, 1.40021e+03, 2.58440e+02],\n",
      "       [3.00000e+00, 1.36644e+03, 4.72330e+02],\n",
      "       [5.00000e+00, 1.21849e+03, 5.25400e+02],\n",
      "       [6.00000e+00, 1.33750e+03, 6.54050e+02],\n",
      "       [7.00000e+00, 1.27799e+03, 6.49230e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap1', 'img086.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.16864e+03, 7.99300e+01],\n",
      "       [1.00000e+00, 1.17990e+03, 1.44260e+02],\n",
      "       [2.00000e+00, 1.16060e+03, 2.58440e+02],\n",
      "       [3.00000e+00, 1.05285e+03, 4.01570e+02],\n",
      "       [4.00000e+00, 1.32946e+03, 4.08000e+02],\n",
      "       [5.00000e+00, 1.21849e+03, 5.57560e+02],\n",
      "       [6.00000e+00, 1.15577e+03, 6.33140e+02],\n",
      "       [7.00000e+00, 1.29568e+03, 6.36360e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk6', 'img039.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 661.62, 287.1 ],\n",
      "       [  1.  , 737.61, 308.68],\n",
      "       [  2.  , 585.63, 394.05],\n",
      "       [  3.  , 720.73, 567.61],\n",
      "       [  4.  , 485.25, 561.05],\n",
      "       [  5.  , 576.25, 658.62],\n",
      "       [  6.  , 651.3 , 737.42],\n",
      "       [  7.  , 532.16, 741.17]], dtype=float32)}}, {'image': ('labeled-data', 'preen35', 'img049.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.68130e+02, 2.92420e+02],\n",
      "       [1.00000e+00, 9.54560e+02, 4.35040e+02],\n",
      "       [2.00000e+00, 1.14039e+03, 3.74530e+02],\n",
      "       [3.00000e+00, 9.35110e+02, 6.87850e+02],\n",
      "       [4.00000e+00, 1.22466e+03, 6.94330e+02],\n",
      "       [5.00000e+00, 1.07989e+03, 8.08860e+02],\n",
      "       [6.00000e+00, 9.65360e+02, 9.92530e+02],\n",
      "       [7.00000e+00, 1.12743e+03, 1.00333e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest42', 'img088.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.96860e+02, 3.53320e+02],\n",
      "       [1.00000e+00, 7.23180e+02, 3.87090e+02],\n",
      "       [2.00000e+00, 1.04803e+03, 4.41770e+02],\n",
      "       [4.00000e+00, 1.10914e+03, 6.78170e+02],\n",
      "       [5.00000e+00, 1.43881e+03, 6.78170e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand19', 'img101.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.01586e+03, 1.79640e+02],\n",
      "       [1.00000e+00, 9.30630e+02, 2.84170e+02],\n",
      "       [2.00000e+00, 1.11879e+03, 4.12820e+02],\n",
      "       [3.00000e+00, 9.14550e+02, 6.99080e+02],\n",
      "       [4.00000e+00, 1.25548e+03, 6.39580e+02],\n",
      "       [5.00000e+00, 1.09949e+03, 8.29340e+02],\n",
      "       [6.00000e+00, 9.93350e+02, 9.80510e+02],\n",
      "       [7.00000e+00, 1.16864e+03, 9.88550e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen48', 'img103.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.35972e+03, 3.31870e+02],\n",
      "       [1.00000e+00, 1.32411e+03, 3.66840e+02],\n",
      "       [2.00000e+00, 1.23827e+03, 2.45380e+02],\n",
      "       [3.00000e+00, 1.16578e+03, 4.04360e+02],\n",
      "       [4.00000e+00, 1.38643e+03, 3.75740e+02],\n",
      "       [5.00000e+00, 1.25353e+03, 5.13730e+02],\n",
      "       [6.00000e+00, 1.22682e+03, 5.49340e+02],\n",
      "       [7.00000e+00, 1.31648e+03, 5.54430e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest8', 'img056.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  819.67,  120.14],\n",
      "       [   2.  ,  959.58,  247.18],\n",
      "       [   4.  ,  711.92,  678.17],\n",
      "       [   5.  , 1432.38,  612.24]], dtype=float32)}}, {'image': ('labeled-data', 'flap1', 'img097.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[1.00000e+00, 1.24744e+03, 7.57000e+00],\n",
      "       [2.00000e+00, 1.17829e+03, 2.06980e+02],\n",
      "       [3.00000e+00, 1.09305e+03, 4.28910e+02],\n",
      "       [4.00000e+00, 1.30372e+03, 4.09610e+02],\n",
      "       [5.00000e+00, 1.22010e+03, 5.76860e+02],\n",
      "       [6.00000e+00, 1.16060e+03, 6.31540e+02],\n",
      "       [7.00000e+00, 1.29408e+03, 6.36360e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen40', 'img071.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.09245e+03, 1.08950e+02],\n",
      "       [1.00000e+00, 1.17140e+03, 1.08950e+02],\n",
      "       [2.00000e+00, 1.15660e+03, 3.58950e+02],\n",
      "       [3.00000e+00, 9.31260e+02, 6.41850e+02],\n",
      "       [4.00000e+00, 1.24706e+03, 6.33630e+02],\n",
      "       [5.00000e+00, 1.10068e+03, 8.04690e+02],\n",
      "       [6.00000e+00, 9.97060e+02, 9.83970e+02],\n",
      "       [7.00000e+00, 1.16318e+03, 9.95480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap1', 'img120.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.33910e+03, 4.35000e+00],\n",
      "       [1.00000e+00, 1.46133e+03, 7.67200e+01],\n",
      "       [2.00000e+00, 1.29086e+03, 1.92510e+02],\n",
      "       [3.00000e+00, 1.01425e+03, 3.01860e+02],\n",
      "       [4.00000e+00, 1.47902e+03, 3.30810e+02],\n",
      "       [5.00000e+00, 1.22010e+03, 5.36650e+02],\n",
      "       [6.00000e+00, 1.15738e+03, 6.23490e+02],\n",
      "       [7.00000e+00, 1.28443e+03, 6.28320e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk80', 'img027.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 586.84, 517.47],\n",
      "       [  1.  , 622.96, 574.94],\n",
      "       [  2.  , 492.43, 544.56],\n",
      "       [  3.  , 478.47, 666.88],\n",
      "       [  5.  , 369.28, 724.35],\n",
      "       [  6.  , 402.94, 794.95],\n",
      "       [  7.  , 454.66, 780.18]], dtype=float32)}}, {'image': ('labeled-data', 'walk64', 'img023.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.19919e+03, 1.45870e+02],\n",
      "       [1.00000e+00, 1.26834e+03, 2.11800e+02],\n",
      "       [2.00000e+00, 1.16060e+03, 2.82560e+02],\n",
      "       [3.00000e+00, 9.77270e+02, 4.25690e+02],\n",
      "       [4.00000e+00, 1.18311e+03, 4.33730e+02],\n",
      "       [5.00000e+00, 1.00139e+03, 5.41480e+02],\n",
      "       [6.00000e+00, 9.06510e+02, 5.89720e+02],\n",
      "       [7.00000e+00, 1.08501e+03, 5.86510e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand79', 'img049.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.78870e+02, 2.92210e+02],\n",
      "       [1.00000e+00, 9.53140e+02, 3.38850e+02],\n",
      "       [2.00000e+00, 9.17760e+02, 4.67500e+02],\n",
      "       [3.00000e+00, 9.06510e+02, 6.29930e+02],\n",
      "       [4.00000e+00, 1.10270e+03, 6.57270e+02],\n",
      "       [5.00000e+00, 1.04642e+03, 7.63410e+02],\n",
      "       [6.00000e+00, 1.03677e+03, 8.02000e+02],\n",
      "       [7.00000e+00, 1.07215e+03, 8.14870e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand11', 'img079.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.07044e+03, 3.53060e+02],\n",
      "       [1.00000e+00, 1.10974e+03, 3.54120e+02],\n",
      "       [2.00000e+00, 1.03592e+03, 4.33790e+02],\n",
      "       [3.00000e+00, 1.11239e+03, 5.35220e+02],\n",
      "       [5.00000e+00, 1.03379e+03, 6.24980e+02],\n",
      "       [6.00000e+00, 1.08584e+03, 6.43030e+02],\n",
      "       [7.00000e+00, 1.01733e+03, 6.48870e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest30', 'img006.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.86360e+02, 4.09410e+02],\n",
      "       [1.00000e+00, 6.03210e+02, 4.55970e+02],\n",
      "       [2.00000e+00, 7.90620e+02, 4.11450e+02],\n",
      "       [4.00000e+00, 9.03360e+02, 5.71580e+02],\n",
      "       [5.00000e+00, 1.07282e+03, 5.10540e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest15', 'img141.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 5.80050e+02, 4.94840e+02],\n",
      "       [1.00000e+00, 4.49790e+02, 6.02590e+02],\n",
      "       [2.00000e+00, 8.40570e+02, 4.73930e+02],\n",
      "       [4.00000e+00, 9.69230e+02, 8.00390e+02],\n",
      "       [5.00000e+00, 1.34715e+03, 7.10340e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk64', 'img004.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 964.4 , 112.1 ],\n",
      "       [  1.  , 998.17, 195.72],\n",
      "       [  2.  , 948.32, 256.83],\n",
      "       [  3.  , 784.29, 398.35],\n",
      "       [  4.  , 982.09, 375.84],\n",
      "       [  5.  , 834.14, 494.84],\n",
      "       [  6.  , 744.08, 559.17],\n",
      "       [  7.  , 932.24, 517.36]], dtype=float32)}}, {'image': ('labeled-data', 'flap2', 'img149.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.73793e+03, 5.25900e+01],\n",
      "       [1.00000e+00, 1.81191e+03, 8.63700e+01],\n",
      "       [2.00000e+00, 1.55942e+03, 1.57130e+02],\n",
      "       [3.00000e+00, 1.51761e+03, 2.63260e+02],\n",
      "       [4.00000e+00, 1.38896e+03, 2.50400e+02],\n",
      "       [5.00000e+00, 1.30855e+03, 5.27000e+02],\n",
      "       [6.00000e+00, 1.43720e+03, 6.54050e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen40', 'img030.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.02831e+03, 7.11200e+01],\n",
      "       [1.00000e+00, 1.04804e+03, 1.35260e+02],\n",
      "       [2.00000e+00, 1.08258e+03, 3.22770e+02],\n",
      "       [3.00000e+00, 9.01660e+02, 6.23760e+02],\n",
      "       [4.00000e+00, 1.25035e+03, 6.02380e+02],\n",
      "       [5.00000e+00, 1.07929e+03, 7.88240e+02],\n",
      "       [6.00000e+00, 9.83900e+02, 9.70810e+02],\n",
      "       [7.00000e+00, 1.15166e+03, 9.79030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand75', 'img080.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00091e+03, 3.40110e+02],\n",
      "       [1.00000e+00, 1.05059e+03, 3.60470e+02],\n",
      "       [2.00000e+00, 9.43900e+02, 4.15030e+02],\n",
      "       [3.00000e+00, 1.01801e+03, 5.50220e+02],\n",
      "       [5.00000e+00, 8.86080e+02, 6.24330e+02],\n",
      "       [6.00000e+00, 9.80550e+02, 6.73200e+02],\n",
      "       [7.00000e+00, 9.09700e+02, 6.72380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk6', 'img056.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 633.48, 293.67],\n",
      "       [  2.  , 637.23, 397.8 ],\n",
      "       [  3.  , 769.51, 576.99],\n",
      "       [  4.  , 519.96, 577.93],\n",
      "       [  5.  , 597.83, 658.62],\n",
      "       [  6.  , 668.19, 735.54]], dtype=float32)}}, {'image': ('labeled-data', 'rest42', 'img147.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.14550e+02, 3.19550e+02],\n",
      "       [1.00000e+00, 8.83990e+02, 4.44990e+02],\n",
      "       [2.00000e+00, 1.02551e+03, 4.03180e+02],\n",
      "       [4.00000e+00, 1.11074e+03, 6.33140e+02],\n",
      "       [5.00000e+00, 1.41790e+03, 6.37970e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap3', 'img142.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.48706e+03, 1.12100e+02],\n",
      "       [1.00000e+00, 1.62375e+03, 2.69700e+02],\n",
      "       [2.00000e+00, 1.20241e+03, 1.89290e+02],\n",
      "       [3.00000e+00, 9.09720e+02, 4.72330e+02],\n",
      "       [4.00000e+00, 1.51600e+03, 4.77150e+02],\n",
      "       [5.00000e+00, 8.85600e+02, 8.02000e+02],\n",
      "       [6.00000e+00, 1.04159e+03, 9.86940e+02],\n",
      "       [7.00000e+00, 1.19276e+03, 9.35480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk57', 'img094.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.91010e+02, 3.06690e+02],\n",
      "       [1.00000e+00, 5.49490e+02, 4.27300e+02],\n",
      "       [2.00000e+00, 9.22590e+02, 4.46600e+02],\n",
      "       [4.00000e+00, 8.74340e+02, 7.95570e+02],\n",
      "       [5.00000e+00, 1.22653e+03, 8.55070e+02],\n",
      "       [7.00000e+00, 9.80480e+02, 1.04323e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand9', 'img015.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.06732e+03, 2.55220e+02],\n",
      "       [1.00000e+00, 1.19759e+03, 2.90600e+02],\n",
      "       [2.00000e+00, 1.01747e+03, 4.20870e+02],\n",
      "       [4.00000e+00, 9.14550e+02, 7.00690e+02],\n",
      "       [5.00000e+00, 1.19115e+03, 8.30950e+02],\n",
      "       [6.00000e+00, 1.15416e+03, 9.14570e+02],\n",
      "       [7.00000e+00, 1.05124e+03, 9.38700e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk6', 'img026.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 658.23, 275.07],\n",
      "       [  1.  , 727.31, 298.1 ],\n",
      "       [  2.  , 588.45, 396.87],\n",
      "       [  3.  , 720.73, 570.43],\n",
      "       [  4.  , 475.87, 567.61],\n",
      "       [  5.  , 575.31, 651.11],\n",
      "       [  6.  , 646.61, 726.16],\n",
      "       [  7.  , 526.53, 735.54]], dtype=float32)}}, {'image': ('labeled-data', 'walk86', 'img065.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 655.47, 436.39],\n",
      "       [  1.  , 640.32, 490.74],\n",
      "       [  2.  , 757.93, 405.2 ],\n",
      "       [  4.  , 718.73, 554.9 ],\n",
      "       [  5.  , 871.98, 577.17],\n",
      "       [  6.  , 863.07, 638.65],\n",
      "       [  7.  , 788.23, 651.13]], dtype=float32)}}, {'image': ('labeled-data', 'rest39', 'img139.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.03194e+03, 4.04780e+02],\n",
      "       [1.00000e+00, 1.19115e+03, 4.59460e+02],\n",
      "       [3.00000e+00, 7.47300e+02, 6.37970e+02],\n",
      "       [4.00000e+00, 1.30372e+03, 5.65600e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand19', 'img077.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.02712e+03, 1.81250e+02],\n",
      "       [1.00000e+00, 9.41890e+02, 2.66480e+02],\n",
      "       [2.00000e+00, 1.11396e+03, 4.19260e+02],\n",
      "       [3.00000e+00, 9.09720e+02, 7.02290e+02],\n",
      "       [4.00000e+00, 1.24905e+03, 6.68520e+02],\n",
      "       [5.00000e+00, 1.09466e+03, 8.26120e+02],\n",
      "       [6.00000e+00, 9.88520e+02, 9.96590e+02],\n",
      "       [7.00000e+00, 1.15095e+03, 9.99810e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap3', 'img078.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.68647e+03, 1.10490e+02],\n",
      "       [1.00000e+00, 1.85533e+03, 2.87390e+02],\n",
      "       [2.00000e+00, 1.36644e+03, 1.86070e+02],\n",
      "       [3.00000e+00, 1.13808e+03, 2.61660e+02],\n",
      "       [5.00000e+00, 9.14550e+02, 7.56970e+02],\n",
      "       [6.00000e+00, 1.18633e+03, 9.64430e+02],\n",
      "       [7.00000e+00, 1.14452e+03, 9.64430e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand19', 'img020.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.06732e+03, 1.87680e+02],\n",
      "       [1.00000e+00, 1.06893e+03, 2.40750e+02],\n",
      "       [2.00000e+00, 1.12522e+03, 4.12820e+02],\n",
      "       [3.00000e+00, 9.24200e+02, 7.00690e+02],\n",
      "       [4.00000e+00, 1.26995e+03, 6.49230e+02],\n",
      "       [5.00000e+00, 1.10914e+03, 8.38990e+02],\n",
      "       [6.00000e+00, 9.96560e+02, 9.94980e+02],\n",
      "       [7.00000e+00, 1.16060e+03, 9.86940e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk89', 'img021.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.07929e+03, 1.45130e+02],\n",
      "       [1.00000e+00, 9.57580e+02, 1.94480e+02],\n",
      "       [2.00000e+00, 1.07107e+03, 3.16190e+02],\n",
      "       [3.00000e+00, 9.31260e+02, 5.30010e+02],\n",
      "       [4.00000e+00, 1.21417e+03, 5.44810e+02],\n",
      "       [5.00000e+00, 1.06614e+03, 6.96130e+02],\n",
      "       [6.00000e+00, 1.01515e+03, 7.60280e+02],\n",
      "       [7.00000e+00, 1.15331e+03, 7.73440e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk68', 'img140.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 2.55200e+02, 3.66190e+02],\n",
      "       [1.00000e+00, 2.02130e+02, 4.77150e+02],\n",
      "       [2.00000e+00, 2.55200e+02, 5.73640e+02],\n",
      "       [3.00000e+00, 5.73900e+01, 8.11650e+02],\n",
      "       [4.00000e+00, 4.77130e+02, 8.55070e+02],\n",
      "       [5.00000e+00, 2.16600e+02, 1.06092e+03],\n",
      "       [6.00000e+00, 1.02420e+02, 1.05448e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk68', 'img110.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 3.66160e+02, 3.38850e+02],\n",
      "       [1.00000e+00, 3.00230e+02, 4.59460e+02],\n",
      "       [2.00000e+00, 3.82240e+02, 5.54340e+02],\n",
      "       [3.00000e+00, 1.97300e+02, 7.74660e+02],\n",
      "       [4.00000e+00, 6.13820e+02, 8.16470e+02],\n",
      "       [5.00000e+00, 4.22450e+02, 1.03840e+03],\n",
      "       [6.00000e+00, 3.22740e+02, 1.02071e+03],\n",
      "       [7.00000e+00, 5.28590e+02, 1.05127e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk68', 'img053.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 557.53, 293.82],\n",
      "       [  1.  , 517.33, 388.7 ],\n",
      "       [  2.  , 565.58, 488.41],\n",
      "       [  3.  , 412.8 , 758.58],\n",
      "       [  4.  , 821.27, 768.23],\n",
      "       [  5.  , 600.96, 964.43],\n",
      "       [  6.  , 493.21, 993.37],\n",
      "       [  7.  , 756.95, 994.98]], dtype=float32)}}, {'image': ('labeled-data', 'rest15', 'img114.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.49200e+02, 4.88410e+02],\n",
      "       [1.00000e+00, 5.07680e+02, 6.04200e+02],\n",
      "       [2.00000e+00, 9.01680e+02, 4.65890e+02],\n",
      "       [4.00000e+00, 1.03034e+03, 7.79490e+02],\n",
      "       [5.00000e+00, 1.41308e+03, 7.02290e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest19', 'img063.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.22975e+03, 2.89000e+02],\n",
      "       [1.00000e+00, 1.40182e+03, 3.22770e+02],\n",
      "       [2.00000e+00, 1.06893e+03, 3.96740e+02],\n",
      "       [3.00000e+00, 1.08341e+03, 7.13550e+02],\n",
      "       [5.00000e+00, 6.95840e+02, 5.47910e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap3', 'img041.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.66074e+03, 1.10490e+02],\n",
      "       [1.00000e+00, 1.80708e+03, 2.98640e+02],\n",
      "       [2.00000e+00, 1.32946e+03, 1.66770e+02],\n",
      "       [3.00000e+00, 1.29729e+03, 4.43380e+02],\n",
      "       [4.00000e+00, 1.14934e+03, 2.58440e+02],\n",
      "       [5.00000e+00, 9.08120e+02, 7.48930e+02],\n",
      "       [6.00000e+00, 1.17829e+03, 9.64430e+02],\n",
      "       [7.00000e+00, 1.13004e+03, 9.51560e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap7', 'img066.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.06732e+03, 4.06390e+02],\n",
      "       [1.00000e+00, 1.13165e+03, 4.65890e+02],\n",
      "       [2.00000e+00, 1.07858e+03, 6.31540e+02],\n",
      "       [3.00000e+00, 8.64690e+02, 8.16470e+02],\n",
      "       [4.00000e+00, 1.11235e+03, 7.97180e+02],\n",
      "       [5.00000e+00, 9.62790e+02, 9.46740e+02],\n",
      "       [6.00000e+00, 9.03290e+02, 1.00785e+03],\n",
      "       [7.00000e+00, 1.01908e+03, 9.82120e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap13', 'img078.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.03516e+03, 3.24380e+02],\n",
      "       [1.00000e+00, 1.09466e+03, 3.43670e+02],\n",
      "       [2.00000e+00, 9.77270e+02, 4.36950e+02],\n",
      "       [3.00000e+00, 1.15095e+03, 5.72030e+02],\n",
      "       [4.00000e+00, 8.13230e+02, 5.59170e+02],\n",
      "       [5.00000e+00, 9.88520e+02, 7.42500e+02],\n",
      "       [6.00000e+00, 1.03677e+03, 8.21300e+02],\n",
      "       [7.00000e+00, 9.40280e+02, 8.16470e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk86', 'img105.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 498.65, 359.76],\n",
      "       [  1.  , 485.28, 415.9 ],\n",
      "       [  2.  , 584.19, 363.33],\n",
      "       [  4.  , 527.16, 480.94],\n",
      "       [  5.  , 667.05, 529.95],\n",
      "       [  6.  , 626.06, 591.43],\n",
      "       [  7.  , 587.75, 598.56]], dtype=float32)}}, {'image': ('labeled-data', 'flap1', 'img071.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.18311e+03, 1.16920e+02],\n",
      "       [1.00000e+00, 1.20080e+03, 1.97330e+02],\n",
      "       [2.00000e+00, 1.16381e+03, 2.72910e+02],\n",
      "       [3.00000e+00, 9.66010e+02, 3.19550e+02],\n",
      "       [4.00000e+00, 1.42916e+03, 3.24380e+02],\n",
      "       [5.00000e+00, 1.22332e+03, 5.70430e+02],\n",
      "       [6.00000e+00, 1.15899e+03, 6.26710e+02],\n",
      "       [7.00000e+00, 1.29890e+03, 6.34750e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand4', 'img114.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.05446e+03, 2.53620e+02],\n",
      "       [1.00000e+00, 9.74050e+02, 2.55220e+02],\n",
      "       [2.00000e+00, 1.11718e+03, 4.28910e+02],\n",
      "       [4.00000e+00, 9.53140e+02, 6.86210e+02],\n",
      "       [5.00000e+00, 1.22814e+03, 8.29340e+02],\n",
      "       [6.00000e+00, 1.17829e+03, 9.11360e+02],\n",
      "       [7.00000e+00, 1.07858e+03, 9.32260e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand67', 'img034.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.20080e+03, 1.13710e+02],\n",
      "       [1.00000e+00, 1.31337e+03, 1.29790e+02],\n",
      "       [2.00000e+00, 1.16060e+03, 2.64870e+02],\n",
      "       [3.00000e+00, 1.02712e+03, 4.88410e+02],\n",
      "       [4.00000e+00, 1.28282e+03, 4.80370e+02],\n",
      "       [5.00000e+00, 1.18633e+03, 6.49230e+02],\n",
      "       [6.00000e+00, 1.10914e+03, 6.63700e+02],\n",
      "       [7.00000e+00, 1.26513e+03, 6.47620e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest44', 'img060.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.71130e+02, 3.50110e+02],\n",
      "       [1.00000e+00, 7.24780e+02, 4.35340e+02],\n",
      "       [2.00000e+00, 1.02712e+03, 4.27300e+02],\n",
      "       [4.00000e+00, 1.09627e+03, 6.60480e+02],\n",
      "       [5.00000e+00, 1.41630e+03, 6.63700e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap1', 'img082.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.16060e+03, 9.60200e+01],\n",
      "       [1.00000e+00, 1.16703e+03, 1.65170e+02],\n",
      "       [2.00000e+00, 1.16221e+03, 2.68090e+02],\n",
      "       [3.00000e+00, 1.03677e+03, 4.12820e+02],\n",
      "       [4.00000e+00, 1.35519e+03, 4.22470e+02],\n",
      "       [5.00000e+00, 1.21849e+03, 5.60780e+02],\n",
      "       [6.00000e+00, 1.15738e+03, 6.33140e+02],\n",
      "       [7.00000e+00, 1.29890e+03, 6.41180e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen46', 'img018.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.80380e+02, 2.24860e+02],\n",
      "       [1.00000e+00, 9.45510e+02, 2.60320e+02],\n",
      "       [2.00000e+00, 8.66890e+02, 1.39150e+02],\n",
      "       [3.00000e+00, 7.98320e+02, 2.98750e+02],\n",
      "       [4.00000e+00, 1.00048e+03, 2.82790e+02],\n",
      "       [5.00000e+00, 8.79300e+02, 3.93920e+02],\n",
      "       [6.00000e+00, 8.57430e+02, 4.41800e+02],\n",
      "       [7.00000e+00, 9.47280e+02, 4.47710e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand82', 'img023.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.61180e+02, 1.28180e+02],\n",
      "       [1.00000e+00, 8.13230e+02, 1.95720e+02],\n",
      "       [2.00000e+00, 1.07054e+03, 3.24380e+02],\n",
      "       [4.00000e+00, 1.08662e+03, 7.37670e+02],\n",
      "       [5.00000e+00, 1.30694e+03, 7.77880e+02],\n",
      "       [6.00000e+00, 1.08019e+03, 9.45130e+02],\n",
      "       [7.00000e+00, 1.11557e+03, 1.00624e+03]], dtype=float32)}}, {'image': ('labeled-data', 'preen40', 'img056.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.05627e+03, 5.96000e+01],\n",
      "       [1.00000e+00, 1.13522e+03, 9.74300e+01],\n",
      "       [2.00000e+00, 1.11712e+03, 3.09610e+02],\n",
      "       [3.00000e+00, 9.11530e+02, 6.15540e+02],\n",
      "       [4.00000e+00, 1.25529e+03, 5.89220e+02],\n",
      "       [5.00000e+00, 1.10397e+03, 7.89880e+02],\n",
      "       [6.00000e+00, 9.98700e+02, 9.83970e+02],\n",
      "       [7.00000e+00, 1.15989e+03, 9.90550e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk89', 'img048.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.73040e+02, 3.32640e+02],\n",
      "       [1.00000e+00, 6.17110e+02, 4.54350e+02],\n",
      "       [2.00000e+00, 8.17780e+02, 4.06650e+02],\n",
      "       [3.00000e+00, 8.34220e+02, 5.71130e+02],\n",
      "       [4.00000e+00, 1.03160e+03, 5.10270e+02],\n",
      "       [5.00000e+00, 1.01844e+03, 6.97780e+02],\n",
      "       [6.00000e+00, 9.60870e+02, 7.70150e+02],\n",
      "       [7.00000e+00, 1.12535e+03, 6.99420e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen37', 'img033.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.51440e+02, 7.22420e+02],\n",
      "       [1.00000e+00, 8.98380e+02, 7.50520e+02],\n",
      "       [2.00000e+00, 8.07620e+02, 5.92780e+02],\n",
      "       [3.00000e+00, 9.80490e+02, 8.54230e+02],\n",
      "       [4.00000e+00, 1.22250e+03, 5.34430e+02],\n",
      "       [5.00000e+00, 1.11014e+03, 8.34790e+02],\n",
      "       [6.00000e+00, 9.48080e+02, 9.94690e+02],\n",
      "       [7.00000e+00, 1.09717e+03, 1.00549e+03]], dtype=float32)}}, {'image': ('labeled-data', 'flap13', 'img032.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.57970e+02, 3.82270e+02],\n",
      "       [1.00000e+00, 8.87210e+02, 3.98350e+02],\n",
      "       [2.00000e+00, 1.05607e+03, 4.69110e+02],\n",
      "       [3.00000e+00, 1.16703e+03, 5.86510e+02],\n",
      "       [4.00000e+00, 8.87210e+02, 5.96160e+02],\n",
      "       [5.00000e+00, 1.08501e+03, 7.89140e+02],\n",
      "       [7.00000e+00, 9.96560e+02, 8.38990e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap4', 'img141.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.36966e+03, 5.90300e+01],\n",
      "       [1.00000e+00, 1.57872e+03, 1.10490e+02],\n",
      "       [2.00000e+00, 1.07697e+03, 2.50400e+02],\n",
      "       [3.00000e+00, 8.90430e+02, 3.59760e+02],\n",
      "       [5.00000e+00, 7.02270e+02, 8.21300e+02],\n",
      "       [6.00000e+00, 8.80780e+02, 9.70860e+02],\n",
      "       [7.00000e+00, 9.90130e+02, 9.54780e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap13', 'img003.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1051.24,  391.92],\n",
      "       [   2.  , 1065.72,  496.45],\n",
      "       [   3.  , 1195.98,  570.43],\n",
      "       [   4.  ,  893.64,  538.26],\n",
      "       [   5.  , 1078.58,  779.49],\n",
      "       [   7.  , 1015.86,  855.07]], dtype=float32)}}, {'image': ('labeled-data', 'stand82', 'img020.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.53140e+02, 1.28180e+02],\n",
      "       [1.00000e+00, 8.06800e+02, 1.90900e+02],\n",
      "       [2.00000e+00, 1.07697e+03, 3.32420e+02],\n",
      "       [4.00000e+00, 1.07697e+03, 7.39280e+02],\n",
      "       [5.00000e+00, 1.30855e+03, 7.97180e+02],\n",
      "       [6.00000e+00, 1.08180e+03, 9.54780e+02],\n",
      "       [7.00000e+00, 1.10914e+03, 1.01267e+03]], dtype=float32)}}, {'image': ('labeled-data', 'flap12', 'img058.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1160.6 ,  408.  ],\n",
      "       [   2.  , 1068.93,  502.88],\n",
      "       [   3.  , 1094.66,  528.61],\n",
      "       [   4.  ,  948.32,  502.88],\n",
      "       [   5.  ,  930.63,  777.88],\n",
      "       [   6.  , 1025.51,  869.54],\n",
      "       [   7.  ,  975.66,  867.94]], dtype=float32)}}, {'image': ('labeled-data', 'rest8', 'img142.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.37360e+02, 1.04060e+02],\n",
      "       [1.00000e+00, 6.58850e+02, 1.15310e+02],\n",
      "       [2.00000e+00, 9.49930e+02, 2.34320e+02],\n",
      "       [4.00000e+00, 7.16740e+02, 6.68520e+02],\n",
      "       [5.00000e+00, 1.44042e+03, 6.23490e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest39', 'img118.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.94960e+02, 4.22470e+02],\n",
      "       [1.00000e+00, 1.11396e+03, 4.86800e+02],\n",
      "       [3.00000e+00, 7.29610e+02, 6.52440e+02],\n",
      "       [4.00000e+00, 1.28282e+03, 5.72030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap4', 'img085.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.34554e+03, 4.77700e+01],\n",
      "       [1.00000e+00, 1.54817e+03, 1.21750e+02],\n",
      "       [2.00000e+00, 1.06250e+03, 2.39140e+02],\n",
      "       [3.00000e+00, 8.93640e+02, 3.05080e+02],\n",
      "       [4.00000e+00, 9.22590e+02, 2.19840e+02],\n",
      "       [5.00000e+00, 7.26390e+02, 8.06830e+02],\n",
      "       [6.00000e+00, 9.29020e+02, 9.67640e+02],\n",
      "       [7.00000e+00, 1.00621e+03, 9.30650e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand82', 'img006.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.46710e+02, 1.23350e+02],\n",
      "       [1.00000e+00, 7.82680e+02, 1.89290e+02],\n",
      "       [2.00000e+00, 1.07376e+03, 3.30810e+02],\n",
      "       [4.00000e+00, 1.07536e+03, 7.45720e+02],\n",
      "       [5.00000e+00, 1.31498e+03, 7.90740e+02],\n",
      "       [6.00000e+00, 1.08180e+03, 9.46740e+02],\n",
      "       [7.00000e+00, 1.09466e+03, 1.01750e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest42', 'img024.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.47000e+02, 3.58150e+02],\n",
      "       [1.00000e+00, 6.78150e+02, 4.01570e+02],\n",
      "       [2.00000e+00, 9.86920e+02, 4.46600e+02],\n",
      "       [4.00000e+00, 1.05446e+03, 6.87820e+02],\n",
      "       [5.00000e+00, 1.38092e+03, 6.76560e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen30', 'img032.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.25707e+03, 1.43320e+02],\n",
      "       [1.00000e+00, 1.21602e+03, 3.11870e+02],\n",
      "       [2.00000e+00, 1.15768e+03, 3.52920e+02],\n",
      "       [3.00000e+00, 9.71850e+02, 6.46800e+02],\n",
      "       [4.00000e+00, 1.32406e+03, 6.44630e+02],\n",
      "       [5.00000e+00, 1.16848e+03, 7.98050e+02],\n",
      "       [6.00000e+00, 1.08205e+03, 9.79560e+02],\n",
      "       [7.00000e+00, 1.23763e+03, 9.86040e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk57', 'img060.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.26191e+03, 3.99960e+02],\n",
      "       [1.00000e+00, 1.17990e+03, 5.44690e+02],\n",
      "       [2.00000e+00, 1.37288e+03, 5.59170e+02],\n",
      "       [3.00000e+00, 1.36805e+03, 7.71450e+02],\n",
      "       [4.00000e+00, 1.65109e+03, 7.66620e+02],\n",
      "       [5.00000e+00, 1.57711e+03, 9.35480e+02],\n",
      "       [6.00000e+00, 1.47419e+03, 1.02071e+03],\n",
      "       [7.00000e+00, 1.70094e+03, 1.00946e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand2', 'img048.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.50510e+02, 1.18530e+02],\n",
      "       [1.00000e+00, 6.87800e+02, 2.08590e+02],\n",
      "       [2.00000e+00, 8.82380e+02, 2.64870e+02],\n",
      "       [3.00000e+00, 7.10310e+02, 5.76860e+02],\n",
      "       [4.00000e+00, 1.01425e+03, 6.52440e+02],\n",
      "       [5.00000e+00, 1.11879e+03, 6.68520e+02],\n",
      "       [6.00000e+00, 8.37360e+02, 8.71150e+02],\n",
      "       [7.00000e+00, 9.37060e+02, 9.30650e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk89', 'img133.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 204.27, 110.59],\n",
      "       [  1.  , 100.65, 146.78],\n",
      "       [  2.  , 191.12, 316.19],\n",
      "       [  3.  , 202.63, 590.87],\n",
      "       [  4.  , 342.44, 599.09],\n",
      "       [  5.  , 319.41, 738.9 ],\n",
      "       [  6.  , 316.12, 804.69],\n",
      "       [  7.  , 400.  , 837.58]], dtype=float32)}}, {'image': ('labeled-data', 'walk82', 'img011.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.83700e+02, 1.90900e+02],\n",
      "       [1.00000e+00, 1.07858e+03, 2.47180e+02],\n",
      "       [2.00000e+00, 8.75950e+02, 2.97040e+02],\n",
      "       [3.00000e+00, 8.58260e+02, 5.10920e+02],\n",
      "       [5.00000e+00, 6.78150e+02, 6.09020e+02],\n",
      "       [6.00000e+00, 7.90720e+02, 6.70130e+02],\n",
      "       [7.00000e+00, 7.71420e+02, 7.16770e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk68', 'img079.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 5.81660e+02, 2.87390e+02],\n",
      "       [1.00000e+00, 6.18640e+02, 3.69400e+02],\n",
      "       [2.00000e+00, 5.25370e+02, 5.14140e+02],\n",
      "       [3.00000e+00, 3.00230e+02, 7.42500e+02],\n",
      "       [4.00000e+00, 7.23180e+02, 7.90740e+02],\n",
      "       [5.00000e+00, 4.93210e+02, 1.00141e+03],\n",
      "       [6.00000e+00, 3.87070e+02, 9.57990e+02],\n",
      "       [7.00000e+00, 6.18640e+02, 1.03036e+03]], dtype=float32)}}, {'image': ('labeled-data', 'preen50', 'img008.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.83700e+02, 4.51420e+02],\n",
      "       [1.00000e+00, 9.27410e+02, 4.46600e+02],\n",
      "       [2.00000e+00, 1.22171e+03, 4.40160e+02],\n",
      "       [3.00000e+00, 1.18150e+03, 8.45420e+02],\n",
      "       [4.00000e+00, 9.46710e+02, 5.09310e+02],\n",
      "       [5.00000e+00, 8.24490e+02, 8.55070e+02],\n",
      "       [6.00000e+00, 1.05124e+03, 9.83720e+02],\n",
      "       [7.00000e+00, 1.01265e+03, 9.66030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk82', 'img081.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1858.54,   78.33],\n",
      "       [   2.  , 1741.15,  192.51],\n",
      "       [   3.  , 1725.06,  435.34],\n",
      "       [   5.  , 1541.73,  518.96],\n",
      "       [   6.  , 1610.88,  636.36],\n",
      "       [   7.  , 1717.02,  613.85]], dtype=float32)}}, {'image': ('labeled-data', 'stand75', 'img064.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00498e+03, 3.38480e+02],\n",
      "       [1.00000e+00, 1.05710e+03, 3.63730e+02],\n",
      "       [2.00000e+00, 9.47160e+02, 4.13400e+02],\n",
      "       [3.00000e+00, 1.01557e+03, 5.57550e+02],\n",
      "       [5.00000e+00, 8.81190e+02, 6.19450e+02],\n",
      "       [6.00000e+00, 9.77290e+02, 6.71570e+02],\n",
      "       [7.00000e+00, 9.06440e+02, 6.71570e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk89', 'img083.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 247.04, 230.66],\n",
      "       [  1.  , 164.8 , 329.35],\n",
      "       [  2.  , 319.41, 357.31],\n",
      "       [  3.  , 340.79, 567.84],\n",
      "       [  4.  , 513.49, 561.26],\n",
      "       [  5.  , 475.66, 710.93],\n",
      "       [  6.  , 478.95, 765.21],\n",
      "       [  7.  , 562.84, 750.41]], dtype=float32)}}, {'image': ('labeled-data', 'rest24', 'img079.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.42160e+02, 3.59600e+02],\n",
      "       [1.00000e+00, 6.10530e+02, 4.03810e+02],\n",
      "       [2.00000e+00, 9.26050e+02, 3.71660e+02],\n",
      "       [4.00000e+00, 1.11697e+03, 5.79660e+02],\n",
      "       [5.00000e+00, 1.36115e+03, 4.86210e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand4', 'img131.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.02390e+03, 2.55220e+02],\n",
      "       [1.00000e+00, 9.45100e+02, 2.68090e+02],\n",
      "       [2.00000e+00, 1.09145e+03, 4.19260e+02],\n",
      "       [4.00000e+00, 9.40280e+02, 6.87820e+02],\n",
      "       [5.00000e+00, 1.22814e+03, 8.26120e+02],\n",
      "       [6.00000e+00, 1.16864e+03, 9.08140e+02],\n",
      "       [7.00000e+00, 1.07054e+03, 9.35480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap3', 'img028.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.61249e+03, 1.18530e+02],\n",
      "       [1.00000e+00, 1.74758e+03, 3.08290e+02],\n",
      "       [2.00000e+00, 1.25387e+03, 1.58730e+02],\n",
      "       [3.00000e+00, 1.39539e+03, 4.91620e+02],\n",
      "       [5.00000e+00, 9.11330e+02, 7.40890e+02],\n",
      "       [6.00000e+00, 1.17025e+03, 9.45130e+02],\n",
      "       [7.00000e+00, 1.11718e+03, 9.37090e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand2', 'img112.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.39260e+02, 1.87680e+02],\n",
      "       [1.00000e+00, 6.52420e+02, 3.22770e+02],\n",
      "       [2.00000e+00, 9.04900e+02, 2.80950e+02],\n",
      "       [3.00000e+00, 7.28000e+02, 5.89720e+02],\n",
      "       [4.00000e+00, 1.04963e+03, 6.74960e+02],\n",
      "       [5.00000e+00, 1.14452e+03, 6.74960e+02],\n",
      "       [6.00000e+00, 8.61480e+02, 8.79190e+02],\n",
      "       [7.00000e+00, 9.54750e+02, 9.40300e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk68', 'img023.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 726.39, 293.82],\n",
      "       [  1.  , 650.81, 419.26],\n",
      "       [  2.  , 748.91, 490.02],\n",
      "       [  3.  , 670.11, 666.92],\n",
      "       [  4.  , 983.7 , 718.38],\n",
      "       [  5.  , 851.83, 924.22],\n",
      "       [  6.  , 785.89, 922.61],\n",
      "       [  7.  , 937.06, 966.03]], dtype=float32)}}, {'image': ('labeled-data', 'stand67', 'img048.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.18633e+03, 1.16920e+02],\n",
      "       [1.00000e+00, 1.26995e+03, 1.33000e+02],\n",
      "       [2.00000e+00, 1.16381e+03, 2.63260e+02],\n",
      "       [3.00000e+00, 1.01747e+03, 4.91620e+02],\n",
      "       [4.00000e+00, 1.27156e+03, 4.80370e+02],\n",
      "       [5.00000e+00, 1.18472e+03, 6.54050e+02],\n",
      "       [6.00000e+00, 1.11557e+03, 6.63700e+02],\n",
      "       [7.00000e+00, 1.25709e+03, 6.54050e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen50', 'img053.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.18472e+03, 2.21450e+02],\n",
      "       [1.00000e+00, 1.32785e+03, 2.63260e+02],\n",
      "       [2.00000e+00, 1.15577e+03, 4.17650e+02],\n",
      "       [3.00000e+00, 1.17990e+03, 7.82700e+02],\n",
      "       [4.00000e+00, 9.70830e+02, 4.99670e+02],\n",
      "       [5.00000e+00, 8.50220e+02, 8.61500e+02],\n",
      "       [6.00000e+00, 1.04320e+03, 9.80510e+02],\n",
      "       [7.00000e+00, 1.00782e+03, 9.66030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen33', 'img064.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.40401e+03, 4.24230e+02],\n",
      "       [1.00000e+00, 1.23763e+03, 5.43080e+02],\n",
      "       [2.00000e+00, 1.17064e+03, 4.30710e+02],\n",
      "       [3.00000e+00, 8.91900e+02, 6.51120e+02],\n",
      "       [4.00000e+00, 1.28733e+03, 6.57600e+02],\n",
      "       [5.00000e+00, 1.09501e+03, 8.13180e+02],\n",
      "       [6.00000e+00, 9.89130e+02, 9.75240e+02],\n",
      "       [7.00000e+00, 1.15336e+03, 9.83880e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk86', 'img031.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.15850e+02, 4.56880e+02],\n",
      "       [1.00000e+00, 8.01590e+02, 5.08560e+02],\n",
      "       [2.00000e+00, 9.29900e+02, 4.48860e+02],\n",
      "       [4.00000e+00, 9.00500e+02, 5.77170e+02],\n",
      "       [5.00000e+00, 1.04484e+03, 6.24400e+02],\n",
      "       [6.00000e+00, 9.97620e+02, 6.93890e+02],\n",
      "       [7.00000e+00, 9.45050e+02, 7.06370e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap4', 'img074.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[1.00000e+00, 1.53208e+03, 3.97300e+01],\n",
      "       [2.00000e+00, 1.04963e+03, 2.23060e+02],\n",
      "       [3.00000e+00, 9.64400e+02, 5.15750e+02],\n",
      "       [4.00000e+00, 1.29568e+03, 5.25400e+02],\n",
      "       [5.00000e+00, 7.95540e+02, 7.56970e+02],\n",
      "       [6.00000e+00, 9.41890e+02, 9.48340e+02],\n",
      "       [7.00000e+00, 1.06893e+03, 9.16180e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand69', 'img075.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00300e+03, 1.97330e+02],\n",
      "       [1.00000e+00, 1.14934e+03, 2.40750e+02],\n",
      "       [2.00000e+00, 9.64400e+02, 4.28910e+02],\n",
      "       [3.00000e+00, 6.99050e+02, 6.58870e+02],\n",
      "       [4.00000e+00, 9.80480e+02, 6.83000e+02],\n",
      "       [5.00000e+00, 8.27710e+02, 8.59900e+02],\n",
      "       [6.00000e+00, 7.28000e+02, 9.12960e+02],\n",
      "       [7.00000e+00, 9.14550e+02, 9.11360e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk62', 'img096.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1147.73,  120.14],\n",
      "       [   2.  , 1088.23,  263.26],\n",
      "       [   3.  , 1200.8 ,  523.79],\n",
      "       [   4.  ,  924.2 ,  494.84],\n",
      "       [   5.  , 1022.3 ,  610.63],\n",
      "       [   6.  , 1138.08,  752.15],\n",
      "       [   7.  ,  945.1 ,  758.58]], dtype=float32)}}, {'image': ('labeled-data', 'walk64', 'img129.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.65270e+03, 3.42070e+02],\n",
      "       [1.00000e+00, 1.66717e+03, 4.64290e+02],\n",
      "       [2.00000e+00, 1.54977e+03, 3.90310e+02],\n",
      "       [3.00000e+00, 1.39057e+03, 4.80370e+02],\n",
      "       [4.00000e+00, 1.53208e+03, 4.70720e+02],\n",
      "       [5.00000e+00, 1.34232e+03, 5.43090e+02],\n",
      "       [6.00000e+00, 1.32302e+03, 5.75250e+02],\n",
      "       [7.00000e+00, 1.41308e+03, 5.55950e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand6', 'img009.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.11330e+02, 2.68090e+02],\n",
      "       [1.00000e+00, 8.03580e+02, 3.11510e+02],\n",
      "       [2.00000e+00, 1.05607e+03, 4.14430e+02],\n",
      "       [4.00000e+00, 8.83990e+02, 6.74960e+02],\n",
      "       [5.00000e+00, 1.17990e+03, 8.13260e+02],\n",
      "       [6.00000e+00, 1.10753e+03, 8.98490e+02],\n",
      "       [7.00000e+00, 1.00943e+03, 9.21010e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen30', 'img148.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.19225e+03, 2.01670e+02],\n",
      "       [1.00000e+00, 1.14471e+03, 3.48600e+02],\n",
      "       [2.00000e+00, 1.09501e+03, 3.61570e+02],\n",
      "       [3.00000e+00, 8.94060e+02, 6.68400e+02],\n",
      "       [4.00000e+00, 1.24195e+03, 6.53280e+02],\n",
      "       [5.00000e+00, 1.08205e+03, 8.02370e+02],\n",
      "       [6.00000e+00, 9.80490e+02, 9.81720e+02],\n",
      "       [7.00000e+00, 1.14039e+03, 9.88210e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand4', 'img022.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.06089e+03, 2.40750e+02],\n",
      "       [1.00000e+00, 1.17025e+03, 2.52010e+02],\n",
      "       [2.00000e+00, 1.06893e+03, 3.99960e+02],\n",
      "       [4.00000e+00, 9.06510e+02, 6.92650e+02],\n",
      "       [5.00000e+00, 1.20080e+03, 8.16470e+02],\n",
      "       [6.00000e+00, 1.14612e+03, 9.00100e+02],\n",
      "       [7.00000e+00, 1.04320e+03, 9.17790e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk68', 'img006.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.58260e+02, 2.60050e+02],\n",
      "       [1.00000e+00, 7.55340e+02, 3.74230e+02],\n",
      "       [2.00000e+00, 9.04900e+02, 4.48200e+02],\n",
      "       [3.00000e+00, 8.35750e+02, 6.62090e+02],\n",
      "       [4.00000e+00, 1.15416e+03, 6.89430e+02],\n",
      "       [5.00000e+00, 9.66010e+02, 8.74370e+02],\n",
      "       [6.00000e+00, 8.88820e+02, 8.85630e+02],\n",
      "       [7.00000e+00, 1.12361e+03, 9.14570e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen31', 'img063.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17929e+03, 2.38400e+02],\n",
      "       [1.00000e+00, 1.15768e+03, 3.85340e+02],\n",
      "       [2.00000e+00, 1.07340e+03, 3.81010e+02],\n",
      "       [3.00000e+00, 8.85410e+02, 6.59760e+02],\n",
      "       [4.00000e+00, 1.23331e+03, 6.57600e+02],\n",
      "       [5.00000e+00, 1.07124e+03, 8.00210e+02],\n",
      "       [6.00000e+00, 9.69690e+02, 9.88210e+02],\n",
      "       [7.00000e+00, 1.13391e+03, 9.99010e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand19', 'img093.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.01265e+03, 1.82860e+02],\n",
      "       [1.00000e+00, 9.41890e+02, 2.71310e+02],\n",
      "       [2.00000e+00, 1.10110e+03, 4.11220e+02],\n",
      "       [3.00000e+00, 9.12940e+02, 6.92650e+02],\n",
      "       [4.00000e+00, 1.25066e+03, 6.52440e+02],\n",
      "       [5.00000e+00, 1.09788e+03, 8.24520e+02],\n",
      "       [6.00000e+00, 9.85310e+02, 9.98200e+02],\n",
      "       [7.00000e+00, 1.15738e+03, 1.00463e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk68', 'img142.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 2.08560e+02, 3.50110e+02],\n",
      "       [1.00000e+00, 1.47450e+02, 4.65890e+02],\n",
      "       [2.00000e+00, 2.08560e+02, 5.70430e+02],\n",
      "       [3.00000e+00, 1.71900e+01, 8.37380e+02],\n",
      "       [4.00000e+00, 4.67480e+02, 8.40600e+02],\n",
      "       [5.00000e+00, 2.26250e+02, 1.05448e+03],\n",
      "       [6.00000e+00, 7.99100e+01, 1.07378e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand2', 'img123.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.23180e+02, 2.48790e+02],\n",
      "       [1.00000e+00, 6.37940e+02, 3.83880e+02],\n",
      "       [2.00000e+00, 9.16160e+02, 2.92210e+02],\n",
      "       [3.00000e+00, 7.53730e+02, 5.99370e+02],\n",
      "       [4.00000e+00, 1.06893e+03, 6.99080e+02],\n",
      "       [5.00000e+00, 1.16864e+03, 6.70130e+02],\n",
      "       [6.00000e+00, 8.80780e+02, 8.79190e+02],\n",
      "       [7.00000e+00, 9.80480e+02, 9.38700e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand9', 'img087.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.83700e+02, 2.56830e+02],\n",
      "       [1.00000e+00, 9.20980e+02, 2.71310e+02],\n",
      "       [2.00000e+00, 1.05928e+03, 4.41770e+02],\n",
      "       [4.00000e+00, 8.92030e+02, 6.78170e+02],\n",
      "       [5.00000e+00, 1.16864e+03, 8.37380e+02],\n",
      "       [6.00000e+00, 1.12200e+03, 9.14570e+02],\n",
      "       [7.00000e+00, 1.02069e+03, 9.33870e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen31', 'img028.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17280e+03, 2.42720e+02],\n",
      "       [1.00000e+00, 1.14255e+03, 3.89660e+02],\n",
      "       [2.00000e+00, 1.07773e+03, 3.72370e+02],\n",
      "       [3.00000e+00, 8.81090e+02, 6.74890e+02],\n",
      "       [4.00000e+00, 1.23114e+03, 6.61920e+02],\n",
      "       [5.00000e+00, 1.06908e+03, 8.06700e+02],\n",
      "       [6.00000e+00, 9.69690e+02, 9.81720e+02],\n",
      "       [7.00000e+00, 1.13391e+03, 9.90370e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap12', 'img112.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1096.27,  401.57],\n",
      "       [   2.  , 1086.62,  493.23],\n",
      "       [   3.  , 1210.45,  654.05],\n",
      "       [   4.  ,  951.54,  628.32],\n",
      "       [   5.  , 1041.59,  811.65],\n",
      "       [   6.  , 1075.36,  874.37],\n",
      "       [   7.  ,  991.74,  866.33]], dtype=float32)}}, {'image': ('labeled-data', 'rest19', 'img109.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.24583e+03, 2.95430e+02],\n",
      "       [1.00000e+00, 1.45811e+03, 3.24380e+02],\n",
      "       [2.00000e+00, 1.08180e+03, 3.95130e+02],\n",
      "       [3.00000e+00, 1.08823e+03, 7.23200e+02],\n",
      "       [5.00000e+00, 7.11920e+02, 5.41480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen46', 'img083.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00403e+03, 1.95300e+02],\n",
      "       [1.00000e+00, 9.70330e+02, 2.38450e+02],\n",
      "       [2.00000e+00, 9.01760e+02, 1.26140e+02],\n",
      "       [3.00000e+00, 8.22560e+02, 2.86330e+02],\n",
      "       [4.00000e+00, 1.02353e+03, 2.81600e+02],\n",
      "       [5.00000e+00, 9.03540e+02, 3.95690e+02],\n",
      "       [6.00000e+00, 8.75760e+02, 4.38250e+02],\n",
      "       [7.00000e+00, 9.63830e+02, 4.42980e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen48', 'img138.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.39470e+03, 2.87990e+02],\n",
      "       [1.00000e+00, 1.36163e+03, 3.28690e+02],\n",
      "       [2.00000e+00, 1.29868e+03, 2.07870e+02],\n",
      "       [3.00000e+00, 1.22110e+03, 3.69380e+02],\n",
      "       [4.00000e+00, 1.42331e+03, 3.50940e+02],\n",
      "       [5.00000e+00, 1.30567e+03, 4.74940e+02],\n",
      "       [6.00000e+00, 1.27706e+03, 5.12460e+02],\n",
      "       [7.00000e+00, 1.36354e+03, 5.20090e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen48', 'img012.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.16323e+03, 3.13420e+02],\n",
      "       [1.00000e+00, 1.12635e+03, 3.41400e+02],\n",
      "       [2.00000e+00, 1.05513e+03, 2.06590e+02],\n",
      "       [3.00000e+00, 9.78820e+02, 3.64930e+02],\n",
      "       [4.00000e+00, 1.18294e+03, 3.52210e+02],\n",
      "       [5.00000e+00, 1.06721e+03, 4.67950e+02],\n",
      "       [6.00000e+00, 1.03860e+03, 5.02290e+02],\n",
      "       [7.00000e+00, 1.13144e+03, 5.11820e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk82', 'img095.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   5.  , 1795.82,  499.67],\n",
      "       [   7.  , 1877.84,  626.71]], dtype=float32)}}, {'image': ('labeled-data', 'preen50', 'img119.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.02551e+03, 4.27300e+02],\n",
      "       [1.00000e+00, 9.27410e+02, 3.45280e+02],\n",
      "       [2.00000e+00, 1.21045e+03, 4.49810e+02],\n",
      "       [3.00000e+00, 1.15256e+03, 8.48640e+02],\n",
      "       [4.00000e+00, 9.86920e+02, 4.69110e+02],\n",
      "       [5.00000e+00, 8.16450e+02, 8.40600e+02],\n",
      "       [6.00000e+00, 1.03355e+03, 9.85330e+02],\n",
      "       [7.00000e+00, 1.00139e+03, 9.70860e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest47', 'img088.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.44080e+02, 4.01570e+02],\n",
      "       [1.00000e+00, 6.10600e+02, 5.39870e+02],\n",
      "       [2.00000e+00, 9.38670e+02, 3.85490e+02],\n",
      "       [4.00000e+00, 1.07215e+03, 6.62090e+02],\n",
      "       [5.00000e+00, 1.47741e+03, 6.12240e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest42', 'img017.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.51830e+02, 3.51710e+02],\n",
      "       [1.00000e+00, 6.78150e+02, 3.96740e+02],\n",
      "       [2.00000e+00, 9.86920e+02, 4.38560e+02],\n",
      "       [4.00000e+00, 1.05446e+03, 6.68520e+02],\n",
      "       [5.00000e+00, 1.38413e+03, 6.68520e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk62', 'img017.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.09145e+03, 2.56830e+02],\n",
      "       [1.00000e+00, 1.15899e+03, 3.27590e+02],\n",
      "       [2.00000e+00, 9.14550e+02, 3.66190e+02],\n",
      "       [3.00000e+00, 9.40280e+02, 6.66920e+02],\n",
      "       [5.00000e+00, 6.95840e+02, 7.52150e+02],\n",
      "       [6.00000e+00, 8.55050e+02, 9.14570e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap4', 'img108.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.36001e+03, 2.20400e+01],\n",
      "       [1.00000e+00, 1.57711e+03, 1.16920e+02],\n",
      "       [2.00000e+00, 1.03034e+03, 2.31100e+02],\n",
      "       [3.00000e+00, 9.85310e+02, 5.46300e+02],\n",
      "       [4.00000e+00, 1.28282e+03, 5.33440e+02],\n",
      "       [5.00000e+00, 7.64990e+02, 7.79490e+02],\n",
      "       [6.00000e+00, 8.79170e+02, 9.59600e+02],\n",
      "       [7.00000e+00, 1.00782e+03, 9.41910e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand2', 'img145.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.18640e+02, 3.53320e+02],\n",
      "       [1.00000e+00, 5.14110e+02, 4.93230e+02],\n",
      "       [2.00000e+00, 8.63090e+02, 3.03470e+02],\n",
      "       [3.00000e+00, 7.53730e+02, 6.12240e+02],\n",
      "       [4.00000e+00, 1.06572e+03, 6.94250e+02],\n",
      "       [5.00000e+00, 1.16060e+03, 6.41180e+02],\n",
      "       [6.00000e+00, 8.69520e+02, 8.85630e+02],\n",
      "       [7.00000e+00, 9.75660e+02, 9.35480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest47', 'img019.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.37650e+02, 3.80660e+02],\n",
      "       [1.00000e+00, 6.07390e+02, 5.23790e+02],\n",
      "       [2.00000e+00, 9.24200e+02, 3.75840e+02],\n",
      "       [4.00000e+00, 1.04963e+03, 6.52440e+02],\n",
      "       [5.00000e+00, 1.44524e+03, 5.97760e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen46', 'img008.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 962.9 , 241.12],\n",
      "       [  1.  , 930.02, 268.52],\n",
      "       [  2.  , 859.37, 146.11],\n",
      "       [  3.  , 783.85, 304.46],\n",
      "       [  4.  , 987.26, 297.76],\n",
      "       [  5.  , 867.29, 407.38],\n",
      "       [  6.  , 845.36, 446.36],\n",
      "       [  7.  , 933.06, 454.28]], dtype=float32)}}, {'image': ('labeled-data', 'preen33', 'img019.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.44074e+03, 4.41520e+02],\n",
      "       [1.00000e+00, 1.26356e+03, 5.36590e+02],\n",
      "       [2.00000e+00, 1.21386e+03, 4.41520e+02],\n",
      "       [3.00000e+00, 9.32950e+02, 6.53280e+02],\n",
      "       [4.00000e+00, 1.32406e+03, 6.66240e+02],\n",
      "       [5.00000e+00, 1.12310e+03, 8.32630e+02],\n",
      "       [6.00000e+00, 1.01938e+03, 1.01197e+03],\n",
      "       [7.00000e+00, 1.18793e+03, 1.02062e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk64', 'img030.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.26352e+03, 1.24960e+02],\n",
      "       [1.00000e+00, 1.36644e+03, 1.87680e+02],\n",
      "       [2.00000e+00, 1.24261e+03, 2.82560e+02],\n",
      "       [3.00000e+00, 1.08019e+03, 4.43380e+02],\n",
      "       [4.00000e+00, 1.23297e+03, 4.22470e+02],\n",
      "       [5.00000e+00, 9.32240e+02, 4.98060e+02],\n",
      "       [6.00000e+00, 1.03677e+03, 5.88110e+02],\n",
      "       [7.00000e+00, 1.09466e+03, 5.83290e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap1', 'img024.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.23297e+03, 7.51100e+01],\n",
      "       [1.00000e+00, 1.33106e+03, 1.33000e+02],\n",
      "       [2.00000e+00, 1.18954e+03, 2.69700e+02],\n",
      "       [3.00000e+00, 9.88520e+02, 4.01570e+02],\n",
      "       [4.00000e+00, 1.41951e+03, 4.27300e+02],\n",
      "       [5.00000e+00, 1.22332e+03, 5.81680e+02],\n",
      "       [6.00000e+00, 1.14612e+03, 6.33140e+02],\n",
      "       [7.00000e+00, 1.30533e+03, 6.42790e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk57', 'img107.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 4.64260e+02, 3.37240e+02],\n",
      "       [1.00000e+00, 3.53300e+02, 4.61070e+02],\n",
      "       [2.00000e+00, 7.16740e+02, 4.33730e+02],\n",
      "       [4.00000e+00, 7.31220e+02, 7.69840e+02],\n",
      "       [5.00000e+00, 1.00943e+03, 8.61500e+02],\n",
      "       [6.00000e+00, 8.45400e+02, 1.01428e+03],\n",
      "       [7.00000e+00, 6.97450e+02, 1.02554e+03]], dtype=float32)}}, {'image': ('labeled-data', 'flap12', 'img088.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.15416e+03, 4.12820e+02],\n",
      "       [1.00000e+00, 1.18954e+03, 4.53030e+02],\n",
      "       [2.00000e+00, 1.05767e+03, 5.01270e+02],\n",
      "       [3.00000e+00, 1.16542e+03, 5.54340e+02],\n",
      "       [4.00000e+00, 9.41890e+02, 4.91620e+02],\n",
      "       [5.00000e+00, 9.74050e+02, 7.93960e+02],\n",
      "       [6.00000e+00, 1.06089e+03, 8.43810e+02],\n",
      "       [7.00000e+00, 9.94960e+02, 8.71150e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen46', 'img095.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.88660e+02, 2.02400e+02],\n",
      "       [1.00000e+00, 9.63830e+02, 2.51460e+02],\n",
      "       [2.00000e+00, 8.89940e+02, 1.46830e+02],\n",
      "       [3.00000e+00, 8.11320e+02, 3.06430e+02],\n",
      "       [4.00000e+00, 1.00935e+03, 2.94020e+02],\n",
      "       [5.00000e+00, 8.94080e+02, 4.09280e+02],\n",
      "       [6.00000e+00, 8.68660e+02, 4.51850e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest44', 'img141.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.58260e+02, 3.59760e+02],\n",
      "       [1.00000e+00, 6.86190e+02, 4.16040e+02],\n",
      "       [2.00000e+00, 1.02712e+03, 4.38560e+02],\n",
      "       [4.00000e+00, 1.10110e+03, 6.70130e+02],\n",
      "       [5.00000e+00, 1.42273e+03, 6.73350e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest65', 'img088.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.30855e+03, 3.08290e+02],\n",
      "       [1.00000e+00, 1.49188e+03, 3.51710e+02],\n",
      "       [2.00000e+00, 1.10914e+03, 4.43380e+02],\n",
      "       [3.00000e+00, 1.25548e+03, 7.61800e+02],\n",
      "       [5.00000e+00, 5.65580e+02, 6.18670e+02],\n",
      "       [6.00000e+00, 8.50220e+02, 8.37380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap14', 'img001.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.69230e+02, 1.36220e+02],\n",
      "       [1.00000e+00, 1.12683e+03, 1.39440e+02],\n",
      "       [2.00000e+00, 8.05190e+02, 3.22770e+02],\n",
      "       [3.00000e+00, 9.22590e+02, 5.31830e+02],\n",
      "       [5.00000e+00, 6.36330e+02, 7.89140e+02],\n",
      "       [6.00000e+00, 8.30920e+02, 9.35480e+02],\n",
      "       [7.00000e+00, 7.64990e+02, 9.27440e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen35', 'img068.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.61640e+02, 3.14030e+02],\n",
      "       [1.00000e+00, 9.61040e+02, 4.41520e+02],\n",
      "       [2.00000e+00, 1.14039e+03, 3.61570e+02],\n",
      "       [3.00000e+00, 9.56720e+02, 6.61920e+02],\n",
      "       [4.00000e+00, 1.23763e+03, 6.61920e+02],\n",
      "       [5.00000e+00, 1.08205e+03, 8.06700e+02],\n",
      "       [6.00000e+00, 9.71850e+02, 9.88210e+02],\n",
      "       [7.00000e+00, 1.11662e+03, 1.00549e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest44', 'img099.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.38960e+02, 3.51710e+02],\n",
      "       [1.00000e+00, 6.73320e+02, 4.11220e+02],\n",
      "       [2.00000e+00, 1.02712e+03, 4.30510e+02],\n",
      "       [4.00000e+00, 1.08984e+03, 6.58870e+02],\n",
      "       [5.00000e+00, 1.41147e+03, 6.65310e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen33', 'img040.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.42994e+03, 4.54480e+02],\n",
      "       [1.00000e+00, 1.24843e+03, 5.47400e+02],\n",
      "       [2.00000e+00, 1.19009e+03, 4.30710e+02],\n",
      "       [3.00000e+00, 9.17830e+02, 6.27350e+02],\n",
      "       [4.00000e+00, 1.31758e+03, 6.74890e+02],\n",
      "       [5.00000e+00, 1.10582e+03, 8.26140e+02],\n",
      "       [6.00000e+00, 1.00858e+03, 9.92530e+02],\n",
      "       [7.00000e+00, 1.16848e+03, 1.00765e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk57', 'img093.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.03880e+02, 3.17940e+02],\n",
      "       [1.00000e+00, 5.80050e+02, 4.49810e+02],\n",
      "       [2.00000e+00, 9.46710e+02, 4.49810e+02],\n",
      "       [4.00000e+00, 8.87210e+02, 8.05220e+02],\n",
      "       [5.00000e+00, 1.23940e+03, 8.58290e+02],\n",
      "       [6.00000e+00, 1.07376e+03, 1.02393e+03],\n",
      "       [7.00000e+00, 9.90130e+02, 1.04323e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand4', 'img000.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.99780e+02, 2.34320e+02],\n",
      "       [1.00000e+00, 1.12039e+03, 2.47180e+02],\n",
      "       [2.00000e+00, 9.78870e+02, 3.74230e+02],\n",
      "       [4.00000e+00, 8.87210e+02, 6.87820e+02],\n",
      "       [5.00000e+00, 1.17829e+03, 8.05220e+02],\n",
      "       [6.00000e+00, 1.11235e+03, 9.03320e+02],\n",
      "       [7.00000e+00, 1.01104e+03, 9.29050e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen84', 'img134.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.06089e+03, 2.92210e+02],\n",
      "       [1.00000e+00, 1.05928e+03, 3.88700e+02],\n",
      "       [2.00000e+00, 1.10592e+03, 4.62680e+02],\n",
      "       [3.00000e+00, 8.75950e+02, 6.18670e+02],\n",
      "       [4.00000e+00, 1.16381e+03, 6.63700e+02],\n",
      "       [5.00000e+00, 9.62790e+02, 8.16470e+02],\n",
      "       [6.00000e+00, 9.06510e+02, 8.45420e+02],\n",
      "       [7.00000e+00, 1.08984e+03, 8.16470e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest44', 'img084.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.42180e+02, 3.51710e+02],\n",
      "       [1.00000e+00, 6.73320e+02, 4.09610e+02],\n",
      "       [2.00000e+00, 1.03677e+03, 4.35340e+02],\n",
      "       [4.00000e+00, 1.09466e+03, 6.70130e+02],\n",
      "       [5.00000e+00, 1.42595e+03, 6.73350e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen35', 'img078.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.61640e+02, 3.29150e+02],\n",
      "       [1.00000e+00, 9.56720e+02, 4.52320e+02],\n",
      "       [2.00000e+00, 1.13823e+03, 3.61570e+02],\n",
      "       [3.00000e+00, 9.58880e+02, 6.55440e+02],\n",
      "       [4.00000e+00, 1.23331e+03, 6.77050e+02],\n",
      "       [5.00000e+00, 1.08637e+03, 8.06700e+02],\n",
      "       [6.00000e+00, 9.58880e+02, 9.79560e+02],\n",
      "       [7.00000e+00, 1.11878e+03, 9.96850e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen50', 'img123.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.01425e+03, 3.95130e+02],\n",
      "       [1.00000e+00, 8.98470e+02, 3.35630e+02],\n",
      "       [2.00000e+00, 1.16221e+03, 4.72330e+02],\n",
      "       [3.00000e+00, 1.15738e+03, 8.38990e+02],\n",
      "       [4.00000e+00, 9.83700e+02, 4.72330e+02],\n",
      "       [5.00000e+00, 8.27710e+02, 8.38990e+02],\n",
      "       [6.00000e+00, 1.03516e+03, 9.82120e+02],\n",
      "       [7.00000e+00, 1.00782e+03, 9.67640e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk6', 'img091.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 366.1 , 230.81],\n",
      "       [  1.  , 330.45, 281.47],\n",
      "       [  2.  , 422.39, 326.5 ],\n",
      "       [  3.  , 506.83, 498.19],\n",
      "       [  4.  , 277.91, 492.56],\n",
      "       [  5.  , 467.42, 579.81],\n",
      "       [  6.  , 450.53, 657.68],\n",
      "       [  7.  , 410.19, 639.85]], dtype=float32)}}, {'image': ('labeled-data', 'preen37', 'img003.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.75210e+02, 6.77050e+02],\n",
      "       [1.00000e+00, 9.04860e+02, 8.15340e+02],\n",
      "       [2.00000e+00, 8.24910e+02, 5.86290e+02],\n",
      "       [3.00000e+00, 9.89130e+02, 8.39110e+02],\n",
      "       [4.00000e+00, 1.24411e+03, 5.30110e+02],\n",
      "       [5.00000e+00, 1.12743e+03, 8.23980e+02],\n",
      "       [6.00000e+00, 9.56720e+02, 9.96850e+02],\n",
      "       [7.00000e+00, 1.11230e+03, 9.96850e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest65', 'img057.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.30533e+03, 3.00250e+02],\n",
      "       [1.00000e+00, 1.48384e+03, 3.38850e+02],\n",
      "       [2.00000e+00, 1.09949e+03, 4.41770e+02],\n",
      "       [3.00000e+00, 1.25226e+03, 7.58580e+02],\n",
      "       [5.00000e+00, 5.73620e+02, 6.15450e+02],\n",
      "       [6.00000e+00, 8.53440e+02, 8.40600e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen35', 'img038.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.76770e+02, 2.55690e+02],\n",
      "       [1.00000e+00, 9.56720e+02, 4.39360e+02],\n",
      "       [2.00000e+00, 1.13607e+03, 3.76690e+02],\n",
      "       [3.00000e+00, 9.30790e+02, 6.74890e+02],\n",
      "       [4.00000e+00, 1.21818e+03, 6.94330e+02],\n",
      "       [5.00000e+00, 1.07557e+03, 8.02370e+02],\n",
      "       [6.00000e+00, 9.65360e+02, 9.99010e+02],\n",
      "       [7.00000e+00, 1.12310e+03, 1.00117e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand75', 'img007.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00987e+03, 3.41740e+02],\n",
      "       [1.00000e+00, 1.07665e+03, 3.69430e+02],\n",
      "       [2.00000e+00, 9.43090e+02, 4.21550e+02],\n",
      "       [3.00000e+00, 1.01720e+03, 5.57550e+02],\n",
      "       [5.00000e+00, 8.78750e+02, 6.22700e+02],\n",
      "       [6.00000e+00, 9.72400e+02, 6.69940e+02],\n",
      "       [7.00000e+00, 9.10510e+02, 6.69940e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk64', 'img045.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.30533e+03, 1.39440e+02],\n",
      "       [1.00000e+00, 1.39378e+03, 1.95720e+02],\n",
      "       [2.00000e+00, 1.26995e+03, 3.01860e+02],\n",
      "       [3.00000e+00, 1.09788e+03, 4.48200e+02],\n",
      "       [4.00000e+00, 1.29729e+03, 4.67500e+02],\n",
      "       [5.00000e+00, 1.11718e+03, 5.55950e+02],\n",
      "       [6.00000e+00, 1.04481e+03, 6.00980e+02],\n",
      "       [7.00000e+00, 1.17990e+03, 5.91330e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand11', 'img043.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.05132e+03, 3.54120e+02],\n",
      "       [1.00000e+00, 1.09434e+03, 3.57310e+02],\n",
      "       [2.00000e+00, 1.02105e+03, 4.35910e+02],\n",
      "       [3.00000e+00, 1.09646e+03, 5.34160e+02],\n",
      "       [5.00000e+00, 1.01733e+03, 6.30820e+02],\n",
      "       [6.00000e+00, 1.06353e+03, 6.43030e+02],\n",
      "       [7.00000e+00, 1.00246e+03, 6.47280e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen40', 'img066.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08423e+03, 9.25000e+01],\n",
      "       [1.00000e+00, 1.16976e+03, 1.05660e+02],\n",
      "       [2.00000e+00, 1.16153e+03, 3.39220e+02],\n",
      "       [3.00000e+00, 9.37840e+02, 6.31990e+02],\n",
      "       [4.00000e+00, 1.27502e+03, 6.13890e+02],\n",
      "       [5.00000e+00, 1.10726e+03, 8.03040e+02],\n",
      "       [6.00000e+00, 9.95410e+02, 9.85610e+02],\n",
      "       [7.00000e+00, 1.16153e+03, 9.92190e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest19', 'img005.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.20723e+03, 2.69700e+02],\n",
      "       [1.00000e+00, 1.33267e+03, 3.34020e+02],\n",
      "       [2.00000e+00, 1.09145e+03, 3.95130e+02],\n",
      "       [3.00000e+00, 1.10592e+03, 7.16770e+02],\n",
      "       [5.00000e+00, 7.28000e+02, 5.51130e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand69', 'img135.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.74050e+02, 2.29490e+02],\n",
      "       [1.00000e+00, 1.11557e+03, 2.66480e+02],\n",
      "       [2.00000e+00, 9.66010e+02, 4.44990e+02],\n",
      "       [3.00000e+00, 6.99050e+02, 6.65310e+02],\n",
      "       [4.00000e+00, 9.85310e+02, 6.87820e+02],\n",
      "       [5.00000e+00, 8.35750e+02, 8.38990e+02],\n",
      "       [6.00000e+00, 7.31220e+02, 9.11360e+02],\n",
      "       [7.00000e+00, 9.19370e+02, 9.16180e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest15', 'img054.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.36040e+02, 4.81980e+02],\n",
      "       [1.00000e+00, 5.84870e+02, 6.10630e+02],\n",
      "       [2.00000e+00, 9.64400e+02, 4.64290e+02],\n",
      "       [4.00000e+00, 1.10431e+03, 7.69840e+02],\n",
      "       [5.00000e+00, 1.49188e+03, 7.03900e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen37', 'img094.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.33550e+02, 7.41870e+02],\n",
      "       [1.00000e+00, 9.69690e+02, 7.76440e+02],\n",
      "       [2.00000e+00, 8.65970e+02, 5.86290e+02],\n",
      "       [3.00000e+00, 1.04099e+03, 8.58560e+02],\n",
      "       [4.00000e+00, 1.27220e+03, 5.32270e+02],\n",
      "       [5.00000e+00, 1.17712e+03, 8.26140e+02],\n",
      "       [6.00000e+00, 1.01290e+03, 9.86040e+02],\n",
      "       [7.00000e+00, 1.17064e+03, 9.99010e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand11', 'img038.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.05185e+03, 3.53590e+02],\n",
      "       [1.00000e+00, 1.08903e+03, 3.56250e+02],\n",
      "       [2.00000e+00, 1.02742e+03, 4.33260e+02],\n",
      "       [3.00000e+00, 1.09646e+03, 5.33100e+02],\n",
      "       [5.00000e+00, 1.02848e+03, 6.28690e+02],\n",
      "       [6.00000e+00, 1.07097e+03, 6.41970e+02],\n",
      "       [7.00000e+00, 1.00458e+03, 6.48340e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest47', 'img017.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.37650e+02, 3.85490e+02],\n",
      "       [1.00000e+00, 6.18640e+02, 5.25400e+02],\n",
      "       [2.00000e+00, 9.32240e+02, 3.80660e+02],\n",
      "       [4.00000e+00, 1.05124e+03, 6.42790e+02],\n",
      "       [5.00000e+00, 1.46937e+03, 6.04200e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk57', 'img098.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.13820e+02, 3.22770e+02],\n",
      "       [1.00000e+00, 4.73910e+02, 4.35340e+02],\n",
      "       [2.00000e+00, 8.48610e+02, 4.54640e+02],\n",
      "       [4.00000e+00, 8.61480e+02, 7.90740e+02],\n",
      "       [5.00000e+00, 1.13648e+03, 8.58290e+02],\n",
      "       [6.00000e+00, 8.72740e+02, 9.80510e+02],\n",
      "       [7.00000e+00, 9.82090e+02, 1.02714e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand9', 'img052.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.05928e+03, 2.50400e+02],\n",
      "       [1.00000e+00, 1.14291e+03, 2.58440e+02],\n",
      "       [2.00000e+00, 1.03677e+03, 4.20870e+02],\n",
      "       [4.00000e+00, 9.11330e+02, 6.78170e+02],\n",
      "       [5.00000e+00, 1.17346e+03, 8.35770e+02],\n",
      "       [6.00000e+00, 1.13648e+03, 9.14570e+02],\n",
      "       [7.00000e+00, 1.03677e+03, 9.35480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand72', 'img072.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08984e+03, 1.63560e+02],\n",
      "       [1.00000e+00, 1.04803e+03, 2.82560e+02],\n",
      "       [2.00000e+00, 1.06572e+03, 3.87090e+02],\n",
      "       [3.00000e+00, 9.54750e+02, 5.99370e+02],\n",
      "       [4.00000e+00, 1.28443e+03, 6.29930e+02],\n",
      "       [5.00000e+00, 1.17025e+03, 7.69840e+02],\n",
      "       [6.00000e+00, 1.10110e+03, 8.08430e+02],\n",
      "       [7.00000e+00, 1.23940e+03, 8.21300e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest42', 'img029.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.61480e+02, 3.66190e+02],\n",
      "       [1.00000e+00, 6.82970e+02, 4.16040e+02],\n",
      "       [2.00000e+00, 9.96560e+02, 4.56250e+02],\n",
      "       [4.00000e+00, 1.05928e+03, 6.78170e+02],\n",
      "       [5.00000e+00, 1.38735e+03, 6.89430e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen40', 'img025.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.06120e+03, 9.08500e+01],\n",
      "       [1.00000e+00, 1.06285e+03, 1.55000e+02],\n",
      "       [2.00000e+00, 1.10561e+03, 3.27700e+02],\n",
      "       [3.00000e+00, 9.37840e+02, 6.10600e+02],\n",
      "       [4.00000e+00, 1.29312e+03, 5.82640e+02],\n",
      "       [5.00000e+00, 1.11219e+03, 7.88240e+02],\n",
      "       [6.00000e+00, 1.00363e+03, 9.59300e+02],\n",
      "       [7.00000e+00, 1.17963e+03, 9.65870e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap14', 'img055.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.82090e+02, 1.88200e+01],\n",
      "       [1.00000e+00, 1.11074e+03, 5.96000e+00],\n",
      "       [2.00000e+00, 8.24490e+02, 2.42360e+02],\n",
      "       [3.00000e+00, 8.50220e+02, 3.37240e+02],\n",
      "       [4.00000e+00, 7.28000e+02, 3.46890e+02],\n",
      "       [5.00000e+00, 6.44380e+02, 7.21590e+02],\n",
      "       [6.00000e+00, 8.30920e+02, 9.35480e+02],\n",
      "       [7.00000e+00, 7.71420e+02, 9.19400e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk82', 'img069.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.68647e+03, 1.02450e+02],\n",
      "       [1.00000e+00, 1.77492e+03, 1.58730e+02],\n",
      "       [2.00000e+00, 1.56264e+03, 2.21450e+02],\n",
      "       [3.00000e+00, 1.51761e+03, 4.33730e+02],\n",
      "       [5.00000e+00, 1.36644e+03, 5.12530e+02],\n",
      "       [6.00000e+00, 1.41147e+03, 5.99370e+02],\n",
      "       [7.00000e+00, 1.48545e+03, 6.57270e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen50', 'img025.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.88520e+02, 2.80950e+02],\n",
      "       [1.00000e+00, 1.04159e+03, 1.49080e+02],\n",
      "       [2.00000e+00, 1.18150e+03, 4.44990e+02],\n",
      "       [3.00000e+00, 1.23618e+03, 7.81100e+02],\n",
      "       [4.00000e+00, 9.69230e+02, 4.61070e+02],\n",
      "       [5.00000e+00, 8.51830e+02, 8.53460e+02],\n",
      "       [6.00000e+00, 1.06089e+03, 9.93370e+02],\n",
      "       [7.00000e+00, 1.01425e+03, 9.70860e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap14', 'img057.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.83700e+02, 5.96000e+00],\n",
      "       [1.00000e+00, 1.09145e+03, 2.74000e+00],\n",
      "       [2.00000e+00, 8.22880e+02, 2.35930e+02],\n",
      "       [3.00000e+00, 7.48910e+02, 3.27590e+02],\n",
      "       [5.00000e+00, 6.45980e+02, 7.19980e+02],\n",
      "       [6.00000e+00, 8.30920e+02, 9.25830e+02],\n",
      "       [7.00000e+00, 7.73030e+02, 9.12960e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap2', 'img074.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.63822e+03, 6.70700e+01],\n",
      "       [1.00000e+00, 1.77653e+03, 1.26570e+02],\n",
      "       [2.00000e+00, 1.41308e+03, 1.53910e+02],\n",
      "       [3.00000e+00, 1.40021e+03, 3.67800e+02],\n",
      "       [5.00000e+00, 1.15577e+03, 4.90020e+02],\n",
      "       [6.00000e+00, 1.23618e+03, 6.04200e+02],\n",
      "       [7.00000e+00, 1.30372e+03, 6.33140e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen31', 'img122.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.22250e+03, 2.55690e+02],\n",
      "       [1.00000e+00, 1.17280e+03, 4.02620e+02],\n",
      "       [2.00000e+00, 1.08637e+03, 3.76690e+02],\n",
      "       [3.00000e+00, 8.83250e+02, 6.66240e+02],\n",
      "       [4.00000e+00, 1.22250e+03, 6.46800e+02],\n",
      "       [5.00000e+00, 1.07340e+03, 8.00210e+02],\n",
      "       [6.00000e+00, 9.63200e+02, 9.70920e+02],\n",
      "       [7.00000e+00, 1.13607e+03, 9.88210e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen30', 'img090.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.21170e+03, 1.69250e+02],\n",
      "       [1.00000e+00, 1.14687e+03, 3.22670e+02],\n",
      "       [2.00000e+00, 1.09933e+03, 3.59410e+02],\n",
      "       [3.00000e+00, 9.17830e+02, 6.46800e+02],\n",
      "       [4.00000e+00, 1.27436e+03, 6.38150e+02],\n",
      "       [5.00000e+00, 1.11230e+03, 7.95890e+02],\n",
      "       [6.00000e+00, 1.00858e+03, 9.79560e+02],\n",
      "       [7.00000e+00, 1.17064e+03, 9.99010e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap4', 'img051.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.26674e+03, 7.83300e+01],\n",
      "       [1.00000e+00, 1.39378e+03, 1.49080e+02],\n",
      "       [2.00000e+00, 1.21528e+03, 3.30810e+02],\n",
      "       [3.00000e+00, 8.34140e+02, 6.02590e+02],\n",
      "       [4.00000e+00, 1.30533e+03, 5.22180e+02],\n",
      "       [5.00000e+00, 9.99780e+02, 8.50250e+02],\n",
      "       [6.00000e+00, 9.45100e+02, 9.43520e+02],\n",
      "       [7.00000e+00, 1.12200e+03, 9.17790e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest19', 'img115.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.23940e+03, 3.01860e+02],\n",
      "       [1.00000e+00, 1.45811e+03, 3.24380e+02],\n",
      "       [2.00000e+00, 1.07054e+03, 4.04780e+02],\n",
      "       [3.00000e+00, 1.08501e+03, 7.21590e+02],\n",
      "       [5.00000e+00, 7.07090e+02, 5.57560e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest65', 'img138.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.32302e+03, 3.14730e+02],\n",
      "       [1.00000e+00, 1.49670e+03, 3.56540e+02],\n",
      "       [2.00000e+00, 1.11718e+03, 4.38560e+02],\n",
      "       [3.00000e+00, 1.26030e+03, 7.66620e+02],\n",
      "       [5.00000e+00, 5.65580e+02, 6.25100e+02],\n",
      "       [6.00000e+00, 8.55050e+02, 8.45420e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest30', 'img135.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.49160e+02, 4.02110e+02],\n",
      "       [1.00000e+00, 5.66590e+02, 4.45920e+02],\n",
      "       [2.00000e+00, 7.34610e+02, 4.05700e+02],\n",
      "       [4.00000e+00, 8.66020e+02, 5.65120e+02],\n",
      "       [5.00000e+00, 1.05487e+03, 5.16290e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap7', 'img053.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.04642e+03, 4.09610e+02],\n",
      "       [1.00000e+00, 1.12683e+03, 4.80370e+02],\n",
      "       [2.00000e+00, 1.06411e+03, 6.33140e+02],\n",
      "       [3.00000e+00, 8.64690e+02, 8.32560e+02],\n",
      "       [4.00000e+00, 1.10592e+03, 7.93960e+02],\n",
      "       [5.00000e+00, 9.56360e+02, 9.46740e+02],\n",
      "       [6.00000e+00, 9.01680e+02, 1.01267e+03],\n",
      "       [7.00000e+00, 1.02712e+03, 9.85330e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap14', 'img077.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.57970e+02, 7.83300e+01],\n",
      "       [1.00000e+00, 1.03677e+03, 7.18900e+01],\n",
      "       [2.00000e+00, 8.35750e+02, 2.90600e+02],\n",
      "       [3.00000e+00, 9.19370e+02, 4.72330e+02],\n",
      "       [4.00000e+00, 6.89400e+02, 4.01570e+02],\n",
      "       [5.00000e+00, 6.28290e+02, 7.32850e+02],\n",
      "       [6.00000e+00, 8.40570e+02, 9.41910e+02],\n",
      "       [7.00000e+00, 7.73030e+02, 9.30650e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand2', 'img056.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.47300e+02, 1.34610e+02],\n",
      "       [1.00000e+00, 6.91010e+02, 2.27890e+02],\n",
      "       [2.00000e+00, 8.88820e+02, 2.76130e+02],\n",
      "       [3.00000e+00, 7.19960e+02, 5.88110e+02],\n",
      "       [4.00000e+00, 1.02069e+03, 6.62090e+02],\n",
      "       [5.00000e+00, 1.11557e+03, 6.60480e+02],\n",
      "       [6.00000e+00, 8.42180e+02, 8.72760e+02],\n",
      "       [7.00000e+00, 9.38670e+02, 9.40300e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap13', 'img145.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  929.02,  335.63],\n",
      "       [   2.  ,  946.71,  425.69],\n",
      "       [   3.  , 1068.93,  565.6 ],\n",
      "       [   4.  ,  782.68,  530.22],\n",
      "       [   5.  ,  993.35,  702.29],\n",
      "       [   6.  ,  959.58,  785.92],\n",
      "       [   7.  ,  908.12,  781.1 ]], dtype=float32)}}, {'image': ('labeled-data', 'stand75', 'img026.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.02290e+03, 3.34410e+02],\n",
      "       [1.00000e+00, 1.08968e+03, 3.63730e+02],\n",
      "       [2.00000e+00, 9.51230e+02, 4.16660e+02],\n",
      "       [3.00000e+00, 1.01801e+03, 5.61620e+02],\n",
      "       [5.00000e+00, 8.74680e+02, 6.17820e+02],\n",
      "       [6.00000e+00, 9.78920e+02, 6.74010e+02],\n",
      "       [7.00000e+00, 9.10510e+02, 6.73200e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand9', 'img041.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08019e+03, 2.58440e+02],\n",
      "       [1.00000e+00, 1.16060e+03, 2.63260e+02],\n",
      "       [2.00000e+00, 1.04963e+03, 4.25690e+02],\n",
      "       [4.00000e+00, 9.08120e+02, 6.86210e+02],\n",
      "       [5.00000e+00, 1.18311e+03, 8.51850e+02],\n",
      "       [6.00000e+00, 1.14612e+03, 9.21010e+02],\n",
      "       [7.00000e+00, 1.04320e+03, 9.46740e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk66', 'img028.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.12458e+03, 2.27000e+02],\n",
      "       [1.00000e+00, 1.03232e+03, 2.72190e+02],\n",
      "       [2.00000e+00, 1.10858e+03, 3.55970e+02],\n",
      "       [3.00000e+00, 1.14341e+03, 5.00000e+02],\n",
      "       [4.00000e+00, 1.30062e+03, 4.76470e+02],\n",
      "       [5.00000e+00, 1.23378e+03, 5.97900e+02],\n",
      "       [6.00000e+00, 1.20271e+03, 6.20500e+02],\n",
      "       [7.00000e+00, 1.28367e+03, 6.36500e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest30', 'img133.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.57780e+02, 4.02110e+02],\n",
      "       [1.00000e+00, 5.73770e+02, 4.45200e+02],\n",
      "       [2.00000e+00, 7.50410e+02, 4.06420e+02],\n",
      "       [4.00000e+00, 8.72480e+02, 5.59370e+02],\n",
      "       [5.00000e+00, 1.05918e+03, 5.17000e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap13', 'img116.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.22590e+02, 3.27590e+02],\n",
      "       [1.00000e+00, 8.71130e+02, 3.43670e+02],\n",
      "       [2.00000e+00, 9.74050e+02, 4.36950e+02],\n",
      "       [3.00000e+00, 1.10914e+03, 4.77150e+02],\n",
      "       [4.00000e+00, 9.56360e+02, 5.30220e+02],\n",
      "       [5.00000e+00, 1.03677e+03, 7.08730e+02],\n",
      "       [6.00000e+00, 9.91740e+02, 7.95570e+02],\n",
      "       [7.00000e+00, 9.37060e+02, 7.95570e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand9', 'img148.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.87210e+02, 3.93530e+02],\n",
      "       [1.00000e+00, 8.22880e+02, 5.01270e+02],\n",
      "       [2.00000e+00, 1.06893e+03, 4.22470e+02],\n",
      "       [4.00000e+00, 9.48320e+02, 7.02290e+02],\n",
      "       [5.00000e+00, 1.21206e+03, 8.27730e+02],\n",
      "       [6.00000e+00, 1.14934e+03, 9.08140e+02],\n",
      "       [7.00000e+00, 1.04963e+03, 9.29050e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand82', 'img005.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.49930e+02, 1.21750e+02],\n",
      "       [1.00000e+00, 7.84290e+02, 1.86070e+02],\n",
      "       [2.00000e+00, 1.06250e+03, 3.21160e+02],\n",
      "       [4.00000e+00, 1.08984e+03, 7.37670e+02],\n",
      "       [5.00000e+00, 1.31981e+03, 7.98780e+02],\n",
      "       [6.00000e+00, 1.08180e+03, 9.51560e+02],\n",
      "       [7.00000e+00, 1.09627e+03, 1.02554e+03]], dtype=float32)}}, {'image': ('labeled-data', 'preen48', 'img118.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.34573e+03, 3.05160e+02],\n",
      "       [1.00000e+00, 1.31585e+03, 3.42680e+02],\n",
      "       [2.00000e+00, 1.24208e+03, 2.19310e+02],\n",
      "       [3.00000e+00, 1.15942e+03, 3.81470e+02],\n",
      "       [4.00000e+00, 1.36863e+03, 3.63660e+02],\n",
      "       [5.00000e+00, 1.24335e+03, 4.99740e+02],\n",
      "       [6.00000e+00, 1.21410e+03, 5.32810e+02],\n",
      "       [7.00000e+00, 1.30249e+03, 5.41080e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk62', 'img146.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1110.74,   60.64],\n",
      "       [   2.  , 1117.18,  202.15],\n",
      "       [   3.  , 1250.66,  422.47],\n",
      "       [   4.  ,  969.23,  432.12],\n",
      "       [   5.  , 1099.49,  554.34],\n",
      "       [   6.  , 1192.76,  620.28],\n",
      "       [   7.  , 1044.81,  650.83]], dtype=float32)}}, {'image': ('labeled-data', 'stand69', 'img099.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00300e+03, 2.02150e+02],\n",
      "       [1.00000e+00, 1.14612e+03, 2.50400e+02],\n",
      "       [2.00000e+00, 9.75660e+02, 4.38560e+02],\n",
      "       [3.00000e+00, 7.02270e+02, 6.66920e+02],\n",
      "       [4.00000e+00, 9.80480e+02, 6.89430e+02],\n",
      "       [5.00000e+00, 8.34140e+02, 8.47030e+02],\n",
      "       [6.00000e+00, 7.32820e+02, 9.14570e+02],\n",
      "       [7.00000e+00, 9.12940e+02, 9.12960e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen40', 'img101.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.07107e+03, 1.48420e+02],\n",
      "       [1.00000e+00, 1.14344e+03, 1.66510e+02],\n",
      "       [2.00000e+00, 1.13686e+03, 3.85270e+02],\n",
      "       [3.00000e+00, 9.16460e+02, 6.51720e+02],\n",
      "       [4.00000e+00, 1.25529e+03, 6.40210e+02],\n",
      "       [5.00000e+00, 1.10068e+03, 8.09620e+02],\n",
      "       [6.00000e+00, 9.93770e+02, 9.88900e+02],\n",
      "       [7.00000e+00, 1.15660e+03, 9.97130e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest30', 'img033.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.68550e+02, 4.03550e+02],\n",
      "       [1.00000e+00, 5.84540e+02, 4.50940e+02],\n",
      "       [2.00000e+00, 7.57590e+02, 4.09290e+02],\n",
      "       [4.00000e+00, 8.85410e+02, 5.69420e+02],\n",
      "       [5.00000e+00, 1.05559e+03, 5.07670e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen33', 'img049.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.42562e+03, 4.58800e+02],\n",
      "       [1.00000e+00, 1.24627e+03, 5.38750e+02],\n",
      "       [2.00000e+00, 1.18361e+03, 4.22070e+02],\n",
      "       [3.00000e+00, 9.17830e+02, 6.25190e+02],\n",
      "       [4.00000e+00, 1.29381e+03, 6.61920e+02],\n",
      "       [5.00000e+00, 1.10150e+03, 8.13180e+02],\n",
      "       [6.00000e+00, 1.00210e+03, 9.90370e+02],\n",
      "       [7.00000e+00, 1.16200e+03, 9.96850e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand72', 'img006.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.12039e+03, 1.42650e+02],\n",
      "       [1.00000e+00, 1.07376e+03, 2.35930e+02],\n",
      "       [2.00000e+00, 1.06572e+03, 3.56540e+02],\n",
      "       [3.00000e+00, 9.64400e+02, 5.76860e+02],\n",
      "       [4.00000e+00, 1.27960e+03, 6.12240e+02],\n",
      "       [5.00000e+00, 1.17668e+03, 7.95570e+02],\n",
      "       [6.00000e+00, 1.11235e+03, 8.30950e+02],\n",
      "       [7.00000e+00, 1.25548e+03, 8.47030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand69', 'img117.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.86920e+02, 2.19840e+02],\n",
      "       [1.00000e+00, 1.13004e+03, 2.60050e+02],\n",
      "       [2.00000e+00, 9.78870e+02, 4.38560e+02],\n",
      "       [3.00000e+00, 7.00660e+02, 6.63700e+02],\n",
      "       [4.00000e+00, 9.91740e+02, 6.91040e+02],\n",
      "       [5.00000e+00, 8.26100e+02, 8.51850e+02],\n",
      "       [6.00000e+00, 7.28000e+02, 9.11360e+02],\n",
      "       [7.00000e+00, 9.16160e+02, 9.03320e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand72', 'img009.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.09788e+03, 1.36220e+02],\n",
      "       [1.00000e+00, 1.05928e+03, 2.27890e+02],\n",
      "       [2.00000e+00, 1.07054e+03, 3.51710e+02],\n",
      "       [3.00000e+00, 9.64400e+02, 5.84900e+02],\n",
      "       [4.00000e+00, 1.28121e+03, 5.86510e+02],\n",
      "       [5.00000e+00, 1.17507e+03, 7.79490e+02],\n",
      "       [6.00000e+00, 1.10753e+03, 8.34160e+02],\n",
      "       [7.00000e+00, 1.24422e+03, 8.47030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest30', 'img092.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.70710e+02, 3.92060e+02],\n",
      "       [1.00000e+00, 5.80230e+02, 4.40890e+02],\n",
      "       [2.00000e+00, 7.43950e+02, 3.96370e+02],\n",
      "       [4.00000e+00, 8.77510e+02, 5.52190e+02],\n",
      "       [5.00000e+00, 1.06062e+03, 5.08390e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand82', 'img041.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.51540e+02, 1.08880e+02],\n",
      "       [1.00000e+00, 7.84290e+02, 1.66770e+02],\n",
      "       [2.00000e+00, 1.08662e+03, 3.37240e+02],\n",
      "       [4.00000e+00, 1.07697e+03, 7.56970e+02],\n",
      "       [5.00000e+00, 1.31659e+03, 8.06830e+02],\n",
      "       [6.00000e+00, 1.08662e+03, 9.48340e+02],\n",
      "       [7.00000e+00, 1.10914e+03, 9.99810e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest15', 'img146.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.05780e+02, 4.99670e+02],\n",
      "       [1.00000e+00, 4.73910e+02, 6.04200e+02],\n",
      "       [2.00000e+00, 8.59870e+02, 4.78760e+02],\n",
      "       [4.00000e+00, 9.86920e+02, 8.00390e+02],\n",
      "       [5.00000e+00, 1.38735e+03, 7.28030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest30', 'img077.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.64240e+02, 3.97090e+02],\n",
      "       [1.00000e+00, 5.76640e+02, 4.40170e+02],\n",
      "       [2.00000e+00, 7.50410e+02, 3.98520e+02],\n",
      "       [4.00000e+00, 8.69610e+02, 5.54340e+02],\n",
      "       [5.00000e+00, 1.04626e+03, 4.93310e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest24', 'img066.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.91920e+02, 3.27450e+02],\n",
      "       [1.00000e+00, 5.52250e+02, 3.76680e+02],\n",
      "       [2.00000e+00, 8.67770e+02, 3.35480e+02],\n",
      "       [4.00000e+00, 1.05769e+03, 5.35450e+02],\n",
      "       [5.00000e+00, 1.30488e+03, 4.54060e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest42', 'img070.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.75950e+02, 3.61360e+02],\n",
      "       [1.00000e+00, 7.10310e+02, 4.04780e+02],\n",
      "       [2.00000e+00, 1.03516e+03, 4.46600e+02],\n",
      "       [4.00000e+00, 1.10914e+03, 6.94250e+02],\n",
      "       [5.00000e+00, 1.42434e+03, 6.79780e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest47', 'img145.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.52120e+02, 4.06390e+02],\n",
      "       [1.00000e+00, 6.52420e+02, 5.41480e+02],\n",
      "       [2.00000e+00, 9.37060e+02, 3.82270e+02],\n",
      "       [4.00000e+00, 1.07858e+03, 6.52440e+02],\n",
      "       [5.00000e+00, 1.47258e+03, 6.05800e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap4', 'img124.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[1.0000e+00, 1.5160e+03, 1.7220e+01],\n",
      "       [2.0000e+00, 1.0239e+03, 2.5362e+02],\n",
      "       [3.0000e+00, 8.7595e+02, 3.9192e+02],\n",
      "       [4.0000e+00, 9.9174e+02, 2.6487e+02],\n",
      "       [5.0000e+00, 7.2318e+02, 7.7788e+02],\n",
      "       [6.0000e+00, 8.8078e+02, 9.5960e+02],\n",
      "       [7.0000e+00, 9.9817e+02, 9.4674e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap13', 'img067.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.03998e+03, 3.27590e+02],\n",
      "       [1.00000e+00, 1.10110e+03, 3.40460e+02],\n",
      "       [2.00000e+00, 1.01265e+03, 4.43380e+02],\n",
      "       [3.00000e+00, 1.17346e+03, 5.89720e+02],\n",
      "       [4.00000e+00, 8.63090e+02, 5.33440e+02],\n",
      "       [5.00000e+00, 1.05446e+03, 7.40890e+02],\n",
      "       [6.00000e+00, 1.01908e+03, 8.37380e+02],\n",
      "       [7.00000e+00, 9.70830e+02, 7.97180e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest30', 'img098.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.57780e+02, 3.96370e+02],\n",
      "       [1.00000e+00, 5.74480e+02, 4.40890e+02],\n",
      "       [2.00000e+00, 7.41080e+02, 3.98520e+02],\n",
      "       [4.00000e+00, 8.74640e+02, 5.53630e+02],\n",
      "       [5.00000e+00, 1.05918e+03, 5.13410e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest39', 'img095.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.88520e+02, 4.11220e+02],\n",
      "       [1.00000e+00, 1.13969e+03, 4.61070e+02],\n",
      "       [3.00000e+00, 7.13530e+02, 6.31540e+02],\n",
      "       [4.00000e+00, 1.25870e+03, 5.65600e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen37', 'img138.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.00540e+02, 7.41870e+02],\n",
      "       [1.00000e+00, 1.04099e+03, 7.78610e+02],\n",
      "       [2.00000e+00, 9.30790e+02, 6.01420e+02],\n",
      "       [3.00000e+00, 1.11014e+03, 8.60720e+02],\n",
      "       [4.00000e+00, 1.34567e+03, 5.27950e+02],\n",
      "       [5.00000e+00, 1.24843e+03, 8.30470e+02],\n",
      "       [6.00000e+00, 1.08205e+03, 9.99010e+02],\n",
      "       [7.00000e+00, 1.23547e+03, 1.00765e+03]], dtype=float32)}}, {'image': ('labeled-data', 'preen35', 'img025.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.70290e+02, 2.53530e+02],\n",
      "       [1.00000e+00, 9.65360e+02, 4.30710e+02],\n",
      "       [2.00000e+00, 1.14471e+03, 3.83180e+02],\n",
      "       [3.00000e+00, 9.32950e+02, 6.74890e+02],\n",
      "       [4.00000e+00, 1.22034e+03, 6.83530e+02],\n",
      "       [5.00000e+00, 1.07340e+03, 8.11020e+02],\n",
      "       [6.00000e+00, 9.65360e+02, 9.92530e+02],\n",
      "       [7.00000e+00, 1.11878e+03, 1.00549e+03]], dtype=float32)}}, {'image': ('labeled-data', 'preen50', 'img088.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1041.59,  438.56],\n",
      "       [   2.  , 1150.95,  438.56],\n",
      "       [   3.  , 1158.99,  822.91],\n",
      "       [   5.  ,  840.57,  834.16],\n",
      "       [   6.  , 1049.63,  988.55],\n",
      "       [   7.  , 1012.65,  966.03]], dtype=float32)}}, {'image': ('labeled-data', 'walk6', 'img104.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 229.13, 228.  ],\n",
      "       [  2.  , 264.78, 337.76],\n",
      "       [  3.  , 379.23, 480.36],\n",
      "       [  4.  , 155.95, 517.89],\n",
      "       [  5.  , 329.51, 603.26],\n",
      "       [  6.  , 384.86, 638.91],\n",
      "       [  7.  , 271.34, 670.81]], dtype=float32)}}, {'image': ('labeled-data', 'stand9', 'img062.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.96560e+02, 2.52010e+02],\n",
      "       [1.00000e+00, 9.40280e+02, 2.63260e+02],\n",
      "       [2.00000e+00, 1.05607e+03, 4.35340e+02],\n",
      "       [4.00000e+00, 8.92030e+02, 6.78170e+02],\n",
      "       [5.00000e+00, 1.15899e+03, 8.26120e+02],\n",
      "       [6.00000e+00, 1.12522e+03, 9.14570e+02],\n",
      "       [7.00000e+00, 1.02712e+03, 9.35480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap2', 'img001.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[1.00000e+00, 1.41308e+03, 3.00800e+01],\n",
      "       [2.00000e+00, 1.26674e+03, 1.98940e+02],\n",
      "       [3.00000e+00, 1.09466e+03, 3.99960e+02],\n",
      "       [4.00000e+00, 1.43077e+03, 3.80660e+02],\n",
      "       [5.00000e+00, 1.23779e+03, 5.41480e+02],\n",
      "       [6.00000e+00, 1.18794e+03, 6.12240e+02],\n",
      "       [7.00000e+00, 1.28764e+03, 6.18670e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest8', 'img080.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  822.88,  121.75],\n",
      "       [   2.  ,  954.75,  252.01],\n",
      "       [   4.  ,  713.53,  670.13],\n",
      "       [   5.  , 1440.42,  615.45]], dtype=float32)}}, {'image': ('labeled-data', 'stand72', 'img123.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08501e+03, 1.95720e+02],\n",
      "       [1.00000e+00, 1.03194e+03, 3.00250e+02],\n",
      "       [2.00000e+00, 1.05285e+03, 4.06390e+02],\n",
      "       [3.00000e+00, 9.41890e+02, 6.04200e+02],\n",
      "       [4.00000e+00, 1.28764e+03, 6.46010e+02],\n",
      "       [5.00000e+00, 1.17185e+03, 7.85920e+02],\n",
      "       [6.00000e+00, 1.08501e+03, 8.24520e+02],\n",
      "       [7.00000e+00, 1.22975e+03, 8.43810e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap7', 'img121.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.62790e+02, 6.62090e+02],\n",
      "       [1.00000e+00, 9.78870e+02, 7.92350e+02],\n",
      "       [2.00000e+00, 9.94960e+02, 7.60190e+02],\n",
      "       [3.00000e+00, 8.16450e+02, 8.67940e+02],\n",
      "       [4.00000e+00, 1.08823e+03, 8.11650e+02],\n",
      "       [5.00000e+00, 9.59580e+02, 9.78900e+02],\n",
      "       [6.00000e+00, 9.06510e+02, 1.02232e+03],\n",
      "       [7.00000e+00, 1.02390e+03, 1.00302e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand4', 'img054.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.07858e+03, 2.48790e+02],\n",
      "       [1.00000e+00, 1.20241e+03, 2.95430e+02],\n",
      "       [2.00000e+00, 1.03677e+03, 3.96740e+02],\n",
      "       [4.00000e+00, 9.04900e+02, 6.74960e+02],\n",
      "       [5.00000e+00, 1.20241e+03, 8.29340e+02],\n",
      "       [6.00000e+00, 1.14130e+03, 9.08140e+02],\n",
      "       [7.00000e+00, 1.03838e+03, 9.27440e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand4', 'img080.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.05607e+03, 2.42360e+02],\n",
      "       [1.00000e+00, 1.10270e+03, 2.39140e+02],\n",
      "       [2.00000e+00, 1.05607e+03, 4.01570e+02],\n",
      "       [4.00000e+00, 9.22590e+02, 6.78170e+02],\n",
      "       [5.00000e+00, 1.19919e+03, 8.21300e+02],\n",
      "       [6.00000e+00, 1.15577e+03, 9.06530e+02],\n",
      "       [7.00000e+00, 1.05285e+03, 9.29050e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap12', 'img051.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1160.6 ,  411.22],\n",
      "       [   2.  , 1096.27,  506.1 ],\n",
      "       [   3.  , 1097.88,  586.51],\n",
      "       [   4.  ,  956.36,  597.76],\n",
      "       [   5.  ,  937.06,  800.39],\n",
      "       [   6.  , 1022.3 ,  874.37]], dtype=float32)}}, {'image': ('labeled-data', 'stand11', 'img117.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.04442e+03, 3.62620e+02],\n",
      "       [1.00000e+00, 9.97680e+02, 3.79090e+02],\n",
      "       [2.00000e+00, 1.06778e+03, 4.33790e+02],\n",
      "       [3.00000e+00, 1.12833e+03, 5.36290e+02],\n",
      "       [4.00000e+00, 1.02105e+03, 5.26730e+02],\n",
      "       [5.00000e+00, 1.05663e+03, 6.09040e+02],\n",
      "       [6.00000e+00, 1.10337e+03, 6.41970e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk57', 'img114.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 3.03440e+02, 3.48500e+02],\n",
      "       [1.00000e+00, 2.02130e+02, 4.91620e+02],\n",
      "       [2.00000e+00, 5.80050e+02, 4.32120e+02],\n",
      "       [4.00000e+00, 6.05780e+02, 7.74660e+02],\n",
      "       [5.00000e+00, 8.79170e+02, 8.56680e+02],\n",
      "       [6.00000e+00, 6.21860e+02, 9.67640e+02],\n",
      "       [7.00000e+00, 6.66890e+02, 1.02554e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk89', 'img067.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 431.25, 281.65],\n",
      "       [  1.  , 376.98, 409.94],\n",
      "       [  2.  , 562.84, 378.69],\n",
      "       [  3.  , 587.51, 557.97],\n",
      "       [  4.  , 765.14, 543.17],\n",
      "       [  5.  , 728.96, 709.29],\n",
      "       [  6.  , 720.73, 783.3 ],\n",
      "       [  7.  , 788.17, 737.25]], dtype=float32)}}, {'image': ('labeled-data', 'rest39', 'img138.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.04481e+03, 4.08000e+02],\n",
      "       [1.00000e+00, 1.20080e+03, 4.64290e+02],\n",
      "       [3.00000e+00, 7.53730e+02, 6.29930e+02],\n",
      "       [4.00000e+00, 1.32141e+03, 5.63990e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen50', 'img019.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.29020e+02, 3.87090e+02],\n",
      "       [1.00000e+00, 8.85600e+02, 2.31100e+02],\n",
      "       [2.00000e+00, 1.19919e+03, 4.51420e+02],\n",
      "       [3.00000e+00, 1.21688e+03, 8.02000e+02],\n",
      "       [5.00000e+00, 8.42180e+02, 8.38990e+02],\n",
      "       [6.00000e+00, 1.05928e+03, 9.83720e+02],\n",
      "       [7.00000e+00, 1.01425e+03, 9.66030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen30', 'img060.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.26356e+03, 1.56290e+02],\n",
      "       [1.00000e+00, 1.21170e+03, 3.29150e+02],\n",
      "       [2.00000e+00, 1.15336e+03, 3.61570e+02],\n",
      "       [3.00000e+00, 9.58880e+02, 6.48960e+02],\n",
      "       [4.00000e+00, 1.31974e+03, 6.42470e+02],\n",
      "       [5.00000e+00, 1.16848e+03, 7.93730e+02],\n",
      "       [6.00000e+00, 1.05828e+03, 9.77400e+02],\n",
      "       [7.00000e+00, 1.23331e+03, 9.92530e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk89', 'img084.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 250.33, 227.37],\n",
      "       [  1.  , 163.16, 322.77],\n",
      "       [  2.  , 309.54, 357.31],\n",
      "       [  3.  , 335.86, 562.91],\n",
      "       [  4.  , 492.11, 566.19],\n",
      "       [  5.  , 452.64, 714.22],\n",
      "       [  6.  , 472.37, 784.95],\n",
      "       [  7.  , 510.2 , 781.66]], dtype=float32)}}, {'image': ('labeled-data', 'walk82', 'img033.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.27960e+03, 1.66770e+02],\n",
      "       [1.00000e+00, 1.36484e+03, 2.13410e+02],\n",
      "       [2.00000e+00, 1.17668e+03, 2.93820e+02],\n",
      "       [3.00000e+00, 1.14452e+03, 5.18960e+02],\n",
      "       [5.00000e+00, 9.85310e+02, 5.76860e+02],\n",
      "       [6.00000e+00, 1.11235e+03, 6.76560e+02],\n",
      "       [7.00000e+00, 1.06732e+03, 6.87820e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen35', 'img004.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.91900e+02, 2.31920e+02],\n",
      "       [1.00000e+00, 9.58880e+02, 4.24230e+02],\n",
      "       [2.00000e+00, 1.14687e+03, 3.76690e+02],\n",
      "       [3.00000e+00, 9.22150e+02, 6.46800e+02],\n",
      "       [4.00000e+00, 1.21602e+03, 6.83530e+02],\n",
      "       [5.00000e+00, 1.06908e+03, 8.17500e+02],\n",
      "       [6.00000e+00, 9.58880e+02, 9.90370e+02],\n",
      "       [7.00000e+00, 1.11014e+03, 9.99010e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen46', 'img136.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.98120e+02, 2.50280e+02],\n",
      "       [1.00000e+00, 9.61470e+02, 2.82200e+02],\n",
      "       [2.00000e+00, 8.87580e+02, 1.66930e+02],\n",
      "       [3.00000e+00, 8.08960e+02, 3.25940e+02],\n",
      "       [4.00000e+00, 1.02413e+03, 3.06430e+02],\n",
      "       [5.00000e+00, 8.97040e+02, 4.28200e+02],\n",
      "       [6.00000e+00, 8.68660e+02, 4.72530e+02],\n",
      "       [7.00000e+00, 9.59100e+02, 4.79040e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen48', 'img046.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.19503e+03, 3.29320e+02],\n",
      "       [1.00000e+00, 1.16132e+03, 3.64300e+02],\n",
      "       [2.00000e+00, 1.07293e+03, 2.30760e+02],\n",
      "       [3.00000e+00, 1.00489e+03, 3.89730e+02],\n",
      "       [4.00000e+00, 1.20520e+03, 3.79560e+02],\n",
      "       [5.00000e+00, 1.09201e+03, 4.94020e+02],\n",
      "       [6.00000e+00, 1.06403e+03, 5.33440e+02],\n",
      "       [7.00000e+00, 1.15624e+03, 5.39800e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap12', 'img074.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17990e+03, 4.04780e+02],\n",
      "       [1.00000e+00, 1.20884e+03, 4.38560e+02],\n",
      "       [2.00000e+00, 1.07858e+03, 4.94840e+02],\n",
      "       [3.00000e+00, 1.20241e+03, 6.50830e+02],\n",
      "       [4.00000e+00, 9.74050e+02, 6.07410e+02],\n",
      "       [5.00000e+00, 9.57970e+02, 7.73050e+02],\n",
      "       [6.00000e+00, 1.03516e+03, 8.67940e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand67', 'img150.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17668e+03, 1.33000e+02],\n",
      "       [1.00000e+00, 1.28764e+03, 1.44260e+02],\n",
      "       [2.00000e+00, 1.15577e+03, 2.69700e+02],\n",
      "       [3.00000e+00, 1.01586e+03, 5.09310e+02],\n",
      "       [4.00000e+00, 1.27156e+03, 4.98060e+02],\n",
      "       [5.00000e+00, 1.17990e+03, 6.62090e+02],\n",
      "       [6.00000e+00, 1.10110e+03, 6.83000e+02],\n",
      "       [7.00000e+00, 1.24905e+03, 6.68520e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen33', 'img145.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.38240e+03, 4.54480e+02],\n",
      "       [1.00000e+00, 1.19657e+03, 5.49560e+02],\n",
      "       [2.00000e+00, 1.14255e+03, 4.17750e+02],\n",
      "       [3.00000e+00, 8.85410e+02, 6.35990e+02],\n",
      "       [4.00000e+00, 1.27868e+03, 6.53280e+02],\n",
      "       [5.00000e+00, 1.08205e+03, 8.00210e+02],\n",
      "       [6.00000e+00, 9.93450e+02, 9.75240e+02],\n",
      "       [7.00000e+00, 1.15336e+03, 9.79560e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand72', 'img024.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08662e+03, 9.60200e+01],\n",
      "       [1.00000e+00, 9.90130e+02, 1.90900e+02],\n",
      "       [2.00000e+00, 1.05124e+03, 3.32420e+02],\n",
      "       [3.00000e+00, 9.69230e+02, 5.78470e+02],\n",
      "       [4.00000e+00, 1.28603e+03, 6.07410e+02],\n",
      "       [5.00000e+00, 1.16221e+03, 7.69840e+02],\n",
      "       [6.00000e+00, 1.09305e+03, 8.29340e+02],\n",
      "       [7.00000e+00, 1.23779e+03, 8.40600e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen40', 'img131.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.03160e+03, 1.66510e+02],\n",
      "       [1.00000e+00, 1.02008e+03, 2.24080e+02],\n",
      "       [2.00000e+00, 1.11548e+03, 3.98430e+02],\n",
      "       [3.00000e+00, 9.21400e+02, 6.78040e+02],\n",
      "       [4.00000e+00, 1.25364e+03, 6.50080e+02],\n",
      "       [5.00000e+00, 1.10068e+03, 8.19490e+02],\n",
      "       [6.00000e+00, 9.93770e+02, 9.95480e+02],\n",
      "       [7.00000e+00, 1.16318e+03, 1.00206e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest42', 'img120.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.08120e+02, 3.74230e+02],\n",
      "       [1.00000e+00, 7.56950e+02, 4.30510e+02],\n",
      "       [2.00000e+00, 1.04963e+03, 4.62680e+02],\n",
      "       [4.00000e+00, 1.11074e+03, 6.86210e+02],\n",
      "       [5.00000e+00, 1.43720e+03, 6.94250e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest65', 'img008.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.30212e+03, 2.98640e+02],\n",
      "       [1.00000e+00, 1.49188e+03, 3.29200e+02],\n",
      "       [2.00000e+00, 1.09949e+03, 4.41770e+02],\n",
      "       [3.00000e+00, 1.26191e+03, 7.66620e+02],\n",
      "       [5.00000e+00, 5.70400e+02, 6.21890e+02],\n",
      "       [6.00000e+00, 8.61480e+02, 8.42210e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk6', 'img145.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 130.26, 155.  ],\n",
      "       [  1.  , 154.93, 250.4 ],\n",
      "       [  2.  , 217.43, 321.12],\n",
      "       [  3.  ,  21.7 , 418.16],\n",
      "       [  4.  , 289.8 , 521.79],\n",
      "       [  5.  , 140.13, 506.98],\n",
      "       [  6.  , 192.76, 584.29],\n",
      "       [  7.  ,  87.5 , 592.51]], dtype=float32)}}, {'image': ('labeled-data', 'stand4', 'img108.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.03838e+03, 2.47180e+02],\n",
      "       [1.00000e+00, 9.67620e+02, 2.43970e+02],\n",
      "       [2.00000e+00, 1.10753e+03, 4.35340e+02],\n",
      "       [4.00000e+00, 9.35450e+02, 6.78170e+02],\n",
      "       [5.00000e+00, 1.22492e+03, 8.21300e+02],\n",
      "       [6.00000e+00, 1.16703e+03, 9.09750e+02],\n",
      "       [7.00000e+00, 1.06572e+03, 9.29050e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk89', 'img075.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 325.99, 258.62],\n",
      "       [  1.  , 270.07, 393.49],\n",
      "       [  2.  , 447.7 , 365.53],\n",
      "       [  3.  , 478.95, 557.97],\n",
      "       [  4.  , 626.98, 569.48],\n",
      "       [  5.  , 597.38, 714.22],\n",
      "       [  6.  , 574.35, 750.41],\n",
      "       [  7.  , 679.61, 798.11]], dtype=float32)}}, {'image': ('labeled-data', 'walk57', 'img046.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.43399e+03, 3.03470e+02],\n",
      "       [1.00000e+00, 1.30694e+03, 4.32120e+02],\n",
      "       [2.00000e+00, 1.46454e+03, 5.31830e+02],\n",
      "       [3.00000e+00, 1.49188e+03, 7.45720e+02],\n",
      "       [4.00000e+00, 1.73311e+03, 8.02000e+02],\n",
      "       [5.00000e+00, 1.68004e+03, 9.30650e+02],\n",
      "       [6.00000e+00, 1.65752e+03, 9.83720e+02],\n",
      "       [7.00000e+00, 1.70094e+03, 1.02393e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest44', 'img109.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.59870e+02, 3.51710e+02],\n",
      "       [1.00000e+00, 6.94230e+02, 4.08000e+02],\n",
      "       [2.00000e+00, 1.03516e+03, 4.33730e+02],\n",
      "       [4.00000e+00, 1.10431e+03, 6.78170e+02],\n",
      "       [5.00000e+00, 1.42595e+03, 6.73350e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand82', 'img060.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.48320e+02, 8.79700e+01],\n",
      "       [1.00000e+00, 7.77850e+02, 1.42650e+02],\n",
      "       [2.00000e+00, 1.08984e+03, 3.32420e+02],\n",
      "       [4.00000e+00, 1.06250e+03, 7.32850e+02],\n",
      "       [5.00000e+00, 1.31337e+03, 8.08430e+02],\n",
      "       [6.00000e+00, 1.08019e+03, 9.53170e+02],\n",
      "       [7.00000e+00, 1.10110e+03, 1.01910e+03]], dtype=float32)}}, {'image': ('labeled-data', 'flap12', 'img145.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1060.89,  390.31],\n",
      "       [   2.  , 1056.07,  488.41],\n",
      "       [   3.  , 1195.98,  522.18],\n",
      "       [   4.  ,  892.03,  535.05],\n",
      "       [   5.  , 1033.55,  766.62],\n",
      "       [   6.  , 1093.05,  859.9 ],\n",
      "       [   7.  ,  991.74,  858.29]], dtype=float32)}}, {'image': ('labeled-data', 'stand6', 'img129.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.69230e+02, 2.52010e+02],\n",
      "       [1.00000e+00, 8.75950e+02, 2.60050e+02],\n",
      "       [2.00000e+00, 1.04803e+03, 4.19260e+02],\n",
      "       [4.00000e+00, 8.95250e+02, 6.74960e+02],\n",
      "       [5.00000e+00, 1.17025e+03, 8.18080e+02],\n",
      "       [6.00000e+00, 1.11557e+03, 9.04920e+02],\n",
      "       [7.00000e+00, 1.01908e+03, 9.27440e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand19', 'img054.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.03194e+03, 1.87680e+02],\n",
      "       [1.00000e+00, 1.03355e+03, 2.40750e+02],\n",
      "       [2.00000e+00, 1.11396e+03, 4.09610e+02],\n",
      "       [3.00000e+00, 9.12940e+02, 6.87820e+02],\n",
      "       [4.00000e+00, 1.25548e+03, 6.60480e+02],\n",
      "       [5.00000e+00, 1.09466e+03, 8.35770e+02],\n",
      "       [6.00000e+00, 9.93350e+02, 9.96590e+02],\n",
      "       [7.00000e+00, 1.15738e+03, 9.98200e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest65', 'img148.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.32463e+03, 3.09900e+02],\n",
      "       [1.00000e+00, 1.49027e+03, 3.53320e+02],\n",
      "       [2.00000e+00, 1.10592e+03, 4.38560e+02],\n",
      "       [3.00000e+00, 1.25709e+03, 7.68230e+02],\n",
      "       [5.00000e+00, 5.72010e+02, 6.29930e+02],\n",
      "       [6.00000e+00, 8.55050e+02, 8.40600e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest39', 'img124.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.96560e+02, 4.16040e+02],\n",
      "       [1.00000e+00, 1.12843e+03, 4.81980e+02],\n",
      "       [3.00000e+00, 7.31220e+02, 6.33140e+02],\n",
      "       [4.00000e+00, 1.28764e+03, 5.62380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk6', 'img069.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 550.92, 261.77],\n",
      "       [  2.  , 564.05, 361.22],\n",
      "       [  3.  , 686.02, 526.33],\n",
      "       [  4.  , 442.09, 541.34],\n",
      "       [  5.  , 587.51, 623.9 ],\n",
      "       [  7.  , 540.6 , 714.91]], dtype=float32)}}, {'image': ('labeled-data', 'flap1', 'img048.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.22653e+03, 8.47600e+01],\n",
      "       [1.00000e+00, 1.32624e+03, 1.47480e+02],\n",
      "       [2.00000e+00, 1.18472e+03, 2.69700e+02],\n",
      "       [3.00000e+00, 1.12361e+03, 1.66770e+02],\n",
      "       [4.00000e+00, 1.33267e+03, 1.90900e+02],\n",
      "       [5.00000e+00, 1.22010e+03, 5.72030e+02],\n",
      "       [6.00000e+00, 1.14773e+03, 6.26710e+02],\n",
      "       [7.00000e+00, 1.30372e+03, 6.36360e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand79', 'img133.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.80480e+02, 2.89000e+02],\n",
      "       [1.00000e+00, 9.30630e+02, 3.34020e+02],\n",
      "       [2.00000e+00, 9.16160e+02, 4.69110e+02],\n",
      "       [3.00000e+00, 8.93640e+02, 6.09020e+02],\n",
      "       [4.00000e+00, 1.10110e+03, 6.60480e+02],\n",
      "       [5.00000e+00, 1.03355e+03, 7.61800e+02],\n",
      "       [6.00000e+00, 1.02873e+03, 8.08430e+02],\n",
      "       [7.00000e+00, 1.06250e+03, 8.19690e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen37', 'img075.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.86020e+02, 7.48350e+02],\n",
      "       [1.00000e+00, 9.30790e+02, 7.76440e+02],\n",
      "       [2.00000e+00, 8.18430e+02, 6.07900e+02],\n",
      "       [3.00000e+00, 9.99940e+02, 8.69360e+02],\n",
      "       [4.00000e+00, 1.24411e+03, 5.49560e+02],\n",
      "       [5.00000e+00, 1.14255e+03, 8.36950e+02],\n",
      "       [6.00000e+00, 9.82650e+02, 1.01197e+03],\n",
      "       [7.00000e+00, 1.13391e+03, 1.01846e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand11', 'img009.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.07840e+03, 3.55190e+02],\n",
      "       [1.00000e+00, 1.10230e+03, 3.73770e+02],\n",
      "       [2.00000e+00, 1.06672e+03, 4.25820e+02],\n",
      "       [3.00000e+00, 1.14426e+03, 5.33100e+02],\n",
      "       [5.00000e+00, 1.06672e+03, 6.72240e+02],\n",
      "       [6.00000e+00, 1.05132e+03, 6.46220e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand11', 'img102.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.04813e+03, 3.61560e+02],\n",
      "       [1.00000e+00, 1.01255e+03, 3.73770e+02],\n",
      "       [2.00000e+00, 1.06566e+03, 4.32190e+02],\n",
      "       [3.00000e+00, 1.12036e+03, 5.38940e+02],\n",
      "       [4.00000e+00, 1.01096e+03, 5.29910e+02],\n",
      "       [5.00000e+00, 1.04442e+03, 6.21790e+02],\n",
      "       [6.00000e+00, 1.09328e+03, 6.40910e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap7', 'img109.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.86920e+02, 5.15750e+02],\n",
      "       [1.00000e+00, 9.99780e+02, 6.13850e+02],\n",
      "       [2.00000e+00, 1.02390e+03, 6.97470e+02],\n",
      "       [3.00000e+00, 8.53440e+02, 8.43810e+02],\n",
      "       [4.00000e+00, 1.10431e+03, 7.98780e+02],\n",
      "       [5.00000e+00, 9.54750e+02, 9.80510e+02],\n",
      "       [6.00000e+00, 9.00070e+02, 1.01750e+03],\n",
      "       [7.00000e+00, 1.01908e+03, 9.94980e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest15', 'img081.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.05490e+02, 4.83580e+02],\n",
      "       [1.00000e+00, 5.51100e+02, 6.05800e+02],\n",
      "       [2.00000e+00, 9.32240e+02, 4.69110e+02],\n",
      "       [4.00000e+00, 1.06732e+03, 7.85920e+02],\n",
      "       [5.00000e+00, 1.45489e+03, 7.18380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk62', 'img064.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.07215e+03, 1.82860e+02],\n",
      "       [1.00000e+00, 1.15899e+03, 2.52010e+02],\n",
      "       [2.00000e+00, 9.98170e+02, 3.05080e+02],\n",
      "       [3.00000e+00, 1.11557e+03, 5.76860e+02],\n",
      "       [5.00000e+00, 8.88820e+02, 6.87820e+02],\n",
      "       [6.00000e+00, 9.99780e+02, 8.22910e+02],\n",
      "       [7.00000e+00, 9.16160e+02, 8.10040e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk82', 'img072.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.73471e+03, 1.08880e+02],\n",
      "       [1.00000e+00, 1.82960e+03, 1.65170e+02],\n",
      "       [2.00000e+00, 1.61249e+03, 2.21450e+02],\n",
      "       [3.00000e+00, 1.55460e+03, 4.20870e+02],\n",
      "       [5.00000e+00, 1.42112e+03, 5.39870e+02],\n",
      "       [6.00000e+00, 1.58033e+03, 6.36360e+02],\n",
      "       [7.00000e+00, 1.48866e+03, 6.42790e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest8', 'img090.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  824.49,  116.92],\n",
      "       [   2.  ,  957.97,  248.79],\n",
      "       [   4.  ,  718.35,  679.78],\n",
      "       [   5.  , 1430.77,  613.85]], dtype=float32)}}, {'image': ('labeled-data', 'stand19', 'img041.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.06250e+03, 1.89290e+02],\n",
      "       [1.00000e+00, 1.15899e+03, 2.19840e+02],\n",
      "       [2.00000e+00, 1.12683e+03, 4.08000e+02],\n",
      "       [3.00000e+00, 9.16160e+02, 6.95860e+02],\n",
      "       [4.00000e+00, 1.26030e+03, 6.46010e+02],\n",
      "       [5.00000e+00, 1.09788e+03, 8.43810e+02],\n",
      "       [6.00000e+00, 9.88520e+02, 9.94980e+02],\n",
      "       [7.00000e+00, 1.16221e+03, 1.00141e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk64', 'img068.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.38896e+03, 1.76420e+02],\n",
      "       [1.00000e+00, 1.45007e+03, 2.63260e+02],\n",
      "       [2.00000e+00, 1.35197e+03, 3.09900e+02],\n",
      "       [3.00000e+00, 1.21528e+03, 4.70720e+02],\n",
      "       [4.00000e+00, 1.32785e+03, 4.62680e+02],\n",
      "       [5.00000e+00, 1.17185e+03, 5.68820e+02],\n",
      "       [6.00000e+00, 1.15577e+03, 6.04200e+02],\n",
      "       [7.00000e+00, 1.19437e+03, 5.96160e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest15', 'img066.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.45690e+02, 4.88410e+02],\n",
      "       [1.00000e+00, 5.94520e+02, 6.15450e+02],\n",
      "       [2.00000e+00, 9.83700e+02, 4.72330e+02],\n",
      "       [4.00000e+00, 1.11396e+03, 7.69840e+02],\n",
      "       [5.00000e+00, 1.50153e+03, 7.08730e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk66', 'img116.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.05492e+03, 2.52420e+02],\n",
      "       [1.00000e+00, 9.98440e+02, 3.16430e+02],\n",
      "       [2.00000e+00, 1.03609e+03, 3.73860e+02],\n",
      "       [3.00000e+00, 1.06716e+03, 5.16940e+02],\n",
      "       [4.00000e+00, 1.19424e+03, 5.04710e+02],\n",
      "       [5.00000e+00, 1.15376e+03, 6.15790e+02],\n",
      "       [6.00000e+00, 1.12364e+03, 6.33670e+02],\n",
      "       [7.00000e+00, 1.20742e+03, 6.57210e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen30', 'img011.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.23114e+03, 1.28200e+02],\n",
      "       [1.00000e+00, 1.19657e+03, 2.98900e+02],\n",
      "       [2.00000e+00, 1.15552e+03, 3.42120e+02],\n",
      "       [3.00000e+00, 9.74010e+02, 6.40310e+02],\n",
      "       [4.00000e+00, 1.32838e+03, 6.23030e+02],\n",
      "       [5.00000e+00, 1.17064e+03, 7.95890e+02],\n",
      "       [6.00000e+00, 1.07124e+03, 9.90370e+02],\n",
      "       [7.00000e+00, 1.23547e+03, 9.90370e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand2', 'img119.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.23180e+02, 2.29490e+02],\n",
      "       [1.00000e+00, 6.47590e+02, 3.67800e+02],\n",
      "       [2.00000e+00, 9.11330e+02, 2.84170e+02],\n",
      "       [3.00000e+00, 7.39260e+02, 5.88110e+02],\n",
      "       [4.00000e+00, 1.06411e+03, 6.74960e+02],\n",
      "       [5.00000e+00, 1.16060e+03, 6.74960e+02],\n",
      "       [6.00000e+00, 8.82380e+02, 8.87230e+02],\n",
      "       [7.00000e+00, 9.74050e+02, 9.41910e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest47', 'img127.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.68200e+02, 3.90310e+02],\n",
      "       [1.00000e+00, 6.49200e+02, 5.33440e+02],\n",
      "       [2.00000e+00, 9.46710e+02, 3.83880e+02],\n",
      "       [4.00000e+00, 1.07858e+03, 6.57270e+02],\n",
      "       [5.00000e+00, 1.47580e+03, 6.09020e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap12', 'img007.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.24583e+03, 4.22470e+02],\n",
      "       [1.00000e+00, 1.31981e+03, 4.67500e+02],\n",
      "       [2.00000e+00, 1.07215e+03, 5.01270e+02],\n",
      "       [3.00000e+00, 1.11396e+03, 6.84610e+02],\n",
      "       [5.00000e+00, 8.74340e+02, 7.71450e+02],\n",
      "       [6.00000e+00, 9.75660e+02, 8.71150e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand79', 'img004.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.69230e+02, 2.95430e+02],\n",
      "       [1.00000e+00, 8.58260e+02, 3.29200e+02],\n",
      "       [2.00000e+00, 9.16160e+02, 4.75540e+02],\n",
      "       [3.00000e+00, 9.09720e+02, 6.31540e+02],\n",
      "       [4.00000e+00, 1.10110e+03, 6.50830e+02],\n",
      "       [5.00000e+00, 1.04481e+03, 7.69840e+02],\n",
      "       [6.00000e+00, 1.03838e+03, 8.05220e+02],\n",
      "       [7.00000e+00, 1.07376e+03, 8.19690e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen30', 'img098.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.20305e+03, 1.71410e+02],\n",
      "       [1.00000e+00, 1.14687e+03, 3.31320e+02],\n",
      "       [2.00000e+00, 1.09933e+03, 3.63730e+02],\n",
      "       [3.00000e+00, 9.02700e+02, 6.57600e+02],\n",
      "       [4.00000e+00, 1.26572e+03, 6.53280e+02],\n",
      "       [5.00000e+00, 1.09501e+03, 8.06700e+02],\n",
      "       [6.00000e+00, 9.99940e+02, 9.68760e+02],\n",
      "       [7.00000e+00, 1.15552e+03, 9.83880e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest65', 'img115.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.31659e+03, 3.13120e+02],\n",
      "       [1.00000e+00, 1.49349e+03, 3.56540e+02],\n",
      "       [2.00000e+00, 1.11718e+03, 4.38560e+02],\n",
      "       [3.00000e+00, 1.25387e+03, 7.63410e+02],\n",
      "       [5.00000e+00, 5.73620e+02, 6.20280e+02],\n",
      "       [6.00000e+00, 8.58260e+02, 8.37380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap14', 'img091.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 916.16,  97.62],\n",
      "       [  2.  , 827.71, 308.29],\n",
      "       [  3.  , 940.28, 636.36],\n",
      "       [  5.  , 625.08, 763.41],\n",
      "       [  6.  , 827.71, 935.48],\n",
      "       [  7.  , 769.81, 932.26]], dtype=float32)}}, {'image': ('labeled-data', 'walk86', 'img017.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.71090e+02, 3.79370e+02],\n",
      "       [1.00000e+00, 8.14070e+02, 4.16790e+02],\n",
      "       [2.00000e+00, 9.55740e+02, 4.47080e+02],\n",
      "       [4.00000e+00, 9.05840e+02, 5.82520e+02],\n",
      "       [5.00000e+00, 1.04752e+03, 6.38650e+02],\n",
      "       [6.00000e+00, 9.96730e+02, 6.96570e+02],\n",
      "       [7.00000e+00, 9.44160e+02, 7.04590e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap3', 'img132.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.55138e+03, 1.10490e+02],\n",
      "       [1.00000e+00, 1.72506e+03, 2.43970e+02],\n",
      "       [2.00000e+00, 1.26834e+03, 2.13410e+02],\n",
      "       [3.00000e+00, 1.14291e+03, 6.12240e+02],\n",
      "       [5.00000e+00, 9.17760e+02, 7.55360e+02],\n",
      "       [6.00000e+00, 1.03516e+03, 9.72470e+02],\n",
      "       [7.00000e+00, 1.21206e+03, 9.48340e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap2', 'img024.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.41469e+03, 6.54600e+01],\n",
      "       [1.00000e+00, 1.54173e+03, 1.61950e+02],\n",
      "       [2.00000e+00, 1.34715e+03, 2.45580e+02],\n",
      "       [3.00000e+00, 1.06732e+03, 3.24380e+02],\n",
      "       [4.00000e+00, 1.47419e+03, 4.08000e+02],\n",
      "       [5.00000e+00, 1.22653e+03, 5.67210e+02],\n",
      "       [6.00000e+00, 1.18633e+03, 6.29930e+02],\n",
      "       [7.00000e+00, 1.27799e+03, 6.05800e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest8', 'img007.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  800.37,  115.31],\n",
      "       [   2.  ,  956.36,  245.58],\n",
      "       [   4.  ,  731.22,  658.87],\n",
      "       [   5.  , 1432.38,  618.67]], dtype=float32)}}, {'image': ('labeled-data', 'flap3', 'img002.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.66556e+03, 1.37830e+02],\n",
      "       [1.00000e+00, 1.75401e+03, 3.48500e+02],\n",
      "       [2.00000e+00, 1.33106e+03, 1.57130e+02],\n",
      "       [3.00000e+00, 1.50153e+03, 5.89720e+02],\n",
      "       [4.00000e+00, 1.06893e+03, 3.46890e+02],\n",
      "       [5.00000e+00, 9.56360e+02, 7.84310e+02],\n",
      "       [6.00000e+00, 1.21688e+03, 9.67640e+02],\n",
      "       [7.00000e+00, 1.07858e+03, 9.53170e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest24', 'img110.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.11500e+02, 4.05820e+02],\n",
      "       [1.00000e+00, 6.88910e+02, 4.13860e+02],\n",
      "       [2.00000e+00, 9.94380e+02, 4.23910e+02],\n",
      "       [4.00000e+00, 1.16018e+03, 6.25890e+02],\n",
      "       [5.00000e+00, 1.40938e+03, 5.42480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand9', 'img122.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.19370e+02, 2.97040e+02],\n",
      "       [1.00000e+00, 8.14840e+02, 3.79050e+02],\n",
      "       [2.00000e+00, 1.05607e+03, 4.28910e+02],\n",
      "       [4.00000e+00, 9.09720e+02, 6.94250e+02],\n",
      "       [5.00000e+00, 1.18633e+03, 8.30950e+02],\n",
      "       [6.00000e+00, 1.12843e+03, 9.11360e+02],\n",
      "       [7.00000e+00, 1.02551e+03, 9.33870e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk6', 'img017.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 658.3 , 278.54],\n",
      "       [  1.  , 717.73, 295.09],\n",
      "       [  2.  , 605.77, 386.76],\n",
      "       [  3.  , 737.01, 573.01],\n",
      "       [  4.  , 489.96, 564.33],\n",
      "       [  5.  , 583.57, 651.18],\n",
      "       [  6.  , 662.71, 737.07],\n",
      "       [  7.  , 534.36, 734.17]], dtype=float32)}}, {'image': ('labeled-data', 'walk6', 'img099.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 248.83, 228.93],\n",
      "       [  1.  , 218.81, 274.9 ],\n",
      "       [  2.  , 306.06, 339.64],\n",
      "       [  3.  , 416.76, 468.17],\n",
      "       [  4.  , 187.85, 521.64],\n",
      "       [  5.  , 358.59, 596.7 ],\n",
      "       [  6.  , 428.96, 641.73],\n",
      "       [  7.  , 277.91, 674.56]], dtype=float32)}}, {'image': ('labeled-data', 'preen37', 'img047.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.51440e+02, 7.41870e+02],\n",
      "       [1.00000e+00, 8.98380e+02, 7.82930e+02],\n",
      "       [2.00000e+00, 8.05460e+02, 6.01420e+02],\n",
      "       [3.00000e+00, 9.76170e+02, 8.52070e+02],\n",
      "       [4.00000e+00, 1.20305e+03, 5.25790e+02],\n",
      "       [5.00000e+00, 1.10366e+03, 8.23980e+02],\n",
      "       [6.00000e+00, 1.10150e+03, 1.00549e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest39', 'img053.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.02230e+03, 3.90310e+02],\n",
      "       [1.00000e+00, 1.18633e+03, 4.25690e+02],\n",
      "       [3.00000e+00, 7.42470e+02, 6.25100e+02],\n",
      "       [4.00000e+00, 1.30533e+03, 5.60780e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap4', 'img061.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.26834e+03, 4.35000e+00],\n",
      "       [1.00000e+00, 1.46937e+03, 3.49000e+01],\n",
      "       [2.00000e+00, 1.23940e+03, 2.39140e+02],\n",
      "       [3.00000e+00, 8.01980e+02, 4.70720e+02],\n",
      "       [4.00000e+00, 1.36162e+03, 4.12820e+02],\n",
      "       [5.00000e+00, 1.02069e+03, 8.35770e+02],\n",
      "       [6.00000e+00, 9.40280e+02, 9.41910e+02],\n",
      "       [7.00000e+00, 1.11074e+03, 9.29050e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand79', 'img112.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.82090e+02, 2.87390e+02],\n",
      "       [1.00000e+00, 8.85600e+02, 3.21160e+02],\n",
      "       [2.00000e+00, 9.27410e+02, 4.65890e+02],\n",
      "       [3.00000e+00, 9.09720e+02, 6.17060e+02],\n",
      "       [4.00000e+00, 1.10592e+03, 6.62090e+02],\n",
      "       [5.00000e+00, 1.04159e+03, 7.68230e+02],\n",
      "       [6.00000e+00, 1.03516e+03, 7.97180e+02],\n",
      "       [7.00000e+00, 1.07376e+03, 8.11650e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen40', 'img006.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.60870e+02, 1.74740e+02],\n",
      "       [1.00000e+00, 1.08423e+03, 1.86250e+02],\n",
      "       [2.00000e+00, 1.06778e+03, 3.88560e+02],\n",
      "       [3.00000e+00, 9.31260e+02, 6.89550e+02],\n",
      "       [4.00000e+00, 1.27996e+03, 6.05670e+02],\n",
      "       [5.00000e+00, 1.12206e+03, 8.12910e+02],\n",
      "       [6.00000e+00, 1.00692e+03, 9.75740e+02],\n",
      "       [7.00000e+00, 1.17469e+03, 9.82320e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest15', 'img133.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 5.55930e+02, 5.01270e+02],\n",
      "       [1.00000e+00, 4.25660e+02, 6.07410e+02],\n",
      "       [2.00000e+00, 8.14840e+02, 4.78760e+02],\n",
      "       [4.00000e+00, 9.41890e+02, 7.98780e+02],\n",
      "       [5.00000e+00, 1.32946e+03, 7.16770e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap13', 'img048.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  978.87,  364.58],\n",
      "       [   2.  , 1030.34,  456.25],\n",
      "       [   3.  , 1178.29,  581.68],\n",
      "       [   4.  ,  864.69,  600.98],\n",
      "       [   5.  , 1083.41,  765.01],\n",
      "       [   6.  , 1046.42,  842.21],\n",
      "       [   7.  ,  998.17,  805.22]], dtype=float32)}}, {'image': ('labeled-data', 'flap2', 'img054.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.58033e+03, 9.28000e+01],\n",
      "       [1.00000e+00, 1.71381e+03, 1.60340e+02],\n",
      "       [2.00000e+00, 1.38413e+03, 1.84460e+02],\n",
      "       [3.00000e+00, 1.25226e+03, 3.56540e+02],\n",
      "       [4.00000e+00, 1.31820e+03, 1.82860e+02],\n",
      "       [5.00000e+00, 1.11396e+03, 5.30220e+02],\n",
      "       [6.00000e+00, 1.25709e+03, 6.47620e+02],\n",
      "       [7.00000e+00, 1.27478e+03, 6.10630e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk66', 'img010.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.15470e+03, 2.29830e+02],\n",
      "       [1.00000e+00, 1.08692e+03, 2.60890e+02],\n",
      "       [2.00000e+00, 1.14717e+03, 3.52200e+02],\n",
      "       [4.00000e+00, 1.30156e+03, 4.84940e+02],\n",
      "       [5.00000e+00, 1.26767e+03, 6.06370e+02],\n",
      "       [6.00000e+00, 1.27802e+03, 6.53440e+02],\n",
      "       [7.00000e+00, 1.32227e+03, 6.57210e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap1', 'img010.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.23297e+03, 8.47600e+01],\n",
      "       [1.00000e+00, 1.32624e+03, 1.44260e+02],\n",
      "       [2.00000e+00, 1.18472e+03, 2.87390e+02],\n",
      "       [3.00000e+00, 1.03194e+03, 4.61070e+02],\n",
      "       [4.00000e+00, 1.37448e+03, 4.65890e+02],\n",
      "       [5.00000e+00, 1.22653e+03, 5.83290e+02],\n",
      "       [6.00000e+00, 1.14452e+03, 6.26710e+02],\n",
      "       [7.00000e+00, 1.30533e+03, 6.36360e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest24', 'img018.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.41680e+02, 2.09880e+02],\n",
      "       [1.00000e+00, 4.95980e+02, 2.80220e+02],\n",
      "       [2.00000e+00, 8.22550e+02, 2.15910e+02],\n",
      "       [4.00000e+00, 1.00543e+03, 4.15870e+02],\n",
      "       [5.00000e+00, 1.25765e+03, 3.35480e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand75', 'img133.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00417e+03, 3.41740e+02],\n",
      "       [1.00000e+00, 1.05629e+03, 3.59650e+02],\n",
      "       [2.00000e+00, 9.38200e+02, 4.22360e+02],\n",
      "       [3.00000e+00, 1.01720e+03, 5.53480e+02],\n",
      "       [5.00000e+00, 8.81190e+02, 6.23520e+02],\n",
      "       [6.00000e+00, 9.79730e+02, 6.71570e+02],\n",
      "       [7.00000e+00, 9.08070e+02, 6.72380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk62', 'img089.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.09627e+03, 1.10490e+02],\n",
      "       [1.00000e+00, 1.17668e+03, 1.69990e+02],\n",
      "       [2.00000e+00, 9.99780e+02, 2.60050e+02],\n",
      "       [3.00000e+00, 1.15738e+03, 4.83580e+02],\n",
      "       [5.00000e+00, 9.51540e+02, 5.78470e+02],\n",
      "       [6.00000e+00, 1.05767e+03, 7.16770e+02],\n",
      "       [7.00000e+00, 9.46710e+02, 7.61800e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen31', 'img127.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.22034e+03, 2.57850e+02],\n",
      "       [1.00000e+00, 1.17064e+03, 3.96140e+02],\n",
      "       [2.00000e+00, 1.09285e+03, 3.76690e+02],\n",
      "       [3.00000e+00, 8.83250e+02, 6.53280e+02],\n",
      "       [4.00000e+00, 1.21602e+03, 6.48960e+02],\n",
      "       [5.00000e+00, 1.06692e+03, 7.87250e+02],\n",
      "       [6.00000e+00, 9.76170e+02, 9.79560e+02],\n",
      "       [7.00000e+00, 1.13823e+03, 9.88210e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand69', 'img148.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.37060e+02, 2.29490e+02],\n",
      "       [1.00000e+00, 1.01425e+03, 2.76130e+02],\n",
      "       [2.00000e+00, 9.64400e+02, 4.53030e+02],\n",
      "       [3.00000e+00, 7.02270e+02, 6.66920e+02],\n",
      "       [4.00000e+00, 9.88520e+02, 6.86210e+02],\n",
      "       [5.00000e+00, 8.32530e+02, 8.43810e+02],\n",
      "       [6.00000e+00, 7.32820e+02, 9.14570e+02],\n",
      "       [7.00000e+00, 9.11330e+02, 9.11360e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand2', 'img079.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.82680e+02, 1.41040e+02],\n",
      "       [1.00000e+00, 7.28000e+02, 2.42360e+02],\n",
      "       [2.00000e+00, 9.09720e+02, 2.66480e+02],\n",
      "       [3.00000e+00, 7.45690e+02, 5.81680e+02],\n",
      "       [4.00000e+00, 1.04803e+03, 6.57270e+02],\n",
      "       [5.00000e+00, 1.14934e+03, 6.73350e+02],\n",
      "       [6.00000e+00, 8.72740e+02, 8.75980e+02],\n",
      "       [7.00000e+00, 9.69230e+02, 9.33870e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk86', 'img085.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 561.91, 388.28],\n",
      "       [  1.  , 613.59, 350.85],\n",
      "       [  2.  , 653.69, 408.77],\n",
      "       [  4.  , 568.15, 539.75],\n",
      "       [  5.  , 708.93, 592.32],\n",
      "       [  7.  , 633.19, 627.07]], dtype=float32)}}, {'image': ('labeled-data', 'preen84', 'img143.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.09466e+03, 2.87390e+02],\n",
      "       [1.00000e+00, 1.13004e+03, 3.74230e+02],\n",
      "       [2.00000e+00, 1.11879e+03, 4.46600e+02],\n",
      "       [3.00000e+00, 8.72740e+02, 6.47620e+02],\n",
      "       [4.00000e+00, 1.16864e+03, 6.57270e+02],\n",
      "       [5.00000e+00, 9.72440e+02, 8.24520e+02],\n",
      "       [6.00000e+00, 9.09720e+02, 8.40600e+02],\n",
      "       [7.00000e+00, 1.09305e+03, 8.18080e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand75', 'img142.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00172e+03, 3.43370e+02],\n",
      "       [1.00000e+00, 1.05629e+03, 3.60470e+02],\n",
      "       [2.00000e+00, 9.39830e+02, 4.19920e+02],\n",
      "       [3.00000e+00, 1.01720e+03, 5.59180e+02],\n",
      "       [5.00000e+00, 8.79560e+02, 6.30850e+02],\n",
      "       [6.00000e+00, 9.77290e+02, 6.71570e+02],\n",
      "       [7.00000e+00, 9.11320e+02, 6.70750e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest47', 'img108.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.56950e+02, 3.98350e+02],\n",
      "       [1.00000e+00, 6.57240e+02, 5.30220e+02],\n",
      "       [2.00000e+00, 9.43490e+02, 3.80660e+02],\n",
      "       [4.00000e+00, 1.08501e+03, 6.52440e+02],\n",
      "       [5.00000e+00, 1.48866e+03, 6.05800e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen31', 'img070.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.18145e+03, 2.36240e+02],\n",
      "       [1.00000e+00, 1.15984e+03, 3.93980e+02],\n",
      "       [2.00000e+00, 1.07340e+03, 3.85340e+02],\n",
      "       [3.00000e+00, 8.81090e+02, 6.57600e+02],\n",
      "       [4.00000e+00, 1.22898e+03, 6.38150e+02],\n",
      "       [5.00000e+00, 1.07557e+03, 8.08860e+02],\n",
      "       [6.00000e+00, 9.71850e+02, 9.77400e+02],\n",
      "       [7.00000e+00, 1.13823e+03, 9.83880e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk80', 'img071.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 844.62, 555.23],\n",
      "       [  1.  , 879.1 , 597.1 ],\n",
      "       [  2.  , 782.23, 612.7 ],\n",
      "       [  3.  , 830.66, 735.02],\n",
      "       [  5.  , 675.5 , 769.5 ],\n",
      "       [  6.  , 750.21, 838.47],\n",
      "       [  7.  , 700.13, 825.33]], dtype=float32)}}, {'image': ('labeled-data', 'preen31', 'img072.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.18145e+03, 2.27600e+02],\n",
      "       [1.00000e+00, 1.16848e+03, 3.76690e+02],\n",
      "       [2.00000e+00, 1.08421e+03, 3.74530e+02],\n",
      "       [3.00000e+00, 8.76770e+02, 6.74890e+02],\n",
      "       [4.00000e+00, 1.23331e+03, 6.66240e+02],\n",
      "       [5.00000e+00, 1.06908e+03, 7.87250e+02],\n",
      "       [6.00000e+00, 9.65360e+02, 9.66600e+02],\n",
      "       [7.00000e+00, 1.14471e+03, 9.75240e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk66', 'img022.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.14246e+03, 2.33590e+02],\n",
      "       [1.00000e+00, 1.05492e+03, 2.78780e+02],\n",
      "       [2.00000e+00, 1.12929e+03, 3.64440e+02],\n",
      "       [3.00000e+00, 1.16035e+03, 5.09410e+02],\n",
      "       [4.00000e+00, 1.27708e+03, 4.91530e+02],\n",
      "       [5.00000e+00, 1.23754e+03, 6.05430e+02],\n",
      "       [6.00000e+00, 1.24507e+03, 6.46850e+02],\n",
      "       [7.00000e+00, 1.29685e+03, 6.55330e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen50', 'img132.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.94960e+02, 3.35630e+02],\n",
      "       [1.00000e+00, 9.51540e+02, 2.00550e+02],\n",
      "       [2.00000e+00, 1.17185e+03, 4.67500e+02],\n",
      "       [3.00000e+00, 1.20402e+03, 8.14870e+02],\n",
      "       [4.00000e+00, 9.80480e+02, 4.67500e+02],\n",
      "       [5.00000e+00, 8.45400e+02, 8.47030e+02],\n",
      "       [6.00000e+00, 1.04642e+03, 9.91770e+02],\n",
      "       [7.00000e+00, 1.00943e+03, 9.70860e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap4', 'img148.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.39539e+03, 5.25900e+01],\n",
      "       [1.00000e+00, 1.59641e+03, 8.31500e+01],\n",
      "       [2.00000e+00, 1.11557e+03, 2.37530e+02],\n",
      "       [3.00000e+00, 8.74340e+02, 2.80950e+02],\n",
      "       [4.00000e+00, 9.32240e+02, 2.10200e+02],\n",
      "       [5.00000e+00, 7.18350e+02, 8.27730e+02],\n",
      "       [6.00000e+00, 8.87210e+02, 9.69250e+02],\n",
      "       [7.00000e+00, 1.00461e+03, 9.56390e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand2', 'img099.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.71420e+02, 1.50690e+02],\n",
      "       [1.00000e+00, 6.89400e+02, 2.40750e+02],\n",
      "       [2.00000e+00, 9.19370e+02, 2.72910e+02],\n",
      "       [3.00000e+00, 7.37650e+02, 5.89720e+02],\n",
      "       [4.00000e+00, 1.05446e+03, 6.71740e+02],\n",
      "       [5.00000e+00, 1.15899e+03, 6.76560e+02],\n",
      "       [6.00000e+00, 8.66300e+02, 8.74370e+02],\n",
      "       [7.00000e+00, 9.64400e+02, 9.38700e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap2', 'img135.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.72024e+03, 6.22400e+01],\n",
      "       [1.00000e+00, 1.81030e+03, 8.95800e+01],\n",
      "       [2.00000e+00, 1.53530e+03, 1.34610e+02],\n",
      "       [3.00000e+00, 1.61088e+03, 4.08000e+02],\n",
      "       [5.00000e+00, 1.25226e+03, 5.12530e+02],\n",
      "       [6.00000e+00, 1.37609e+03, 6.47620e+02],\n",
      "       [7.00000e+00, 1.39378e+03, 6.31540e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand69', 'img021.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.90130e+02, 2.39140e+02],\n",
      "       [1.00000e+00, 1.13165e+03, 2.80950e+02],\n",
      "       [2.00000e+00, 9.38670e+02, 4.48200e+02],\n",
      "       [3.00000e+00, 6.97450e+02, 6.60480e+02],\n",
      "       [4.00000e+00, 9.85310e+02, 6.71740e+02],\n",
      "       [5.00000e+00, 8.35750e+02, 8.43810e+02],\n",
      "       [6.00000e+00, 7.28000e+02, 9.06530e+02],\n",
      "       [7.00000e+00, 9.03290e+02, 9.00100e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand79', 'img027.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.78870e+02, 2.89000e+02],\n",
      "       [1.00000e+00, 8.96860e+02, 3.35630e+02],\n",
      "       [2.00000e+00, 9.22590e+02, 4.69110e+02],\n",
      "       [3.00000e+00, 9.14550e+02, 6.12240e+02],\n",
      "       [4.00000e+00, 1.10431e+03, 6.37970e+02],\n",
      "       [5.00000e+00, 1.04159e+03, 7.65010e+02],\n",
      "       [6.00000e+00, 1.04159e+03, 8.00390e+02],\n",
      "       [7.00000e+00, 1.07376e+03, 8.22910e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest24', 'img091.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.97430e+02, 3.82710e+02],\n",
      "       [1.00000e+00, 6.61780e+02, 4.21900e+02],\n",
      "       [2.00000e+00, 9.73280e+02, 3.96780e+02],\n",
      "       [4.00000e+00, 1.16018e+03, 5.98760e+02],\n",
      "       [5.00000e+00, 1.40637e+03, 5.16360e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest8', 'img147.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.34140e+02, 1.12100e+02],\n",
      "       [1.00000e+00, 6.57240e+02, 1.21750e+02],\n",
      "       [2.00000e+00, 9.43490e+02, 2.42360e+02],\n",
      "       [4.00000e+00, 7.13530e+02, 6.50830e+02],\n",
      "       [5.00000e+00, 1.43077e+03, 6.17060e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest39', 'img134.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.03034e+03, 4.12820e+02],\n",
      "       [1.00000e+00, 1.17025e+03, 4.69110e+02],\n",
      "       [3.00000e+00, 7.37650e+02, 6.39580e+02],\n",
      "       [4.00000e+00, 1.28925e+03, 5.72030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen46', 'img126.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.89250e+02, 2.27220e+02],\n",
      "       [1.00000e+00, 9.53780e+02, 2.71560e+02],\n",
      "       [2.00000e+00, 8.73390e+02, 1.65150e+02],\n",
      "       [3.00000e+00, 7.95360e+02, 3.26530e+02],\n",
      "       [4.00000e+00, 1.01408e+03, 3.04070e+02],\n",
      "       [5.00000e+00, 8.88760e+02, 4.34700e+02],\n",
      "       [6.00000e+00, 8.56840e+02, 4.70170e+02],\n",
      "       [7.00000e+00, 9.50240e+02, 4.75490e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen46', 'img047.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.01408e+03, 2.09490e+02],\n",
      "       [1.00000e+00, 9.76250e+02, 2.48500e+02],\n",
      "       [2.00000e+00, 8.91720e+02, 1.29100e+02],\n",
      "       [3.00000e+00, 8.18420e+02, 2.91060e+02],\n",
      "       [4.00000e+00, 1.02058e+03, 2.81600e+02],\n",
      "       [5.00000e+00, 9.02950e+02, 3.85050e+02],\n",
      "       [6.00000e+00, 8.79890e+02, 4.35290e+02],\n",
      "       [7.00000e+00, 9.70930e+02, 4.40020e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand67', 'img071.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17346e+03, 1.24960e+02],\n",
      "       [1.00000e+00, 1.26674e+03, 1.42650e+02],\n",
      "       [2.00000e+00, 1.14452e+03, 2.69700e+02],\n",
      "       [3.00000e+00, 1.03034e+03, 5.01270e+02],\n",
      "       [4.00000e+00, 1.27639e+03, 4.94840e+02],\n",
      "       [5.00000e+00, 1.19276e+03, 6.57270e+02],\n",
      "       [6.00000e+00, 1.10753e+03, 6.70130e+02],\n",
      "       [7.00000e+00, 1.25387e+03, 6.57270e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest15', 'img027.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.82970e+02, 4.83580e+02],\n",
      "       [1.00000e+00, 5.33410e+02, 6.09020e+02],\n",
      "       [2.00000e+00, 9.12940e+02, 4.67500e+02],\n",
      "       [4.00000e+00, 1.06250e+03, 7.31240e+02],\n",
      "       [5.00000e+00, 1.44203e+03, 7.11940e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand72', 'img098.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08662e+03, 2.03760e+02],\n",
      "       [1.00000e+00, 1.04481e+03, 3.27590e+02],\n",
      "       [2.00000e+00, 1.05607e+03, 4.06390e+02],\n",
      "       [3.00000e+00, 9.45100e+02, 6.12240e+02],\n",
      "       [4.00000e+00, 1.28443e+03, 6.50830e+02],\n",
      "       [5.00000e+00, 1.16703e+03, 7.87530e+02],\n",
      "       [6.00000e+00, 1.09145e+03, 8.18080e+02],\n",
      "       [7.00000e+00, 1.23618e+03, 8.35770e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen40', 'img010.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00363e+03, 1.63220e+02],\n",
      "       [1.00000e+00, 9.55940e+02, 2.38880e+02],\n",
      "       [2.00000e+00, 1.07436e+03, 3.95140e+02],\n",
      "       [3.00000e+00, 9.32910e+02, 6.63240e+02],\n",
      "       [4.00000e+00, 1.28160e+03, 6.17180e+02],\n",
      "       [5.00000e+00, 1.11219e+03, 8.11270e+02],\n",
      "       [6.00000e+00, 1.00363e+03, 9.74100e+02],\n",
      "       [7.00000e+00, 1.17469e+03, 9.88900e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk80', 'img017.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 514.59, 515.83],\n",
      "       [  1.  , 549.07, 573.29],\n",
      "       [  2.  , 402.12, 547.84],\n",
      "       [  3.  , 397.2 , 673.45],\n",
      "       [  5.  , 293.75, 715.32],\n",
      "       [  6.  , 388.99, 786.75],\n",
      "       [  7.  , 327.41, 778.54]], dtype=float32)}}, {'image': ('labeled-data', 'walk89', 'img105.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 186.18, 202.7 ],\n",
      "       [  1.  ,  85.85, 288.23],\n",
      "       [  2.  , 182.89, 349.08],\n",
      "       [  3.  , 194.41, 566.19],\n",
      "       [  4.  , 363.82, 569.48],\n",
      "       [  5.  , 337.5 , 729.03],\n",
      "       [  6.  , 311.18, 803.04],\n",
      "       [  7.  , 459.21, 819.49]], dtype=float32)}}, {'image': ('labeled-data', 'rest65', 'img064.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.30694e+03, 3.05080e+02],\n",
      "       [1.00000e+00, 1.49027e+03, 3.42070e+02],\n",
      "       [2.00000e+00, 1.10914e+03, 4.44990e+02],\n",
      "       [3.00000e+00, 1.25387e+03, 7.60190e+02],\n",
      "       [5.00000e+00, 5.67180e+02, 6.20280e+02],\n",
      "       [6.00000e+00, 8.58260e+02, 8.37380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand6', 'img049.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  957.97,  252.01],\n",
      "       [   2.  ,  993.35,  406.39],\n",
      "       [   4.  ,  874.34,  686.21],\n",
      "       [   5.  , 1149.34,  810.04],\n",
      "       [   6.  , 1096.27,  900.1 ],\n",
      "       [   7.  ,  996.56,  924.22]], dtype=float32)}}, {'image': ('labeled-data', 'rest24', 'img048.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.0000e+00, 6.8891e+02, 2.6314e+02],\n",
      "       [1.0000e+00, 5.5325e+02, 3.1941e+02],\n",
      "       [2.0000e+00, 8.6777e+02, 2.7017e+02],\n",
      "       [4.0000e+00, 1.0597e+03, 4.8119e+02],\n",
      "       [5.0000e+00, 1.3099e+03, 3.8874e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen30', 'img079.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.23763e+03, 1.69250e+02],\n",
      "       [1.00000e+00, 1.17280e+03, 3.26990e+02],\n",
      "       [2.00000e+00, 1.12526e+03, 3.63730e+02],\n",
      "       [3.00000e+00, 9.26470e+02, 6.64080e+02],\n",
      "       [4.00000e+00, 1.28949e+03, 6.55440e+02],\n",
      "       [5.00000e+00, 1.14039e+03, 8.04540e+02],\n",
      "       [6.00000e+00, 1.02803e+03, 9.75240e+02],\n",
      "       [7.00000e+00, 1.19873e+03, 9.83880e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen37', 'img023.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.68730e+02, 7.15940e+02],\n",
      "       [1.00000e+00, 8.94060e+02, 7.46190e+02],\n",
      "       [2.00000e+00, 8.07620e+02, 5.99260e+02],\n",
      "       [3.00000e+00, 9.80490e+02, 8.43430e+02],\n",
      "       [4.00000e+00, 1.22034e+03, 5.21470e+02],\n",
      "       [5.00000e+00, 1.10798e+03, 8.41270e+02],\n",
      "       [6.00000e+00, 9.58880e+02, 9.96850e+02],\n",
      "       [7.00000e+00, 1.10798e+03, 1.00549e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk86', 'img042.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 741.9 , 469.36],\n",
      "       [  1.  , 722.29, 522.82],\n",
      "       [  2.  , 855.06, 439.06],\n",
      "       [  4.  , 845.25, 587.86],\n",
      "       [  5.  , 996.73, 614.59],\n",
      "       [  6.  , 988.71, 690.33],\n",
      "       [  7.  , 899.61, 688.55]], dtype=float32)}}, {'image': ('labeled-data', 'walk64', 'img032.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.26995e+03, 1.28180e+02],\n",
      "       [1.00000e+00, 1.36805e+03, 1.89290e+02],\n",
      "       [2.00000e+00, 1.24744e+03, 2.84170e+02],\n",
      "       [3.00000e+00, 1.10431e+03, 4.38560e+02],\n",
      "       [4.00000e+00, 1.23779e+03, 4.28910e+02],\n",
      "       [5.00000e+00, 1.06732e+03, 5.51130e+02],\n",
      "       [6.00000e+00, 1.05607e+03, 5.99370e+02],\n",
      "       [7.00000e+00, 1.09949e+03, 5.88110e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk80', 'img007.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 416.08, 485.45],\n",
      "       [  1.  , 450.56, 542.92],\n",
      "       [  2.  , 318.38, 534.71],\n",
      "       [  3.  , 333.16, 661.14],\n",
      "       [  5.  , 196.88, 703.83],\n",
      "       [  6.  , 265.84, 785.92]], dtype=float32)}}, {'image': ('labeled-data', 'walk68', 'img066.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 4.12800e+02, 2.64870e+02],\n",
      "       [1.00000e+00, 4.11190e+02, 3.38850e+02],\n",
      "       [2.00000e+00, 3.75810e+02, 4.83580e+02],\n",
      "       [3.00000e+00, 2.72890e+02, 7.37670e+02],\n",
      "       [4.00000e+00, 6.07390e+02, 7.48930e+02],\n",
      "       [5.00000e+00, 4.96420e+02, 9.91770e+02],\n",
      "       [6.00000e+00, 4.33710e+02, 1.01589e+03],\n",
      "       [7.00000e+00, 6.10600e+02, 1.03197e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand82', 'img031.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.53140e+02, 1.21750e+02],\n",
      "       [1.00000e+00, 7.89110e+02, 1.84460e+02],\n",
      "       [2.00000e+00, 1.06893e+03, 3.34020e+02],\n",
      "       [4.00000e+00, 1.07858e+03, 7.44110e+02],\n",
      "       [5.00000e+00, 1.32141e+03, 8.19690e+02],\n",
      "       [6.00000e+00, 1.08501e+03, 9.51560e+02],\n",
      "       [7.00000e+00, 1.09305e+03, 1.02714e+03]], dtype=float32)}}, {'image': ('labeled-data', 'preen31', 'img022.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17712e+03, 2.44880e+02],\n",
      "       [1.00000e+00, 1.12094e+03, 3.87500e+02],\n",
      "       [2.00000e+00, 1.07773e+03, 3.65890e+02],\n",
      "       [3.00000e+00, 8.83250e+02, 6.57600e+02],\n",
      "       [4.00000e+00, 1.23979e+03, 6.51120e+02],\n",
      "       [5.00000e+00, 1.07124e+03, 8.04540e+02],\n",
      "       [6.00000e+00, 9.69690e+02, 9.81720e+02],\n",
      "       [7.00000e+00, 1.14039e+03, 9.94690e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen30', 'img044.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.26572e+03, 1.62770e+02],\n",
      "       [1.00000e+00, 1.22466e+03, 3.24830e+02],\n",
      "       [2.00000e+00, 1.16416e+03, 3.57250e+02],\n",
      "       [3.00000e+00, 9.71850e+02, 6.46800e+02],\n",
      "       [4.00000e+00, 1.32838e+03, 6.48960e+02],\n",
      "       [5.00000e+00, 1.16848e+03, 7.98050e+02],\n",
      "       [6.00000e+00, 1.06908e+03, 9.77400e+02],\n",
      "       [7.00000e+00, 1.23979e+03, 9.79560e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest65', 'img122.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.31659e+03, 3.16330e+02],\n",
      "       [1.00000e+00, 1.49670e+03, 3.61360e+02],\n",
      "       [2.00000e+00, 1.11235e+03, 4.44990e+02],\n",
      "       [3.00000e+00, 1.25548e+03, 7.56970e+02],\n",
      "       [5.00000e+00, 5.60750e+02, 6.23490e+02],\n",
      "       [6.00000e+00, 8.56650e+02, 8.37380e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen35', 'img144.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.50840e+02, 3.29150e+02],\n",
      "       [1.00000e+00, 9.54560e+02, 4.84730e+02],\n",
      "       [2.00000e+00, 1.13607e+03, 3.63730e+02],\n",
      "       [3.00000e+00, 9.61040e+02, 6.53280e+02],\n",
      "       [4.00000e+00, 1.23114e+03, 6.70560e+02],\n",
      "       [5.00000e+00, 1.07989e+03, 7.91570e+02],\n",
      "       [6.00000e+00, 9.61040e+02, 9.75240e+02],\n",
      "       [7.00000e+00, 1.11446e+03, 9.73080e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand72', 'img130.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08984e+03, 1.97330e+02],\n",
      "       [1.00000e+00, 1.03838e+03, 3.08290e+02],\n",
      "       [2.00000e+00, 1.06732e+03, 3.99960e+02],\n",
      "       [3.00000e+00, 9.53140e+02, 5.96160e+02],\n",
      "       [4.00000e+00, 1.29247e+03, 6.52440e+02],\n",
      "       [5.00000e+00, 1.17185e+03, 7.90740e+02],\n",
      "       [6.00000e+00, 1.09466e+03, 8.19690e+02],\n",
      "       [7.00000e+00, 1.23940e+03, 8.38990e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand19', 'img005.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.96560e+02, 2.03760e+02],\n",
      "       [1.00000e+00, 9.46710e+02, 2.90600e+02],\n",
      "       [2.00000e+00, 1.09949e+03, 4.20870e+02],\n",
      "       [3.00000e+00, 9.19370e+02, 6.87820e+02],\n",
      "       [4.00000e+00, 1.26674e+03, 6.49230e+02],\n",
      "       [5.00000e+00, 1.10431e+03, 8.34160e+02],\n",
      "       [6.00000e+00, 9.86920e+02, 9.88550e+02],\n",
      "       [7.00000e+00, 1.16221e+03, 9.91770e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap3', 'img105.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.61249e+03, 9.92300e+01],\n",
      "       [1.00000e+00, 1.77974e+03, 2.76130e+02],\n",
      "       [2.00000e+00, 1.30855e+03, 1.90900e+02],\n",
      "       [3.00000e+00, 1.09466e+03, 3.53320e+02],\n",
      "       [5.00000e+00, 9.08120e+02, 7.39280e+02],\n",
      "       [6.00000e+00, 1.10914e+03, 9.62820e+02],\n",
      "       [7.00000e+00, 1.18794e+03, 9.37090e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand6', 'img014.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.93640e+02, 2.71310e+02],\n",
      "       [1.00000e+00, 7.92330e+02, 3.14730e+02],\n",
      "       [2.00000e+00, 1.04803e+03, 4.16040e+02],\n",
      "       [4.00000e+00, 8.72740e+02, 6.95860e+02],\n",
      "       [5.00000e+00, 1.16221e+03, 8.16470e+02],\n",
      "       [6.00000e+00, 1.09466e+03, 9.08140e+02],\n",
      "       [7.00000e+00, 9.94960e+02, 9.30650e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest47', 'img118.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.61770e+02, 4.04780e+02],\n",
      "       [1.00000e+00, 6.52420e+02, 5.41480e+02],\n",
      "       [2.00000e+00, 9.43490e+02, 3.85490e+02],\n",
      "       [4.00000e+00, 1.08180e+03, 6.60480e+02],\n",
      "       [5.00000e+00, 1.47902e+03, 6.15450e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand79', 'img093.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.83700e+02, 2.90600e+02],\n",
      "       [1.00000e+00, 9.32240e+02, 3.29200e+02],\n",
      "       [2.00000e+00, 9.32240e+02, 4.69110e+02],\n",
      "       [3.00000e+00, 9.03290e+02, 6.12240e+02],\n",
      "       [4.00000e+00, 1.10110e+03, 6.52440e+02],\n",
      "       [5.00000e+00, 1.03998e+03, 7.65010e+02],\n",
      "       [6.00000e+00, 1.03677e+03, 8.05220e+02],\n",
      "       [7.00000e+00, 1.07054e+03, 8.21300e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk66', 'img087.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08787e+03, 2.22300e+02],\n",
      "       [1.00000e+00, 9.97490e+02, 2.45830e+02],\n",
      "       [2.00000e+00, 1.05868e+03, 3.49380e+02],\n",
      "       [4.00000e+00, 1.21307e+03, 4.98120e+02],\n",
      "       [5.00000e+00, 1.16976e+03, 6.02610e+02],\n",
      "       [6.00000e+00, 1.17165e+03, 6.42150e+02],\n",
      "       [7.00000e+00, 1.21118e+03, 6.51560e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap2', 'img034.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.52887e+03, 8.63700e+01],\n",
      "       [1.00000e+00, 1.64626e+03, 1.73210e+02],\n",
      "       [2.00000e+00, 1.31659e+03, 1.95720e+02],\n",
      "       [3.00000e+00, 1.13969e+03, 3.09900e+02],\n",
      "       [5.00000e+00, 1.14291e+03, 4.88410e+02],\n",
      "       [6.00000e+00, 1.24583e+03, 6.29930e+02],\n",
      "       [7.00000e+00, 1.30051e+03, 6.20280e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen33', 'img052.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.42130e+03, 4.58800e+02],\n",
      "       [1.00000e+00, 1.23114e+03, 5.49560e+02],\n",
      "       [2.00000e+00, 1.18577e+03, 4.26390e+02],\n",
      "       [3.00000e+00, 9.09180e+02, 6.14380e+02],\n",
      "       [4.00000e+00, 1.28733e+03, 6.57600e+02],\n",
      "       [5.00000e+00, 1.10366e+03, 8.19660e+02],\n",
      "       [6.00000e+00, 9.99940e+02, 9.96850e+02],\n",
      "       [7.00000e+00, 1.17064e+03, 1.00117e+03]], dtype=float32)}}, {'image': ('labeled-data', 'stand82', 'img080.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.40280e+02, 1.02450e+02],\n",
      "       [1.00000e+00, 7.66600e+02, 1.57130e+02],\n",
      "       [2.00000e+00, 1.07376e+03, 3.35630e+02],\n",
      "       [4.00000e+00, 1.06089e+03, 7.55360e+02],\n",
      "       [5.00000e+00, 1.30533e+03, 8.05220e+02],\n",
      "       [6.00000e+00, 1.07858e+03, 9.45130e+02],\n",
      "       [7.00000e+00, 1.08341e+03, 1.03679e+03]], dtype=float32)}}, {'image': ('labeled-data', 'walk80', 'img100.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 723.94, 515.01],\n",
      "       [  2.  , 704.24, 572.47],\n",
      "       [  3.  , 787.15, 689.05],\n",
      "       [  4.  , 633.63, 688.23],\n",
      "       [  5.  , 674.68, 745.7 ],\n",
      "       [  6.  , 744.46, 801.52],\n",
      "       [  7.  , 666.47, 815.48]], dtype=float32)}}, {'image': ('labeled-data', 'rest8', 'img059.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  ,  818.06,  126.57],\n",
      "       [   2.  ,  962.79,  253.62],\n",
      "       [   4.  ,  718.35,  663.7 ],\n",
      "       [   5.  , 1432.38,  615.45]], dtype=float32)}}, {'image': ('labeled-data', 'walk66', 'img048.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.03797e+03, 2.19470e+02],\n",
      "       [1.00000e+00, 9.56070e+02, 2.50540e+02],\n",
      "       [2.00000e+00, 1.02479e+03, 3.46560e+02],\n",
      "       [4.00000e+00, 1.17635e+03, 4.84940e+02],\n",
      "       [5.00000e+00, 1.14341e+03, 6.01670e+02],\n",
      "       [6.00000e+00, 1.16223e+03, 6.45910e+02],\n",
      "       [7.00000e+00, 1.20365e+03, 6.50620e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap14', 'img112.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 890.43, 120.14],\n",
      "       [  1.  , 858.26,  99.23],\n",
      "       [  2.  , 843.79, 319.55],\n",
      "       [  3.  , 969.23, 658.87],\n",
      "       [  5.  , 633.12, 781.1 ],\n",
      "       [  6.  , 838.96, 940.3 ],\n",
      "       [  7.  , 777.85, 940.3 ]], dtype=float32)}}, {'image': ('labeled-data', 'stand82', 'img143.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.41890e+02, 1.07270e+02],\n",
      "       [1.00000e+00, 7.69810e+02, 1.57130e+02],\n",
      "       [2.00000e+00, 1.08662e+03, 3.46890e+02],\n",
      "       [4.00000e+00, 1.07536e+03, 7.39280e+02],\n",
      "       [5.00000e+00, 1.31659e+03, 8.03610e+02],\n",
      "       [6.00000e+00, 1.07858e+03, 9.48340e+02],\n",
      "       [7.00000e+00, 1.10431e+03, 1.00946e+03]], dtype=float32)}}, {'image': ('labeled-data', 'flap3', 'img058.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.71059e+03, 1.24960e+02],\n",
      "       [1.00000e+00, 1.86176e+03, 3.06690e+02],\n",
      "       [2.00000e+00, 1.29247e+03, 2.21450e+02],\n",
      "       [3.00000e+00, 1.19276e+03, 2.97040e+02],\n",
      "       [5.00000e+00, 9.03290e+02, 7.42500e+02],\n",
      "       [6.00000e+00, 1.19437e+03, 9.66030e+02],\n",
      "       [7.00000e+00, 1.13487e+03, 9.54780e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk62', 'img004.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.05446e+03, 2.58440e+02],\n",
      "       [1.00000e+00, 1.13969e+03, 3.27590e+02],\n",
      "       [2.00000e+00, 8.55050e+02, 3.75840e+02],\n",
      "       [3.00000e+00, 7.97150e+02, 6.57270e+02],\n",
      "       [4.00000e+00, 9.35450e+02, 7.05510e+02],\n",
      "       [5.00000e+00, 6.33120e+02, 7.45720e+02],\n",
      "       [6.00000e+00, 7.23180e+02, 9.04920e+02],\n",
      "       [7.00000e+00, 8.37360e+02, 8.74370e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk64', 'img084.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.35197e+03, 2.06980e+02],\n",
      "       [1.00000e+00, 1.37931e+03, 3.05080e+02],\n",
      "       [2.00000e+00, 1.35519e+03, 3.61360e+02],\n",
      "       [3.00000e+00, 1.22010e+03, 5.04490e+02],\n",
      "       [4.00000e+00, 1.39700e+03, 4.64290e+02],\n",
      "       [5.00000e+00, 1.18954e+03, 5.59170e+02],\n",
      "       [6.00000e+00, 1.17025e+03, 5.94550e+02],\n",
      "       [7.00000e+00, 1.20563e+03, 5.75250e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen37', 'img127.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.87570e+02, 7.28910e+02],\n",
      "       [1.00000e+00, 1.02587e+03, 7.78610e+02],\n",
      "       [2.00000e+00, 9.15670e+02, 5.90610e+02],\n",
      "       [3.00000e+00, 1.10150e+03, 8.60720e+02],\n",
      "       [4.00000e+00, 1.34135e+03, 5.34430e+02],\n",
      "       [5.00000e+00, 1.23114e+03, 8.34790e+02],\n",
      "       [6.00000e+00, 1.07557e+03, 9.92530e+02],\n",
      "       [7.00000e+00, 1.22682e+03, 9.94690e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand19', 'img000.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.99780e+02, 2.00550e+02],\n",
      "       [1.00000e+00, 9.35450e+02, 2.80950e+02],\n",
      "       [2.00000e+00, 1.10110e+03, 4.14430e+02],\n",
      "       [3.00000e+00, 9.09720e+02, 6.84610e+02],\n",
      "       [4.00000e+00, 1.27317e+03, 6.34750e+02],\n",
      "       [5.00000e+00, 1.10270e+03, 8.27730e+02],\n",
      "       [6.00000e+00, 9.93350e+02, 9.80510e+02],\n",
      "       [7.00000e+00, 1.16381e+03, 9.88550e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap13', 'img126.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.48320e+02, 3.37240e+02],\n",
      "       [1.00000e+00, 8.88820e+02, 3.51710e+02],\n",
      "       [2.00000e+00, 9.96560e+02, 4.48200e+02],\n",
      "       [3.00000e+00, 1.13487e+03, 4.78760e+02],\n",
      "       [4.00000e+00, 8.37360e+02, 5.14140e+02],\n",
      "       [5.00000e+00, 1.02873e+03, 7.19980e+02],\n",
      "       [6.00000e+00, 9.85310e+02, 8.05220e+02],\n",
      "       [7.00000e+00, 9.32240e+02, 7.66620e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest15', 'img078.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 7.32820e+02, 4.83580e+02],\n",
      "       [1.00000e+00, 5.73620e+02, 6.05800e+02],\n",
      "       [2.00000e+00, 9.66010e+02, 4.70720e+02],\n",
      "       [4.00000e+00, 1.09466e+03, 7.89140e+02],\n",
      "       [5.00000e+00, 1.47741e+03, 7.13550e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand79', 'img001.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.43490e+02, 2.95430e+02],\n",
      "       [1.00000e+00, 8.45400e+02, 3.27590e+02],\n",
      "       [2.00000e+00, 8.96860e+02, 4.69110e+02],\n",
      "       [3.00000e+00, 8.83990e+02, 6.21890e+02],\n",
      "       [4.00000e+00, 1.08662e+03, 6.46010e+02],\n",
      "       [5.00000e+00, 1.03034e+03, 7.74660e+02],\n",
      "       [6.00000e+00, 1.05124e+03, 8.21300e+02],\n",
      "       [7.00000e+00, 1.01104e+03, 8.05220e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen50', 'img028.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.59580e+02, 2.72910e+02],\n",
      "       [1.00000e+00, 1.00300e+03, 1.28180e+02],\n",
      "       [2.00000e+00, 1.18150e+03, 4.54640e+02],\n",
      "       [3.00000e+00, 1.22814e+03, 7.90740e+02],\n",
      "       [4.00000e+00, 9.83700e+02, 4.56250e+02],\n",
      "       [5.00000e+00, 8.47000e+02, 8.47030e+02],\n",
      "       [6.00000e+00, 1.06089e+03, 9.86940e+02],\n",
      "       [7.00000e+00, 1.01908e+03, 9.77290e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen48', 'img021.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.16387e+03, 3.20420e+02],\n",
      "       [1.00000e+00, 1.13334e+03, 3.47130e+02],\n",
      "       [2.00000e+00, 1.05195e+03, 2.11680e+02],\n",
      "       [3.00000e+00, 9.79460e+02, 3.70020e+02],\n",
      "       [4.00000e+00, 1.18040e+03, 3.57940e+02],\n",
      "       [5.00000e+00, 1.06658e+03, 4.76210e+02],\n",
      "       [6.00000e+00, 1.03987e+03, 5.11820e+02],\n",
      "       [7.00000e+00, 1.13016e+03, 5.18180e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest44', 'img046.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.45400e+02, 3.62970e+02],\n",
      "       [1.00000e+00, 7.10310e+02, 4.48200e+02],\n",
      "       [2.00000e+00, 1.01908e+03, 4.30510e+02],\n",
      "       [4.00000e+00, 1.08501e+03, 6.58870e+02],\n",
      "       [5.00000e+00, 1.40986e+03, 6.68520e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest24', 'img037.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.51730e+02, 2.44040e+02],\n",
      "       [1.00000e+00, 5.18080e+02, 3.04330e+02],\n",
      "       [2.00000e+00, 8.32600e+02, 2.48060e+02],\n",
      "       [4.00000e+00, 1.02553e+03, 4.45010e+02],\n",
      "       [5.00000e+00, 1.27272e+03, 3.66630e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap14', 'img027.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00300e+03, 1.39440e+02],\n",
      "       [1.00000e+00, 1.14130e+03, 1.61950e+02],\n",
      "       [2.00000e+00, 7.89110e+02, 3.06690e+02],\n",
      "       [3.00000e+00, 1.03838e+03, 5.68820e+02],\n",
      "       [5.00000e+00, 6.33120e+02, 7.76270e+02],\n",
      "       [6.00000e+00, 8.35750e+02, 9.43520e+02],\n",
      "       [7.00000e+00, 7.74640e+02, 9.32260e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap14', 'img034.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00300e+03, 1.21750e+02],\n",
      "       [1.00000e+00, 1.15256e+03, 1.53910e+02],\n",
      "       [2.00000e+00, 7.93940e+02, 2.93820e+02],\n",
      "       [3.00000e+00, 1.04320e+03, 5.59170e+02],\n",
      "       [5.00000e+00, 6.42770e+02, 7.42500e+02],\n",
      "       [6.00000e+00, 8.37360e+02, 9.33870e+02],\n",
      "       [7.00000e+00, 7.69810e+02, 9.22610e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen37', 'img106.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.48680e+02, 7.33230e+02],\n",
      "       [1.00000e+00, 9.80490e+02, 7.65640e+02],\n",
      "       [2.00000e+00, 8.76770e+02, 5.90610e+02],\n",
      "       [3.00000e+00, 1.05396e+03, 8.52070e+02],\n",
      "       [4.00000e+00, 1.29813e+03, 5.32270e+02],\n",
      "       [5.00000e+00, 1.20089e+03, 8.19660e+02],\n",
      "       [6.00000e+00, 1.03451e+03, 9.92530e+02],\n",
      "       [7.00000e+00, 1.19441e+03, 1.00333e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest15', 'img092.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 6.76540e+02, 4.81980e+02],\n",
      "       [1.00000e+00, 5.38240e+02, 5.97760e+02],\n",
      "       [2.00000e+00, 9.19370e+02, 4.65890e+02],\n",
      "       [4.00000e+00, 1.05285e+03, 7.76270e+02],\n",
      "       [5.00000e+00, 1.44524e+03, 7.03900e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk82', 'img003.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 866.3 , 205.37],\n",
      "       [  1.  , 946.71, 264.87],\n",
      "       [  2.  , 742.47, 317.94],\n",
      "       [  3.  , 763.38, 539.87],\n",
      "       [  5.  , 565.58, 631.54],\n",
      "       [  6.  , 658.85, 737.67],\n",
      "       [  7.  , 695.84, 719.98]], dtype=float32)}}, {'image': ('labeled-data', 'walk66', 'img123.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.60780e+02, 2.75010e+02],\n",
      "       [1.00000e+00, 9.12770e+02, 3.49380e+02],\n",
      "       [2.00000e+00, 9.57010e+02, 3.86090e+02],\n",
      "       [4.00000e+00, 1.13588e+03, 4.79290e+02],\n",
      "       [5.00000e+00, 1.10199e+03, 6.05430e+02],\n",
      "       [6.00000e+00, 1.06621e+03, 6.42150e+02],\n",
      "       [7.00000e+00, 1.18483e+03, 6.58150e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand11', 'img127.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.02689e+03, 3.65280e+02],\n",
      "       [1.00000e+00, 9.80690e+02, 3.90240e+02],\n",
      "       [2.00000e+00, 1.06619e+03, 4.29010e+02],\n",
      "       [3.00000e+00, 1.12833e+03, 5.38940e+02],\n",
      "       [5.00000e+00, 1.04654e+03, 6.20730e+02],\n",
      "       [6.00000e+00, 1.10071e+03, 6.38780e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk62', 'img045.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.02069e+03, 1.86070e+02],\n",
      "       [1.00000e+00, 1.08501e+03, 2.48790e+02],\n",
      "       [2.00000e+00, 9.32240e+02, 3.22770e+02],\n",
      "       [3.00000e+00, 1.06893e+03, 6.02590e+02],\n",
      "       [5.00000e+00, 8.45400e+02, 7.03900e+02],\n",
      "       [6.00000e+00, 9.80480e+02, 8.38990e+02],\n",
      "       [7.00000e+00, 8.18060e+02, 8.47030e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand9', 'img131.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.11330e+02, 3.09900e+02],\n",
      "       [1.00000e+00, 8.21270e+02, 4.03180e+02],\n",
      "       [2.00000e+00, 1.07697e+03, 4.25690e+02],\n",
      "       [4.00000e+00, 9.27410e+02, 6.94250e+02],\n",
      "       [5.00000e+00, 1.19437e+03, 8.29340e+02],\n",
      "       [6.00000e+00, 1.14130e+03, 9.08140e+02],\n",
      "       [7.00000e+00, 1.04159e+03, 9.29050e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk68', 'img101.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 2.48770e+02, 3.01860e+02],\n",
      "       [1.00000e+00, 1.63530e+02, 3.99960e+02],\n",
      "       [2.00000e+00, 2.42330e+02, 5.14140e+02],\n",
      "       [3.00000e+00, 1.57100e+02, 7.74660e+02],\n",
      "       [4.00000e+00, 5.26980e+02, 7.98780e+02],\n",
      "       [5.00000e+00, 3.95110e+02, 1.03679e+03],\n",
      "       [6.00000e+00, 3.29170e+02, 1.04001e+03],\n",
      "       [7.00000e+00, 5.04460e+02, 1.05448e+03]], dtype=float32)}}, {'image': ('labeled-data', 'preen46', 'img101.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.89840e+02, 2.22490e+02],\n",
      "       [1.00000e+00, 9.60880e+02, 2.70960e+02],\n",
      "       [2.00000e+00, 8.84030e+02, 1.54510e+02],\n",
      "       [3.00000e+00, 8.08370e+02, 3.16480e+02],\n",
      "       [4.00000e+00, 1.00994e+03, 3.03480e+02],\n",
      "       [5.00000e+00, 8.89350e+02, 4.14600e+02],\n",
      "       [6.00000e+00, 8.70440e+02, 4.66030e+02],\n",
      "       [7.00000e+00, 9.58510e+02, 4.70760e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand11', 'img095.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1060.35,  358.9 ],\n",
      "       [   2.  , 1060.88,  435.38],\n",
      "       [   3.  , 1117.17,  536.29],\n",
      "       [   5.  , 1040.17,  626.04],\n",
      "       [   6.  , 1093.28,  639.32]], dtype=float32)}}, {'image': ('labeled-data', 'walk80', 'img090.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 764.99, 540.46],\n",
      "       [  2.  , 755.96, 604.49],\n",
      "       [  3.  , 811.78, 730.1 ],\n",
      "       [  5.  , 673.86, 772.79],\n",
      "       [  6.  , 749.39, 838.47],\n",
      "       [  7.  , 688.64, 822.05]], dtype=float32)}}, {'image': ('labeled-data', 'preen84', 'img138.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.09305e+03, 2.92210e+02],\n",
      "       [1.00000e+00, 1.12200e+03, 3.87090e+02],\n",
      "       [2.00000e+00, 1.10914e+03, 4.53030e+02],\n",
      "       [3.00000e+00, 8.80780e+02, 6.15450e+02],\n",
      "       [4.00000e+00, 1.16864e+03, 6.60480e+02],\n",
      "       [5.00000e+00, 9.72440e+02, 8.14870e+02],\n",
      "       [6.00000e+00, 9.06510e+02, 8.40600e+02],\n",
      "       [7.00000e+00, 1.07376e+03, 8.13260e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen31', 'img147.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.22034e+03, 2.49200e+02],\n",
      "       [1.00000e+00, 1.17929e+03, 4.09110e+02],\n",
      "       [2.00000e+00, 1.08637e+03, 3.83180e+02],\n",
      "       [3.00000e+00, 8.76770e+02, 6.53280e+02],\n",
      "       [4.00000e+00, 1.21386e+03, 6.46800e+02],\n",
      "       [5.00000e+00, 1.06908e+03, 8.00210e+02],\n",
      "       [6.00000e+00, 9.69690e+02, 9.79560e+02],\n",
      "       [7.00000e+00, 1.13607e+03, 9.86040e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand67', 'img122.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.16542e+03, 1.29790e+02],\n",
      "       [1.00000e+00, 1.23457e+03, 1.39440e+02],\n",
      "       [2.00000e+00, 1.14773e+03, 2.72910e+02],\n",
      "       [3.00000e+00, 1.01586e+03, 5.12530e+02],\n",
      "       [4.00000e+00, 1.26995e+03, 5.01270e+02],\n",
      "       [5.00000e+00, 1.17346e+03, 6.63700e+02],\n",
      "       [6.00000e+00, 1.09627e+03, 6.78170e+02],\n",
      "       [7.00000e+00, 1.25870e+03, 6.62090e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk66', 'img121.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.91850e+02, 2.72190e+02],\n",
      "       [1.00000e+00, 9.36300e+02, 3.43730e+02],\n",
      "       [2.00000e+00, 9.84310e+02, 3.85150e+02],\n",
      "       [3.00000e+00, 1.02197e+03, 5.14120e+02],\n",
      "       [4.00000e+00, 1.15941e+03, 5.00000e+02],\n",
      "       [5.00000e+00, 1.10669e+03, 6.03550e+02],\n",
      "       [6.00000e+00, 1.06339e+03, 6.43090e+02],\n",
      "       [7.00000e+00, 1.19047e+03, 6.52500e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk64', 'img013.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.08984e+03, 1.44260e+02],\n",
      "       [1.00000e+00, 1.16864e+03, 2.29490e+02],\n",
      "       [2.00000e+00, 1.06250e+03, 2.85780e+02],\n",
      "       [3.00000e+00, 8.82380e+02, 4.33730e+02],\n",
      "       [4.00000e+00, 1.08984e+03, 3.67800e+02],\n",
      "       [5.00000e+00, 9.24200e+02, 5.39870e+02],\n",
      "       [6.00000e+00, 8.75950e+02, 5.76860e+02],\n",
      "       [7.00000e+00, 9.33850e+02, 5.75250e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand67', 'img086.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17507e+03, 1.24960e+02],\n",
      "       [1.00000e+00, 1.26995e+03, 1.39440e+02],\n",
      "       [2.00000e+00, 1.15738e+03, 2.63260e+02],\n",
      "       [3.00000e+00, 1.01265e+03, 5.01270e+02],\n",
      "       [4.00000e+00, 1.27317e+03, 4.93230e+02],\n",
      "       [5.00000e+00, 1.17668e+03, 6.58870e+02],\n",
      "       [6.00000e+00, 1.09949e+03, 6.71740e+02],\n",
      "       [7.00000e+00, 1.26352e+03, 6.50830e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest19', 'img093.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.24101e+03, 2.92210e+02],\n",
      "       [1.00000e+00, 1.43720e+03, 3.22770e+02],\n",
      "       [2.00000e+00, 1.07536e+03, 3.95130e+02],\n",
      "       [3.00000e+00, 1.08180e+03, 7.15160e+02],\n",
      "       [5.00000e+00, 6.95840e+02, 5.46300e+02]], dtype=float32)}}, {'image': ('labeled-data', 'rest44', 'img039.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.47000e+02, 3.74230e+02],\n",
      "       [1.00000e+00, 7.11920e+02, 4.59460e+02],\n",
      "       [2.00000e+00, 1.01747e+03, 4.38560e+02],\n",
      "       [4.00000e+00, 1.08501e+03, 6.74960e+02],\n",
      "       [5.00000e+00, 1.40986e+03, 6.74960e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk80', 'img051.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[  0.  , 790.44, 538.81],\n",
      "       [  1.  , 822.45, 593.82],\n",
      "       [  2.  , 684.53, 566.73],\n",
      "       [  3.  , 700.95, 714.5 ],\n",
      "       [  5.  , 567.96, 738.31],\n",
      "       [  6.  , 672.22, 827.79],\n",
      "       [  7.  , 600.79, 797.42]], dtype=float32)}}, {'image': ('labeled-data', 'preen35', 'img072.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 8.61640e+02, 3.31320e+02],\n",
      "       [1.00000e+00, 9.67520e+02, 4.35040e+02],\n",
      "       [2.00000e+00, 1.14039e+03, 3.57250e+02],\n",
      "       [3.00000e+00, 9.61040e+02, 6.51120e+02],\n",
      "       [4.00000e+00, 1.23331e+03, 6.66240e+02],\n",
      "       [5.00000e+00, 1.09069e+03, 7.87250e+02],\n",
      "       [6.00000e+00, 9.65360e+02, 9.75240e+02],\n",
      "       [7.00000e+00, 1.12094e+03, 9.88210e+02]], dtype=float32)}}, {'image': ('labeled-data', 'walk57', 'img081.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 9.30630e+02, 3.51710e+02],\n",
      "       [1.00000e+00, 8.06800e+02, 5.04490e+02],\n",
      "       [2.00000e+00, 1.16221e+03, 4.64290e+02],\n",
      "       [4.00000e+00, 1.19115e+03, 8.11650e+02],\n",
      "       [5.00000e+00, 1.46454e+03, 8.82410e+02],\n",
      "       [6.00000e+00, 1.19276e+03, 1.00624e+03],\n",
      "       [7.00000e+00, 1.35679e+03, 1.05127e+03]], dtype=float32)}}, {'image': ('labeled-data', 'rest65', 'img109.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.32141e+03, 3.11510e+02],\n",
      "       [1.00000e+00, 1.49188e+03, 3.54930e+02],\n",
      "       [2.00000e+00, 1.11235e+03, 4.44990e+02],\n",
      "       [3.00000e+00, 1.25709e+03, 7.55360e+02],\n",
      "       [5.00000e+00, 5.68790e+02, 6.18670e+02],\n",
      "       [6.00000e+00, 8.53440e+02, 8.35770e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap7', 'img039.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.00461e+03, 4.19260e+02],\n",
      "       [1.00000e+00, 9.40280e+02, 5.17360e+02],\n",
      "       [2.00000e+00, 1.04963e+03, 6.36360e+02],\n",
      "       [3.00000e+00, 8.63090e+02, 8.26120e+02],\n",
      "       [4.00000e+00, 1.10270e+03, 7.77880e+02],\n",
      "       [5.00000e+00, 9.62790e+02, 9.45130e+02],\n",
      "       [6.00000e+00, 9.01680e+02, 1.00785e+03],\n",
      "       [7.00000e+00, 1.02390e+03, 9.77290e+02]], dtype=float32)}}, {'image': ('labeled-data', 'stand67', 'img053.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.17990e+03, 1.21750e+02],\n",
      "       [1.00000e+00, 1.26352e+03, 1.34610e+02],\n",
      "       [2.00000e+00, 1.16060e+03, 2.63260e+02],\n",
      "       [3.00000e+00, 1.01908e+03, 4.96450e+02],\n",
      "       [4.00000e+00, 1.27799e+03, 4.78760e+02],\n",
      "       [5.00000e+00, 1.18472e+03, 6.55660e+02],\n",
      "       [6.00000e+00, 1.11396e+03, 6.65310e+02],\n",
      "       [7.00000e+00, 1.26191e+03, 6.50830e+02]], dtype=float32)}}, {'image': ('labeled-data', 'flap13', 'img071.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[0.00000e+00, 1.04642e+03, 3.25980e+02],\n",
      "       [1.00000e+00, 1.10592e+03, 3.40460e+02],\n",
      "       [2.00000e+00, 1.00139e+03, 4.44990e+02],\n",
      "       [3.00000e+00, 1.11396e+03, 4.81980e+02],\n",
      "       [4.00000e+00, 8.77560e+02, 4.81980e+02],\n",
      "       [5.00000e+00, 1.01586e+03, 7.40890e+02],\n",
      "       [6.00000e+00, 1.03838e+03, 8.22910e+02],\n",
      "       [7.00000e+00, 9.49930e+02, 8.16470e+02]], dtype=float32)}}, {'image': ('labeled-data', 'preen50', 'img077.png'), 'size': array([   3, 1080, 1920]), 'joints': {0: array([[   0.  , 1054.46,  367.8 ],\n",
      "       [   2.  , 1155.77,  422.47],\n",
      "       [   3.  , 1162.21,  811.65],\n",
      "       [   5.  ,  850.22,  838.99],\n",
      "       [   6.  , 1052.85,  988.55],\n",
      "       [   7.  , 1014.25,  972.47]], dtype=float32)}}], array([ 90, 254, 283, 445, 461,  15, 316, 489, 159, 153, 241, 250, 390,\n",
      "       289, 171, 329, 468, 355, 154,  37, 205, 366, 240, 108,  45, 438,\n",
      "        21, 367,  96, 233, 428, 118, 124, 191, 374, 492, 311, 451, 353,\n",
      "       238, 322,  46, 403, 221,  76,   1, 213, 325, 418, 102, 363, 170,\n",
      "       343, 144, 132,  12, 327, 173, 224, 342,  78, 276, 387, 425, 301,\n",
      "       196,  10, 469, 271,  75, 142,  65, 340, 484, 175, 362, 264, 100,\n",
      "       491, 295, 300, 235, 475, 219, 330, 326, 421, 157, 348,  54, 220,\n",
      "       402, 379, 200, 179, 372,  56, 440,  60, 208, 107, 336,  71, 474,\n",
      "         6, 412, 113, 236, 299, 155, 272,   7, 137,   8, 463, 432, 375,\n",
      "       284, 210, 188, 430,  49, 134, 365, 413, 239,  59, 406, 391, 411,\n",
      "       485, 229, 297,  55, 293, 490, 458, 457, 453, 186, 194,  52,  74,\n",
      "        26, 488,   4, 318, 331, 245,   5, 141, 383, 135, 493, 122,  22,\n",
      "        68,  20, 382,  14, 278, 225,  64, 381, 231,  81, 401, 302, 499,\n",
      "       471, 455, 160, 478, 364, 496, 206, 319,  51, 306, 452, 332, 164,\n",
      "       106, 481,  63, 344, 427, 439, 320,  89, 312, 450,  93, 298, 459,\n",
      "       308, 395,  92,  18, 198, 145, 158, 150, 479, 167, 255, 230, 422,\n",
      "        66, 309, 253, 140, 101, 433,   2, 408,  17, 146, 249, 263,  30,\n",
      "       114, 247, 103, 405, 310, 176, 246, 116, 168, 415, 120, 261, 112,\n",
      "       360, 435, 282, 136, 190, 347, 181, 126, 281, 252, 407, 393, 354,\n",
      "       232, 133,  33, 476, 162,  34,  44,  97,  85,  61, 199, 268, 218,\n",
      "        73,  35, 303,  29, 361, 392, 443, 217,  27, 399, 380, 156, 429,\n",
      "       345, 138, 212, 104, 350, 346, 351, 215, 385, 189, 214, 204, 234,\n",
      "       259,  67,  24, 216, 223, 129, 111, 166, 417, 394,  40, 274, 357,\n",
      "        79, 313, 315,  13, 287, 409, 494, 228, 161,  83, 497, 473, 110,\n",
      "       149, 152,  16, 339, 109, 352, 139, 237, 260, 419, 317, 495, 400,\n",
      "       248, 386,  19, 328, 296, 269, 226, 414,   3, 378, 125, 280, 286,\n",
      "        77, 184, 424, 477, 275, 294, 436, 182, 447,  80, 307, 258,  11,\n",
      "       371,  86, 266,  36, 480,  58,  41, 270,  50, 209, 397, 410, 416,\n",
      "       123, 222,  62, 377, 130, 187,  23,  43, 441,   0, 201, 368, 426,\n",
      "        98, 349, 304, 487, 178, 369, 256,  94, 465,  95, 442, 169,  69,\n",
      "       305,  48, 341, 373, 207, 279, 227, 148, 143, 334, 180, 356, 131,\n",
      "       462, 498, 262, 324, 203,  84, 121, 482, 434, 460, 454, 384,  91,\n",
      "        82, 267, 119, 358, 291,  57, 321, 257, 376, 446,  42, 105, 388,\n",
      "       467, 273, 444,  38, 389,  53, 420, 437, 128, 290,  28, 183, 370,\n",
      "       163, 151, 244, 202,  31,  32, 127, 185, 470, 449, 288, 423, 398,\n",
      "       456, 147, 285, 466, 177,  99, 338, 448, 431, 335, 197, 243, 464,\n",
      "       115, 404, 265,  72, 333,  25, 165]), array([337, 483, 174, 486,  39, 193, 314, 396,  88, 472,  70,  87, 292,\n",
      "       242, 277, 211,   9, 359, 195, 251, 323, 192, 117,  47, 172]), 0.95]\n"
     ]
    }
   ],
   "source": [
    "#file_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/training-datasets/iteration-0/UnaugmentedDataSet_DLC_simple_datasetSep2/DLC_simple_dataset_model195shuffle1.pickle'\n",
    "file_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/DLC_model/DLC_simple_dataset-model1-2024-09-02/training-datasets/iteration-0/UnaugmentedDataSet_DLC_simple_datasetSep2/Documentation_data-DLC_simple_dataset_95shuffle1.pickle'\n",
    "# Load the pickle file\n",
    "\n",
    "data = load_pickle(file_path)\n",
    "\n",
    "print(data)\n",
    "\n",
    "# # Save it as a JSON file for easier editing\n",
    "# json_file_path = file_path.replace('.pickle', '.json')\n",
    "# with open(json_file_path, 'w') as json_file:\n",
    "#     json.dump(data, json_file, indent=4)\n",
    "\n",
    "# print(f\"Pickle data saved as JSON to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0. Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all_cols(df):\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ids into list \n",
    "def load_file_to_list(file_path):\n",
    "    \"\"\"\n",
    "    loads a text file to a list with each entry on a new line becoming a new entry in the list.\n",
    "\n",
    "    :param file_path: Path to the file where the list should be saved.\n",
    "    :return list of data from file\n",
    "    \"\"\"\n",
    "    # Open the file for writing\n",
    "    lst = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Write each item on a new line\n",
    "        for line in file:\n",
    "            lst.append(line.strip())\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(df, path):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame to a .json file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be converted to JSON.\n",
    "    path (str): The path (including file name) where the .json file will be saved.\n",
    "    \"\"\"\n",
    "    df.to_json(path, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_to_df(path):\n",
    "    \"\"\"\n",
    "    Converts a .json file to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The path to the .json file that will be read.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame created from the JSON file.\n",
    "    \"\"\"\n",
    "    print(path)\n",
    "    df = pd.read_json(path, orient='records')\n",
    "    print(f\"JSON file has been successfully converted to DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dtypes_df_full_annotation_abs(df):\n",
    "    df['vid_id'] = df['vid_id'].astype(str)\n",
    "    df['img_id'] = df['img_id'].astype(str)\n",
    "    df['bbox_id'] = df['bbox_id'].astype(str)\n",
    "    df['bbox_c_x'] = df['bbox_c_x'].astype('float32')\n",
    "    df['bbox_c_y'] = df['bbox_c_y'].astype('float32')\n",
    "    df['bbox_w'] = df['bbox_w'].astype('float32')\n",
    "    df['bbox_h'] = df['bbox_h'].astype('float32')\n",
    "    df['Head_x'] = df['Head_x'].astype('float32')\n",
    "    df['Head_y'] = df['Head_y'].astype('float32')\n",
    "    df['Beak_x'] = df['Beak_x'].astype('float32')\n",
    "    df['Beak_y'] = df['Beak_y'].astype('float32')\n",
    "    df['Body_top_x'] = df['Body_top_x'].astype('float32')\n",
    "    df['Body_top_y'] = df['Body_top_y'].astype('float32')\n",
    "    df['RFlipper_mid_x'] = df['RFlipper_mid_x'].astype('float32')\n",
    "    df['RFlipper_mid_y'] = df['RFlipper_mid_y'].astype('float32')\n",
    "    df['LFlipper_mid_x'] = df['LFlipper_mid_x'].astype('float32')\n",
    "    df['LFlipper_mid_y'] = df['LFlipper_mid_y'].astype('float32')\n",
    "    df['Body_bottom_x'] = df['Body_bottom_x'].astype('float32')\n",
    "    df['Body_bottom_y'] = df['Body_bottom_y'].astype('float32')\n",
    "    df['RFoot_x'] = df['RFoot_x'].astype('float32')\n",
    "    df['RFoot_y'] = df['RFoot_y'].astype('float32')\n",
    "    df['LFoot_x'] = df['LFoot_x'].astype('float32')\n",
    "    df['LFoot_y'] = df['LFoot_y'].astype('float32')\n",
    "    df['kp_outside_best_bbox'] = df['kp_outside_best_bbox'].astype('float32')\n",
    "    df['kp_missing'] = df['kp_missing'].astype('float32')\n",
    "    df['kp_primary_missing'] = df['kp_primary_missing'].astype(bool)\n",
    "    df['img_width'] = df['img_width'].astype('float32')\n",
    "    df['img_height'] = df['img_height'].astype('float32')\n",
    "    df['bbox_max_h_w'] = df['bbox_max_h_w'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnorm_keypoints(img_size, keypoints, kp_to_null=None):\n",
    "    \"\"\"\n",
    "    De-normalizes keypoints based on image size and returns the de-normalized keypoints along with \n",
    "    the positions of any missing or nullified keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    - img_size: Tuple of the image dimensions (height, width).\n",
    "    - keypoints: List of normalized keypoints (with values between -1 and 1).\n",
    "    - kp_to_null: Optional. List of indices where the keypoints should be nulled (set to NaN).\n",
    "\n",
    "    Returns:\n",
    "    - new_keypoints: List of de-normalized keypoints where each coordinate is scaled back to the \n",
    "                     image's pixel dimensions.\n",
    "    - missing_kp: List of indices where the keypoints were either originally set to -10 (indicating \n",
    "                  missing keypoints) or explicitly nullified by the kp_to_null list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract image width and height\n",
    "    readjust_x = img_size[0]  # width of the image\n",
    "    readjust_y = img_size[1]  # height of the image\n",
    "\n",
    "    new_keypoints = []  # List to store the de-normalized keypoints\n",
    "    missing_kp = []     # List to store the indices of missing or nullified keypoints\n",
    "\n",
    "    # Iterate through each keypoint\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        # Null keypoints if they are -10 or if they are specified in kp_to_null\n",
    "        if keypoint == -10 or (kp_to_null and i in kp_to_null):\n",
    "            keypoint = np.nan  # Set keypoint to NaN\n",
    "            missing_kp.append(i)  # Record the index of the missing or nullified keypoint\n",
    "\n",
    "        # De-normalize the x-coordinates\n",
    "        if i % 2 == 0:  # Even indices are x-coordinates\n",
    "            keypoint = keypoint * readjust_x + readjust_x / 2\n",
    "        # De-normalize the y-coordinates\n",
    "        else:  # Odd indices are y-coordinates\n",
    "            keypoint = keypoint * readjust_y + readjust_y / 2\n",
    "\n",
    "        new_keypoints.append(keypoint)  # Append the de-normalized keypoint to the list\n",
    "\n",
    "    return new_keypoints, missing_kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_keypoints(img_size, keypoints):\n",
    "    \"\"\"\n",
    "    Normalizes keypoints based on image size and replaces any NaN values with -10.\n",
    "\n",
    "    Parameters:\n",
    "    - img_size: Tuple of the image dimensions (width, height).\n",
    "    - keypoints: List of de-normalized keypoints where each coordinate is in pixel dimensions.\n",
    "\n",
    "    Returns:\n",
    "    - norm_keypoints: List of normalized keypoints where each coordinate is scaled to the range \n",
    "                      [-1, 1] relative to the image size, with NaNs replaced by -10.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract image width and height\n",
    "    readjust_x = img_size[0]  # width of the image\n",
    "    readjust_y = img_size[1]  # height of the image\n",
    "\n",
    "    norm_keypoints = []  # List to store the normalized keypoints\n",
    "\n",
    "    # Iterate through each keypoint\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        # Replace NaN values with -10\n",
    "        if np.isnan(keypoint):\n",
    "            keypoint = -10.0\n",
    "        else:\n",
    "            # Normalize the x-coordinates\n",
    "            if i % 2 == 0:  # Even indices are x-coordinates\n",
    "                keypoint = (keypoint - readjust_x / 2) / readjust_x\n",
    "            # Normalize the y-coordinates\n",
    "            else:  # Odd indices are y-coordinates\n",
    "                keypoint = (keypoint - readjust_y / 2) / readjust_y\n",
    "\n",
    "        norm_keypoints.append(keypoint)  # Append the normalized keypoint to the list\n",
    "\n",
    "    return norm_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_and_keypoint(img, keypoints, nkeypoints=8, keypoint_labels=None):\n",
    "  fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "  plt.imshow(img)\n",
    "  print(keypoints)\n",
    "  x_keypoints = keypoints[::2]\n",
    "  y_keypoints = keypoints[1::2]\n",
    "  print(x_keypoints)\n",
    "  print(y_keypoints)\n",
    "  plt.scatter(x_keypoints, y_keypoints, marker='.', c=np.arange(nkeypoints), cmap='jet')\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "  if keypoint_labels is not None:\n",
    "      for i, (x, y) in enumerate(zip(x_keypoints, y_keypoints)):\n",
    "          plt.text(x, y, keypoint_labels[i], fontsize=12, color='white', \n",
    "                    bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Extract and save only the desired keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the video names\n",
    "def get_unique_video_names(directory):\n",
    "    \"\"\"\n",
    "    Scans the given directory for video files and returns a list of unique file names without the extension.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (str): The path to the directory containing the video files.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of unique video file names without extensions.\n",
    "    \"\"\"\n",
    "    unique_names = set()\n",
    "    \n",
    "    # Supported video file extensions\n",
    "    video_extensions = {'.mp4', '.mjpeg'}\n",
    "    \n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Split the filename and extension\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        # Check if the file has a video extension\n",
    "        if ext.lower() in video_extensions:\n",
    "            unique_names.add(name)  # Add the name to the set (ensures uniqueness)\n",
    "    \n",
    "    # Convert the set to a list and return\n",
    "    return list(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple\n",
    "list_of_vids = get_unique_video_names('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_videos')\n",
    "print(len(list_of_vids))\n",
    "print(list_of_vids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy and rename csv files that contain annotations\n",
    "def copy_csv_files(ids, source_dir, destination_dir):\n",
    "    \"\"\"\n",
    "    Copies CSV files from sub-directories that match the given IDs and renames them to the ID.\n",
    "    \n",
    "    Parameters:\n",
    "    ids (list): A list of IDs (sub-directory names) to search for.\n",
    "    source_dir (str): The path to the root directory containing sub-directories.\n",
    "    destination_dir (str): The path to the directory where the CSV files should be copied and renamed.\n",
    "    \"\"\"\n",
    "    # Ensure the destination directory exists\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    for id_ in ids:\n",
    "        subdir_path = os.path.join(source_dir, id_)\n",
    "        \n",
    "        if os.path.isdir(subdir_path):\n",
    "            # Find the CSV file in the sub-directory\n",
    "            for file_name in os.listdir(subdir_path):\n",
    "                if file_name.endswith('.csv'):\n",
    "                    csv_file_path = os.path.join(subdir_path, file_name)\n",
    "                    \n",
    "                    # Create the destination file path\n",
    "                    destination_file_path = os.path.join(destination_dir, f\"{id_}.csv\")\n",
    "                    \n",
    "                    # Copy the CSV file to the destination directory with the new name\n",
    "                    shutil.copy(csv_file_path, destination_file_path)\n",
    "                    print(f\"Copied {csv_file_path} to {destination_file_path}\")\n",
    "                    break  # Assuming there is only one CSV file per sub-directory\n",
    "        else:\n",
    "            print(f\"Sub-directory '{id_}' not found in '{source_dir}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = '/home/matthew/Desktop/Masters/Masters-data/Roanne Penguins 2022/Penguin Project Annotation and Videos/Penguin Annotations/P1_labeled-data'\n",
    "destination_directory = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/DLC_Annotations'\n",
    "\n",
    "copy_csv_files(list_of_vids, source_directory, destination_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load csv to a dataframe and remove the keypoints not required\n",
    "need it to look like:\n",
    "vid_id,img_id,Head,Head,Beak,Beak,Body_top,Body_top,RFlipper_mid,RFlipper_mid,LFlipper_mid,LFlipper_mid,Body_bottom,Body_bottom,RFoot,RFoot,LFoot,LFoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv into a df\n",
    "df = pd.read_csv('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/DLC_Annotations/flap1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test df function\n",
    "# print(df.head())\n",
    "# print(df.info())\n",
    "# print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step through list of videos/csv ids and create a list of dfs\n",
    "list_of_kp_df_raw = []\n",
    "\n",
    "# step through the list of ids and load each csv into a df and add to list\n",
    "for _id in list_of_vids:\n",
    "    \n",
    "    # load csv to a temp df\n",
    "    print(_id)\n",
    "    df = pd.read_csv(f'/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/DLC_Annotations/{_id}.csv')\n",
    "    list_of_kp_df_raw.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list_of_kp_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append master df from csv to df with correct kp and column names\n",
    "######  THIS WILL HAVE TO BE UPDATED WHEN HAVE LOTS OF PENGUINS. JUST ADD AN IF STATEMENT TO PUT ADDITIONAL COLUMNS\n",
    "######  AS A ADDITIONAL ENTRY AND ADD A BOUNDING BBOX NUMBER COLUMN. THIS WILL LEAD TO SOME EMPTY ROWS IF A PENGUIN\n",
    "######  ENTRERS THE FRAME. SO FINALLY REMOVE ANY EMPTY ROWS\n",
    "def consolidate_dataframes(source_dfs):\n",
    "    \"\"\"\n",
    "    Consolidates data from multiple source DataFrames into a single DataFrame with specific columns.\n",
    "    \n",
    "    Parameters:\n",
    "    source_dfs (list of pd.DataFrame): List of source DataFrames to be consolidated.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A consolidated DataFrame with the selected columns.\n",
    "    \"\"\"\n",
    "    consolidated_df = pd.DataFrame(columns=[\n",
    "        'vid_id', 'image_id', 'Head_x', 'Head_y', 'Beak_x', 'Beak_y',\n",
    "        'Body_top_x', 'Body_top_y', 'RFlipper_mid_x', 'RFlipper_mid_y',\n",
    "        'LFlipper_mid_x', 'LFlipper_mid_y', 'Body_bottom_x', 'Body_bottom_y',\n",
    "        'RFoot_x', 'RFoot_y', 'LFoot_x', 'LFoot_y'\n",
    "    ])\n",
    "\n",
    "    for df in source_dfs:\n",
    "        # Skip the first 4 rows (headers)\n",
    "        df = df.iloc[3:]\n",
    "        \n",
    "        # Create a temporary DataFrame to hold the required columns\n",
    "        temp_df = pd.DataFrame({\n",
    "            'vid_id': df.iloc[:, 1],  # Column 2 (index 1)\n",
    "            'image_id': range(len(df)),  # Sequential image_id starting from 0\n",
    "            'Head_x': df.iloc[:, 3],  # Column 4 (index 3)\n",
    "            'Head_y': df.iloc[:, 4],  # Column 5 (index 4)\n",
    "            'Beak_x': df.iloc[:, 5],  # Column 6 (index 5)\n",
    "            'Beak_y': df.iloc[:, 6],  # Column 7 (index 6)\n",
    "            'Body_top_x': df.iloc[:, 7],  # Column 8 (index 7)\n",
    "            'Body_top_y': df.iloc[:, 8],  # Column 9 (index 8)\n",
    "            'RFlipper_mid_x': df.iloc[:, 13],  # Column 14 (index 13)\n",
    "            'RFlipper_mid_y': df.iloc[:, 14],  # Column 15 (index 14)\n",
    "            'LFlipper_mid_x': df.iloc[:, 15],  # Column 16 (index 15)\n",
    "            'LFlipper_mid_y': df.iloc[:, 16],  # Column 17 (index 16)\n",
    "            'Body_bottom_x': df.iloc[:, 23],  # Column 24 (index 23)\n",
    "            'Body_bottom_y': df.iloc[:, 24],  # Column 25 (index 24)\n",
    "            'RFoot_x': df.iloc[:, 27],  # Column 28 (index 27)\n",
    "            'RFoot_y': df.iloc[:, 28],  # Column 29 (index 28)\n",
    "            'LFoot_x': df.iloc[:, 29],  # Column 30 (index 29)\n",
    "            'LFoot_y': df.iloc[:, 30]  # Column 31 (index 30)\n",
    "        })\n",
    "\n",
    "        # Append the temp_df to the consolidated DataFrame\n",
    "        consolidated_df = pd.concat([consolidated_df, temp_df], ignore_index=True)\n",
    "\n",
    "    return consolidated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_kp_df = consolidate_dataframes(list_of_kp_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_kp_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_kp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/reduced_kp_raw_Simple.json'\n",
    "df_to_json(master_kp_df, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Save df as a json (and vice versa) and set the datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(df, path):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame to a .json file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be converted to JSON.\n",
    "    path (str): The path (including file name) where the .json file will be saved.\n",
    "    \"\"\"\n",
    "    df.to_json(path, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def json_to_df(path):\n",
    "    \"\"\"\n",
    "    Converts a .json file to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The path to the .json file that will be read.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame created from the JSON file.\n",
    "    \"\"\"\n",
    "    print(path)\n",
    "    df = pd.read_json(path, orient='records')\n",
    "    print(f\"JSON file has been successfully converted to DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dtypes_df_full_annotation_abs(df):\n",
    "    df['vid_id'] = df['vid_id'].astype(str)\n",
    "    df['img_id'] = df['img_id'].astype(str)\n",
    "    df['bbox_id'] = df['bbox_id'].astype(str)\n",
    "    df['bbox_c_x'] = df['bbox_c_x'].astype('float32')\n",
    "    df['bbox_c_y'] = df['bbox_c_y'].astype('float32')\n",
    "    df['bbox_w'] = df['bbox_w'].astype('float32')\n",
    "    df['bbox_h'] = df['bbox_h'].astype('float32')\n",
    "    df['Head_x'] = df['Head_x'].astype('float32')\n",
    "    df['Head_y'] = df['Head_y'].astype('float32')\n",
    "    df['Beak_x'] = df['Beak_x'].astype('float32')\n",
    "    df['Beak_y'] = df['Beak_y'].astype('float32')\n",
    "    df['Body_top_x'] = df['Body_top_x'].astype('float32')\n",
    "    df['Body_top_y'] = df['Body_top_y'].astype('float32')\n",
    "    df['RFlipper_mid_x'] = df['RFlipper_mid_x'].astype('float32')\n",
    "    df['RFlipper_mid_y'] = df['RFlipper_mid_y'].astype('float32')\n",
    "    df['LFlipper_mid_x'] = df['LFlipper_mid_x'].astype('float32')\n",
    "    df['LFlipper_mid_y'] = df['LFlipper_mid_y'].astype('float32')\n",
    "    df['Body_bottom_x'] = df['Body_bottom_x'].astype('float32')\n",
    "    df['Body_bottom_y'] = df['Body_bottom_y'].astype('float32')\n",
    "    df['RFoot_x'] = df['RFoot_x'].astype('float32')\n",
    "    df['RFoot_y'] = df['RFoot_y'].astype('float32')\n",
    "    df['LFoot_x'] = df['LFoot_x'].astype('float32')\n",
    "    df['LFoot_y'] = df['LFoot_y'].astype('float32')\n",
    "    df['kp_outside_best_bbox'] = df['kp_outside_best_bbox'].astype('float32')\n",
    "    df['kp_missing'] = df['kp_missing'].astype('float32')\n",
    "    df['kp_primary_missing'] = df['kp_primary_missing'].astype(bool)\n",
    "    df['img_width'] = df['img_width'].astype('float32')\n",
    "    df['img_height'] = df['img_height'].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Create a single annotation with bbox and keypoints in the correct form and linked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load keypoints df and make them the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the reduced_kp_raw_Simple_df\n",
    "df_reduced_kp_raw = json_to_df('/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/reduced_kp_raw_Simple.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_kp_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the format of the keypoints\n",
    "    # 1st string\n",
    "df_reduced_kp_raw.iloc[:, 0] = df_reduced_kp_raw.iloc[:, 0].astype(str)\n",
    "\n",
    "# Ensure the second column is an integer\n",
    "df_reduced_kp_raw.iloc[:, 1] = df_reduced_kp_raw.iloc[:, 1].astype(str)\n",
    "\n",
    "# Format the remaining columns as floats with minimal decimal points\n",
    "for col in df_reduced_kp_raw.columns[2:]:\n",
    "    df_reduced_kp_raw[col] = df_reduced_kp_raw[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_kp_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bboxes into a df to be used and correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step through bounding box text files and create a df with the following output\n",
    "# vid_id,img_id,bbox_c_x, bbox_c_y, bbox_w, bbox_h\n",
    "def bbox_txt_files_to_df(directory):\n",
    "    \"\"\"\n",
    "    Processes all text files in the given directory and returns a DataFrame with the columns:\n",
    "    vid_id, img_id, bbox_c_x, bbox_c_y, bbox_w, bbox_h.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing the text files.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The processed DataFrame.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    #count = 0\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            # Extract vid_id and img_id from the filename\n",
    "            parts = filename.split('_')\n",
    "            vid_id = parts[1].split('.')[0]\n",
    "            img_id = parts[-1].split('.')[0]\n",
    "            #print(img_id)\n",
    "\n",
    "            # Read the text file\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                for line in file:\n",
    "                    figures = line.strip().split()\n",
    "                    if len(figures) == 5:\n",
    "                        bbox_c_x, bbox_c_y, bbox_w, bbox_h = map(float, figures[1:])\n",
    "                        data.append([vid_id, img_id, np.float32(bbox_c_x), np.float32(bbox_c_y), np.float32(bbox_w), np.float32(bbox_h)])\n",
    "                        #count += 1\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=['vid_id', 'img_id', 'bbox_c_x', 'bbox_c_y', 'bbox_w', 'bbox_h'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_annotations'\n",
    "\n",
    "bbox_df_raw = bbox_txt_files_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see where there are more than one bounding box per image\n",
    "duplicates = bbox_df_raw[bbox_df_raw.duplicated(['vid_id', 'img_id'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image size stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step through image files and create a df with the following output\n",
    "# vid_id,img_id,img_wid,img_height\n",
    "\n",
    "def image_files_to_df(directory):\n",
    "    \"\"\"\n",
    "    Processes all .jpg image files in the given directory and returns a DataFrame with the columns:\n",
    "    vid_id, img_id, img_wid, img_height.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): Path to the directory containing the image files.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The processed DataFrame.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.jpg'):\n",
    "            # Extract vid_id and img_id from the filename\n",
    "            parts = filename.split('_')\n",
    "            vid_id = parts[1].split('.')[0]\n",
    "            img_id = parts[-1].split('.')[0]\n",
    "\n",
    "            # Read the image file and get its dimensions\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with Image.open(filepath) as img:\n",
    "                img_wid, img_height = img.size\n",
    "\n",
    "            # Append the data to the list\n",
    "            data.append([vid_id, img_id, np.float32(img_wid), np.float32(img_height)])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=['vid_id', 'img_id', 'img_wid', 'img_height'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_images'\n",
    "\n",
    "imgsize_df_raw = image_files_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsize_df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsize_df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match keypoint to bbox, check for keypoints outside of bbox, match bbox ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all keypoint entries in df\n",
    "# for each keypoint filter the bbox df to have only those ids\n",
    "# do the same for the image df\n",
    "# find the bbox that will contain the most keypoints from the bbox df\n",
    "#   this will require the rescaling of the bbox\n",
    "#   find how many keypoints are outside the bbox\n",
    "#   find how many keypoints are missing\n",
    "#   find whether the primary kp are missing (True/False)\n",
    "# check if this is img_id = 0 \n",
    "#   yes: increment bbox_id starting at 0\n",
    "#   no: find bbox from previous vid_id, img_id - 1 in final df that has the best fit and make bbox id = to that bbox_id, unless distance is over 20% of img size, then make bbox_id last bbox_id + 1\n",
    "# remove the bounding box from the original bbox df - I REMOVED THIS STEP AS UNNECESSARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each keypoint filter the bbox df to have only those ids\n",
    "\n",
    "def filter_dataframe_based_on_another(vid_id, img_id, df_to_filter):\n",
    "    \"\"\"\n",
    "    filters df_to_filter to show entries with the same vid_id and img_id.\n",
    "\n",
    "    Parameters:\n",
    "    vid_id: string with vid_id\n",
    "    image_id: string with img_id\n",
    "    df_to_filter (pd.DataFrame): The DataFrame to filter based on vid_id and img_id.\n",
    "\n",
    "    Returns:\n",
    "    list of pd.DataFrame: A list of filtered DataFrames, one for each row in df_main.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Filter df_to_filter based on the current row's vid_id and img_id\n",
    "    filtered_df = df_to_filter[(df_to_filter['vid_id'] == vid_id) & (df_to_filter['img_id'] == img_id)]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the bounding box\n",
    "def denorm_bbox_df(df_bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Denormalizes bounding boxes in a DataFrame from normalized values to absolute pixel values.\n",
    "\n",
    "    Parameters:\n",
    "    df_bboxes (pd.DataFrame): The DataFrame containing bounding box coordinates.\n",
    "                              Expected columns: ['vid_id', 'image_id', 'bbox_c_x', 'bbox_c_y', 'bbox_w', 'bbox_h']\n",
    "    img_width (int): The width of the image.\n",
    "    img_height (int): The height of the image.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with denormalized bounding boxes.\n",
    "                  Columns: ['vid_id', 'image_id', 'bbox_c_x', 'bbox_c_y', 'bbox_w', 'bbox_h']\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original DataFrame\n",
    "    denorm_df = df_bbox.copy()\n",
    "    #print(denorm_df)\n",
    "    #print(img_width)\n",
    "\n",
    "    # Denormalize the bounding box coordinates\n",
    "    #print(denorm_df['bbox_c_x'])# * img_width)\n",
    "    denorm_df['bbox_c_x'] = denorm_df['bbox_c_x'] * img_width\n",
    "    denorm_df['bbox_c_y'] = denorm_df['bbox_c_y'] * img_height\n",
    "    denorm_df['bbox_w'] = denorm_df['bbox_w'] * img_width\n",
    "    denorm_df['bbox_h'] = denorm_df['bbox_h'] * img_height\n",
    "\n",
    "    return denorm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the bbox that will contain the most keypoints from the bbox df\n",
    "#   find how many keypoints are outside the bbox\n",
    "#   find how many keypoints are missing\n",
    "#   find whether the primary kp are missing (True/False)\n",
    "\n",
    "def find_best_bbox(bbox_df, keypoints_df):\n",
    "    \"\"\"\n",
    "    Finds the bounding box that contains the most keypoints and returns it along with the number\n",
    "    of keypoints that fall outside that bounding box.\n",
    "\n",
    "    Parameters:\n",
    "    bbox_df (pd.DataFrame): DataFrame with bounding boxes in absolute coordinates. \n",
    "                            \n",
    "    keypoints_df (pd.DataFrame): DataFrame with keypoints.\n",
    "                                \n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing the best bounding box and the number of keypoints outside it.\n",
    "    \"\"\"\n",
    "    best_bbox = None\n",
    "    max_keypoints_inside = -1\n",
    "    min_distance_to_origin = float('inf')\n",
    "    keypoints_outside_best_bbox = 0\n",
    "    nan_keypoint_pairs = 0\n",
    "    missing_primary_keypoint = False\n",
    "\n",
    "    # Extract keypoints and check for NaN pairs and missing primary keypoints\n",
    "    keypoints = []\n",
    "    for i in range(0, 16, 2):  # Since there are 8 keypoints (16 columns), we step by 2\n",
    "        #print(i)\n",
    "        x = keypoints_df.iloc[i+2]  \n",
    "        y = keypoints_df.iloc[i+3]\n",
    "        keypoints.append((x, y))\n",
    "\n",
    "        # Check if either x or y is NaN\n",
    "        if pd.isna(x) or pd.isna(y):\n",
    "            nan_keypoint_pairs += 1\n",
    "            if i == 4 or i == 10:\n",
    "                missing_primary_keypoint = True\n",
    "\n",
    "        #break\n",
    "    count =0\n",
    "    for _, bbox in bbox_df.iterrows():\n",
    "        #vid_id, img_id = bbox['vid_id'], bbox['img_id']\n",
    "        count=+1\n",
    "        #print(count)\n",
    "        bbox_c_x, bbox_c_y, bbox_w, bbox_h = bbox['bbox_c_x'], bbox['bbox_c_y'], bbox['bbox_w'], bbox['bbox_h']\n",
    "        \n",
    "        # Calculate the bounding box corners (x_min, y_min, x_max, y_max)\n",
    "        x_min = bbox_c_x - bbox_w / 2\n",
    "        y_min = bbox_c_y - bbox_h / 2\n",
    "        x_max = bbox_c_x + bbox_w / 2\n",
    "        y_max = bbox_c_y + bbox_h / 2\n",
    "\n",
    "        # print('xy minmax')\n",
    "        # print(x_min, y_min, x_max, y_max)\n",
    "        # print('keypoints')\n",
    "        # print(keypoints)\n",
    "        \n",
    "        # Count keypoints inside the current bbox\n",
    "        keypoints_inside = sum(x_min <= x <= x_max and y_min <= y <= y_max for x, y in keypoints)\n",
    "        #print('keypoint inside')\n",
    "        #print(keypoints_inside)\n",
    "        \n",
    "        # Calculate the distance of the bbox to the origin (0,0)\n",
    "        distance_to_origin = (x_min**2 + y_min**2)**0.5\n",
    "        \n",
    "        # Update the best bbox if this one has more keypoints inside, or the same number but is closer to the origin\n",
    "        if (keypoints_inside > max_keypoints_inside) or \\\n",
    "           (keypoints_inside == max_keypoints_inside and distance_to_origin < min_distance_to_origin):\n",
    "            best_bbox = bbox\n",
    "            max_keypoints_inside = keypoints_inside\n",
    "            min_distance_to_origin = distance_to_origin\n",
    "            \n",
    "            keypoints_outside_best_bbox = len(keypoints) - nan_keypoint_pairs - keypoints_inside\n",
    "    \n",
    "    # Convert the best bbox to a DataFrame or a list\n",
    "    if best_bbox is not None:\n",
    "        best_bbox_df = pd.DataFrame([{\n",
    "            'vid_id': best_bbox['vid_id'],\n",
    "            'img_id': best_bbox['img_id'],\n",
    "            'bbox_c_x': best_bbox['bbox_c_x'],\n",
    "            'bbox_c_y': best_bbox['bbox_c_y'],\n",
    "            'bbox_w': best_bbox['bbox_w'],\n",
    "            'bbox_h': best_bbox['bbox_h'],\n",
    "        }])\n",
    "        return best_bbox_df, keypoints_outside_best_bbox, nan_keypoint_pairs, missing_primary_keypoint\n",
    "    else:\n",
    "        return None, 8, 0, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_bbox_id(best_bbox_df, final_df_prev):\n",
    "    \"\"\"\n",
    "    Finds the bbox_id in final_df_prev that is closest in distance to the bounding box in best_bbox_df \n",
    "    UNLESS it is more that 20% of image size off, then it returns max bbox_id +1.\n",
    "\n",
    "    Parameters:\n",
    "    best_bbox_df (pd.DataFrame): DataFrame with a single entry for the best bounding box.\n",
    "    final_df_prev (pd.DataFrame): DataFrame with multiple entries, each having a bounding box.\n",
    "\n",
    "    Returns:\n",
    "    str: The bbox_id of the closest bounding box in final_df_prev UNLESS \n",
    "    \"\"\"\n",
    "    # Extract the values from the single entry in best_bbox_df\n",
    "    best_bbox_c_x = best_bbox_df['bbox_c_x'].iloc[0]\n",
    "    best_bbox_c_y = best_bbox_df['bbox_c_y'].iloc[0]\n",
    "    best_bbox_w = best_bbox_df['bbox_w'].iloc[0]\n",
    "    best_bbox_h = best_bbox_df['bbox_h'].iloc[0]\n",
    "\n",
    "    # Initialize variables to track the closest bbox\n",
    "    min_distance = float('inf')\n",
    "    closest_bbox_id = None\n",
    "\n",
    "    # Find the max distance can be (rsm of the image size x 0.25 - this is 25% of image size)\n",
    "    max_distance = np.sqrt(\n",
    "        final_df_prev['img_width'].iloc[0] ** 2 +\n",
    "        final_df_prev['img_height'].iloc[0] ** 2\n",
    "    ) * 0.25\n",
    "\n",
    "    # Iterate through each entry in final_df_prev to calculate the distance\n",
    "    for index, row in final_df_prev.iterrows():\n",
    "        # Calculate the Euclidean distance (root squared mean)\n",
    "        distance = np.sqrt(\n",
    "            (row['bbox_c_x'] - best_bbox_c_x) ** 2 +\n",
    "            (row['bbox_c_y'] - best_bbox_c_y) ** 2 +\n",
    "            (row['bbox_w'] - best_bbox_w) ** 2 +\n",
    "            (row['bbox_h'] - best_bbox_h) ** 2\n",
    "        )\n",
    "        \n",
    "        # Update the closest_bbox_id if this distance is the smallest found\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_bbox_id = row['bbox_id']\n",
    "    \n",
    "    # return max bbox_id + 1 if the distance is greater than the max distance\n",
    "    if min_distance > max_distance:\n",
    "        closest_bbox_id = str(int(final_df_prev['bbox_id'].max())+1)\n",
    "\n",
    "    return closest_bbox_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all keypoint entries in df and use the above functions to create final df\n",
    "def process_dataframe(df_kp, df_bbox, df_imgsize):\n",
    "    \"\"\"\n",
    "    Iterates over each row in the DataFrame and processes the data.\n",
    "\n",
    "    Parameters:\n",
    "    df_kp, df_bbox, df_imgsize: keypoint df, bbox df, img size df \n",
    "\n",
    "    Returns:\n",
    "    df_full_annotation: Full annotation compiled df\n",
    "    \"\"\"\n",
    "    #test\n",
    "    count = 0\n",
    "    bbox_count = 0\n",
    "    prev_img_id = -1\n",
    "    df_final = pd.DataFrame(columns=[\n",
    "        'vid_id', 'img_id', 'bbox_id', 'bbox_c_x', 'bbox_c_y', 'bbox_w', 'bbox_h',\n",
    "        'Head_x', 'Head_y', 'Beak_x', 'Beak_y','Body_top_x', 'Body_top_y','RFlipper_mid_x',\t\n",
    "        'RFlipper_mid_y', 'LFlipper_mid_x', 'LFlipper_mid_y', 'Body_bottom_x', 'Body_bottom_y', \n",
    "        'RFoot_x', 'RFoot_y', 'LFoot_x', 'LFoot_y','kp_outside_best_bbox', 'kp_missing', 'kp_primary_missing',\n",
    "        'img_width', 'img_height',\n",
    "    ])\n",
    "\n",
    "    for index, row in df_kp.iterrows():\n",
    "        #test\n",
    "        count += 1\n",
    "\n",
    "        # Access data in each row using row['column_name'] - get the vid_id and img_id\n",
    "        vid_id = row['vid_id']\n",
    "        img_id = row['image_id']\n",
    "\n",
    "        # print(row)\n",
    "\n",
    "        #filter the bbox and img_size df to only have specific img and vid id\n",
    "        df_bbox_filtered = filter_dataframe_based_on_another(vid_id, img_id, df_bbox)\n",
    "        df_imgsize_filtered = filter_dataframe_based_on_another(vid_id, img_id, df_imgsize)\n",
    "\n",
    "        # get image size \n",
    "        img_width = df_imgsize_filtered['img_wid']\n",
    "        img_height = df_imgsize_filtered['img_height']\n",
    "        # convert them to scalars that can be used in math operations\n",
    "        img_width = img_width.iloc[0]  # Convert to scalar\n",
    "        img_height = img_height.iloc[0]  # Convert to scalar\n",
    "\n",
    "        # denormalise the bbox so that the they are absolute coords\n",
    "        df_bbox_filtered_abs = denorm_bbox_df(df_bbox_filtered, img_width, img_height)\n",
    "\n",
    "        # find the bbox that will contain the most keypoints from the bbox df\n",
    "        # and find how many keypoints are outside the bbox\n",
    "        df_best_bbox, kp_outside_best_bbox, kp_missing, kp_primary_missing = find_best_bbox(df_bbox_filtered_abs, row)\n",
    "\n",
    "        # check if first image in video sequence (for matching bboxes and kp, if it is first then we don't need matching)\n",
    "        if img_id == '0':\n",
    "            #yes: just increment bbox_id starting at 0\n",
    "            if prev_img_id != img_id: # if we are on the first bbox of an img\n",
    "                bbox_count = 0\n",
    "            else: # if we not on the first one\n",
    "                bbox_count += 1\n",
    "            # set the bbox id\n",
    "            bbox_id = str(bbox_count)  \n",
    "        \n",
    "        else:\n",
    "            # no: find bbox from previous vid_id, img_id - 1 in final df that has the best fit \n",
    "            # and make bbox id = to that bbox_id, unless distance is over 20% of img size, \n",
    "            # then make bbox_id last bbox_id + 1\n",
    "\n",
    "            # 1. filter for all the entries in the df_final that are from the previous image\n",
    "            df_final_filtered_prev = filter_dataframe_based_on_another(prev_vid_id, prev_img_id, df_final)\n",
    "\n",
    "            # 2. find bbox df_final_filtered_prev that has the best fit to current best bbox \n",
    "            # and make bbox id = to that bbox_id, unless distance is over 25% of img size, \n",
    "            # then make bbox_id last bbox_id + 1\n",
    "            bbox_id = find_closest_bbox_id(df_best_bbox, df_final_filtered_prev)\n",
    "\n",
    "\n",
    "        # Store the result in a dictionary and then append to the DataFrame\n",
    "        result = {\n",
    "            'vid_id': vid_id,\n",
    "            'img_id': img_id,\n",
    "            'bbox_id': bbox_id,\n",
    "            'bbox_c_x': df_best_bbox['bbox_c_x'].iloc[0],\n",
    "            'bbox_c_y': df_best_bbox['bbox_c_y'].iloc[0],\n",
    "            'bbox_w': df_best_bbox['bbox_w'].iloc[0],\n",
    "            'bbox_h': df_best_bbox['bbox_h'].iloc[0],\n",
    "            'Head_x': row.iloc[2],\n",
    "            'Head_y': row.iloc[3],\n",
    "            'Beak_x': row.iloc[4],\n",
    "            'Beak_y': row.iloc[5],\n",
    "            'Body_top_x': row.iloc[6],\n",
    "            'Body_top_y': row.iloc[7],\n",
    "            'RFlipper_mid_x': row.iloc[8],\n",
    "            'RFlipper_mid_y': row.iloc[9],\n",
    "            'LFlipper_mid_x': row.iloc[10],\n",
    "            'LFlipper_mid_y': row.iloc[11],\n",
    "            'Body_bottom_x': row.iloc[12],\n",
    "            'Body_bottom_y': row.iloc[13],\n",
    "            'RFoot_x': row.iloc[14],\n",
    "            'RFoot_y': row.iloc[15],\n",
    "            'LFoot_x': row.iloc[16],\n",
    "            'LFoot_y': row.iloc[17],\n",
    "            'kp_outside_best_bbox': float(kp_outside_best_bbox),\n",
    "            'kp_missing': float(kp_missing),\n",
    "            'kp_primary_missing': kp_primary_missing,\n",
    "            'img_width': img_width,\n",
    "            'img_height': img_height\n",
    "        }\n",
    "        \n",
    "        df_final = df_final.append(result, ignore_index=True)\n",
    "\n",
    "        # keep track of the last img_id so if we are on the second bbox for an image we know\n",
    "        prev_img_id = img_id\n",
    "        prev_vid_id = vid_id\n",
    "        \n",
    "    return df_final\n",
    "        \n",
    "\n",
    "\n",
    "        #test\n",
    "        # if count == 50:\n",
    "        #     return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs = process_dataframe(df_reduced_kp_raw, bbox_df_raw, imgsize_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check that all kp are contained in the bbox\n",
    "A. in the simple we will adjust bbox \n",
    "B. in others we will adjust bboxs to make them slightly bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the bboxs that have kps outside of the box\n",
    "df_kp_outside_bbox = df_full_annotation_abs[(df_full_annotation_abs['kp_outside_best_bbox'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kp_outside_bbox.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple\n",
    "we will just show all the bbox that are an issue and adjust the bbox and save those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all the issue bbox rows\n",
    "df_kp_outside_bbox.head(44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPORTANT: display all cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_kp_outside_bbox.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i need to adjust the Head_y value of row 390 as this is a negative (which it cant be)\n",
    "# first lets see that we have the right row number (390) -> seen in the above output\n",
    "print(df_full_annotation_abs.loc[390, 'Head_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay now just set it to 1.5\n",
    "df_full_annotation_abs.loc[390, 'Head_y'] = float(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the kp_outside flag\n",
    "df_full_annotation_abs.loc[390, 'kp_outside_best_bbox'] = float(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_full_annotation_abs.loc[390])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets just check that it is the correct dtype\n",
    "df_full_annotation_abs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save df_final as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/df_full_annotation_abs_Simple.json'\n",
    "df_to_json(df_full_annotation_abs, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Get json annotation abs to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/df_full_annotation_abs_Simple.json'\n",
    "df_full_annotation_abs = json_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs = set_dtypes_df_full_annotation_abs(df_full_annotation_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_abs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Crop images by bbox\n",
    "save them with the name vid_id_img_id_bbox_id_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all kp entries in the df_full_annotation_abs\n",
    "# for each entry crop and save image with the naming criteria using a function that takes bbox coords as input and ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to crop image by bbox and save using id naming convention\n",
    "def crop_and_save_image(image_path, save_directory, vid_id, img_id, bbox_id, bbox_c_x, bbox_c_y, bbox_w, bbox_h):\n",
    "    \"\"\"\n",
    "    Crops an image based on the provided bounding box coordinates and saves it with a specific naming convention.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - save_directory (str): Directory where the cropped image will be saved.\n",
    "    - vid_id (str): Video ID used for naming the cropped image.\n",
    "    - img_id (str): Image ID used for naming the cropped image.\n",
    "    - bbox_id (str): Bounding box ID used for naming the cropped image.\n",
    "    - bbox_c_x (float): X-coordinate of the bounding box center.\n",
    "    - bbox_c_y (float): Y-coordinate of the bounding box center.\n",
    "    - bbox_w (float): Width of the bounding box.\n",
    "    - bbox_h (float): Height of the bounding box.\n",
    "    \n",
    "    The cropped image will be saved as `vid_id_img_id_bbox_id_crop_raw.jpg` in the save directory.\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Calculate the bounding box corners\n",
    "    left = bbox_c_x - (bbox_w / 2)\n",
    "    top = bbox_c_y - (bbox_h / 2)\n",
    "    right = bbox_c_x + (bbox_w / 2)\n",
    "    bottom = bbox_c_y + (bbox_h / 2)\n",
    "    \n",
    "    # Crop the image\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    \n",
    "    # Ensure the save directory exists\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    \n",
    "    # Create the file name\n",
    "    save_filename = f\"{vid_id}_{img_id}_{bbox_id}_crop_raw.jpg\"\n",
    "    save_path = os.path.join(save_directory, save_filename)\n",
    "    \n",
    "    # Save the cropped image\n",
    "    cropped_image.save(save_path, format='JPEG')\n",
    "    \n",
    "    print(f\"Cropped image saved as {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that gets the path to the image \n",
    "def find_image_path(image_directory, vid_id, img_id):\n",
    "    \"\"\"\n",
    "    Searches through all .jpg files in the specified directory and returns the path to the image\n",
    "    that matches the provided vid_id and img_id.\n",
    "\n",
    "    Parameters:\n",
    "    - image_directory (str): Path to the directory containing the images.\n",
    "    - vid_id (str): The video ID to match in the image file name.\n",
    "    - img_id (str): The image ID to match in the image file name.\n",
    "\n",
    "    Returns:\n",
    "    - str: The path to the matching image file, or None if no match is found.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(image_directory):\n",
    "        if filename.endswith('.jpg'):\n",
    "            # Split the filename and check if it matches the vid_id and img_id\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) >= 3:  # Ensure there are enough parts to avoid index errors\n",
    "                file_vid_id = parts[1].split('.')[0]\n",
    "                file_img_id = parts[-1].split('.')[0]\n",
    "                if file_vid_id == vid_id and file_img_id == img_id:\n",
    "                    return os.path.join(image_directory, filename)\n",
    "    \n",
    "    # If no matching image is found, return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that steps through df and calls above function\n",
    "def crop_img_from_df(df, img_dir, save_dir):\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # Access data in each row using row['column_name'] - get the vid_id and img_id\n",
    "        vid_id = row['vid_id']\n",
    "        img_id = row['img_id']\n",
    "        bbox_id = row['bbox_id']\n",
    "        bbox_c_x = row['bbox_c_x']\n",
    "        bbox_c_y = row['bbox_c_y']\n",
    "        bbox_w = row['bbox_w']\n",
    "        bbox_h = row['bbox_h']\n",
    "\n",
    "        #print(type(vid_id))\n",
    "        #print(type(img_id))\n",
    "\n",
    "        # get the relevant image path\n",
    "        img_path = find_image_path(img_dir, vid_id, img_id)\n",
    "        #print(img_path)\n",
    "\n",
    "        #crop and save the relevant bbox in the save directory\n",
    "        crop_and_save_image(img_path, save_dir, vid_id, img_id, bbox_id, bbox_c_x, bbox_c_y, bbox_w, bbox_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/Simple_ObjectDetect1/raw_images'\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_raw'\n",
    "#df_full_annotation_abs.info()\n",
    "\n",
    "crop_img_from_df(df_full_annotation_abs, img_dir, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Resize cropped images and add padding\n",
    "resize bbox_img to fit into 220x220 but do not allow distortion of the img. Use padding rather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad_images(source_dir, save_dir):\n",
    "    \"\"\"\n",
    "    Resizes and pads images from the source directory to 220x220 pixels and saves them to the save directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_dir (str): Path to the directory containing the source images.\n",
    "    - save_dir (str): Path to the directory where the resized images will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Process each image in the source directory\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            img_path = os.path.join(source_dir, filename)\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Resize while maintaining aspect ratio\n",
    "            img.thumbnail((220, 220), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Calculate padding to make the image 220x220\n",
    "            delta_w = 220 - img.size[0]\n",
    "            delta_h = 220 - img.size[1]\n",
    "            padding = (delta_w // 2, delta_h // 2, delta_w - (delta_w // 2), delta_h - (delta_h // 2))\n",
    "            \n",
    "            # Add padding to the image\n",
    "            padded_img = ImageOps.expand(img, padding, fill='black')\n",
    "            \n",
    "            # Rename the image\n",
    "            parts = filename.split('_')\n",
    "            base_name = '_'.join(parts[:-1])\n",
    "            extension = filename.split('.')[-1]\n",
    "            new_filename = f\"{base_name}_220x220.{extension}\"\n",
    "            \n",
    "            # Save the new image\n",
    "            save_path = os.path.join(save_dir, new_filename)\n",
    "            padded_img.save(save_path, format='JPEG')\n",
    "            print(f\"Saved resized image as {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_raw'\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_crop_220'\n",
    "resize_and_pad_images(source_dir, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Normalise the df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the keypoints (normalise by the abs value of the bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max(w/h) of the bbox\n",
    "# shift x and y coords for each annotation by the centre of the bbox\n",
    "# devide by the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_kp_by_bbox_df(df_orginal):\n",
    "\n",
    "    # copy so don't effect the original df\n",
    "    df = df_orginal.copy()\n",
    "    \n",
    "    # first create a new col that is the max value of the height and width\n",
    "    df['bbox_max_h_w'] = df[['bbox_w', 'bbox_h']].max(axis=1) \n",
    "\n",
    "    ## shift coords \n",
    "    # list of kp y cols\n",
    "    y_columns = [\n",
    "        'Head_y', 'Beak_y', 'Body_top_y', 'RFlipper_mid_y', \n",
    "        'LFlipper_mid_y', 'Body_bottom_y', 'RFoot_y', 'LFoot_y'\n",
    "    ]\n",
    "    # list of kp x cols\n",
    "    x_columns = [\n",
    "        'Head_x', 'Beak_x', 'Body_top_x', 'RFlipper_mid_x', \n",
    "        'LFlipper_mid_x', 'Body_bottom_x', 'RFoot_x', 'LFoot_x'\n",
    "    ]\n",
    "    # Subtract bbox_c_y from the selected '_y' columns\n",
    "    df[y_columns] = df[y_columns].subtract(df['bbox_c_y'], axis=0)\n",
    "    # Subtract bbox_c_x from the selected '_x' columns\n",
    "    df[x_columns] = df[x_columns].subtract(df['bbox_c_x'], axis=0)\n",
    "\n",
    "    # scale (devide) by the max of bbox width and hight (bbox_max_h_w)\n",
    "    df[y_columns] = df[y_columns].div(df['bbox_max_h_w'], axis=0)\n",
    "    df[x_columns] = df[x_columns].div(df['bbox_max_h_w'], axis=0)\n",
    "\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_kpnorm = norm_kp_by_bbox_df(df_full_annotation_abs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \n",
    "# with pd.option_context('display.max_columns', None):\n",
    "#     print(df_full_annotation_abs.head(2))\n",
    "\n",
    "# with pd.option_context('display.max_columns', None):\n",
    "#     print(df_test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the bbox (normalise by the size of the img)\n",
    "normalising how it is done in the obj dect (between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_bbox_by_img_df(df_orginal):\n",
    "\n",
    "    # copy so don't effect the original df\n",
    "    df = df_orginal.copy()\n",
    "\n",
    "    ## shift coords \n",
    "    # list of kp y cols\n",
    "    y_columns = [\n",
    "        'bbox_c_y', 'bbox_h'\n",
    "    ]\n",
    "    # list of kp x cols \n",
    "    # I AM NORMILISING THE bbox_max_h_w BY THE IMAGE WIDTH\n",
    "    x_columns = [\n",
    "        'bbox_c_x', 'bbox_w', 'bbox_max_h_w'\n",
    "    ]\n",
    "\n",
    "    # scale (devide) by the width and hight of the image\n",
    "    df[y_columns] = df[y_columns].div(df['img_height'], axis=0)\n",
    "    df[x_columns] = df[x_columns].div(df['img_width'], axis=0)\n",
    "\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm = norm_bbox_by_img_df(df_full_annotation_kpnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_full_annotation_abs.head(2))\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_full_annotation_norm.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the norm full annotation df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/df_full_annotation_norm_Simple.json'\n",
    "df_to_json(df_full_annotation_norm, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build Basic Regression PE Model (DeepPose based)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. load the normalised annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/df_full_annotation_norm_Simple.json'\n",
    "df_full_annotation_norm = json_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm = set_dtypes_df_full_annotation_abs(df_full_annotation_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the db easier to work with I am going to create a list with the kp col names, bbox col names, id col names\n",
    "id_cols = df_full_annotation_norm.iloc[:, :3].columns.to_list()\n",
    "bbox_cols = df_full_annotation_norm.iloc[:, 3:7].columns.to_list()\n",
    "kp_cols = df_full_annotation_norm.iloc[:, 7:23].columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_cols)\n",
    "print(bbox_cols)\n",
    "print(kp_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Remove rows where too many (or primary) keypoints are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full_annotation_norm.iloc[359])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any examples with more NaN values than the chosen threshold\n",
    "# The nan values are there when a keypoint is occluded\n",
    "\n",
    "def remove_rows_with_too_many_nans(df, columns_to_check, nan_threshold):\n",
    "    \"\"\"\n",
    "    Remove rows from the DataFrame where the number of NaN values in specified columns exceeds the threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to process.\n",
    "    - columns_to_check: A list of column names to check for NaN values.\n",
    "    - nan_threshold: The maximum allowed number of NaN values in the specified columns. Rows with more NaNs will be removed.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with rows exceeding the NaN threshold removed.\n",
    "    \"\"\"\n",
    "    # Count NaNs only in the specified columns\n",
    "    nan_counts = df[columns_to_check].isna().sum(axis=1)\n",
    "    print(type(nan_counts))\n",
    "\n",
    "    # Identify rows where NaN count is below or equal to the threshold\n",
    "    rows_to_keep = nan_counts <= nan_threshold\n",
    "    print(rows_to_keep[rows_to_keep==False].index)\n",
    "\n",
    "    # Filter the DataFrame to keep only the desired rows\n",
    "    filtered_df = df[rows_to_keep]\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function to remove the rows with more than 14 keypoint coords missing\n",
    "# the number of keypoints is 14 and each has 2 coords so there are 28 coords\n",
    "df_full_annotation_norm = remove_rows_with_too_many_nans(df_full_annotation_norm, kp_cols, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full_annotation_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any examples where the primary kp are missing. When these are missing we will not be able to use our PCK metric\n",
    "# The nan values are there when a keypoint is occluded\n",
    "\n",
    "def remove_rows_with_missing_primary_kp(df):\n",
    "    \"\"\"\n",
    "    Remove rows from the DataFrame where kp_primary_missing is set to true\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "    - A new DataFrame with rows not missing the primary kp.\n",
    "    \"\"\"\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        print(df[df['kp_primary_missing'] == True])\n",
    "\n",
    "    # Identify rows where NaN count is below or equal to the threshold\n",
    "    rows_to_keep = df['kp_primary_missing'] == False\n",
    "\n",
    "    # Filter the DataFrame to keep only the desired rows\n",
    "    filtered_df = df[rows_to_keep]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm = remove_rows_with_missing_primary_kp(df_full_annotation_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Replace nan with out of range (-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to train the data keypoints cannot have the value nan\n",
    "# this function removes the value nan from the keypoint df\n",
    "def convert_nans_to_neg_ten(df, columns):\n",
    "\n",
    "    df_adjusted = df.copy()\n",
    "\n",
    "    # Iterate over the specified columns\n",
    "    for col in columns:\n",
    "        # Replace NaN values with -10\n",
    "        df_adjusted[col].fillna(-10, inplace=True)\n",
    "\n",
    "    return df_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm = convert_nans_to_neg_ten(df_full_annotation_norm, kp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Split data into train, val and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. get list of ids that are in each set from obj detect folder and save to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of ids from image_obj detect\n",
    "def extract_image_ids(folder_path):\n",
    "    \"\"\"\n",
    "    Extracts image IDs from filenames in the given folder. \n",
    "    The filenames are assumed to be in the format something_vidid.something_imgid.something.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of strings in the format 'vidid_imgid'.\n",
    "    \"\"\"\n",
    "    image_ids = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if os.path.isfile(os.path.join(folder_path, filename)):  # Ensure it's a file\n",
    "            parts = filename.split('_')  # Split by the underscore\n",
    "            vidid = parts[1].split('.')[0]  # Extract vidid (part after first underscore and before first dot)\n",
    "            imgid = parts[2].split('.')[0]  # Extract imgid (part after second underscore and before second dot)\n",
    "            image_ids.append(f'{vidid}_{imgid}')\n",
    "\n",
    "    return image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unique ids list as a text file\n",
    "def save_list_to_file(list_data, file_path):\n",
    "    \"\"\"\n",
    "    Saves a list to a text file with each entry on a new line.\n",
    "\n",
    "    :param list_data: List of strings to be saved to a file.\n",
    "    :param file_path: Path to the file where the list should be saved.\n",
    "    \"\"\"\n",
    "    # Open the file for writing\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Write each item on a new line\n",
    "        for item in list_data:\n",
    "            file.write(f\"{item}\\n\")  # Add a newline after each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test imgs\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/YoloV8_dataset_Simple_parent/YoloV8_dataset_Simple/images/test'\n",
    "ids_test = extract_image_ids(path)\n",
    "print(ids_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val imgs\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/YoloV8_dataset_Simple_parent/YoloV8_dataset_Simple/images/val'\n",
    "ids_val = extract_image_ids(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train imgs\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/YoloV8_dataset_Simple_parent/YoloV8_dataset_Simple/images/train'\n",
    "ids_train = extract_image_ids(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lists to txt files\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_test.txt'\n",
    "save_list_to_file(ids_test, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_val.txt'\n",
    "\n",
    "save_list_to_file(ids_val, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_train.txt'\n",
    "\n",
    "save_list_to_file(ids_train, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. split df based on train, val, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_id_parts(df, id_list, col1, col2):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only rows where:\n",
    "    - col1 matches idpart1\n",
    "    - col2 matches idpart2\n",
    "    The ID parts are derived from the id_list, where each ID is in the format 'idpart1_idpart2'.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be filtered.\n",
    "    id_list (list): The list of IDs in the format 'idpart1_idpart2'.\n",
    "    col1 (str): The name of the first column containing idpart1.\n",
    "    col2 (str): The name of the second column containing idpart2.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A filtered DataFrame containing only the rows matching the ID parts.\n",
    "    \"\"\"\n",
    "    # Split the IDs into idpart1 and idpart2\n",
    "    id_parts = [id.split('_') for id in id_list]\n",
    "    \n",
    "    # Convert the list of tuples into a DataFrame\n",
    "    id_df = pd.DataFrame(id_parts, columns=[col1, col2])\n",
    "    \n",
    "    # Perform an inner merge to filter the DataFrame\n",
    "    filtered_df = pd.merge(df, id_df, how='inner', on=[col1, col2])\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "df_full_annotation_norm_test = filter_df_by_id_parts(df_full_annotation_norm, ids_test, 'vid_id', 'img_id')\n",
    "\n",
    "print(df_full_annotation_norm_test.info())\n",
    "\n",
    "display_all_cols(df_full_annotation_norm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val set\n",
    "df_full_annotation_norm_val = filter_df_by_id_parts(df_full_annotation_norm, ids_val, 'vid_id', 'img_id')\n",
    "\n",
    "print(df_full_annotation_norm_val.info())\n",
    "display_all_cols(df_full_annotation_norm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "df_full_annotation_norm_train = filter_df_by_id_parts(df_full_annotation_norm, ids_train, 'vid_id', 'img_id')\n",
    "\n",
    "print(df_full_annotation_norm_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df_full_annotation_norm_test.info())\n",
    "\n",
    "display_all_cols(df_full_annotation_norm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3. Save the df annotations to the processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/test_annotation_simple.json'\n",
    "df_to_json(df_full_annotation_norm_test, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save val\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/val_annotation_simple.json'\n",
    "df_to_json(df_full_annotation_norm_val, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/train_annotation_simple.json'\n",
    "df_to_json(df_full_annotation_norm_train, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4. save imgs to the processed folder based on split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images_by_ids(src_folder, dst_folder, id_list):\n",
    "    \"\"\"\n",
    "    Moves images from the source folder to the destination folder based on the specified IDs.\n",
    "    The image filenames are expected to be in the format 'something_vidid.something_imgid.something'.\n",
    "\n",
    "    Parameters:\n",
    "    src_folder (str): Path to the source folder containing the images.\n",
    "    dst_folder (str): Path to the destination folder where images will be moved.\n",
    "    id_list (list): List of IDs in the format 'idpart1_idpart2'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Ensure the destination directory exists\n",
    "    if not os.path.exists(dst_folder):\n",
    "        os.makedirs(dst_folder)\n",
    "\n",
    "    for filename in os.listdir(src_folder):\n",
    "        if os.path.isfile(os.path.join(src_folder, filename)):  # Check if it's a file\n",
    "            parts = filename.split('_')\n",
    "            vidid = parts[0]\n",
    "            imgid = parts[1]\n",
    "            \n",
    "            # Check if the extracted id combination is in the list\n",
    "            if f'{vidid}_{imgid}' in id_list:\n",
    "                src_path = os.path.join(src_folder, filename)\n",
    "                dst_path = os.path.join(dst_folder, filename)\n",
    "                shutil.move(src_path, dst_path)\n",
    "                print(f'Moved: {filename} to {dst_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "src_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_crop_220'\n",
    "dst_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/test'\n",
    "\n",
    "move_images_by_ids(src_folder, dst_folder, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "src_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_crop_220'\n",
    "dst_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/val'\n",
    "\n",
    "move_images_by_ids(src_folder, dst_folder, ids_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "src_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/raw/PE_Simple/Cropped_bbox_img_crop_220'\n",
    "dst_folder = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/train'\n",
    "\n",
    "move_images_by_ids(src_folder, dst_folder, ids_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5. save final list of all vid_id, img_id, bbox_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of full ids\n",
    "def full_ids_to_list(df, cols_to_combine):\n",
    "    \"\"\"\n",
    "    Combines the values of specified columns in each row of the DataFrame, \n",
    "    separated by an underscore, and returns a list of these combined values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    cols_to_combine (list): List of column names to combine.\n",
    "\n",
    "    Returns:\n",
    "    list: A list where each item is a combined string of the specified columns' values.\n",
    "    \"\"\"\n",
    "    # Use DataFrame's apply method to combine the columns row-wise\n",
    "    combined_list = df[cols_to_combine].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    \n",
    "    # Convert the combined Series to a list\n",
    "    return combined_list.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "ids_test_bbox = full_ids_to_list(df_full_annotation_norm_test, id_cols)\n",
    "print(ids_test_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ids_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "ids_val_bbox = full_ids_to_list(df_full_annotation_norm_val, id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "ids_train_bbox = full_ids_to_list(df_full_annotation_norm_train, id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lists to txt files\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_test_bbox.txt'\n",
    "save_list_to_file(ids_test_bbox, save_dir)\n",
    "\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_val_bbox.txt'\n",
    "\n",
    "save_list_to_file(ids_val_bbox, save_dir)\n",
    "\n",
    "save_dir = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_train_bbox.txt'\n",
    "\n",
    "save_list_to_file(ids_train_bbox, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Load image data into arr for train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1. load ids to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ids into list \n",
    "def load_file_to_list(file_path):\n",
    "    \"\"\"\n",
    "    loads a text file to a list with each entry on a new line becoming a new entry in the list.\n",
    "\n",
    "    :param file_path: Path to the file where the list should be saved.\n",
    "    :return list of data from file\n",
    "    \"\"\"\n",
    "    # Open the file for writing\n",
    "    lst = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Write each item on a new line\n",
    "        for line in file:\n",
    "            lst.append(line.strip())\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_test_bbox.txt'\n",
    "ids_test_bbox = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_val_bbox.txt'\n",
    "ids_val_bbox = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_train_bbox.txt'\n",
    "ids_train_bbox = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_test.txt'\n",
    "ids_test = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_val.txt'\n",
    "ids_val = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/ids_train.txt'\n",
    "ids_train = load_file_to_list(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2. load image data to arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the image data into an arr\n",
    "# in the same order as the annotations and ids are stored (use id list for this)\n",
    "\n",
    "# The load image data function may take a while to run\n",
    "\n",
    "def load_image_data(ids_to_load, image_folder, crop_ext):\n",
    "\n",
    "  # list for loading image data\n",
    "  selected_imgs = []\n",
    "\n",
    "  # for loop for loading image data that is present in the list of ids\n",
    "  for i, img_id in enumerate(ids_to_load):\n",
    "\n",
    "    # load the image\n",
    "    img_path = os.path.join(image_folder, img_id+crop_ext)\n",
    "    print(img_path)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    #print(img)\n",
    "\n",
    "    # change the img to RGB from BGR as plt uses RGB colour scale\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # scaling the pixel values to [0, 1] (you don't need to scal them back)\n",
    "    img = img/255\n",
    "\n",
    "    selected_imgs.append(img)\n",
    "\n",
    "  # Convert the list of images to a NumPy array\n",
    "  selected_imgs_array = np.array(selected_imgs)\n",
    "  \n",
    "  return selected_imgs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "image_folder_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/test'\n",
    "crop_extension = '_crop_220x220.jpg'\n",
    "\n",
    "test_imgs_array = load_image_data(ids_test_bbox, image_folder_path, crop_extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "image_folder_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/val'\n",
    "crop_extension = '_crop_220x220.jpg'\n",
    "\n",
    "val_imgs_array = load_image_data(ids_val_bbox, image_folder_path, crop_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "image_folder_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/images/train'\n",
    "crop_extension = '_crop_220x220.jpg'\n",
    "\n",
    "train_imgs_array = load_image_data(ids_train_bbox, image_folder_path, crop_extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3. load annotations to df and then keypoints to arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3.1. loading dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json to a df test\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/test_annotation_simple.json'\n",
    "df_full_annotation_norm_test = json_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm_test = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json to a df val\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/val_annotation_simple.json'\n",
    "df_full_annotation_norm_val = json_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm_val = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json to a df train\n",
    "path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/data/processed/PE_Simple/annotation/train_annotation_simple.json'\n",
    "df_full_annotation_norm_train = json_to_df(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_annotation_norm_train = set_dtypes_df_full_annotation_abs(df_full_annotation_norm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make the db easier to work with I am going to create a list with the kp col names, bbox col names, id col names\n",
    "id_cols = df_full_annotation_norm_test.iloc[:, :3].columns.to_list()\n",
    "bbox_cols = df_full_annotation_norm_test.iloc[:, 3:7].columns.to_list()\n",
    "kp_cols = df_full_annotation_norm_test.iloc[:, 7:23].columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kp_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3.2. loading the keypoint annotation into an arr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_lists(df_to_list, list_of_cols):\n",
    "\n",
    "  # create temp lists\n",
    "  keypoints_temp = []\n",
    "\n",
    "  # step through the rows and\n",
    "  for _, row in df_to_list.iterrows():\n",
    "\n",
    "    # extract the data arrays\n",
    "    keypoints_data = row[list_of_cols].values\n",
    "\n",
    "    # adding data to the list\n",
    "    keypoints_temp.append(keypoints_data)\n",
    "\n",
    "  # Convert the list to a NumPy array and make sure that they are float32\n",
    "  keypoints_array = np.array(keypoints_temp, dtype=np.float32)\n",
    "  \n",
    "  return keypoints_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_kp_array = create_data_lists(df_full_annotation_norm_test, kp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val\n",
    "val_kp_array = create_data_lists(df_full_annotation_norm_val, kp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_kp_array = create_data_lists(df_full_annotation_norm_train, kp_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.51. Augment train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = train_imgs_array.shape[0]\n",
    "print(num_imgs)\n",
    "num_kp = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize keypoints for an array of images\n",
    "def unnorm_keypoints_arr(kp_arr, img_arr):\n",
    "    \"\"\"\n",
    "    Denormalizes keypoints for each image in the array based on the corresponding image size.\n",
    "    It converts normalized keypoints (range [-1, 1]) back to pixel coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - kp_arr: Array of normalized keypoints, where each entry is a list of keypoints for an image.\n",
    "              The keypoints are expected to be in the format [x1, y1, x2, y2, ...].\n",
    "    - img_arr: Array of images. The size of each image is used to scale the keypoints back \n",
    "               to their pixel coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - kp_abs_arr: Array of denormalized keypoints where each entry corresponds to the denormalized \n",
    "                  keypoints for the corresponding image in `img_arr`.\n",
    "    \"\"\"\n",
    "\n",
    "    kp_abs_list = []  # List to store the denormalized keypoints for each image\n",
    "\n",
    "    # Iterate through each set of keypoints and corresponding image\n",
    "    for i, kp in enumerate(kp_arr):\n",
    "        img_size = img_arr[i].shape  # Get the size of the current image (height, width, channels)\n",
    "\n",
    "        # Denormalize the keypoints based on the image size\n",
    "        kp_abs, missing_kp = unnorm_keypoints(img_size, kp_arr[i])\n",
    "\n",
    "        # Save the denormalized keypoints to the list\n",
    "        kp_abs_list.append(kp_abs)\n",
    "    \n",
    "    # Convert the list of denormalized keypoints to a NumPy array\n",
    "    kp_abs_arr = np.array(kp_abs_list)\n",
    "\n",
    "    return kp_abs_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize keypoints for an array of images\n",
    "def norm_keypoints_arr(kp_arr, img_arr):\n",
    "    \"\"\"\n",
    "    Normalizes keypoints for each image in the array based on the corresponding image size.\n",
    "    It converts keypoints from pixel coordinates back to normalized coordinates (range [-1, 1]).\n",
    "\n",
    "    Parameters:\n",
    "    - kp_arr: Array of keypoints, where each entry is a list of keypoints for an image.\n",
    "              The keypoints are expected to be in the format [x1, y1, x2, y2, ...] \n",
    "              with pixel coordinates.\n",
    "    - img_arr: Array of images. The size of each image is used to scale the keypoints \n",
    "               to normalized coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - kp_norm_arr: Array of normalized keypoints where each entry corresponds to the normalized \n",
    "                   keypoints for the corresponding image in `img_arr`.\n",
    "    \"\"\"\n",
    "        \n",
    "    kp_norm_list = []  # List to store the normalized keypoints for each image\n",
    "\n",
    "    # Iterate through each set of keypoints and corresponding image\n",
    "    for i, kp in enumerate(kp_arr):\n",
    "        img_size = img_arr[i].shape  # Get the size of the current image (height, width, channels)\n",
    "\n",
    "        # Normalize the keypoints based on the image size\n",
    "        kp_norm = norm_keypoints(img_size, kp_arr[i])\n",
    "\n",
    "        # Save the normalized keypoints to the list\n",
    "        kp_norm_list.append(kp_norm)\n",
    "    \n",
    "    # Convert the list of normalized keypoints to a NumPy array\n",
    "    kp_norm_arr = np.array(kp_norm_list)  \n",
    "\n",
    "    return kp_norm_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation to images and keypoints\n",
    "def apply_aug(img_arr_orig, kp_arr_orig, aug, num_of_kp=8):\n",
    "    \"\"\"\n",
    "    Applies augmentation to a batch of images and their corresponding keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    - img_arr_orig: Original array of images. Shape should be (num_imgs, height, width, channels).\n",
    "    - kp_arr_orig: Original array of keypoints. Shape should be (num_imgs, num_of_kp*2), where each \n",
    "                   keypoint is represented by its x and y coordinates in pixel values.\n",
    "    - aug: An imgaug augmentation sequence or augmenter to apply to the images and keypoints.\n",
    "    - num_of_kp: Optional. Number of keypoints per image (default is 8).\n",
    "\n",
    "    Returns:\n",
    "    - img_arr_aug: Augmented array of images. Same shape as `img_arr_orig`.\n",
    "    - kp_arr_aug: Augmented array of keypoints. Same shape as `kp_arr_orig`.\n",
    "    \"\"\"\n",
    "    # print(img_arr_orig.shape)\n",
    "    #print(kp_arr_orig.shape)\n",
    "    \n",
    "    # Initialize lists to store augmented images and keypoints\n",
    "    aug_img = []  # List for augmented images\n",
    "    aug_kp = []   # List for augmented keypoints\n",
    "\n",
    "    # Get the number of images in the batch\n",
    "    num_imgs = img_arr_orig.shape[0]\n",
    "    #print(num_imgs)\n",
    "\n",
    "    # Loop over each image and its corresponding keypoints\n",
    "    for i in range(num_imgs):\n",
    "        image = img_arr_orig[i]  # Extract the i-th image\n",
    "        #print(image.shape)\n",
    "        \n",
    "        # Convert keypoints to KeypointsOnImage format for imgaug\n",
    "        keypoints = kp_arr_orig[i]\n",
    "        #print(keypoints)\n",
    "        kps = [Keypoint(x=keypoints[j*2], y=keypoints[j*2+1]) for j in range(num_of_kp)]\n",
    "        kps_on_image = KeypointsOnImage(kps, shape=image.shape)\n",
    "        \n",
    "        # Apply the augmentation to the image and keypoints\n",
    "        image_aug, kps_aug = aug(image=image, keypoints=kps_on_image)\n",
    "        \n",
    "        # Convert augmented keypoints back to the original flattened format [x1, y1, x2, y2, ...]\n",
    "        keypoints_aug = []\n",
    "        for kp in kps_aug.keypoints:\n",
    "            keypoints_aug.extend([kp.x, kp.y])\n",
    "        \n",
    "        # Append the augmented image and keypoints to their respective lists\n",
    "        aug_img.append(image_aug)\n",
    "        aug_kp.append(keypoints_aug)\n",
    "\n",
    "    # Convert the lists of augmented images and keypoints back to NumPy arrays\n",
    "    img_arr_aug = np.array(aug_img)\n",
    "    kp_arr_aug = np.array(aug_kp)\n",
    "\n",
    "    return img_arr_aug, kp_arr_aug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.51.1. Apply a lrflip to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify augmentation\n",
    "seq_lrflip = iaa.Sequential([\n",
    "    iaa.Fliplr(1.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply augmentation\n",
    "\n",
    "# unnorm kp\n",
    "train_kp_array_abs = unnorm_keypoints_arr(train_kp_array, train_imgs_array)\n",
    "\n",
    "# apply augmentation\n",
    "train_imgs_array_aug_lrflip, train_kp_array_aug_lrflip_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_lrflip)\n",
    "\n",
    "# norm the aug kp\n",
    "train_kp_array_aug_lrflip_norm = norm_keypoints_arr(train_kp_array_aug_lrflip_abs, train_imgs_array_aug_lrflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lrflip\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array_aug_lrflip[150], train_kp_array_aug_lrflip_abs[150], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check original\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array[150], train_kp_array_abs[150], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.51.2. Apply a random rotation (5:20 deg) clockwise and anticlockwise(-20:-5 deg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify augmentation\n",
    "seq_rotate_clock = iaa.Sequential([\n",
    "    iaa.Affine(rotate=(5, 20)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply augmentation\n",
    "\n",
    "# unnorm kp\n",
    "train_kp_array_abs = unnorm_keypoints_arr(train_kp_array, train_imgs_array)\n",
    "\n",
    "# apply augmentation\n",
    "train_imgs_array_aug_rclock, train_kp_array_aug_rclock_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_rotate_clock)\n",
    "\n",
    "# norm the aug kp\n",
    "train_kp_array_aug_rclock_norm = norm_keypoints_arr(train_kp_array_aug_rclock_abs, train_imgs_array_aug_rclock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rclock\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array_aug_rclock[300], train_kp_array_aug_rclock_abs[300], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check original\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array[300], train_kp_array_abs[300], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify augmentation\n",
    "seq_rotate_anticlock = iaa.Sequential([\n",
    "    iaa.Affine(rotate=(-20, -5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply augmentation\n",
    "\n",
    "# unnorm kp\n",
    "train_kp_array_abs = unnorm_keypoints_arr(train_kp_array, train_imgs_array)\n",
    "\n",
    "# apply augmentation\n",
    "train_imgs_array_aug_ranticlock, train_kp_array_aug_ranticlock_abs = apply_aug(train_imgs_array, train_kp_array_abs, seq_rotate_anticlock)\n",
    "\n",
    "# norm the aug kp\n",
    "train_kp_array_aug_ranticlock_norm = norm_keypoints_arr(train_kp_array_aug_ranticlock_abs, train_imgs_array_aug_ranticlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rclock\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array_aug_ranticlock[300], train_kp_array_aug_ranticlock_abs[300], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check original\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array[300], train_kp_array_abs[300], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.51.3. Apply a translation either up and down or left and right by the amount of padding in img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD FUNCTION\n",
    "# def detect_padding(image):\n",
    "#     \"\"\"\n",
    "#     Detects if padding is on the x-axis (left and right) or y-axis (top and bottom) \n",
    "#     of the image and calculates the padding size on one side.\n",
    "\n",
    "#     Parameters:\n",
    "#     - image: A NumPy array representing the image. The shape should be (height, width, channels).\n",
    "\n",
    "#     Returns:\n",
    "#     - is_padding_x: True if padding is on the x-axis, False if padding is on the y-axis.\n",
    "#     - padding_size: The size of the padding on one side in pixels.\n",
    "#     \"\"\"\n",
    "\n",
    "#     height, width, _ = image.shape\n",
    "    \n",
    "#     # Check for padding along the x-axis (left and right)\n",
    "#     left_column = image[:, 0, :]  # The first column (left side)\n",
    "#     right_column = image[:, -1, :]  # The last column (right side)\n",
    "#     # Check for padding along the x-axis (left and right)\n",
    "#     top_row = image[0, :, :]  # The first column (left side)\n",
    "#     bottom_row = image[-1, :, :]  # The last column (right side)\n",
    "#     print(left_column)\n",
    "#     #print(right_column)\n",
    "    \n",
    "#     # Check if the columns are fully black (indicating padding)\n",
    "#     if np.all(left_column < 1) and np.all(right_column < 1):\n",
    "#         # Padding is along the x-axis\n",
    "#         is_padding_x = True\n",
    "#         # Calculate padding size\n",
    "#         print(image[:, 30, 0]*255)\n",
    "#         plot_img(image)\n",
    "#         padding_size = np.sum(image[:, 0, 0] < 1) // 2  # Count black pixels on one side\n",
    "#     else:\n",
    "#         #plot_img(image)\n",
    "#         # Padding is along the y-axis (top and bottom)\n",
    "#         is_padding_x = False\n",
    "#         # Calculate padding size\n",
    "#         padding_size = np.sum(image[0, :, 0] < 1) // 2  # Count black pixels on one side\n",
    "#         #print(padding_size)\n",
    "\n",
    "#     return is_padding_x, padding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_padding(image):\n",
    "    \"\"\"\n",
    "    Detects if padding is on the x-axis (left and right) or y-axis (top and bottom)\n",
    "    of the image and calculates the padding size on one side.\n",
    "\n",
    "    Parameters:\n",
    "    - image: A NumPy array representing the image. The shape should be (width, height, channels).\n",
    "\n",
    "    Returns:\n",
    "    - is_padding_x: True if padding is on the x-axis, False if padding is on the y-axis.\n",
    "    - padding_size: The size of the padding on one side in pixels.\n",
    "    \"\"\"\n",
    "\n",
    "    width, height, _ = image.shape\n",
    "    \n",
    "    # Check for padding along the x-axis (left and right)\n",
    "    left_column = image[:, 0, :]#image[0, :, :]  # The first column (left side)\n",
    "    right_column = image[:, -1, :] #image[-1, :, :]  # The last column (right side)\n",
    "\n",
    "    # Check for padding along the y-axis (top and bottom)\n",
    "    top_row = image[:, 0, :]  # The first row (top side)\n",
    "    bottom_row = image[:, -1, :]  # The last row (bottom side)\n",
    "    #print(image[:, 5, :] *255)\n",
    "    #print(left_column*255)\n",
    "    \n",
    "    # Check if the columns are fully black (indicating padding)\n",
    "    if np.all(left_column*255 < 30) and np.all(right_column*255 < 30):\n",
    "        # Padding is along the x-axis\n",
    "        is_padding_x = True\n",
    "        #plot_img(image)\n",
    "        # Calculate padding size\n",
    "        #padding_size = np.sum(image[0, :, 0]*255 < 30) // 2  # Count black pixels on one side\n",
    "        # if padding_size > 60:\n",
    "        sum1 = np.sum(image[5, :, 0]*255 < 20) // 2\n",
    "        sum2 = np.sum(image[10, :, 0]*255 < 20) // 2\n",
    "        sum3 = np.sum(image[60, :, 0]*255 < 20) // 2\n",
    "        sum4 = np.sum(image[110, :, 0]*255 < 20) // 2\n",
    "        sum5 = np.sum(image[-60, :, 0]*255 < 20) // 2\n",
    "        sum6 = np.sum(image[-10, :, 0]*255 < 20) // 2\n",
    "        sum7 = np.sum(image[-5, :, 0]*255 < 20) // 2\n",
    "        padding_size = min(sum1, sum2, sum3, sum4, sum5, sum6, sum7)\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        # if padding_size > 60:\n",
    "        #     sum1 = np.sum(image[5, :, 0]*255 < 10) // 2\n",
    "        #     sum2 = np.sum(image[10, :, 0]*255 < 10) // 2\n",
    "        #     sum3 = np.sum(image[60, :, 0]*255 < 10) // 2\n",
    "        #     sum4 = np.sum(image[110, :, 0]*255 < 10) // 2\n",
    "        #     sum5 = np.sum(image[-60, :, 0]*255 < 10) // 2\n",
    "        #     sum6 = np.sum(image[-10, :, 0]*255 < 10) // 2\n",
    "        #     sum7 = np.sum(image[-5, :, 0]*255 < 10) // 2\n",
    "        #     average = (sum1 + sum2 + sum3 + sum4 + sum5 + sum6 + sum7) // 7  # Floor division for rounding down\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        if padding_size > 20: \n",
    "            padding_size = 20\n",
    "\n",
    "    else:\n",
    "        # Padding is along the y-axis (top and bottom)\n",
    "        is_padding_x = False\n",
    "        # Calculate padding size\n",
    "        padding_size = np.sum(image[:, 0, 0]*255 < 30) // 2  # Count black pixels on one side\n",
    "        # if padding_size > 60:\n",
    "        sum1 = np.sum(image[:, 5, 0]*255 < 20) // 2\n",
    "        sum2 = np.sum(image[:, 10, 0]*255 < 20) // 2\n",
    "        sum3 = np.sum(image[:, 60, 0]*255 < 20) // 2\n",
    "        sum4 = np.sum(image[:, 110, 0]*255 < 20) // 2\n",
    "        sum5 = np.sum(image[:, -60, 0]*255 < 20) // 2\n",
    "        sum6 = np.sum(image[:, -10, 0]*255 < 20) // 2\n",
    "        sum7 = np.sum(image[:, -5, 0]*255 < 20) // 2\n",
    "        padding_size = min(sum1, sum2, sum3, sum4, sum5, sum6, sum7)\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        # if padding_size > 60:\n",
    "        #     sum1 = np.sum(image[:, 5, 0]*255 < 10) // 2\n",
    "        #     sum2 = np.sum(image[:, 10, 0]*255 < 10) // 2\n",
    "        #     sum3 = np.sum(image[:, 60, 0]*255 < 10) // 2\n",
    "        #     sum4 = np.sum(image[:, 110, 0]*255 < 10) // 2\n",
    "        #     sum5 = np.sum(image[:, -60, 0]*255 < 10) // 2\n",
    "        #     sum6 = np.sum(image[:, -10, 0]*255 < 10) // 2\n",
    "        #     sum7 = np.sum(image[:, -5, 0]*255 < 10) // 2\n",
    "        #     average = (sum1 + sum2 + sum3 + sum4 + sum5 + sum6 + sum7) // 7  # Floor division for rounding down\n",
    "        #     padding_size = max(average - 5, 1)\n",
    "        if padding_size > 20: \n",
    "            padding_size = 20\n",
    "\n",
    "    return is_padding_x, padding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img_arr = train_imgs_array[0:2]\n",
    "# test_img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_kp_arr = train_kp_array[0:2]\n",
    "# test_kp_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnorm kp\n",
    "train_kp_array_abs = unnorm_keypoints_arr(train_kp_array, train_imgs_array)\n",
    "# print(train_kp_array_abs)\n",
    "# apply augmentation\n",
    "\n",
    "# Get the number of images in the batch\n",
    "num_imgs = train_imgs_array.shape[0]\n",
    "# print(num_imgs)\n",
    "\n",
    "# creat empty arrays\n",
    "train_imgs_array_aug_trans = np.empty((0, train_imgs_array.shape[1], train_imgs_array.shape[2], train_imgs_array.shape[3]), dtype=train_imgs_array.dtype)\n",
    "train_kp_array_aug_trans = np.empty((0, train_kp_array_abs.shape[1]), dtype=train_kp_array_abs.dtype)\n",
    "\n",
    "# print(train_imgs_array_aug_trans.shape)\n",
    "# print(train_kp_array_aug_trans.shape)\n",
    "\n",
    "# Loop over each image and its corresponding keypoints\n",
    "for i in range(num_imgs):\n",
    "    image = train_imgs_array[i]  # Extract the i-th image\n",
    "    kp = train_kp_array_abs[i]\n",
    "    # print(i)\n",
    "    # print(image.shape)\n",
    "    # print(kp.shape)\n",
    "\n",
    "    is_padding_x, padding_size = detect_padding(image)\n",
    "    print(f'this: {i}')\n",
    "    print(is_padding_x)\n",
    "    print(padding_size)\n",
    "\n",
    "    if is_padding_x:\n",
    "        seq_trans_x_left = iaa.Sequential([\n",
    "            iaa.TranslateX(px=(-padding_size, -padding_size)),\n",
    "        ])\n",
    "        seq_trans_x_right = iaa.Sequential([\n",
    "            iaa.TranslateX(px=(padding_size, padding_size)),\n",
    "        ])\n",
    "\n",
    "        # Convert to shape (1, 220, 220, 3) and (1, 16)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        #print(is_padding_x)\n",
    "        #print(image.shape)\n",
    "        #print(i)\n",
    "        kp = np.expand_dims(kp, axis=0)\n",
    "\n",
    "        # apply augmentations\n",
    "        single_trans_x_left_img_arr, single_trans_x_left_kp_arr = apply_aug(image, kp, seq_trans_x_left)\n",
    "        single_trans_x_right_img_arr, single_trans_x_right_kp_arr = apply_aug(image, kp, seq_trans_x_right)\n",
    "\n",
    "        #save to image array\n",
    "        train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_x_left_img_arr), axis=0)\n",
    "        train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_x_right_img_arr), axis=0)\n",
    "        #save to kp array\n",
    "        train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_x_left_kp_arr), axis=0)\n",
    "        train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_x_right_kp_arr), axis=0)\n",
    "\n",
    "    else :\n",
    "        seq_trans_y_up = iaa.Sequential([\n",
    "            iaa.TranslateY(px=(-padding_size, -padding_size)),\n",
    "        ])\n",
    "        seq_trans_y_down = iaa.Sequential([\n",
    "            iaa.TranslateY(px=(padding_size, padding_size)),\n",
    "        ])\n",
    "\n",
    "        # Convert to shape (1, 220, 220, 3) and (1, 16)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        #print(is_padding_x)\n",
    "        #print(image.shape)\n",
    "        #print(i)\n",
    "        kp = np.expand_dims(kp, axis=0)\n",
    "\n",
    "        # apply augmentations\n",
    "        single_trans_y_up_img_arr, single_trans_y_up_kp_arr = apply_aug(image, kp, seq_trans_y_up)\n",
    "        single_trans_y_down_img_arr, single_trans_y_down_kp_arr = apply_aug(image, kp, seq_trans_y_down)\n",
    "\n",
    "        #save to image array\n",
    "        train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_y_up_img_arr), axis=0)\n",
    "        train_imgs_array_aug_trans = np.concatenate((train_imgs_array_aug_trans, single_trans_y_down_img_arr), axis=0)\n",
    "        #save to kp array\n",
    "        train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_y_up_kp_arr), axis=0)\n",
    "        train_kp_array_aug_trans = np.concatenate((train_kp_array_aug_trans, single_trans_y_down_kp_arr), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_check = 231\n",
    "\n",
    "# print(train_imgs_array[image_check,:, -1, :]*255)\n",
    "# print(train_imgs_array[image_check,0, :, :]*255)\n",
    "# print(np.sum(train_imgs_array[image_check, :, 0, 0]*255 < 30))\n",
    "# print(np.sum(train_imgs_array[image_check, :, 0, 0]*255 < 30)//2)\n",
    "# plot_img(train_imgs_array[image_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_imgs_array_aug_trans.shape)\n",
    "print(train_kp_array_aug_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm the aug kp\n",
    "train_kp_array_aug_trans_norm = norm_keypoints_arr(train_kp_array_aug_trans, train_imgs_array_aug_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check trans\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array_aug_trans[561], train_kp_array_aug_trans[561], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check trans\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array_aug_trans[560], train_kp_array_aug_trans[560], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check original\n",
    "labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "plot_img_and_keypoint(train_imgs_array[280], train_kp_array_abs[280], nkeypoints=8, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.51.4 Combine the simple augmentation datasets with the original dataset to create simple_aug_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat empty arrays\n",
    "#train_imgs_array_aug_simple = np.empty((0, train_imgs_array.shape[1], train_imgs_array.shape[2], train_imgs_array.shape[3]), dtype=train_imgs_array.dtype)\n",
    "#train_kp_array_aug_simple = np.empty((0, train_kp_array_abs.shape[1]), dtype=train_kp_array_abs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine arrays\n",
    "#save to image array\n",
    "train_imgs_array_aug_simple = np.concatenate((train_imgs_array, train_imgs_array_aug_lrflip), axis=0)\n",
    "train_imgs_array_aug_simple = np.concatenate((train_imgs_array_aug_simple, train_imgs_array_aug_rclock), axis=0)\n",
    "train_imgs_array_aug_simple = np.concatenate((train_imgs_array_aug_simple, train_imgs_array_aug_ranticlock), axis=0)\n",
    "train_imgs_array_aug_simple = np.concatenate((train_imgs_array_aug_simple, train_imgs_array_aug_trans), axis=0)\n",
    "#save to kp array\n",
    "train_kp_array_aug_simple = np.concatenate((train_kp_array, train_kp_array_aug_lrflip_norm), axis=0)\n",
    "train_kp_array_aug_simple = np.concatenate((train_kp_array_aug_simple, train_kp_array_aug_rclock_norm), axis=0)\n",
    "train_kp_array_aug_simple = np.concatenate((train_kp_array_aug_simple, train_kp_array_aug_ranticlock_norm), axis=0)\n",
    "train_kp_array_aug_simple = np.concatenate((train_kp_array_aug_simple, train_kp_array_aug_trans_norm), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_imgs_array_aug_simple.shape)\n",
    "print(train_kp_array_aug_simple.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.51.5. Ensure that all kp are within the image frame and shift them in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_out_of_img_kp_rows(arr):\n",
    "    \"\"\"\n",
    "    Finds the number and positions of rows that contain numbers lower than -0.5 but not -10 and greater than 0.5\n",
    "    THese are keypoints that are outside the frame but not the missing ones.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: A NumPy array of shape (n, 16).\n",
    "\n",
    "    Returns:\n",
    "    - count_neg_rows: The number of rows that contain negative numbers.\n",
    "    - neg_row_indices: A list of indices of rows that contain negative numbers.\n",
    "    \"\"\"\n",
    "    # Check which rows contain negative numbers\n",
    "    neg_row_mask = np.any(((arr < -0.5) & (arr > -9.0)) | (arr > 0.5), axis=1)\n",
    "    \n",
    "    # Get the indices of rows that contain negative numbers\n",
    "    neg_row_indices = np.where(neg_row_mask)[0]\n",
    "    \n",
    "    # Count the number of rows with negative numbers\n",
    "    count_neg_rows = len(neg_row_indices)\n",
    "    \n",
    "    return count_neg_rows, neg_row_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_types = set(type(element) for element in train_kp_array_aug_simple.flatten())\n",
    "print(unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes\n",
    "train_kp_array_aug_simple.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all the kp are within the image\n",
    "print(find_out_of_img_kp_rows(train_kp_array_aug_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_out_of_img_kp(arr):\n",
    "    \"\"\"\n",
    "    Finds and replaces the elements in the array that are outside the frame but not the missing ones.\n",
    "    Specifically, elements greater than 0.5 are replaced with 0.5, and elements less than -0.5 but \n",
    "    greater than -9.0 are replaced with -0.5.\n",
    "\n",
    "    Parameters:\n",
    "    - arr: A NumPy array of shape (n, 16).\n",
    "\n",
    "    Returns:\n",
    "    - modified_arr: The modified NumPy array with replaced values.\n",
    "    - count_replacements: The number of elements that were replaced.\n",
    "    \"\"\"\n",
    "    # Make a copy of the array to avoid modifying the original array\n",
    "    modified_arr = arr.copy()\n",
    "\n",
    "    # Replace elements greater than 0.5 with 0.5\n",
    "    count_pos_replacements = np.sum(modified_arr > 0.5)\n",
    "    modified_arr[modified_arr > 0.5] = 0.49\n",
    "\n",
    "    # Replace elements less than -0.5 but greater than -9.0 with -0.5\n",
    "    count_neg_replacements = np.sum((modified_arr < -0.5) & (modified_arr > -9.0))\n",
    "    modified_arr[(modified_arr < -0.5) & (modified_arr > -9.0)] = -0.49\n",
    "\n",
    "    # Total count of replacements\n",
    "    count_replacements = count_pos_replacements + count_neg_replacements\n",
    "\n",
    "    return modified_arr, count_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kp_array_aug_simple, num_replacements = replace_out_of_img_kp(train_kp_array_aug_simple)\n",
    "print(num_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_out_of_img_kp_rows(train_kp_array_aug_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img):\n",
    "  fig = plt.figure(figsize=(8, 25), dpi=100)\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.imshow(img)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_and_keypoint(img, keypoints, nkeypoints, keypoint_labels):\n",
    "  fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "  plt.imshow(img)\n",
    "  x_keypoints = keypoints[::2]\n",
    "  y_keypoints = keypoints[1::2]\n",
    "  plt.scatter(x_keypoints, y_keypoints, marker='.', c=np.arange(nkeypoints), cmap='jet')\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "  if keypoint_labels is not None:\n",
    "      for i, (x, y) in enumerate(zip(x_keypoints, y_keypoints)):\n",
    "          plt.text(x, y, keypoint_labels[i], fontsize=12, color='white', \n",
    "                    bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unnorm_keypoints(img_size, keypoints, kp_to_null=None):\n",
    "\n",
    "#   readjust_x = img_size[0]\n",
    "#   readjust_y = img_size[1]\n",
    "#   #print(readjust_x)\n",
    "#   #print(readjust_y)\n",
    "#   new_keypoints = []\n",
    "#   missing_kp = []\n",
    "\n",
    "#   for i, keypoint in enumerate(keypoints):\n",
    "#     # Null keypoints at specified indices\n",
    "#     #print(kp_to_null)\n",
    "#     if keypoint == -10 or (kp_to_null and i in kp_to_null):\n",
    "#       keypoint = np.nan\n",
    "#       #print(missing_kp)\n",
    "#       missing_kp.append(i)\n",
    "\n",
    "#     if i % 2 == 0:\n",
    "#       keypoint = keypoint * readjust_x + readjust_x/2\n",
    "#       #print(i, keypoint, 'x')\n",
    "#     else:\n",
    "#       keypoint = keypoint * readjust_y + readjust_y/2\n",
    "#       #print(i, keypoint, 'y')\n",
    "#     #print(keypoint)\n",
    "#     new_keypoints.append(keypoint)\n",
    "#   #print(new_keypoints)\n",
    "#   return new_keypoints, missing_kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img = 259\n",
    "chosen_img = train_imgs_array[display_img]\n",
    "chosen_img_size = chosen_img.shape\n",
    "print(chosen_img_size)\n",
    "#print(original_img_shape)\n",
    "chosen_img_keypoints = train_kp_array[display_img]\n",
    "nkeypoints = 8\n",
    "keypoint_labels = kp_cols[::2]\n",
    "\n",
    "display_keypoints, missing_kp = unnorm_keypoints(chosen_img_size, chosen_img_keypoints)\n",
    "\n",
    "plot_img_and_keypoint(chosen_img, display_keypoints, 8, keypoint_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img = 1013\n",
    "chosen_img = train_imgs_array_aug_simple[display_img]\n",
    "chosen_img_size = chosen_img.shape\n",
    "print(chosen_img_size)\n",
    "print(chosen_img_size)\n",
    "#print(original_img_shape)\n",
    "chosen_img_keypoints = train_kp_array_aug_simple_adjust[display_img]\n",
    "nkeypoints = 8\n",
    "keypoint_labels = kp_cols[::2]\n",
    "\n",
    "display_keypoints, missing_kp = unnorm_keypoints(chosen_img_size, chosen_img_keypoints)\n",
    "\n",
    "plot_img_and_keypoint(chosen_img, display_keypoints, 8, keypoint_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1. Define the Loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error, ignoring the invisible keypoints.\n",
    "    Assuming that -10.0 indicates an invisible keypoint.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints from both\n",
    "    # the predictions and the true values\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # Compute the Mean Squared Error only on the visible keypoints\n",
    "    mse = F.mse_loss(y_pred_masked, y_true_masked, reduction='sum') / mask.sum()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL IMPLEMENTATION OF THE ABOVE\n",
    "# def masked_rmse_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Computes the Root Mean Square Error (RMSE) loss, ignoring the invisible keypoints (denoted by -10).\n",
    "    \n",
    "#     Parameters:\n",
    "#     y_true (torch.Tensor): The ground truth keypoints (batch_size, num_keypoints*2).\n",
    "#     y_pred (torch.Tensor): The predicted keypoints (batch_size, num_keypoints*2).\n",
    "\n",
    "#     Returns:\n",
    "#     torch.Tensor: The computed RMSE loss.\n",
    "#     \"\"\"\n",
    "#     # Create a mask where keypoints are visible (not equal to -10)\n",
    "#     mask = (y_true != -10.0).float()\n",
    "\n",
    "#     # Apply the mask to filter out invisible keypoints\n",
    "#     y_true_masked = y_true * mask\n",
    "#     y_pred_masked = y_pred * mask\n",
    "\n",
    "#     # Compute the squared differences\n",
    "#     squared_diff = (y_pred_masked - y_true_masked) ** 2\n",
    "\n",
    "#     # Compute the mean of squared differences for visible keypoints\n",
    "#     loss = torch.sum(squared_diff) / torch.sum(mask)\n",
    "\n",
    "#     # Return the square root of the loss to get RMSE\n",
    "#     return torch.sqrt(loss)\n",
    "\n",
    "# # Example usage:\n",
    "# # Assume y_true and y_pred are your ground truth and predicted keypoints, respectively.\n",
    "# # y_true = torch.tensor([...])\n",
    "# # y_pred = torch.tensor([...])\n",
    "# # loss = masked_rmse_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2. Define the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCK\n",
    "# put in a function that will use the max bbox if primary kp is missing\n",
    "def pck_metric(y_true, y_pred, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Computes the Percentage of Correct Keypoints (PCK) metric.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (torch.Tensor): The ground truth keypoints (batch_size, num_keypoints*2).\n",
    "    y_pred (torch.Tensor): The predicted keypoints (batch_size, num_keypoints*2).\n",
    "    threshold (float): The distance threshold for a keypoint to be considered correct.\n",
    "                       Typically set relative to the size of the bounding box (e.g., 0.2).\n",
    "    \n",
    "    Returns:\n",
    "    float: The percentage of correct keypoints.\n",
    "    \"\"\"\n",
    "    # Create a mask where keypoints are visible (not equal to -10)\n",
    "    mask = (y_true != -10.0).float().to(y_true.device)\n",
    "    #print(mask)\n",
    "\n",
    "    # Apply the mask to filter out invisible keypoints\n",
    "    y_true_masked = y_true * mask\n",
    "    y_pred_masked = y_pred * mask\n",
    "\n",
    "    # print(y_true_masked)\n",
    "    # print(y_pred_masked)\n",
    "\n",
    "    # Compute the Euclidean distance between the predicted and true keypoints\n",
    "    distances = torch.sqrt((y_pred_masked[:, ::2] - y_true_masked[:, ::2]) ** 2 +\n",
    "                           (y_pred_masked[:, 1::2] - y_true_masked[:, 1::2]) ** 2)\n",
    "    \n",
    "    #print(distances)\n",
    "    \n",
    "    # Normalize the distances (relative to the max and min y coord)\n",
    "    Norm_max_min_kp = torch.max(y_true_masked[:, 1::2], dim=1)[0] - torch.min(y_true_masked[:, 1::2], dim=1)[0]\n",
    "    # Normalise based on the distance between the head and the bottom of the body (position 0, 1 and )\n",
    "    #print(y_true[:, 0],y_true[:,10],y_true[:, 1],y_true[:, 11])\n",
    "    #print((y_true[:, 0] - y_true[:,10]) ** 2)\n",
    "    #print((y_true[:, 1] - y_true[:, 11]) ** 2)\n",
    "    Norm_head_lowerbody = torch.sqrt((y_true[:, 0] - y_true[:,10]) ** 2 +\n",
    "                        (y_true[:, 1] - y_true[:, 11]) ** 2)\n",
    "    #print(Norm_head_lowerbody)\n",
    "    normalized_distances = distances / Norm_head_lowerbody[:, None]\n",
    "    #print(distances)\n",
    "    #print(normalized_distances)\n",
    "\n",
    "    # Count the correct keypoints (distance <= threshold)\n",
    "    correct_keypoints = (normalized_distances <= threshold).float() * mask[:, ::2]\n",
    "    #print(correct_keypoints)\n",
    "\n",
    "    # Calculate the PCK as the percentage of correct keypoints\n",
    "    pck = correct_keypoints.sum() / mask[:, ::2].sum()\n",
    "    return pck#.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create two tensors to check pck\n",
    "\n",
    "# # Create two PyTorch tensors with the sizes (1, 16)\n",
    "# # Initialize them with random values between -1 and 1\n",
    "# tensor1_true = torch.rand(1, 16) * 2 - 1\n",
    "# tensor2_pred = tensor1_true.clone()\n",
    "\n",
    "# # creating a tensor with 2 predictions for an image (test that it will work for multiple inputs)\n",
    "# # tensor2_pred = tensor1_true.clone()\n",
    "\n",
    "# # Introduce some differences in tensor2\n",
    "# tensor2_pred[0, :8] += torch.randn(8) * 0.1  # Slightly off for the first element of the first row\n",
    "\n",
    "# tensor2_pred[0, :8] += torch.randn(8) * 0.1  # Slightly off for the first 8 elements of the first row\n",
    "# #tensor2_pred[1, 8:] += torch.randn(8) * 0.1  # Slightly off for the last 8 elements of the second row\n",
    "\n",
    "# # Ensure the values are still within the range [-1, 1]\n",
    "# tensor2_pred = torch.clamp(tensor2_pred, min=-1, max=1)\n",
    "\n",
    "# print(tensor1_true, tensor2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tensor1_true, tensor2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor1_true[0, 5] = -10\n",
    "# tensor1_true[0, 4] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pck_metric(tensor1_true, tensor2_pred, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.3. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First model (not correct sizes)\n",
    "# class DeepPoseModel(nn.Module):\n",
    "#     def __init__(self, nkeypoints=8):\n",
    "#         # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "#         super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "#         # The feature extractor part of the model, composed of several convolutional layers.\n",
    "#         self.features = nn.Sequential(\n",
    "#             # Conv2d: Input channels = 3 (RGB image), Output channels = 96, kernel size = 11x11,\n",
    "#             # stride = 4, padding = 4. \n",
    "#             # Input: (batch_size, 3, 220, 220)\n",
    "#             # Output: (batch_size, 96, 55, 55)\n",
    "#             nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # output size: (batch_size, 96, 27, 27)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 96, Output channels = 256, kernel size = 5x5,\n",
    "#             # stride = 2, padding = 2.\n",
    "#             # Input: (batch_size, 96, 27, 27)\n",
    "#             # Output: (batch_size, 256, 27, 27)\n",
    "#             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "#             # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "#             nn.LocalResponseNorm(5),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             # output size: (batch_size, 96, 13, 13)\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # Conv2d: Input channels = 256, Output channels = 384, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 256, 13, 13)\n",
    "#             # Output: (batch_size, 384, 13, 13)\n",
    "#             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 384, Output channels = 384, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 384, 13, 13)\n",
    "#             # Output: (batch_size, 384, 13, 13)\n",
    "#             nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Conv2d: Input channels = 384, Output channels = 256, kernel size = 3x3,\n",
    "#             # stride = 1, padding = 1.\n",
    "#             # Input: (batch_size, 384, 13, 13)\n",
    "#             # Output: (batch_size, 256, 13, 13)\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "#             # Input: (batch_size, 256, 13, 13)\n",
    "#             # Output: (batch_size, 256, 6, 6)\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         # The classifier part of the model, composed of fully connected layers.\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten the input tensor\n",
    "#             # Input: (batch_size, 256, 6, 6)\n",
    "#             # Output: (batch_size, 256 * 6 * 6) = (batch_size, 9216)\n",
    "#             nn.Flatten(),\n",
    "            \n",
    "#             # Linear layer with input size 6400 and output size 4096\n",
    "#             # Input: (batch_size, 6400)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             nn.Linear(256 * 6 * 6, 4096),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             nn.Dropout(0.6),\n",
    "            \n",
    "#             # Linear layer with input size 4096 and output size 4096\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, 4096)\n",
    "#             nn.Linear(4096, 4096),\n",
    "            \n",
    "#             # ReLU activation function applied in place (no extra memory allocation)\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # Dropout layer with 60% dropout rate\n",
    "#             nn.Dropout(0.6),\n",
    "            \n",
    "#             # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "#             # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "#             # Input: (batch_size, 4096)\n",
    "#             # Output: (batch_size, nkeypoints * 2)\n",
    "#             nn.Linear(4096, nkeypoints * 2)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Define the forward pass through the network.\n",
    "#         # Pass input `x` through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPoseModel(nn.Module):\n",
    "    def __init__(self, nkeypoints=8):\n",
    "        # Initializes the DeepPoseModel with the dataset and training configuration.\n",
    "        super(DeepPoseModel, self).__init__()\n",
    "        \n",
    "        # The feature extractor part of the model, composed of several convolutional layers.\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv2d: Input channels = 3 (RGB image), Output channels = 96, kernel size = 11x11,\n",
    "            # stride = 4, padding = 4. \n",
    "            # Input: (batch_size, 3, 220, 220)\n",
    "            # Output: (batch_size, 96, 55, 55)\n",
    "            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=4),\n",
    "            \n",
    "            # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "            nn.LocalResponseNorm(5),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            # output size: (batch_size, 96, 27, 27)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2d: Input channels = 96, Output channels = 256, kernel size = 5x5,\n",
    "            # stride = 2, padding = 2.\n",
    "            # Input: (batch_size, 96, 27, 27)\n",
    "            # Output: (batch_size, 256, 27, 27)\n",
    "            nn.Conv2d(48, 128, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "            # Local Response Normalization (LRN) over 5 neighboring channels\n",
    "            nn.LocalResponseNorm(5),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            # output size: (batch_size, 96, 13, 13)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Conv2d: Input channels = 256, Output channels = 384, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 256, 13, 13)\n",
    "            # Output: (batch_size, 384, 13, 13)\n",
    "            nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv2d: Input channels = 384, Output channels = 384, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 384, 13, 13)\n",
    "            # Output: (batch_size, 384, 13, 13)\n",
    "            nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv2d: Input channels = 384, Output channels = 256, kernel size = 3x3,\n",
    "            # stride = 1, padding = 1.\n",
    "            # Input: (batch_size, 384, 13, 13)\n",
    "            # Output: (batch_size, 256, 13, 13)\n",
    "            nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # MaxPool2d: Kernel size = 3x3, stride = 2.\n",
    "            # Input: (batch_size, 256, 13, 13)\n",
    "            # Output: (batch_size, 256, 6, 6)\n",
    "            # Max pooling with 3x3 kernel and stride 2\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        # The classifier part of the model, composed of fully connected layers.\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Flatten the input tensor\n",
    "            # Input: (batch_size, 256, 6, 6)\n",
    "            # Output: (batch_size, 256 * 6 * 6) = (batch_size, 9216)\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            # Linear layer with input size 6400 and output size 4096\n",
    "            # Input: (batch_size, 6400)\n",
    "            # Output: (batch_size, 4096)\n",
    "            nn.Linear(128 * 6 * 6, 4096),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Dropout layer with 60% dropout rate\n",
    "            nn.Dropout(0.6),\n",
    "            \n",
    "            # Linear layer with input size 4096 and output size 4096\n",
    "            # Input: (batch_size, 4096)\n",
    "            # Output: (batch_size, 4096)\n",
    "            nn.Linear(4096, 4096),\n",
    "            \n",
    "            # ReLU activation function applied in place (no extra memory allocation)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Dropout layer with 60% dropout rate\n",
    "            nn.Dropout(0.6),\n",
    "            \n",
    "            # Final linear layer with input size 4096 and output size nkeypoints * 2\n",
    "            # Output is (nkeypoints * 2) coordinates (x, y) for each keypoint\n",
    "            # Input: (batch_size, 4096)\n",
    "            # Output: (batch_size, nkeypoints * 2)\n",
    "            nn.Linear(4096, nkeypoints * 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass through the network.\n",
    "        # Pass input `x` through the feature extractor\n",
    "        x = self.features(x)\n",
    "        # Pass the result through the classifier to get the final output\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepPose Model Summary\n",
    "# model = DeepPoseModel()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 220, 220), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original ALexNet model\n",
    "# class AlexNet(nn.Module):\n",
    "#     def __init__(self, num_classes=1000):\n",
    "#         super(AlexNet, self).__init__()\n",
    "        \n",
    "#         # Define the feature extractor part of the network\n",
    "#         self.features = nn.Sequential(\n",
    "#             # 1st Convolutional Layer: 3 input channels (RGB), 64 output channels, 11x11 kernel size, stride 4, padding 2\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # 2nd Convolutional Layer: 64 input channels, 192 output channels, 5x5 kernel size, stride 1, padding 2\n",
    "#             nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "#             # 3rd Convolutional Layer: 192 input channels, 384 output channels, 3x3 kernel size, stride 1, padding 1\n",
    "#             nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # 4th Convolutional Layer: 384 input channels, 256 output channels, 3x3 kernel size, stride 1, padding 1\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            \n",
    "#             # 5th Convolutional Layer: 256 input channels, 256 output channels, 3x3 kernel size, stride 1, padding 1\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             # Max pooling with 3x3 kernel and stride 2\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         # Define the classifier part of the network\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Flatten the input\n",
    "#             nn.Flatten(),\n",
    "#             # 1st Fully Connected Layer: input size 256 * 6 * 6, output size 4096\n",
    "#             nn.Linear(256 * 6 * 6, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "            \n",
    "#             # 2nd Fully Connected Layer: input size 4096, output size 4096\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.5),\n",
    "            \n",
    "#             # 3rd Fully Connected Layer (output layer): input size 4096, output size num_classes\n",
    "#             nn.Linear(4096, num_classes)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Pass the input through the feature extractor\n",
    "#         x = self.features(x)\n",
    "#         # Pass the result through the classifier to get the final output\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet Summary\n",
    "# model = AlexNet(num_classes=1000)  # Example model\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)  # Move model to GPU\n",
    "# summary(model, input_size=(3, 224, 224), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.0 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_PyTorch(img_arr, kp_arr, batch_size, train_flag=True):\n",
    "    '''\n",
    "    Load data into PT dataset and dataLoader in specified batch size\n",
    "    \n",
    "    Params\n",
    "    img_arr: images loaded into an array (i,255,255,3) and are converted to (i,3,255,255)\n",
    "    kp_arr: array of keypoints (i, num_kp*2)\n",
    "    batch_size: batch size \n",
    "\n",
    "    Return:\n",
    "    PT_Dataset: containing input (x) and groundtruth (y)\n",
    "    PT_DataLoader: Dataloader containing dataset and batch size\n",
    "\n",
    "    '''\n",
    "\n",
    "    # create tensors from arrays and load them to the GPU\n",
    "    img_tensor = torch.tensor(img_arr, dtype=torch.float32).permute(0, 3, 1, 2).to('cuda')\n",
    "    kp_tensor = torch.tensor(kp_arr, dtype=torch.float32).to('cuda')\n",
    "\n",
    "    # Create a TensorDataset and DataLoader for training data\n",
    "    dataset = TensorDataset(img_tensor, kp_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train_flag)\n",
    "\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timestamped_dir(descriptor, base_dir='/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/'):\n",
    "    \"\"\"\n",
    "    Creates a directory with a timestamp appended to the base directory name.\n",
    "    Returns the path to the created directory.\n",
    "    \n",
    "    Parameters:\n",
    "    descriptor: string describing the run generally model_dataDescriptor\n",
    "    base_dir (str): The base directory name. Default is './training_results'.\n",
    "    \n",
    "    Returns:\n",
    "    str: The path to the created directory.\n",
    "    \"\"\"\n",
    "    # Get the current datetime and format it as YYYY-MM-DD_HH-MM-SS\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    base_dir_descriptor = f\"{base_dir}{descriptor}\"\n",
    "    \n",
    "    # Create the final directory name with the timestamp\n",
    "    final_dir = f\"{base_dir_descriptor}_{timestamp}\"\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    \n",
    "    return final_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_data, val_data, save_dir, data_descriptor='Loss', show_plot=False):\n",
    "    # Plot the loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_data, label=f'Training {data_descriptor}')\n",
    "    plt.plot(val_data, label=f'Validation {data_descriptor}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(f'{data_descriptor}')\n",
    "    plt.title(f'Training and Validation {data_descriptor} Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, f'{data_descriptor}_plot.png')\n",
    "    plt.savefig(plot_path)\n",
    "    print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "    # Optionally, display the plot\n",
    "    if show_plot == True:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_stats_and_models(model, epoch, val_loss, val_pck, save_dir, \n",
    "                     best_val_loss=None, best_val_pck=None, \n",
    "                     final_model=False, train_loss_list=None, val_loss_list=None, train_pck_list=None, val_pck_list=None):\n",
    "    \"\"\"\n",
    "    Saves the best models based on validation loss, PCK value, and final model.\n",
    "    Saves the train and val curves and results for training\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to be saved.\n",
    "    - epoch (int): The current epoch number.\n",
    "    - val_loss (float): The current validation loss.\n",
    "    - val_pck (float): The current validation PCK value.\n",
    "    - save_dir (str): The directory where the models will be saved.\n",
    "    - best_val_loss (float): The best validation loss seen so far.\n",
    "    - best_val_pck (float): The best validation PCK value seen so far.\n",
    "    - final_model (bool): If True, saves the final model after all epochs.\n",
    "    - train_loss_list (list): List of all the loss values from each epoch\n",
    "    \n",
    "    Returns:\n",
    "    - best_val_loss (float): Updated best validation loss.\n",
    "    - best_val_pck (float): Updated best validation PCK value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the current model has the lowest validation loss\n",
    "    if best_val_loss is None or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model_name = f'best_val_loss_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth'\n",
    "        model_save_path_best_val_loss = os.path.join(save_dir, model_name)\n",
    "        torch.save(model.state_dict(), model_save_path_best_val_loss)\n",
    "        print(f'New best model saved with lowest validation loss to {model_save_path_best_val_loss}')\n",
    "    \n",
    "    # Check if the current model has the highest validation PCK\n",
    "    if best_val_pck is None or val_pck > best_val_pck:\n",
    "        best_val_pck = val_pck\n",
    "        model_save_path_best_val_pck = os.path.join(save_dir, f'best_val_pck_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth')\n",
    "        torch.save(model.state_dict(), model_save_path_best_val_pck)\n",
    "        print(f'New best model saved with highest validation PCK to {model_save_path_best_val_pck}')\n",
    "    \n",
    "    # Save the final model and perform final stats evaluation and save\n",
    "    if final_model:\n",
    "        final_model_path = os.path.join(save_dir, f'final_model_epoch_{epoch}_PCK_{val_pck:.4f}_loss_{val_loss:.4f}.pth')\n",
    "        torch.save(model.state_dict(), final_model_path)\n",
    "        print(f'Final model saved to {final_model_path}')\n",
    "        plot_training_curves(train_loss_list, val_loss_list, save_dir, 'Loss', show_plot=True)\n",
    "        plot_training_curves(train_pck_list, val_pck_list, save_dir, data_descriptor='PCK@0.1', show_plot=True)\n",
    "    \n",
    "    return best_val_loss, best_val_pck, model_save_path_best_val_loss, model_save_path_best_val_pck, final_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# this is where the training loop will go \n",
    "def train_loop(model, optimizer, train_dataloader, val_dataloader, num_epochs, descriptor):\n",
    "    # Create the directory to save the results to\n",
    "    save_dir = create_timestamped_dir(descriptor)\n",
    "\n",
    "    # Assuming the model, loss function, and optimizer are already defined\n",
    "    #model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "\n",
    "    # Define your optimizer\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    # Load data into PT dataset and dataloader\n",
    "    #train_dataset, train_dataloader = load_data_PyTorch(train_imgs_array, train_kp_array, 8)\n",
    "    #val_dataset, val_dataloader = load_data_PyTorch(val_imgs_array, val_kp_array, 8, train_flag=False)\n",
    "\n",
    "    # Training loop (variables)\n",
    "    #num_epochs = 30  # Adjust the number of epochs as needed\n",
    "\n",
    "    # Lists to store the training and validation loss for each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_pck_list = []\n",
    "    val_pck_list = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_pck = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        running_pck_005 = 0.0\n",
    "        running_pck_01 = 0.0\n",
    "        running_pck_02 = 0.0\n",
    "        running_pck_val_005 = 0.0\n",
    "        running_pck_val_01 = 0.0\n",
    "        running_pck_val_02 = 0.0\n",
    "        \n",
    "        for batch_images, batch_keypoints in train_dataloader:\n",
    "            # Move the data to the GPU\n",
    "            batch_images = batch_images.to('cuda')\n",
    "            batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(batch_images)\n",
    "            # Compute the loss\n",
    "            loss = masked_mse(batch_keypoints, outputs)\n",
    "            #print(loss)\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Accumulate the loss\n",
    "            running_train_loss += loss.item()\n",
    "            #print(running_train_loss)\n",
    "\n",
    "            # compute metrics\n",
    "            #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "            pck_01 = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "            #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "            #running_pck_005 += pck_005.item()\n",
    "            running_pck_01 += pck_01.item()\n",
    "            #running_pck_02 += pck_02.item()\n",
    "\n",
    "                \n",
    "        avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "        #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "        avg_pck_01 = running_pck_01 / len(train_dataloader)\n",
    "        #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "        # populate train losses list for evaluation\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        train_pck_list.append(avg_pck_01)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_keypoints in val_dataloader:\n",
    "                # Move the data to the GPU\n",
    "                batch_images = batch_images.to('cuda')\n",
    "                batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "                outputs = model(batch_images)\n",
    "                loss = masked_mse(batch_keypoints, outputs)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # compute metrics\n",
    "                #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "                pck_01_val = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "                #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "                #running_pck_val_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "                running_pck_val_01 += pck_01_val.item()\n",
    "                #running_pck_val_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "        \n",
    "        avg_val_loss = running_val_loss / len(val_dataloader)\n",
    "        #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "        avg_val_pck_01 = running_pck_val_01 / len(val_dataloader)\n",
    "        #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "        # populate val losses list for evaluation\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        val_pck_list.append(avg_val_pck_01)\n",
    "\n",
    "        # save best performing models based on the PCK and loss as well as the stats\n",
    "        best_val_loss, best_val_pck = save_stats_and_models(\n",
    "        model, epoch + 1, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "        best_val_loss, best_val_pck)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train PCK0.1: {avg_pck_01:.4f}, Val PCK0.1: {avg_val_pck_01:.4f}')\n",
    "        \n",
    "    save_stats_and_models(model, num_epochs, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "                    best_val_loss, best_val_pck, final_model=True, train_loss_list=train_losses, \n",
    "                    val_loss_list=val_losses, train_pck_list=train_pck_list, val_pck_list=val_pck_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# this is where the training loop will go \n",
    "def train_loop_mixed_precision(model, optimizer, train_dataloader, val_dataloader, num_epochs, descriptor):\n",
    "    # Create the directory to save the results to\n",
    "    save_dir = create_timestamped_dir(descriptor)\n",
    "\n",
    "    # Assuming the model, loss function, and optimizer are already defined\n",
    "    #model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "\n",
    "    # Define your optimizer\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    # Load data into PT dataset and dataloader\n",
    "    #train_dataset, train_dataloader = load_data_PyTorch(train_imgs_array, train_kp_array, 8)\n",
    "    #val_dataset, val_dataloader = load_data_PyTorch(val_imgs_array, val_kp_array, 8, train_flag=False)\n",
    "\n",
    "    # Training loop (variables)\n",
    "    #num_epochs = 30  # Adjust the number of epochs as needed\n",
    "\n",
    "    # Lists to store the training and validation loss for each epoch\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_pck_list = []\n",
    "    val_pck_list = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_pck = None\n",
    "\n",
    "    scaler = GradScaler() \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        running_pck_005 = 0.0\n",
    "        running_pck_01 = 0.0\n",
    "        running_pck_02 = 0.0\n",
    "        running_pck_val_005 = 0.0\n",
    "        running_pck_val_01 = 0.0\n",
    "        running_pck_val_02 = 0.0\n",
    "        \n",
    "        for batch_images, batch_keypoints in train_dataloader:\n",
    "            # Move the data to the GPU\n",
    "            batch_images = batch_images.to('cuda')\n",
    "            batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                outputs = model(batch_images)\n",
    "                # Compute the loss\n",
    "                loss = masked_mse(batch_keypoints, outputs)\n",
    "                #print(loss)\n",
    "\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update\n",
    "            # Accumulate the loss\n",
    "            running_train_loss += loss.item()\n",
    "            #print(running_train_loss)\n",
    "\n",
    "            # compute metrics\n",
    "            #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "            pck_01 = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "            #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "            #running_pck_005 += pck_005.item()\n",
    "            running_pck_01 += pck_01.item()\n",
    "            #running_pck_02 += pck_02.item()\n",
    "\n",
    "                \n",
    "        avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "        #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "        avg_pck_01 = running_pck_01 / len(train_dataloader)\n",
    "        #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "        # populate train losses list for evaluation\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        train_pck_list.append(avg_pck_01)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_keypoints in val_dataloader:\n",
    "                # Move the data to the GPU\n",
    "                batch_images = batch_images.to('cuda')\n",
    "                batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "                outputs = model(batch_images)\n",
    "                loss = masked_mse(batch_keypoints, outputs)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # compute metrics\n",
    "                #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "                pck_01_val = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "                #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "                #running_pck_val_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "                running_pck_val_01 += pck_01_val.item()\n",
    "                #running_pck_val_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "        \n",
    "        avg_val_loss = running_val_loss / len(val_dataloader)\n",
    "        #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "        avg_val_pck_01 = running_pck_val_01 / len(val_dataloader)\n",
    "        #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "        # populate val losses list for evaluation\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        val_pck_list.append(avg_val_pck_01)\n",
    "\n",
    "        # save best performing models based on the PCK and loss as well as the stats\n",
    "        best_val_loss, best_val_pck = save_stats_and_models(\n",
    "        model, epoch + 1, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "        best_val_loss, best_val_pck)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train PCK0.1: {avg_pck_01:.4f}, Val PCK0.1: {avg_val_pck_01:.4f}')\n",
    "        \n",
    "    save_stats_and_models(model, num_epochs, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "                    best_val_loss, best_val_pck, final_model=True, train_loss_list=train_losses, \n",
    "                    val_loss_list=val_losses, train_pck_list=train_pck_list, val_pck_list=val_pck_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8.1. Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the model, loss function, and optimizer are already defined\n",
    "model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Load data into PT dataset and dataloader\n",
    "bacth_size = 2 #batch_size\n",
    "train_dataset, train_dataloader = load_data_PyTorch(train_imgs_array_aug_simple, train_kp_array_aug_simple, bacth_size)\n",
    "val_dataset, val_dataloader = load_data_PyTorch(val_imgs_array, val_kp_array, bacth_size, train_flag=False)\n",
    "\n",
    "# Training loop (variables)\n",
    "num_epochs = 30  # Adjust the number of epochs as needed\n",
    "\n",
    "descriptor = 'DeepPose_Simple_SimpleAug'\n",
    "\n",
    "#train_loop(model, optimizer, train_dataloader, val_dataloader, num_epochs, descriptor)\n",
    "\n",
    "train_loop_mixed_precision(model, optimizer, train_dataloader, val_dataloader, num_epochs, descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop test\n",
    "# Assuming the model, loss function, and optimizer are already defined\n",
    "print('start loop')\n",
    "model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Load data into PT dataset and dataloader\n",
    "batch_size = 32 #batch_size\n",
    "# create tensors from arrays \n",
    "img_tensor = torch.tensor(train_imgs_array_aug_simple, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "kp_tensor = torch.tensor(train_kp_array_aug_simple, dtype=torch.float32)#.to('cuda')\n",
    "\n",
    "# Create a TensorDataset and DataLoader for training data\n",
    "dataset = TensorDataset(img_tensor, kp_tensor)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "img_tensor = torch.tensor(val_imgs_array, dtype=torch.float32).permute(0, 3, 1, 2)#.to('cuda')\n",
    "kp_tensor = torch.tensor(val_kp_array, dtype=torch.float32)#.to('cuda')\n",
    "\n",
    "# Create a TensorDataset and DataLoader for training data\n",
    "dataset = TensorDataset(img_tensor, kp_tensor)\n",
    "val_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#train_dataset, train_dataloader = load_data_PyTorch(train_imgs_array_aug_simple, train_kp_array_aug_simple, bacth_size)\n",
    "#val_dataset, val_dataloader = load_data_PyTorch(val_imgs_array, val_kp_array, bacth_size, train_flag=False)\n",
    "\n",
    "# Training loop (variables)\n",
    "num_epochs = 30  # Adjust the number of epochs as needed\n",
    "\n",
    "descriptor = 'DeepPose_Simple_SimpleAug'\n",
    "\n",
    "#train_loop(model, optimizer, train_dataloader, val_dataloader, num_epochs, descriptor)\n",
    "\n",
    "save_dir = create_timestamped_dir(descriptor)\n",
    "\n",
    "# Assuming the model, loss function, and optimizer are already defined\n",
    "#model = DeepPoseModel(nkeypoints=8).to('cuda')  # Move the model to GPU\n",
    "\n",
    "# Define your optimizer\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Load data into PT dataset and dataloader\n",
    "#train_dataset, train_dataloader = load_data_PyTorch(train_imgs_array, train_kp_array, 8)\n",
    "#val_dataset, val_dataloader = load_data_PyTorch(val_imgs_array, val_kp_array, 8, train_flag=False)\n",
    "\n",
    "# Training loop (variables)\n",
    "#num_epochs = 30  # Adjust the number of epochs as needed\n",
    "\n",
    "# Lists to store the training and validation loss for each epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_pck_list = []\n",
    "val_pck_list = []\n",
    "\n",
    "best_val_loss = None\n",
    "best_val_pck = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_pck_005 = 0.0\n",
    "    running_pck_01 = 0.0\n",
    "    running_pck_02 = 0.0\n",
    "    running_pck_val_005 = 0.0\n",
    "    running_pck_val_01 = 0.0\n",
    "    running_pck_val_02 = 0.0\n",
    "    \n",
    "    for batch_images, batch_keypoints in train_dataloader:\n",
    "        #print(batch_images.shape)\n",
    "        # Move the data to the GPU\n",
    "        batch_images = batch_images.to('cuda')\n",
    "        batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(batch_images)\n",
    "        # Compute the loss\n",
    "        loss = masked_mse(batch_keypoints, outputs)\n",
    "        #print(loss)\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Accumulate the loss\n",
    "        running_train_loss += loss.item()\n",
    "        #print(running_train_loss)\n",
    "\n",
    "        # compute metrics\n",
    "        #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "        pck_01 = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "        #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "        #running_pck_005 += pck_005.item()\n",
    "        running_pck_01 += pck_01.item()\n",
    "        #running_pck_02 += pck_02.item()\n",
    "\n",
    "            \n",
    "    avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "    #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "    avg_pck_01 = running_pck_01 / len(train_dataloader)\n",
    "    #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "    # populate train losses list for evaluation\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_pck_list.append(avg_pck_01)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_keypoints in val_dataloader:\n",
    "            # Move the data to the GPU\n",
    "            batch_images = batch_images.to('cuda')\n",
    "            batch_keypoints = batch_keypoints.to('cuda')\n",
    "\n",
    "            outputs = model(batch_images)\n",
    "            loss = masked_mse(batch_keypoints, outputs)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # compute metrics\n",
    "            #pck_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "            pck_01_val = pck_metric(batch_keypoints, outputs, 0.1)\n",
    "            #pck_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "            #running_pck_val_005 = pck_metric(batch_keypoints, outputs, 0.05)\n",
    "            running_pck_val_01 += pck_01_val.item()\n",
    "            #running_pck_val_02 = pck_metric(batch_keypoints, outputs, 0.2)\n",
    "\n",
    "    \n",
    "    avg_val_loss = running_val_loss / len(val_dataloader)\n",
    "    #avg_pck_005 = running_pck_005 / len(train_dataloader)\n",
    "    avg_val_pck_01 = running_pck_val_01 / len(val_dataloader)\n",
    "    #avg_pck_02 = running_pck_02 / len(train_dataloader)\n",
    "\n",
    "    # populate val losses list for evaluation\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    val_pck_list.append(avg_val_pck_01)\n",
    "\n",
    "    # save best performing models based on the PCK and loss as well as the stats\n",
    "    best_val_loss, best_val_pck = save_stats_and_models(\n",
    "    model, epoch + 1, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "    best_val_loss, best_val_pck)\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train PCK0.1: {avg_pck_01:.4f}, Val PCK0.1: {avg_val_pck_01:.4f}')\n",
    "    \n",
    "save_stats_and_models(model, num_epochs, avg_val_loss, avg_val_pck_01, save_dir, \n",
    "                best_val_loss, best_val_pck, final_model=True, train_loss_list=train_losses, \n",
    "                val_loss_list=val_losses, train_pck_list=train_pck_list, val_pck_list=val_pck_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. Evaluating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_class, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a PyTorch model from a .pth file.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path (str): The path to the .pth model file.\n",
    "    - model_class (torch.nn.Module): The class of the model to instantiate.\n",
    "    - device (str): The device to load the model onto ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module): The loaded PyTorch model.\n",
    "    \"\"\"\n",
    "    # Instantiate the model class\n",
    "    model = model_class().to(device)\n",
    "    \n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, images, img_is_tensor=False, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generates predictions from a PyTorch model given an array of images.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The PyTorch model to use for predictions.\n",
    "    - images (np.array): Array of images (e.g., shape: (num_images, 220, 220, 3)).\n",
    "    - device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - predictions (np.array): Array of predictions (e.g., keypoints for each image).\n",
    "    \"\"\"\n",
    "    # Convert images to PyTorch tensor and move to the specified device\n",
    "    if not img_is_tensor:\n",
    "        images_tensor = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "    # Forward pass through the model to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images_tensor)\n",
    "    \n",
    "    # Convert predictions back to a NumPy array and move to CPU if necessary\n",
    "    predictions = predictions.cpu().numpy() if device == 'cuda' else predictions.numpy()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(img, pred_keypoints, true_keypoints, save_dir, img_num, nkeypoints=8, keypoint_labels=None, connections = [(0, 1), (0, 2), (2, 3), (2, 4), (2, 5), (5, 6), (5, 7)]):\n",
    "    \"\"\"\n",
    "    Plots predicted keypoints vs. ground truth keypoints on the same image.\n",
    "\n",
    "    Parameters:\n",
    "    - img: The image on which to plot the keypoints.\n",
    "    - pred_keypoints: The predicted keypoints (flattened x, y coordinates).\n",
    "    - true_keypoints: The ground truth keypoints (flattened x, y coordinates).\n",
    "    - save_dir: Directory to save the result to\n",
    "    - img_num: image number that is getting compared\n",
    "    - nkeypoints:  Optional The number of keypoints (default=8).\n",
    "    - keypoint_labels: Optional list of keypoint labels to display next to the keypoints.\n",
    "    - connections: OPtional list of tupels defining the connections between kps\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Extract x and y coordinates for predicted keypoints\n",
    "    pred_x_keypoints = pred_keypoints[::2]\n",
    "    pred_y_keypoints = pred_keypoints[1::2]\n",
    "    \n",
    "    # Extract x and y coordinates for ground truth keypoints\n",
    "    true_x_keypoints = true_keypoints[::2]\n",
    "    true_y_keypoints = true_keypoints[1::2]\n",
    "\n",
    "    # Plot skeleton for true keypoints\n",
    "    for (i, j) in connections:\n",
    "        plt.plot([true_x_keypoints[i], true_x_keypoints[j]], \n",
    "                 [true_y_keypoints[i], true_y_keypoints[j]], \n",
    "                 'r-', linewidth=1)\n",
    "\n",
    "    # Plot skeleton for predicted keypoints\n",
    "    for (i, j) in connections:\n",
    "        plt.plot([pred_x_keypoints[i], pred_x_keypoints[j]], \n",
    "                 [pred_y_keypoints[i], pred_y_keypoints[j]], \n",
    "                 'g-', linewidth=1)\n",
    "    \n",
    "    # Plot predicted keypoints\n",
    "    plt.scatter(pred_x_keypoints, pred_y_keypoints, marker='o', c='g', s=100, label='Predicted', edgecolor='black')\n",
    "    \n",
    "    # Plot ground truth keypoints\n",
    "    plt.scatter(true_x_keypoints, true_y_keypoints, marker='x', c='r', s=100, label='Ground Truth')\n",
    "    \n",
    "    # If labels are provided, add them to the plot\n",
    "    if keypoint_labels is not None:\n",
    "        for i, (x, y) in enumerate(zip(true_x_keypoints, true_y_keypoints)):\n",
    "            plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "                     bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # If labels are provided, add them to the plot\n",
    "    if keypoint_labels is not None:\n",
    "        for i, (x, y) in enumerate(zip(pred_x_keypoints, pred_y_keypoints)):\n",
    "            plt.text(x, y, keypoint_labels[i], fontsize=8, color='white',\n",
    "                     bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # Add a legend to differentiate between predicted and ground truth keypoints\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, f'Comparison of predicted and ground truth for img {img_num}.png')\n",
    "    plt.savefig(plot_path)\n",
    "    #print(f'{data_descriptor} plot saved to {plot_path}')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(model_path, start_img, end_img, model_class=DeepPoseModel, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads a model, predicts keypoints for a range of images, and plots the predicted keypoints \n",
    "    versus ground truth keypoints on the same image. The images with plotted keypoints are then \n",
    "    saved to a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path: The file path to the saved model's .pth file.\n",
    "    - start_img: The starting index of the images in the validation set to process.\n",
    "    - end_img: The ending index of the images in the validation set to process (exclusive).\n",
    "    - model_class: Optional. The class of the model architecture to instantiate and load \n",
    "                   with the saved weights (default=DeepPoseModel).\n",
    "    - device: Optional. The device to run the model on ('cuda' for GPU, 'cpu' for CPU; default='cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - None. The function saves the images with plotted keypoints to the directory derived from the \n",
    "            model path.\n",
    "    \"\"\"\n",
    "\n",
    "    # get img lists\n",
    "    img_arr = val_imgs_array[start_img:end_img,:,:,:]\n",
    "    true_kp_arr = val_kp_array[start_img:end_img,:]\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(model_path, model_class, device=device)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = predict(model, img_arr, device=device)\n",
    "    #print(predictions)\n",
    "\n",
    "    # DeNorm predictions \n",
    "    predictions_abs = []\n",
    "    true_kp_arr_abs = []\n",
    "    for i, kp in enumerate(predictions):\n",
    "\n",
    "        img_size = img_arr[i].shape\n",
    "        #print(img_size)\n",
    "\n",
    "        #unNorm each prediction\n",
    "        true_kp_abs, missing_kp = unnorm_keypoints(img_size, true_kp_arr[i])\n",
    "        #print(missing_kp)\n",
    "        kp_abs, missing_kp = unnorm_keypoints(img_size, kp, kp_to_null=missing_kp)\n",
    "        #print(missing_kp)\n",
    "        \n",
    "\n",
    "        # save result to new list\n",
    "        predictions_abs.append(kp_abs)\n",
    "        true_kp_arr_abs.append(true_kp_abs)\n",
    "\n",
    "    #print(predictions_abs)\n",
    "\n",
    "    # get the save directory parent (where the images will be saved)\n",
    "    save_dir = model_path.rsplit('/',1)[0]\n",
    "\n",
    "    # labels\n",
    "    labels = ['Head', 'Beak', 'Body_top', 'RFlipper', 'LFlipper', 'Body_bottom', 'LFoot', 'RFoot']\n",
    "\n",
    "    for i, kp in enumerate(predictions_abs):\n",
    "\n",
    "        plot_comparison(img_arr[i], predictions_abs[i], true_kp_arr_abs[i], save_dir, img_num=i+start_img)#, keypoint_labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions and draw them \n",
    "model_path = '/home/matthew/Desktop/Master_Dev/masters_penguin_pose_estimation/runs/PE/DeepPose_Simple_SimpleAug_2024-08-22_15-17-20/final_model_epoch_30_PCK_0.5700_loss_0.0083.pth'\n",
    "start_img = 55\n",
    "end_img = 58\n",
    "\n",
    "predict_and_plot(model_path, start_img, end_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare some poses\n",
    "val_imgs_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_kp_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
